{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db823349-554f-4e1c-8b9f-47a4503d8fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Stock_Dataset.ipynb\n",
      "importing Jupyter notebook from feature_extract.ipynb\n",
      "importing Jupyter notebook from AttLSTM.ipynb\n",
      "importing Jupyter notebook from attention.ipynb\n",
      "importing Jupyter notebook from Transformer_Encoder.ipynb\n",
      "importing Jupyter notebook from metric.ipynb\n",
      "importing Jupyter notebook from loss_fn1.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\USER\\\\JupyterProjects\\\\DTML')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from Stock_Dataset import StockDataset\n",
    "import argparse\n",
    "from AttLSTM import att_LSTM\n",
    "from Transformer_Encoder import Transformer\n",
    "import numpy as np\n",
    "import time\n",
    "from metric import metric_acc as ACC\n",
    "from metric import metric_mcc as MCC\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "from loss_fn1 import Selective_Regularization\n",
    "\n",
    "def train(att_LSTM,transformer,                                  ## Model\n",
    "          att_LSTM_optimizer, transformer_optimizer,   ## Optimizer\n",
    "          Partition, args):                                      ## Data, loss function, argument\n",
    "    trainloader = DataLoader(Partition['train'],\n",
    "                             batch_size = args.batch_size,\n",
    "                             shuffle=False, drop_last=True)\n",
    "\n",
    "    att_LSTM.train()\n",
    "    transformer.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for i, (x,y) in enumerate(trainloader):\n",
    "        data_out_list = []\n",
    "        for i in range(len(x)):\n",
    "\n",
    "            att_LSTM_optimizer.zero_grad()\n",
    "            transformer_optimizer.zero_grad()\n",
    "            \n",
    "            x_input = x[i].to(args.device)\n",
    "            \n",
    "            if i == 1:\n",
    "                true_y = y[i].squeeze().float().to(args.device)\n",
    "\n",
    "            att_LSTM.hidden = [hidden.to(args.device) for hidden in att_LSTM.init_hidden()]\n",
    "            \n",
    "            # 'list' object has no attribute 'float'\n",
    "\n",
    "            hidden_context, attn_weights = att_LSTM(x_input)\n",
    "\n",
    "            data_out_list.append(hidden_context)\n",
    "\n",
    "        index_output = data_out_list[0]  # torch.Size([128, 10])\n",
    "        stock_output = data_out_list[1] # torch.Size([128, 10])\n",
    "        \n",
    "\n",
    "        Norm_ = nn.LayerNorm(10, device=args.device)\n",
    "        \n",
    "        index_output = args.r * Norm_(index_output) + args.b\n",
    "        stock_output = args.r * Norm_(stock_output) + args.b\n",
    "\n",
    "\n",
    "        Transformer_input = index_output* args.market_beta + stock_output\n",
    "        \n",
    "\n",
    "        output1,wp,bp = transformer(Transformer_input.transpose(0,1).unsqueeze(2))\n",
    "\n",
    "\n",
    "        # output_ = torch.where(output1 >= 0.5, 1.0, 0.0)\n",
    "        # output_.requires_grad=True\n",
    "        \n",
    "        loss = args.loss_fn(output1, true_y, wp.squeeze(), bp)\n",
    "        loss.backward()\n",
    "\n",
    "        att_LSTM_optimizer.step() ## parameter 갱신\n",
    "        transformer_optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    return att_LSTM, transformer, train_loss\n",
    "\n",
    "\n",
    "def validation(att_LSTM,transformer,\n",
    "               partition, args):\n",
    "    valloader = DataLoader(partition['val'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    att_LSTM.eval()\n",
    "    transformer.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(valloader):\n",
    "\n",
    "            data_out_list = []\n",
    "            for i in range(len(x)):\n",
    "                \n",
    "                x_input = x[i].to(args.device)\n",
    "\n",
    "                if i==1:\n",
    "                    true_y = y[i].squeeze().float().to(args.device)\n",
    "                att_LSTM.hidden = [hidden.to(args.device) for hidden in att_LSTM.init_hidden()]\n",
    "\n",
    "\n",
    "                hidden_context, attn_weights = att_LSTM(x_input)\n",
    "\n",
    "                data_out_list.append(hidden_context)\n",
    "\n",
    "            index_output = data_out_list[0]  # torch.Size([128, 10])\n",
    "            stock_output = data_out_list[1]  # torch.Size([128, 10])\n",
    "\n",
    "            Norm_ = nn.LayerNorm(10, device=args.device)\n",
    "            index_output = args.r * Norm_(index_output) + args.b\n",
    "            stock_output = args.r * Norm_(stock_output) + args.b\n",
    "\n",
    "            Transformer_input = index_output* args.market_beta  + stock_output\n",
    "\n",
    "            output1,wp,bp = transformer(Transformer_input.transpose(0,1).unsqueeze(2))\n",
    "\n",
    "            loss = args.loss_fn(output1, true_y, wp.squeeze(), bp)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "        return att_LSTM, transformer, val_loss\n",
    "\n",
    "\n",
    "def test(att_LSTM, transformer,\n",
    "               partition, args):\n",
    "    testloader = DataLoader(partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    att_LSTM.eval()\n",
    "    transformer.eval()\n",
    "\n",
    "    ACC_metric = 0.0\n",
    "    MCC_metric = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(testloader):\n",
    "\n",
    "            data_out_list = []\n",
    "            for i in range(len(x)):\n",
    "                \n",
    "                x_input = x[i].to(args.device)\n",
    "                # feature transform\n",
    "                if i==1:\n",
    "                    true_y = y[i].squeeze().float().to(args.device)\n",
    "\n",
    "                att_LSTM.hidden = [hidden.to(args.device) for hidden in att_LSTM.init_hidden()]\n",
    "\n",
    "\n",
    "                hidden_context, attn_weights = att_LSTM(x_input)\n",
    "\n",
    "                data_out_list.append(hidden_context)\n",
    "\n",
    "            index_output = data_out_list[0]  # torch.Size([128, 10])\n",
    "            stock_output = data_out_list[1]  # torch.Size([128, 10])\n",
    "\n",
    "            Norm_ = nn.LayerNorm(10, device=args.device)\n",
    "            index_output = args.r * Norm_(index_output) + args.b\n",
    "            stock_output = args.r * Norm_(stock_output) + args.b\n",
    "\n",
    "            Transformer_input = index_output * args.market_beta + stock_output\n",
    "\n",
    "            output1,wp,bp = transformer(Transformer_input.transpose(0,1).unsqueeze(2))\n",
    "\n",
    "            output_ = torch.where(output1 >= 0.5, 1.0, 0.0)\n",
    "\n",
    "            output_.requires_grad = True\n",
    "\n",
    "            ACC_metric += ACC(output_, true_y)\n",
    "            MCC_metric += MCC(output_, true_y)\n",
    "\n",
    "        ACC_metric = ACC_metric / len(testloader)\n",
    "        MCC_metric = MCC_metric / len(testloader)\n",
    "\n",
    "        return ACC_metric, MCC_metric\n",
    "\n",
    "\n",
    "\n",
    "def experiment(partition, args):\n",
    "    att_LSTM = args.att_LSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size,\n",
    "                             args.dropout, args.use_bn, args.attention_head, args.attn_size, activation=\"ReLU\")\n",
    "    transformer = args.transformer(args.trans_feature_size, args.trans_num_laysers,args.dropout, args.batch_size, args.x_frames, args.trans_nhead)\n",
    "    \n",
    "    \n",
    "    args.r = nn.init.xavier_normal_(torch.empty(args.batch_size, args.hid_dim)).to(args.device) \n",
    "    args.b = torch.zeros(1).to(args.device)\n",
    "    \n",
    "    att_LSTM.to(args.device)\n",
    "    transformer.to(args.device)\n",
    "\n",
    "    if args.optim == 'SGD':\n",
    "        att_LSTM_optimizer = optim.SGD(att_LSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "        transformer_optimizer = optim.SGD(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        att_LSTM_optimizer = optim.RMSprop(att_LSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "        transformer_optimizer = optim.RMSprop(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        att_LSTM_optimizer = optim.Adam(att_LSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "        transformer_optimizer = optim.Adam(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "\n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    # ===================================== #\n",
    "    for epoch in range(args.epoch):\n",
    "        ts = time.time()\n",
    "        att_LSTM, transformer, train_loss = train(att_LSTM, transformer,\n",
    "                                                  att_LSTM_optimizer, transformer_optimizer,\n",
    "                                                  partition, args)\n",
    "\n",
    "        att_LSTM, transformer, val_loss = validation(att_LSTM, transformer, partition, args)\n",
    "\n",
    "        te = time.time()\n",
    "\n",
    "        ## 각 에폭마다 모델을 저장하기 위한 코드\n",
    "        torch.save(att_LSTM.state_dict(), args.new_file_path + '\\\\' + str(epoch) +'_epoch' +'_att_LSTM' +'.pt')\n",
    "        torch.save(transformer.state_dict(), args.new_file_path + '\\\\' + str(epoch) +'_epoch' +'_transformer' +'.pt')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "              .format(epoch, train_loss, val_loss, te - ts))\n",
    "\n",
    "    ## val_losses에서 가장 값이 최소인 위치를 저장함\n",
    "    site_val_losses = val_losses.index(min(val_losses)) ## 10 epoch일 경우 0번째~9번째 까지로 나옴\n",
    "    att_LSTM = args.att_LSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size,\n",
    "                             args.dropout, args.use_bn, args.attention_head, args.attn_size,\n",
    "                             activation=\"ReLU\")\n",
    "    transformer = args.transformer(args.trans_feature_size, args.trans_num_laysers,\n",
    "                                   args.dropout, args.batch_size, args.x_frames, args.trans_nhead)\n",
    "    att_LSTM.to(args.device)\n",
    "    transformer.to(args.device)\n",
    "\n",
    "    att_LSTM.load_state_dict(torch.load(args.new_file_path + '\\\\' + str(site_val_losses) +'_epoch' +'_att_LSTM'+ '.pt'))\n",
    "    transformer.load_state_dict(torch.load(args.new_file_path + '\\\\' + str(site_val_losses) + '_epoch' + '_transformer' + '.pt'))\n",
    "\n",
    "    ACC,MCC = test(att_LSTM, transformer, partition, args)\n",
    "    print('ACC: {}, MCC: {}'.format(ACC, MCC))\n",
    "\n",
    "    with open(args.new_file_path + '\\\\'+ str(site_val_losses)+'Epoch_test_metric' +'.csv', 'w') as fd:\n",
    "        print('ACC: {}, MCC: {}'.format(ACC, MCC), file=fd)\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['ACC'] = ACC\n",
    "    result['MCC'] = MCC\n",
    "\n",
    "    return vars(args), result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd115972-567d-475b-ac08-15dedfc7d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '^KS11' : KOSPI\n",
    "# '^KQ11' : 코스닥\n",
    "# '^IXIC' : 나스닥\n",
    "# '^GSPC' : SNP 500 지수\n",
    "# '^DJI' : 다우존수 산업지수\n",
    "# '^HSI' : 홍콩 항생 지수\n",
    "# '^N225' : 니케이지수\n",
    "# '^GDAXI' : 독일 DAX\n",
    "# '^FTSE' : 영국 FTSE\n",
    "# '^FCHI' : 프랑스 CAC\n",
    "# '^IBEX' : 스페인 IBEX\n",
    "# '^TWII' : 대만 기권\n",
    "# '^AEX' : 네덜란드 AEX\n",
    "# '^BSESN' : 인도 센섹스\n",
    "# 'RTSI.ME' : 러시아 RTXI\n",
    "# '^BVSP' : 브라질 보베스파 지수\n",
    "# 'GC=F' : 금 가격\n",
    "# 'CL=F' : 원유 가격 (2000/ 8 / 20일 부터 데이터가 있음)\n",
    "# 'BTC-USD' : 비트코인 암호화폐\n",
    "# 'ETH-USD' : 이더리움 암호화폐\n",
    "\n",
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# ========= experiment setting ========== #\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.save_file_path = \"C:\\\\Users\\\\USER\\\\JupyterProjects\\\\DTML\\\\result\"\n",
    "\n",
    "## ======== data ============= #\n",
    "args.index = ['^IXIC']\n",
    "# [] 되는거\n",
    "# ['CHL', 'DCM' , 'DOW', 'NTT' , 'SYT', 'TOT'] 안되는거\n",
    "args.stock_list =  ['AAPL', 'AMZN', 'BA', 'BAC', 'BHP', 'BRK-B', 'CMCSA', 'CVX', 'D','DIS','DUK', 'EXC', 'GE', 'GOOGL', 'HD', 'INTC', 'JNJ', 'JPM', 'KO', 'MA', 'MMM', 'MO', 'MRK', 'MSFT', 'NGG','NVS', 'ORCL', 'PEP', 'PFE', 'PG', 'PTR', 'RDS-B', 'RIO', 'SO', 'SPY', 'T', 'TM', 'UNH', 'UPS', 'VALE', 'VZ', 'WFC', 'WMT', 'XOM']\n",
    "#args.stock_list =  ['AMZN']\n",
    "#, 'BA', 'BAC']\n",
    "args.data_count = len(args.stock_list)\n",
    "args.train_start = \"2007-01-03\"\n",
    "args.train_end = \"2015-01-01\"\n",
    "args.validation_start = \"2015-01-01\"\n",
    "args.validation_end = \"2016-01-03\"\n",
    "args.test_start = \"2016-01-04\"\n",
    "args.test_end = \"2016-12-31\"\n",
    "\n",
    "# ====== hyperparameter ======= #\n",
    "args.batch_size = 128\n",
    "args.x_frames = 10\n",
    "args.y_frames = 1\n",
    "args.input_dim = 10\n",
    "args.output_dim = 1\n",
    "args.dropout = 0.15\n",
    "args.use_bn = True\n",
    "args.loss_fn = Selective_Regularization  ## loss function for classification : cross entropy\n",
    "args.optim = 'Adam'\n",
    "args.lr = 0.0005\n",
    "args.l2 = 0.00001 #?\n",
    "args.epoch = 100\n",
    "# ============= model ================== #\n",
    "args.att_LSTM = att_LSTM\n",
    "args.transformer = Transformer\n",
    "\n",
    "# ====== att_lstm hyperparameter ======= #\n",
    "args.hid_dim = 10\n",
    "args.attention_head = 1\n",
    "args.attn_size = 10\n",
    "args.num_layers = 1\n",
    "args.decoder_x_frames = 1\n",
    "\n",
    "# ====== transformer hyperparameter ======= #\n",
    "args.trans_feature_size = 250\n",
    "args.trans_num_laysers = 1\n",
    "args.trans_nhead = 10\n",
    "args.market_beta = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fdb0bdf-b811-4e2c-a9a4-1289b194c7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.06375/5.05429. Took 1.21 sec\n",
      "Epoch 1, Loss(train/val) 5.05816/5.04938. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 5.05228/5.04945. Took 1.00 sec\n",
      "Epoch 3, Loss(train/val) 5.05286/5.04753. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 5.05236/5.04721. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 5.04989/5.04637. Took 1.00 sec\n",
      "Epoch 6, Loss(train/val) 5.05024/5.04536. Took 1.00 sec\n",
      "Epoch 7, Loss(train/val) 5.05090/5.04238. Took 1.02 sec\n",
      "Epoch 8, Loss(train/val) 5.04875/5.04046. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 5.04685/5.03928. Took 1.02 sec\n",
      "Epoch 10, Loss(train/val) 5.04722/5.03841. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 5.04603/5.03875. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 5.04589/5.03888. Took 1.00 sec\n",
      "Epoch 13, Loss(train/val) 5.04345/5.04015. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 5.04425/5.04359. Took 1.02 sec\n",
      "Epoch 15, Loss(train/val) 5.04510/5.04585. Took 1.02 sec\n",
      "Epoch 16, Loss(train/val) 5.04304/5.04680. Took 1.03 sec\n",
      "Epoch 17, Loss(train/val) 5.04481/5.04879. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 5.04276/5.05303. Took 1.02 sec\n",
      "Epoch 19, Loss(train/val) 5.04251/5.05594. Took 1.02 sec\n",
      "Epoch 20, Loss(train/val) 5.04199/5.05771. Took 1.03 sec\n",
      "Epoch 21, Loss(train/val) 5.04254/5.06209. Took 1.03 sec\n",
      "Epoch 22, Loss(train/val) 5.04154/5.06871. Took 1.02 sec\n",
      "Epoch 23, Loss(train/val) 5.04190/5.05729. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 5.04119/5.05372. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 5.04127/5.05501. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 5.03981/5.05360. Took 1.02 sec\n",
      "Epoch 27, Loss(train/val) 5.03908/5.05600. Took 1.07 sec\n",
      "Epoch 28, Loss(train/val) 5.03857/5.05912. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 5.03903/5.05743. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 5.03777/5.06113. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 5.03650/5.05928. Took 1.02 sec\n",
      "Epoch 32, Loss(train/val) 5.03674/5.05619. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 5.03665/5.05308. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 5.03649/5.05540. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 5.03986/5.05601. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 5.03650/5.06181. Took 1.02 sec\n",
      "Epoch 37, Loss(train/val) 5.03623/5.06011. Took 1.02 sec\n",
      "Epoch 38, Loss(train/val) 5.03542/5.06399. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 5.03383/5.06827. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 5.03673/5.05275. Took 1.02 sec\n",
      "Epoch 41, Loss(train/val) 5.03726/5.05380. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 5.03471/5.05129. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 5.03539/5.05696. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 5.03266/5.05793. Took 1.02 sec\n",
      "Epoch 45, Loss(train/val) 5.03258/5.05149. Took 1.00 sec\n",
      "Epoch 46, Loss(train/val) 5.03186/5.06046. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 5.03367/5.06051. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 5.03546/5.06898. Took 1.02 sec\n",
      "Epoch 49, Loss(train/val) 5.03317/5.05228. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 5.03175/5.05978. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 5.03167/5.05720. Took 1.02 sec\n",
      "Epoch 52, Loss(train/val) 5.03329/5.05790. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 5.02872/5.06393. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 5.03161/5.05267. Took 1.02 sec\n",
      "Epoch 55, Loss(train/val) 5.03131/5.06287. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 5.03050/5.05509. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 5.03080/5.05947. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 5.03074/5.05301. Took 1.00 sec\n",
      "Epoch 59, Loss(train/val) 5.02736/5.05699. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 5.03021/5.06347. Took 1.03 sec\n",
      "Epoch 61, Loss(train/val) 5.02825/5.05476. Took 1.00 sec\n",
      "Epoch 62, Loss(train/val) 5.03113/5.05920. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 5.02727/5.06148. Took 1.02 sec\n",
      "Epoch 64, Loss(train/val) 5.02870/5.05118. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 5.03896/5.04820. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 5.03241/5.06309. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 5.02891/5.04962. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 5.03164/5.05640. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 5.03259/5.06275. Took 1.02 sec\n",
      "Epoch 70, Loss(train/val) 5.03096/5.05954. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 5.02623/5.09868. Took 1.00 sec\n",
      "Epoch 72, Loss(train/val) 5.03221/5.05157. Took 1.07 sec\n",
      "Epoch 73, Loss(train/val) 5.02747/5.06140. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 5.02515/5.09633. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 5.02572/5.06302. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 5.02807/5.04588. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 5.02957/5.05435. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 5.02683/5.09579. Took 1.00 sec\n",
      "Epoch 79, Loss(train/val) 5.02572/5.05395. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 5.01969/5.09880. Took 1.02 sec\n",
      "Epoch 81, Loss(train/val) 5.02922/5.05716. Took 1.00 sec\n",
      "Epoch 82, Loss(train/val) 5.02693/5.05137. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 5.02652/5.08725. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 5.02381/5.05982. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 5.02178/5.09912. Took 1.02 sec\n",
      "Epoch 86, Loss(train/val) 5.02761/5.04185. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 5.01865/5.04691. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 5.01982/5.12300. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 5.02199/5.05098. Took 1.02 sec\n",
      "Epoch 90, Loss(train/val) 5.02253/5.04108. Took 1.02 sec\n",
      "Epoch 91, Loss(train/val) 5.02425/5.10880. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 5.01774/5.11321. Took 1.02 sec\n",
      "Epoch 93, Loss(train/val) 5.02266/5.06265. Took 1.02 sec\n",
      "Epoch 94, Loss(train/val) 5.02220/5.08195. Took 1.00 sec\n",
      "Epoch 95, Loss(train/val) 5.02161/5.05341. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 5.01574/5.11273. Took 1.02 sec\n",
      "Epoch 97, Loss(train/val) 5.02353/5.05784. Took 1.02 sec\n",
      "Epoch 98, Loss(train/val) 5.02621/5.04521. Took 1.02 sec\n",
      "Epoch 99, Loss(train/val) 5.02075/5.06086. Took 1.02 sec\n",
      "ACC: 0.515625, MCC: 0.04349428527208621\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.02313/4.99726. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.98430/4.97344. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 4.97784/4.97140. Took 1.05 sec\n",
      "Epoch 3, Loss(train/val) 4.97770/4.97453. Took 1.03 sec\n",
      "Epoch 4, Loss(train/val) 4.97639/4.97605. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 4.97517/4.97854. Took 1.02 sec\n",
      "Epoch 6, Loss(train/val) 4.97610/4.98115. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.97735/4.98200. Took 1.02 sec\n",
      "Epoch 8, Loss(train/val) 4.97617/4.97912. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.97520/4.97716. Took 1.02 sec\n",
      "Epoch 10, Loss(train/val) 4.97570/4.97514. Took 1.02 sec\n",
      "Epoch 11, Loss(train/val) 4.97513/4.97439. Took 1.02 sec\n",
      "Epoch 12, Loss(train/val) 4.97618/4.96741. Took 1.07 sec\n",
      "Epoch 13, Loss(train/val) 4.97370/4.96689. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 4.97352/4.96467. Took 1.03 sec\n",
      "Epoch 15, Loss(train/val) 4.97388/4.96990. Took 1.02 sec\n",
      "Epoch 16, Loss(train/val) 4.97382/4.96631. Took 1.02 sec\n",
      "Epoch 17, Loss(train/val) 4.97402/4.96788. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 4.97259/4.96547. Took 1.02 sec\n",
      "Epoch 19, Loss(train/val) 4.97262/4.96275. Took 1.02 sec\n",
      "Epoch 20, Loss(train/val) 4.97166/4.96106. Took 1.02 sec\n",
      "Epoch 21, Loss(train/val) 4.97314/4.96636. Took 1.02 sec\n",
      "Epoch 22, Loss(train/val) 4.97249/4.96371. Took 1.02 sec\n",
      "Epoch 23, Loss(train/val) 4.97240/4.95676. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 4.97247/4.95796. Took 1.02 sec\n",
      "Epoch 25, Loss(train/val) 4.97197/4.96402. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 4.97145/4.96055. Took 1.00 sec\n",
      "Epoch 27, Loss(train/val) 4.97192/4.95766. Took 1.02 sec\n",
      "Epoch 28, Loss(train/val) 4.97016/4.95731. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.97077/4.96073. Took 1.03 sec\n",
      "Epoch 30, Loss(train/val) 4.97135/4.95916. Took 1.03 sec\n",
      "Epoch 31, Loss(train/val) 4.97004/4.96265. Took 1.02 sec\n",
      "Epoch 32, Loss(train/val) 4.97006/4.96142. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.97031/4.96078. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.96746/4.95929. Took 1.02 sec\n",
      "Epoch 35, Loss(train/val) 4.96859/4.95516. Took 1.02 sec\n",
      "Epoch 36, Loss(train/val) 4.97015/4.95803. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.97012/4.95738. Took 1.02 sec\n",
      "Epoch 38, Loss(train/val) 4.97003/4.96044. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.96987/4.95989. Took 1.02 sec\n",
      "Epoch 40, Loss(train/val) 4.96904/4.95492. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.96901/4.95914. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 4.96712/4.95234. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.96797/4.95559. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.96625/4.95636. Took 1.00 sec\n",
      "Epoch 45, Loss(train/val) 4.96794/4.95511. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 4.97065/4.96170. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.96779/4.95649. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 4.96705/4.95508. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.96466/4.95965. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 4.96540/4.95545. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.96950/4.95872. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 4.96564/4.95786. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 4.96666/4.95705. Took 1.04 sec\n",
      "Epoch 54, Loss(train/val) 4.96599/4.95355. Took 1.04 sec\n",
      "Epoch 55, Loss(train/val) 4.96469/4.95736. Took 1.03 sec\n",
      "Epoch 56, Loss(train/val) 4.96461/4.95602. Took 1.02 sec\n",
      "Epoch 57, Loss(train/val) 4.96629/4.96788. Took 1.04 sec\n",
      "Epoch 58, Loss(train/val) 4.96780/4.95562. Took 1.02 sec\n",
      "Epoch 59, Loss(train/val) 4.96617/4.96278. Took 1.08 sec\n",
      "Epoch 60, Loss(train/val) 4.96644/4.96482. Took 1.03 sec\n",
      "Epoch 61, Loss(train/val) 4.96596/4.96112. Took 1.03 sec\n",
      "Epoch 62, Loss(train/val) 4.96287/4.95801. Took 1.03 sec\n",
      "Epoch 63, Loss(train/val) 4.96481/4.96116. Took 1.03 sec\n",
      "Epoch 64, Loss(train/val) 4.96553/4.95176. Took 1.03 sec\n",
      "Epoch 65, Loss(train/val) 4.96694/4.96587. Took 1.02 sec\n",
      "Epoch 66, Loss(train/val) 4.96363/4.95735. Took 1.03 sec\n",
      "Epoch 67, Loss(train/val) 4.96443/4.96327. Took 1.03 sec\n",
      "Epoch 68, Loss(train/val) 4.96134/4.97057. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 4.96720/4.96830. Took 1.03 sec\n",
      "Epoch 70, Loss(train/val) 4.96460/4.95634. Took 1.02 sec\n",
      "Epoch 71, Loss(train/val) 4.96384/4.95514. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 4.96404/4.96105. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.96371/4.96531. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.96271/4.96975. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.96282/4.96178. Took 1.02 sec\n",
      "Epoch 76, Loss(train/val) 4.96957/4.96189. Took 1.02 sec\n",
      "Epoch 77, Loss(train/val) 4.96550/4.96267. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.96637/4.95613. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 4.97354/4.96520. Took 1.02 sec\n",
      "Epoch 80, Loss(train/val) 4.96936/4.96366. Took 1.02 sec\n",
      "Epoch 81, Loss(train/val) 4.96684/4.95975. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.96477/4.95586. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 4.96917/4.95205. Took 1.02 sec\n",
      "Epoch 84, Loss(train/val) 4.96535/4.95824. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 4.96554/4.96101. Took 1.02 sec\n",
      "Epoch 86, Loss(train/val) 4.96345/4.95713. Took 1.03 sec\n",
      "Epoch 87, Loss(train/val) 4.97123/4.98335. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.97405/4.97666. Took 1.03 sec\n",
      "Epoch 89, Loss(train/val) 4.97148/4.97689. Took 1.03 sec\n",
      "Epoch 90, Loss(train/val) 4.97067/4.97644. Took 1.02 sec\n",
      "Epoch 91, Loss(train/val) 4.97059/4.97492. Took 1.04 sec\n",
      "Epoch 92, Loss(train/val) 4.96987/4.97384. Took 1.02 sec\n",
      "Epoch 93, Loss(train/val) 4.96975/4.97163. Took 1.03 sec\n",
      "Epoch 94, Loss(train/val) 4.96846/4.97549. Took 1.02 sec\n",
      "Epoch 95, Loss(train/val) 4.96896/4.97274. Took 1.02 sec\n",
      "Epoch 96, Loss(train/val) 4.96569/4.97591. Took 1.02 sec\n",
      "Epoch 97, Loss(train/val) 4.96679/4.97188. Took 1.02 sec\n",
      "Epoch 98, Loss(train/val) 4.97050/4.97146. Took 1.02 sec\n",
      "Epoch 99, Loss(train/val) 4.96818/4.96275. Took 1.02 sec\n",
      "ACC: 0.546875, MCC: 0.09790425704486858\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.05745/5.02653. Took 1.10 sec\n",
      "Epoch 1, Loss(train/val) 5.01882/5.01533. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 5.01496/5.01259. Took 1.00 sec\n",
      "Epoch 3, Loss(train/val) 5.01247/5.01095. Took 1.03 sec\n",
      "Epoch 4, Loss(train/val) 5.01302/5.01009. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 5.01274/5.00977. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 5.01262/5.00921. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 5.01306/5.00872. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 5.01372/5.00835. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 5.01280/5.00856. Took 1.03 sec\n",
      "Epoch 10, Loss(train/val) 5.01274/5.00807. Took 1.02 sec\n",
      "Epoch 11, Loss(train/val) 5.01154/5.00680. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 5.01236/5.00627. Took 1.02 sec\n",
      "Epoch 13, Loss(train/val) 5.01133/5.00724. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 5.01113/5.00614. Took 1.00 sec\n",
      "Epoch 15, Loss(train/val) 5.01218/5.00658. Took 1.02 sec\n",
      "Epoch 16, Loss(train/val) 5.01162/5.00717. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 5.01091/5.00707. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 5.01094/5.00614. Took 1.00 sec\n",
      "Epoch 19, Loss(train/val) 5.01153/5.00699. Took 1.00 sec\n",
      "Epoch 20, Loss(train/val) 5.01032/5.00740. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 5.01064/5.00698. Took 1.02 sec\n",
      "Epoch 22, Loss(train/val) 5.00971/5.00737. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 5.01014/5.00858. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 5.00969/5.00957. Took 1.02 sec\n",
      "Epoch 25, Loss(train/val) 5.00879/5.01152. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 5.00966/5.01267. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 5.00880/5.01399. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 5.00835/5.01569. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 5.00758/5.01786. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 5.00731/5.01980. Took 1.02 sec\n",
      "Epoch 31, Loss(train/val) 5.00901/5.01799. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 5.00681/5.01811. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 5.00722/5.02018. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 5.00819/5.02399. Took 1.00 sec\n",
      "Epoch 35, Loss(train/val) 5.00564/5.02612. Took 1.02 sec\n",
      "Epoch 36, Loss(train/val) 5.00566/5.02462. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 5.00624/5.02601. Took 1.02 sec\n",
      "Epoch 38, Loss(train/val) 5.00554/5.02958. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 5.00425/5.02540. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 5.00565/5.03159. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 5.00433/5.03068. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 5.00397/5.03173. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 5.00331/5.03455. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 5.00534/5.02904. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 5.00273/5.03122. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 5.00315/5.04002. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 5.00259/5.03907. Took 1.07 sec\n",
      "Epoch 48, Loss(train/val) 5.00123/5.03218. Took 1.00 sec\n",
      "Epoch 49, Loss(train/val) 5.00326/5.03716. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 5.00273/5.03861. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 5.00161/5.03600. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 5.00048/5.04037. Took 1.03 sec\n",
      "Epoch 53, Loss(train/val) 5.00248/5.03086. Took 1.02 sec\n",
      "Epoch 54, Loss(train/val) 5.00055/5.04410. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 5.00222/5.03319. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 5.00098/5.04021. Took 1.02 sec\n",
      "Epoch 57, Loss(train/val) 5.00114/5.03811. Took 1.02 sec\n",
      "Epoch 58, Loss(train/val) 5.00053/5.03874. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 5.00153/5.03427. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 5.00197/5.03729. Took 1.00 sec\n",
      "Epoch 61, Loss(train/val) 5.00291/5.03628. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 4.99940/5.04068. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 5.00210/5.03264. Took 1.02 sec\n",
      "Epoch 64, Loss(train/val) 5.00032/5.03653. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 5.00009/5.03924. Took 1.00 sec\n",
      "Epoch 66, Loss(train/val) 4.99977/5.04211. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 5.00083/5.03710. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 4.99937/5.04132. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 5.00239/5.04601. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 4.99816/5.04224. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 5.00097/5.03915. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 5.01154/5.01247. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 5.01000/5.01145. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 5.00624/5.01378. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 5.00479/5.01633. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 5.00306/5.01665. Took 1.00 sec\n",
      "Epoch 77, Loss(train/val) 5.00316/5.02132. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 5.00211/5.03050. Took 1.02 sec\n",
      "Epoch 79, Loss(train/val) 5.00317/5.03614. Took 1.02 sec\n",
      "Epoch 80, Loss(train/val) 5.00046/5.03956. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 5.00186/5.03258. Took 1.00 sec\n",
      "Epoch 82, Loss(train/val) 4.99858/5.04239. Took 1.00 sec\n",
      "Epoch 83, Loss(train/val) 5.00064/5.03867. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 5.00029/5.04162. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 5.00019/5.04662. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 5.00001/5.04474. Took 1.00 sec\n",
      "Epoch 87, Loss(train/val) 4.99914/5.04524. Took 1.00 sec\n",
      "Epoch 88, Loss(train/val) 4.99936/5.04677. Took 1.00 sec\n",
      "Epoch 89, Loss(train/val) 4.99943/5.04450. Took 1.00 sec\n",
      "Epoch 90, Loss(train/val) 4.99927/5.05025. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 4.99782/5.04738. Took 1.00 sec\n",
      "Epoch 92, Loss(train/val) 4.99786/5.05286. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 4.99916/5.04174. Took 1.00 sec\n",
      "Epoch 94, Loss(train/val) 4.99917/5.04625. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 4.99708/5.04748. Took 1.08 sec\n",
      "Epoch 96, Loss(train/val) 4.99962/5.03957. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.99844/5.04654. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 4.99880/5.04974. Took 1.02 sec\n",
      "Epoch 99, Loss(train/val) 4.99827/5.04354. Took 1.00 sec\n",
      "ACC: 0.5, MCC: 0.024781286764175334\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.00345/4.96226. Took 1.01 sec\n",
      "Epoch 1, Loss(train/val) 4.96527/4.95607. Took 1.00 sec\n",
      "Epoch 2, Loss(train/val) 4.96573/4.95611. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 4.96395/4.95726. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 4.96420/4.95668. Took 1.00 sec\n",
      "Epoch 5, Loss(train/val) 4.96378/4.95764. Took 1.00 sec\n",
      "Epoch 6, Loss(train/val) 4.96419/4.95699. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.96201/4.95699. Took 1.02 sec\n",
      "Epoch 8, Loss(train/val) 4.96333/4.95766. Took 1.00 sec\n",
      "Epoch 9, Loss(train/val) 4.96402/4.95771. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 4.96217/4.95780. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.96135/4.95916. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 4.96154/4.96021. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.96042/4.96271. Took 1.02 sec\n",
      "Epoch 14, Loss(train/val) 4.96088/4.96346. Took 1.04 sec\n",
      "Epoch 15, Loss(train/val) 4.95876/4.96580. Took 1.02 sec\n",
      "Epoch 16, Loss(train/val) 4.95683/4.96884. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 4.95719/4.96669. Took 1.00 sec\n",
      "Epoch 18, Loss(train/val) 4.95888/4.97106. Took 1.00 sec\n",
      "Epoch 19, Loss(train/val) 4.95666/4.97258. Took 1.03 sec\n",
      "Epoch 20, Loss(train/val) 4.95771/4.97570. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.95642/4.97297. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.95355/4.97633. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.95082/4.98082. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 4.95477/4.98105. Took 1.02 sec\n",
      "Epoch 25, Loss(train/val) 4.95306/4.98166. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 4.95072/4.97684. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 4.94975/4.98153. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 4.95347/4.98173. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.95123/4.97758. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 4.95208/4.98954. Took 1.02 sec\n",
      "Epoch 31, Loss(train/val) 4.95280/4.98112. Took 1.03 sec\n",
      "Epoch 32, Loss(train/val) 4.94963/4.98203. Took 1.03 sec\n",
      "Epoch 33, Loss(train/val) 4.95015/4.98359. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.95043/4.98602. Took 1.02 sec\n",
      "Epoch 35, Loss(train/val) 4.94814/4.99901. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 4.94913/4.98562. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.95240/4.98209. Took 1.09 sec\n",
      "Epoch 38, Loss(train/val) 4.94939/4.97784. Took 1.02 sec\n",
      "Epoch 39, Loss(train/val) 4.94957/4.98820. Took 1.03 sec\n",
      "Epoch 40, Loss(train/val) 4.94574/4.99529. Took 1.02 sec\n",
      "Epoch 41, Loss(train/val) 4.94684/4.99083. Took 1.02 sec\n",
      "Epoch 42, Loss(train/val) 4.94649/4.99886. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.95049/4.99230. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.94586/4.98751. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.94582/4.99169. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 4.94466/4.99895. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.94647/4.99439. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 4.94575/5.00079. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.94735/4.99860. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 4.94494/4.99725. Took 1.00 sec\n",
      "Epoch 51, Loss(train/val) 4.94509/4.99482. Took 1.02 sec\n",
      "Epoch 52, Loss(train/val) 4.94322/5.00482. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 4.94596/4.99942. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 4.94299/5.00316. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 4.94592/4.99480. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 4.94427/5.00442. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 4.94418/4.99815. Took 1.02 sec\n",
      "Epoch 58, Loss(train/val) 4.93957/5.01081. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.94558/5.00299. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.94190/5.01128. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 4.94298/5.01125. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 4.94454/5.00316. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.94356/5.00887. Took 1.02 sec\n",
      "Epoch 64, Loss(train/val) 4.94349/5.01252. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 4.94176/5.01129. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 4.94293/4.99588. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 4.94231/5.00805. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 4.94331/5.00453. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 4.94421/5.00527. Took 1.02 sec\n",
      "Epoch 70, Loss(train/val) 4.94056/5.01832. Took 1.00 sec\n",
      "Epoch 71, Loss(train/val) 4.94088/5.00394. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 4.94149/5.01561. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.94712/4.99645. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.94398/5.02053. Took 1.00 sec\n",
      "Epoch 75, Loss(train/val) 4.94088/5.01288. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 4.93980/5.01733. Took 1.00 sec\n",
      "Epoch 77, Loss(train/val) 4.94182/5.01866. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.94232/4.99663. Took 1.02 sec\n",
      "Epoch 79, Loss(train/val) 4.94235/5.01008. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.93851/5.01870. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 4.93636/5.00318. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.95481/4.95646. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 4.96015/4.96519. Took 1.00 sec\n",
      "Epoch 84, Loss(train/val) 4.95698/4.97072. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 4.95262/4.98862. Took 1.07 sec\n",
      "Epoch 86, Loss(train/val) 4.95410/4.98746. Took 1.02 sec\n",
      "Epoch 87, Loss(train/val) 4.94758/4.99628. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.94365/5.00410. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.94592/5.00224. Took 1.00 sec\n",
      "Epoch 90, Loss(train/val) 4.94501/5.00200. Took 1.00 sec\n",
      "Epoch 91, Loss(train/val) 4.94274/5.01189. Took 1.00 sec\n",
      "Epoch 92, Loss(train/val) 4.94036/5.01778. Took 1.00 sec\n",
      "Epoch 93, Loss(train/val) 4.94156/4.99963. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.94423/5.00321. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 4.94408/4.99664. Took 1.00 sec\n",
      "Epoch 96, Loss(train/val) 4.94372/4.99637. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.94477/4.99985. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 4.94581/5.00739. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 4.94490/5.00316. Took 1.01 sec\n",
      "ACC: 0.578125, MCC: 0.16283800671889478\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.14866/5.07999. Took 1.01 sec\n",
      "Epoch 1, Loss(train/val) 5.08786/5.08966. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 5.08637/5.08382. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 5.08649/5.08074. Took 1.03 sec\n",
      "Epoch 4, Loss(train/val) 5.08685/5.08001. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 5.08601/5.08042. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 5.08657/5.08088. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 5.08620/5.08143. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 5.08548/5.08121. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 5.08479/5.08033. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 5.08373/5.08005. Took 1.00 sec\n",
      "Epoch 11, Loss(train/val) 5.08507/5.08112. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 5.08505/5.07996. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 5.08410/5.07936. Took 1.02 sec\n",
      "Epoch 14, Loss(train/val) 5.08304/5.07921. Took 1.00 sec\n",
      "Epoch 15, Loss(train/val) 5.08378/5.07858. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 5.08360/5.07802. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 5.08198/5.07822. Took 1.00 sec\n",
      "Epoch 18, Loss(train/val) 5.08141/5.07776. Took 1.00 sec\n",
      "Epoch 19, Loss(train/val) 5.08108/5.07851. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 5.08074/5.07956. Took 1.02 sec\n",
      "Epoch 21, Loss(train/val) 5.08068/5.07894. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 5.08058/5.07827. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 5.08137/5.07832. Took 1.02 sec\n",
      "Epoch 24, Loss(train/val) 5.08022/5.08133. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 5.07985/5.08006. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 5.08009/5.07947. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 5.08045/5.07852. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 5.07979/5.07836. Took 1.02 sec\n",
      "Epoch 29, Loss(train/val) 5.07916/5.07781. Took 1.09 sec\n",
      "Epoch 30, Loss(train/val) 5.07909/5.07829. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 5.07999/5.07600. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 5.07916/5.08042. Took 1.02 sec\n",
      "Epoch 33, Loss(train/val) 5.08017/5.07858. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 5.07939/5.07868. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 5.07985/5.07840. Took 1.02 sec\n",
      "Epoch 36, Loss(train/val) 5.07995/5.07826. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 5.07914/5.07751. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 5.07828/5.07640. Took 1.02 sec\n",
      "Epoch 39, Loss(train/val) 5.07935/5.07414. Took 1.00 sec\n",
      "Epoch 40, Loss(train/val) 5.07821/5.07868. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 5.07705/5.07588. Took 1.00 sec\n",
      "Epoch 42, Loss(train/val) 5.07874/5.07834. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 5.07852/5.07790. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 5.07831/5.07772. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 5.07715/5.08216. Took 1.00 sec\n",
      "Epoch 46, Loss(train/val) 5.07790/5.08559. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 5.07780/5.07869. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 5.07735/5.07931. Took 1.02 sec\n",
      "Epoch 49, Loss(train/val) 5.07667/5.08353. Took 1.00 sec\n",
      "Epoch 50, Loss(train/val) 5.08107/5.08219. Took 1.02 sec\n",
      "Epoch 51, Loss(train/val) 5.07800/5.08282. Took 1.00 sec\n",
      "Epoch 52, Loss(train/val) 5.07818/5.08580. Took 1.00 sec\n",
      "Epoch 53, Loss(train/val) 5.07780/5.08471. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 5.07574/5.08839. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 5.07815/5.08856. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 5.07562/5.08745. Took 1.00 sec\n",
      "Epoch 57, Loss(train/val) 5.07870/5.08667. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 5.07548/5.08631. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 5.07489/5.09157. Took 1.00 sec\n",
      "Epoch 60, Loss(train/val) 5.07554/5.09367. Took 1.00 sec\n",
      "Epoch 61, Loss(train/val) 5.07725/5.09058. Took 1.00 sec\n",
      "Epoch 62, Loss(train/val) 5.07385/5.09495. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 5.07268/5.09060. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 5.08015/5.09886. Took 1.02 sec\n",
      "Epoch 65, Loss(train/val) 5.07851/5.09449. Took 1.02 sec\n",
      "Epoch 66, Loss(train/val) 5.07737/5.09373. Took 1.00 sec\n",
      "Epoch 67, Loss(train/val) 5.07439/5.08953. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 5.07627/5.09393. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 5.07332/5.09222. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 5.07617/5.09586. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 5.07358/5.09236. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 5.07833/5.08754. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 5.07206/5.09541. Took 1.03 sec\n",
      "Epoch 74, Loss(train/val) 5.08191/5.09561. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 5.07582/5.09434. Took 1.03 sec\n",
      "Epoch 76, Loss(train/val) 5.07563/5.09428. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 5.07640/5.09212. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 5.07247/5.10043. Took 1.08 sec\n",
      "Epoch 79, Loss(train/val) 5.07435/5.09891. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 5.07416/5.09566. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 5.07268/5.09754. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 5.07411/5.09506. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 5.07251/5.09770. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 5.07118/5.09807. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 5.07245/5.09690. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 5.07367/5.10147. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 5.07020/5.09279. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 5.06937/5.10600. Took 1.00 sec\n",
      "Epoch 89, Loss(train/val) 5.07585/5.09574. Took 1.02 sec\n",
      "Epoch 90, Loss(train/val) 5.07200/5.09812. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 5.07213/5.10021. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 5.07361/5.09835. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 5.07219/5.09620. Took 1.02 sec\n",
      "Epoch 94, Loss(train/val) 5.07232/5.10008. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 5.07090/5.10550. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 5.07207/5.10233. Took 1.03 sec\n",
      "Epoch 97, Loss(train/val) 5.07110/5.09896. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 5.07044/5.10272. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 5.06972/5.10352. Took 1.01 sec\n",
      "ACC: 0.4921875, MCC: -0.04222003309207491\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.01798/4.97433. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.97221/4.98004. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 4.97326/4.98028. Took 1.03 sec\n",
      "Epoch 3, Loss(train/val) 4.97373/4.97941. Took 1.00 sec\n",
      "Epoch 4, Loss(train/val) 4.97356/4.97947. Took 1.03 sec\n",
      "Epoch 5, Loss(train/val) 4.97145/4.98170. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.97062/4.98237. Took 1.00 sec\n",
      "Epoch 7, Loss(train/val) 4.96956/4.98024. Took 1.00 sec\n",
      "Epoch 8, Loss(train/val) 4.96859/4.98103. Took 1.02 sec\n",
      "Epoch 9, Loss(train/val) 4.96916/4.98005. Took 1.00 sec\n",
      "Epoch 10, Loss(train/val) 4.96796/4.98003. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.96900/4.97909. Took 1.00 sec\n",
      "Epoch 12, Loss(train/val) 4.96866/4.97365. Took 1.00 sec\n",
      "Epoch 13, Loss(train/val) 4.96727/4.97220. Took 1.00 sec\n",
      "Epoch 14, Loss(train/val) 4.96658/4.97299. Took 1.02 sec\n",
      "Epoch 15, Loss(train/val) 4.96619/4.97257. Took 1.00 sec\n",
      "Epoch 16, Loss(train/val) 4.96531/4.97389. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 4.96488/4.97454. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 4.96457/4.97495. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 4.96456/4.97491. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 4.96790/4.97791. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.96942/4.97646. Took 1.08 sec\n",
      "Epoch 22, Loss(train/val) 4.96762/4.98559. Took 1.00 sec\n",
      "Epoch 23, Loss(train/val) 4.97008/4.97571. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 4.96763/4.98204. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 4.96737/4.98009. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 4.96728/4.98046. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 4.96635/4.97712. Took 1.00 sec\n",
      "Epoch 28, Loss(train/val) 4.96459/4.98247. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.96585/4.97720. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 4.96591/4.97493. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.96313/4.97789. Took 1.02 sec\n",
      "Epoch 32, Loss(train/val) 4.96503/4.97867. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.96183/4.98147. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.96251/4.97788. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 4.96262/4.97971. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 4.96348/4.97874. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.96458/4.97815. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 4.96435/4.98078. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.96091/4.98146. Took 1.02 sec\n",
      "Epoch 40, Loss(train/val) 4.96307/4.97905. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.96281/4.97527. Took 1.02 sec\n",
      "Epoch 42, Loss(train/val) 4.96013/4.98249. Took 1.03 sec\n",
      "Epoch 43, Loss(train/val) 4.96133/4.97601. Took 1.02 sec\n",
      "Epoch 44, Loss(train/val) 4.95911/4.97621. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.95826/4.98089. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 4.96225/4.97756. Took 1.02 sec\n",
      "Epoch 47, Loss(train/val) 4.96120/4.97769. Took 1.02 sec\n",
      "Epoch 48, Loss(train/val) 4.95938/4.98105. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.96396/4.97779. Took 1.02 sec\n",
      "Epoch 50, Loss(train/val) 4.95987/4.98139. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.96023/4.98177. Took 1.00 sec\n",
      "Epoch 52, Loss(train/val) 4.96120/4.98403. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 4.95794/4.98299. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 4.95772/4.98285. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 4.95923/4.99050. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 4.95820/4.98524. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 4.95973/4.98837. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 4.95983/4.98867. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.96163/4.98837. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.96134/4.99296. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 4.95970/4.99900. Took 1.00 sec\n",
      "Epoch 62, Loss(train/val) 4.95905/5.00869. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.95994/4.99837. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 4.96146/4.99645. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 4.95910/4.99911. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 4.95857/5.00510. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 4.95869/5.00444. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 4.95650/5.00605. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 4.95686/5.00802. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 4.95692/5.01059. Took 1.00 sec\n",
      "Epoch 71, Loss(train/val) 4.96000/5.00392. Took 1.08 sec\n",
      "Epoch 72, Loss(train/val) 4.95774/4.99869. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.95655/5.00576. Took 1.00 sec\n",
      "Epoch 74, Loss(train/val) 4.95425/5.01717. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.95363/5.01145. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 4.95602/5.00156. Took 1.00 sec\n",
      "Epoch 77, Loss(train/val) 4.95696/5.00941. Took 1.02 sec\n",
      "Epoch 78, Loss(train/val) 4.95699/5.00387. Took 1.00 sec\n",
      "Epoch 79, Loss(train/val) 4.95494/5.01714. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.95496/5.01293. Took 1.02 sec\n",
      "Epoch 81, Loss(train/val) 4.95697/4.99816. Took 1.02 sec\n",
      "Epoch 82, Loss(train/val) 4.95796/5.00511. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 4.95528/5.00615. Took 1.00 sec\n",
      "Epoch 84, Loss(train/val) 4.96075/4.98849. Took 1.00 sec\n",
      "Epoch 85, Loss(train/val) 4.95930/4.99472. Took 1.00 sec\n",
      "Epoch 86, Loss(train/val) 4.95773/4.99637. Took 1.02 sec\n",
      "Epoch 87, Loss(train/val) 4.95545/5.00077. Took 1.00 sec\n",
      "Epoch 88, Loss(train/val) 4.95276/5.01896. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.95608/5.00389. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 4.95499/5.00829. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 4.95399/5.01474. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 4.95724/5.00054. Took 1.00 sec\n",
      "Epoch 93, Loss(train/val) 4.95496/5.00021. Took 1.00 sec\n",
      "Epoch 94, Loss(train/val) 4.95381/5.01091. Took 1.02 sec\n",
      "Epoch 95, Loss(train/val) 4.95086/5.01219. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 4.95347/5.00710. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.95251/5.02238. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 4.95226/5.01479. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 4.95782/5.00193. Took 1.02 sec\n",
      "ACC: 0.5, MCC: 0.00392156862745098\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.08268/5.07870. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 5.06713/5.07244. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 5.06191/5.06649. Took 1.02 sec\n",
      "Epoch 3, Loss(train/val) 5.06145/5.05968. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 5.05781/5.05951. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 5.05676/5.06191. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 5.05724/5.06543. Took 1.03 sec\n",
      "Epoch 7, Loss(train/val) 5.05744/5.06785. Took 1.02 sec\n",
      "Epoch 8, Loss(train/val) 5.05866/5.06759. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 5.05943/5.06107. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 5.05787/5.05768. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 5.05473/5.06104. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 5.05649/5.06062. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 5.05543/5.05952. Took 1.02 sec\n",
      "Epoch 14, Loss(train/val) 5.05614/5.06172. Took 1.02 sec\n",
      "Epoch 15, Loss(train/val) 5.05478/5.06123. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 5.05502/5.05879. Took 1.09 sec\n",
      "Epoch 17, Loss(train/val) 5.05493/5.05838. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 5.05496/5.05877. Took 1.03 sec\n",
      "Epoch 19, Loss(train/val) 5.05171/5.06045. Took 1.02 sec\n",
      "Epoch 20, Loss(train/val) 5.05352/5.06250. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 5.05575/5.06342. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 5.05501/5.06102. Took 1.03 sec\n",
      "Epoch 23, Loss(train/val) 5.05352/5.06086. Took 1.02 sec\n",
      "Epoch 24, Loss(train/val) 5.05311/5.06150. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 5.05012/5.07075. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 5.05102/5.06722. Took 1.03 sec\n",
      "Epoch 27, Loss(train/val) 5.05482/5.06402. Took 1.02 sec\n",
      "Epoch 28, Loss(train/val) 5.05152/5.06264. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 5.05054/5.06573. Took 1.00 sec\n",
      "Epoch 30, Loss(train/val) 5.05104/5.06395. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 5.05071/5.06552. Took 1.02 sec\n",
      "Epoch 32, Loss(train/val) 5.05253/5.06554. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 5.05100/5.06326. Took 1.02 sec\n",
      "Epoch 34, Loss(train/val) 5.04855/5.06894. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 5.04856/5.07105. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 5.05233/5.06498. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 5.04974/5.06961. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 5.04855/5.07172. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 5.04890/5.07198. Took 1.02 sec\n",
      "Epoch 40, Loss(train/val) 5.04893/5.07060. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 5.04895/5.07460. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 5.04663/5.06946. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 5.04663/5.07626. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 5.04754/5.06806. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 5.04710/5.07478. Took 1.02 sec\n",
      "Epoch 46, Loss(train/val) 5.04444/5.06927. Took 1.03 sec\n",
      "Epoch 47, Loss(train/val) 5.04894/5.07027. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 5.04832/5.06846. Took 1.04 sec\n",
      "Epoch 49, Loss(train/val) 5.05003/5.05942. Took 1.03 sec\n",
      "Epoch 50, Loss(train/val) 5.04601/5.08360. Took 1.03 sec\n",
      "Epoch 51, Loss(train/val) 5.04247/5.07642. Took 1.02 sec\n",
      "Epoch 52, Loss(train/val) 5.04415/5.07596. Took 1.02 sec\n",
      "Epoch 53, Loss(train/val) 5.04403/5.08268. Took 1.02 sec\n",
      "Epoch 54, Loss(train/val) 5.04336/5.07193. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 5.04108/5.06585. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 5.04508/5.06387. Took 1.02 sec\n",
      "Epoch 57, Loss(train/val) 5.04448/5.06821. Took 1.02 sec\n",
      "Epoch 58, Loss(train/val) 5.03962/5.07882. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 5.03913/5.08025. Took 1.00 sec\n",
      "Epoch 60, Loss(train/val) 5.04097/5.07144. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 5.04189/5.07550. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 5.05072/5.07644. Took 1.02 sec\n",
      "Epoch 63, Loss(train/val) 5.05096/5.07874. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 5.04920/5.07574. Took 1.02 sec\n",
      "Epoch 65, Loss(train/val) 5.04994/5.08149. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 5.04933/5.08077. Took 1.03 sec\n",
      "Epoch 67, Loss(train/val) 5.04661/5.09049. Took 1.09 sec\n",
      "Epoch 68, Loss(train/val) 5.04734/5.07478. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 5.04893/5.08301. Took 1.03 sec\n",
      "Epoch 70, Loss(train/val) 5.04864/5.07815. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 5.04715/5.09026. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 5.04530/5.09078. Took 1.02 sec\n",
      "Epoch 73, Loss(train/val) 5.04637/5.09221. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 5.04660/5.09380. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 5.04593/5.08696. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 5.04375/5.09661. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 5.04614/5.07764. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 5.04452/5.09661. Took 1.02 sec\n",
      "Epoch 79, Loss(train/val) 5.04292/5.10062. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 5.04418/5.09680. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 5.04321/5.09045. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 5.04898/5.07727. Took 1.04 sec\n",
      "Epoch 83, Loss(train/val) 5.04867/5.09068. Took 1.02 sec\n",
      "Epoch 84, Loss(train/val) 5.04328/5.09074. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 5.04295/5.09318. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 5.04433/5.08649. Took 1.02 sec\n",
      "Epoch 87, Loss(train/val) 5.04193/5.09447. Took 1.02 sec\n",
      "Epoch 88, Loss(train/val) 5.04193/5.08670. Took 1.02 sec\n",
      "Epoch 89, Loss(train/val) 5.04415/5.09087. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 5.04181/5.10755. Took 1.02 sec\n",
      "Epoch 91, Loss(train/val) 5.04099/5.09849. Took 1.03 sec\n",
      "Epoch 92, Loss(train/val) 5.04054/5.09916. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 5.04254/5.10046. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 5.04425/5.08920. Took 1.02 sec\n",
      "Epoch 95, Loss(train/val) 5.04291/5.09025. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 5.04377/5.09530. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 5.04369/5.09681. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 5.03862/5.11188. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 5.04052/5.11162. Took 1.02 sec\n",
      "ACC: 0.5703125, MCC: 0.1348602233939787\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.12528/5.09789. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 5.10990/5.11347. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 5.10745/5.14116. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 5.10374/5.11466. Took 1.02 sec\n",
      "Epoch 4, Loss(train/val) 5.09863/5.10736. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 5.09900/5.11270. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 5.09835/5.11432. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 5.09958/5.11822. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 5.09925/5.11473. Took 1.00 sec\n",
      "Epoch 9, Loss(train/val) 5.09883/5.11230. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 5.09739/5.11138. Took 1.02 sec\n",
      "Epoch 11, Loss(train/val) 5.09772/5.11245. Took 1.00 sec\n",
      "Epoch 12, Loss(train/val) 5.09758/5.11472. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 5.09805/5.11245. Took 1.08 sec\n",
      "Epoch 14, Loss(train/val) 5.09687/5.11305. Took 1.02 sec\n",
      "Epoch 15, Loss(train/val) 5.09565/5.11380. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 5.09572/5.11431. Took 1.02 sec\n",
      "Epoch 17, Loss(train/val) 5.09593/5.11359. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 5.09583/5.11531. Took 1.02 sec\n",
      "Epoch 19, Loss(train/val) 5.09516/5.11113. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 5.09469/5.11313. Took 1.02 sec\n",
      "Epoch 21, Loss(train/val) 5.09494/5.11267. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 5.09451/5.11678. Took 1.02 sec\n",
      "Epoch 23, Loss(train/val) 5.09518/5.11646. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 5.09382/5.11810. Took 1.02 sec\n",
      "Epoch 25, Loss(train/val) 5.09339/5.11963. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 5.09309/5.11876. Took 1.02 sec\n",
      "Epoch 27, Loss(train/val) 5.09418/5.11849. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 5.09229/5.11966. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 5.09338/5.12012. Took 1.03 sec\n",
      "Epoch 30, Loss(train/val) 5.09250/5.11952. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 5.09115/5.11782. Took 1.02 sec\n",
      "Epoch 32, Loss(train/val) 5.09206/5.12177. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 5.09143/5.12362. Took 1.02 sec\n",
      "Epoch 34, Loss(train/val) 5.08982/5.11704. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 5.08943/5.11604. Took 1.02 sec\n",
      "Epoch 36, Loss(train/val) 5.08867/5.12436. Took 1.02 sec\n",
      "Epoch 37, Loss(train/val) 5.09126/5.12095. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 5.08994/5.12093. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 5.09274/5.12263. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 5.09081/5.12243. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 5.08857/5.12168. Took 1.02 sec\n",
      "Epoch 42, Loss(train/val) 5.08967/5.11754. Took 1.02 sec\n",
      "Epoch 43, Loss(train/val) 5.08889/5.12537. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 5.08970/5.12092. Took 1.02 sec\n",
      "Epoch 45, Loss(train/val) 5.08824/5.12732. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 5.08856/5.12072. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 5.08792/5.12295. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 5.08766/5.13518. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 5.08668/5.12488. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 5.08709/5.12953. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 5.08631/5.12998. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 5.08665/5.12713. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 5.08520/5.12773. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 5.08717/5.12914. Took 1.02 sec\n",
      "Epoch 55, Loss(train/val) 5.08352/5.13255. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 5.08497/5.13447. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 5.08498/5.13115. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 5.08772/5.12482. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 5.08649/5.12528. Took 1.02 sec\n",
      "Epoch 60, Loss(train/val) 5.08631/5.12995. Took 1.02 sec\n",
      "Epoch 61, Loss(train/val) 5.08695/5.13998. Took 1.02 sec\n",
      "Epoch 62, Loss(train/val) 5.08703/5.12954. Took 1.00 sec\n",
      "Epoch 63, Loss(train/val) 5.09502/5.12508. Took 1.00 sec\n",
      "Epoch 64, Loss(train/val) 5.08905/5.11409. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 5.08856/5.12378. Took 1.08 sec\n",
      "Epoch 66, Loss(train/val) 5.08677/5.12870. Took 1.00 sec\n",
      "Epoch 67, Loss(train/val) 5.08438/5.14270. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 5.08564/5.13548. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 5.08435/5.13386. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 5.08507/5.13387. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 5.08294/5.13803. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 5.08333/5.13946. Took 1.02 sec\n",
      "Epoch 73, Loss(train/val) 5.08330/5.13643. Took 1.02 sec\n",
      "Epoch 74, Loss(train/val) 5.08575/5.14205. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 5.08080/5.14187. Took 1.00 sec\n",
      "Epoch 76, Loss(train/val) 5.08217/5.14569. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 5.08939/5.12708. Took 1.04 sec\n",
      "Epoch 78, Loss(train/val) 5.08130/5.14304. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 5.08570/5.13122. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 5.08200/5.13818. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 5.08044/5.15043. Took 1.03 sec\n",
      "Epoch 82, Loss(train/val) 5.07959/5.14226. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 5.08406/5.13613. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 5.08127/5.14152. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 5.07837/5.14002. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 5.08092/5.13612. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 5.07827/5.14217. Took 1.02 sec\n",
      "Epoch 88, Loss(train/val) 5.08060/5.14175. Took 1.02 sec\n",
      "Epoch 89, Loss(train/val) 5.08149/5.14489. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 5.07905/5.13966. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 5.08024/5.13352. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 5.07910/5.15281. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 5.07647/5.15374. Took 1.02 sec\n",
      "Epoch 94, Loss(train/val) 5.08382/5.13930. Took 1.03 sec\n",
      "Epoch 95, Loss(train/val) 5.07837/5.14053. Took 1.04 sec\n",
      "Epoch 96, Loss(train/val) 5.07565/5.15703. Took 1.00 sec\n",
      "Epoch 97, Loss(train/val) 5.07971/5.14344. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 5.07584/5.14793. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 5.07842/5.14687. Took 1.01 sec\n",
      "ACC: 0.5078125, MCC: 0.012734178792822844\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.05297/5.01576. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 5.02904/5.01558. Took 1.00 sec\n",
      "Epoch 2, Loss(train/val) 5.02393/5.01783. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 5.02288/5.01826. Took 1.00 sec\n",
      "Epoch 4, Loss(train/val) 5.02400/5.01741. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 5.02288/5.01643. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 5.02246/5.01605. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 5.02296/5.01531. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 5.02156/5.01530. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 5.02104/5.01585. Took 1.00 sec\n",
      "Epoch 10, Loss(train/val) 5.02094/5.01540. Took 1.03 sec\n",
      "Epoch 11, Loss(train/val) 5.02059/5.01559. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 5.02122/5.01631. Took 1.08 sec\n",
      "Epoch 13, Loss(train/val) 5.01952/5.01669. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 5.02068/5.01737. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 5.01991/5.01753. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 5.02079/5.01872. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 5.01844/5.02004. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 5.01850/5.01990. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 5.01820/5.01905. Took 1.02 sec\n",
      "Epoch 20, Loss(train/val) 5.01928/5.01930. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 5.01767/5.01939. Took 1.04 sec\n",
      "Epoch 22, Loss(train/val) 5.01797/5.02060. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 5.01807/5.02030. Took 1.02 sec\n",
      "Epoch 24, Loss(train/val) 5.01772/5.01937. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 5.01768/5.01870. Took 1.03 sec\n",
      "Epoch 26, Loss(train/val) 5.01765/5.01957. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 5.01784/5.01820. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 5.01784/5.01819. Took 1.02 sec\n",
      "Epoch 29, Loss(train/val) 5.01676/5.02203. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 5.01640/5.02083. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 5.01577/5.02217. Took 1.02 sec\n",
      "Epoch 32, Loss(train/val) 5.01397/5.02105. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 5.01585/5.01899. Took 1.02 sec\n",
      "Epoch 34, Loss(train/val) 5.01428/5.02197. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 5.01432/5.02157. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 5.01485/5.02062. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 5.01605/5.02256. Took 1.00 sec\n",
      "Epoch 38, Loss(train/val) 5.01443/5.02091. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 5.01508/5.02219. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 5.01475/5.02100. Took 1.02 sec\n",
      "Epoch 41, Loss(train/val) 5.01409/5.02303. Took 1.03 sec\n",
      "Epoch 42, Loss(train/val) 5.01511/5.02405. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 5.01210/5.02330. Took 1.02 sec\n",
      "Epoch 44, Loss(train/val) 5.01451/5.02255. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 5.01328/5.02122. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 5.01423/5.02063. Took 1.02 sec\n",
      "Epoch 47, Loss(train/val) 5.01132/5.02377. Took 1.02 sec\n",
      "Epoch 48, Loss(train/val) 5.01230/5.01989. Took 1.02 sec\n",
      "Epoch 49, Loss(train/val) 5.01246/5.02324. Took 1.03 sec\n",
      "Epoch 50, Loss(train/val) 5.01215/5.02198. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 5.01212/5.01786. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 5.01190/5.01893. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 5.01021/5.02315. Took 1.02 sec\n",
      "Epoch 54, Loss(train/val) 5.01001/5.01944. Took 1.00 sec\n",
      "Epoch 55, Loss(train/val) 5.01169/5.01490. Took 1.02 sec\n",
      "Epoch 56, Loss(train/val) 5.01034/5.01863. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 5.00962/5.02318. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 5.01213/5.02253. Took 1.02 sec\n",
      "Epoch 59, Loss(train/val) 5.01059/5.01899. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 5.01044/5.02392. Took 1.02 sec\n",
      "Epoch 61, Loss(train/val) 5.01182/5.01947. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 5.00974/5.01963. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 5.01087/5.01994. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 5.01040/5.02202. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 5.00898/5.01865. Took 1.08 sec\n",
      "Epoch 66, Loss(train/val) 5.00852/5.02000. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 5.00923/5.01942. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 5.00952/5.02563. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 5.01031/5.02017. Took 1.02 sec\n",
      "Epoch 70, Loss(train/val) 5.00853/5.02181. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 5.01029/5.02409. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 5.00665/5.02014. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 5.00983/5.02337. Took 1.02 sec\n",
      "Epoch 74, Loss(train/val) 5.00730/5.02141. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 5.00979/5.01925. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 5.00763/5.01841. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 5.00966/5.01675. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 5.00874/5.01825. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 5.00951/5.01732. Took 1.00 sec\n",
      "Epoch 80, Loss(train/val) 5.00799/5.01920. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 5.00753/5.02027. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 5.00707/5.01430. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 5.00646/5.01637. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 5.00727/5.02109. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 5.00669/5.02005. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 5.00853/5.01572. Took 1.02 sec\n",
      "Epoch 87, Loss(train/val) 5.00585/5.01377. Took 1.02 sec\n",
      "Epoch 88, Loss(train/val) 5.00686/5.01707. Took 1.00 sec\n",
      "Epoch 89, Loss(train/val) 5.00648/5.01430. Took 1.03 sec\n",
      "Epoch 90, Loss(train/val) 5.00664/5.01459. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 5.00726/5.01457. Took 1.02 sec\n",
      "Epoch 92, Loss(train/val) 5.00355/5.01829. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 5.00491/5.02002. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 5.00619/5.02533. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 5.00461/5.02619. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 5.00852/5.01869. Took 1.00 sec\n",
      "Epoch 97, Loss(train/val) 5.00682/5.01980. Took 1.02 sec\n",
      "Epoch 98, Loss(train/val) 5.00495/5.02056. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 5.00879/5.01847. Took 1.01 sec\n",
      "ACC: 0.5234375, MCC: 0.03290901715515332\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.12706/5.03761. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 5.05686/5.03971. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 5.05235/5.04605. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 5.05004/5.04444. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 5.05018/5.04531. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 5.05106/5.04933. Took 1.00 sec\n",
      "Epoch 6, Loss(train/val) 5.05090/5.05206. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 5.04837/5.05447. Took 1.00 sec\n",
      "Epoch 8, Loss(train/val) 5.04966/5.05317. Took 1.02 sec\n",
      "Epoch 9, Loss(train/val) 5.04916/5.05560. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 5.04924/5.05596. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 5.04700/5.06003. Took 1.00 sec\n",
      "Epoch 12, Loss(train/val) 5.04552/5.05955. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 5.04666/5.06368. Took 1.08 sec\n",
      "Epoch 14, Loss(train/val) 5.04544/5.06270. Took 1.00 sec\n",
      "Epoch 15, Loss(train/val) 5.04378/5.05986. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 5.04408/5.06224. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 5.04378/5.05969. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 5.04412/5.06201. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 5.04292/5.06036. Took 1.02 sec\n",
      "Epoch 20, Loss(train/val) 5.04340/5.06149. Took 1.02 sec\n",
      "Epoch 21, Loss(train/val) 5.04321/5.05921. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 5.04364/5.06228. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 5.04251/5.06389. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 5.04228/5.05981. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 5.04253/5.06273. Took 1.02 sec\n",
      "Epoch 26, Loss(train/val) 5.04214/5.06244. Took 1.02 sec\n",
      "Epoch 27, Loss(train/val) 5.04169/5.05923. Took 1.02 sec\n",
      "Epoch 28, Loss(train/val) 5.04204/5.06052. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 5.04083/5.05865. Took 1.00 sec\n",
      "Epoch 30, Loss(train/val) 5.03915/5.06500. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 5.04106/5.06042. Took 1.00 sec\n",
      "Epoch 32, Loss(train/val) 5.03931/5.06119. Took 1.02 sec\n",
      "Epoch 33, Loss(train/val) 5.03930/5.05765. Took 1.02 sec\n",
      "Epoch 34, Loss(train/val) 5.04159/5.05447. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 5.03994/5.06158. Took 1.02 sec\n",
      "Epoch 36, Loss(train/val) 5.03890/5.05533. Took 1.02 sec\n",
      "Epoch 37, Loss(train/val) 5.04147/5.05837. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 5.03998/5.06216. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 5.03991/5.05313. Took 1.00 sec\n",
      "Epoch 40, Loss(train/val) 5.03952/5.06016. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 5.04238/5.04943. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 5.03906/5.06190. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 5.04008/5.05343. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 5.04296/5.06017. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 5.04441/5.04219. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 5.04289/5.05779. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 5.04091/5.04805. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 5.04185/5.05521. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 5.04109/5.05266. Took 1.03 sec\n",
      "Epoch 50, Loss(train/val) 5.04129/5.05718. Took 1.03 sec\n",
      "Epoch 51, Loss(train/val) 5.04288/5.05312. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 5.04103/5.05339. Took 1.02 sec\n",
      "Epoch 53, Loss(train/val) 5.04073/5.05672. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 5.03963/5.05150. Took 1.00 sec\n",
      "Epoch 55, Loss(train/val) 5.04088/5.05809. Took 1.00 sec\n",
      "Epoch 56, Loss(train/val) 5.03812/5.05652. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 5.03972/5.05342. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 5.03955/5.05518. Took 1.00 sec\n",
      "Epoch 59, Loss(train/val) 5.03927/5.05273. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 5.04154/5.04715. Took 1.02 sec\n",
      "Epoch 61, Loss(train/val) 5.04027/5.05690. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 5.03833/5.05299. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 5.03722/5.05224. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 5.03724/5.05207. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 5.03747/5.05039. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 5.03848/5.05637. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 5.03600/5.05344. Took 1.08 sec\n",
      "Epoch 68, Loss(train/val) 5.03564/5.05142. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 5.03770/5.05275. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 5.03593/5.05326. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 5.04291/5.05400. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 5.04665/5.05730. Took 1.00 sec\n",
      "Epoch 73, Loss(train/val) 5.04673/5.05708. Took 1.02 sec\n",
      "Epoch 74, Loss(train/val) 5.04327/5.06123. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 5.04375/5.05760. Took 1.00 sec\n",
      "Epoch 76, Loss(train/val) 5.04350/5.05686. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 5.04462/5.05476. Took 1.02 sec\n",
      "Epoch 78, Loss(train/val) 5.04412/5.05796. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 5.04502/5.05366. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 5.04368/5.05813. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 5.04217/5.05275. Took 1.00 sec\n",
      "Epoch 82, Loss(train/val) 5.04102/5.05623. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 5.04097/5.05873. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 5.04291/5.05223. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 5.04011/5.05246. Took 1.00 sec\n",
      "Epoch 86, Loss(train/val) 5.03798/5.05256. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 5.04205/5.05777. Took 1.00 sec\n",
      "Epoch 88, Loss(train/val) 5.04108/5.05391. Took 1.02 sec\n",
      "Epoch 89, Loss(train/val) 5.03997/5.05496. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 5.03769/5.05552. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 5.03902/5.05412. Took 1.00 sec\n",
      "Epoch 92, Loss(train/val) 5.04281/5.06005. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 5.04107/5.05570. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 5.03899/5.05908. Took 1.03 sec\n",
      "Epoch 95, Loss(train/val) 5.03864/5.05566. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 5.03925/5.05596. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 5.03991/5.05765. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 5.04161/5.05121. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 5.04027/5.05913. Took 1.01 sec\n",
      "ACC: 0.453125, MCC: -0.08304547985373996\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.02837/5.00251. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 5.00113/5.01359. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 5.01403/4.98849. Took 1.00 sec\n",
      "Epoch 3, Loss(train/val) 4.99871/4.99029. Took 1.00 sec\n",
      "Epoch 4, Loss(train/val) 4.99290/4.98907. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 4.99553/4.98933. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.99480/4.99022. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.99413/4.99080. Took 1.02 sec\n",
      "Epoch 8, Loss(train/val) 4.99371/4.99228. Took 1.00 sec\n",
      "Epoch 9, Loss(train/val) 4.99353/4.99233. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 4.99313/4.99352. Took 1.00 sec\n",
      "Epoch 11, Loss(train/val) 4.99293/4.99284. Took 1.00 sec\n",
      "Epoch 12, Loss(train/val) 4.99256/4.99318. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.99233/4.99303. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 4.99045/4.99339. Took 1.00 sec\n",
      "Epoch 15, Loss(train/val) 4.99158/4.99288. Took 1.00 sec\n",
      "Epoch 16, Loss(train/val) 4.99150/4.99188. Took 1.08 sec\n",
      "Epoch 17, Loss(train/val) 4.99060/4.99310. Took 1.00 sec\n",
      "Epoch 18, Loss(train/val) 4.99130/4.99230. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 4.99048/4.99335. Took 1.00 sec\n",
      "Epoch 20, Loss(train/val) 4.98920/4.99355. Took 1.00 sec\n",
      "Epoch 21, Loss(train/val) 4.98905/4.99362. Took 1.02 sec\n",
      "Epoch 22, Loss(train/val) 4.98910/4.99183. Took 1.00 sec\n",
      "Epoch 23, Loss(train/val) 4.98963/4.99381. Took 1.00 sec\n",
      "Epoch 24, Loss(train/val) 4.99046/4.99500. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 4.98887/4.99300. Took 1.00 sec\n",
      "Epoch 26, Loss(train/val) 4.98801/4.99239. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 4.98705/4.99176. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 4.98834/4.98934. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.98739/4.99064. Took 1.02 sec\n",
      "Epoch 30, Loss(train/val) 4.98797/4.98991. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.98714/4.99167. Took 1.03 sec\n",
      "Epoch 32, Loss(train/val) 4.98452/4.99310. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.98626/4.99114. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.98627/4.99283. Took 1.00 sec\n",
      "Epoch 35, Loss(train/val) 4.98662/4.99387. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 4.98717/4.99429. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.98684/4.99245. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 4.98621/4.99411. Took 1.00 sec\n",
      "Epoch 39, Loss(train/val) 4.98638/4.99547. Took 1.03 sec\n",
      "Epoch 40, Loss(train/val) 4.98678/4.99366. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.98549/4.99372. Took 1.00 sec\n",
      "Epoch 42, Loss(train/val) 4.98477/4.99402. Took 1.00 sec\n",
      "Epoch 43, Loss(train/val) 4.98436/4.99466. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.98400/4.99546. Took 1.02 sec\n",
      "Epoch 45, Loss(train/val) 4.98409/4.99632. Took 1.02 sec\n",
      "Epoch 46, Loss(train/val) 4.98339/4.99663. Took 1.00 sec\n",
      "Epoch 47, Loss(train/val) 4.98382/4.99557. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 4.98289/4.99495. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.98702/5.00160. Took 1.00 sec\n",
      "Epoch 50, Loss(train/val) 4.98698/4.99578. Took 1.00 sec\n",
      "Epoch 51, Loss(train/val) 4.98524/4.99602. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 4.98399/4.99913. Took 1.02 sec\n",
      "Epoch 53, Loss(train/val) 4.98432/5.00286. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 4.98534/5.00507. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 4.98617/5.00230. Took 1.00 sec\n",
      "Epoch 56, Loss(train/val) 4.98400/5.01033. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 4.98542/5.00409. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 4.98570/5.00360. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.98452/5.00759. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.98320/5.01330. Took 1.02 sec\n",
      "Epoch 61, Loss(train/val) 4.98538/5.00475. Took 1.00 sec\n",
      "Epoch 62, Loss(train/val) 4.98376/5.00762. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.98341/5.00853. Took 1.02 sec\n",
      "Epoch 64, Loss(train/val) 4.98408/5.01507. Took 1.00 sec\n",
      "Epoch 65, Loss(train/val) 4.98214/5.00980. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 4.98200/5.01558. Took 1.02 sec\n",
      "Epoch 67, Loss(train/val) 4.98295/5.01440. Took 1.00 sec\n",
      "Epoch 68, Loss(train/val) 4.98164/5.01060. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 4.98349/5.01135. Took 1.02 sec\n",
      "Epoch 70, Loss(train/val) 4.98294/5.01207. Took 1.10 sec\n",
      "Epoch 71, Loss(train/val) 4.98158/5.01739. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 4.98041/5.02118. Took 1.02 sec\n",
      "Epoch 73, Loss(train/val) 4.98095/5.01086. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.98480/5.01142. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.98035/5.01238. Took 1.00 sec\n",
      "Epoch 76, Loss(train/val) 4.97928/5.02030. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 4.98240/5.02433. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.98038/5.02565. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 4.98233/5.01660. Took 1.02 sec\n",
      "Epoch 80, Loss(train/val) 4.97870/5.02448. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 4.98084/5.01923. Took 1.02 sec\n",
      "Epoch 82, Loss(train/val) 4.98123/5.01968. Took 1.02 sec\n",
      "Epoch 83, Loss(train/val) 4.97816/5.03035. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.97914/5.03069. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 4.98075/5.00684. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 4.98150/5.01116. Took 1.04 sec\n",
      "Epoch 87, Loss(train/val) 4.97920/5.02086. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.98044/5.01690. Took 1.00 sec\n",
      "Epoch 89, Loss(train/val) 4.98016/5.02293. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 4.97757/5.00984. Took 1.02 sec\n",
      "Epoch 91, Loss(train/val) 4.97697/5.01934. Took 1.02 sec\n",
      "Epoch 92, Loss(train/val) 4.98066/5.02108. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 4.97861/5.01797. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.97831/5.02299. Took 1.00 sec\n",
      "Epoch 95, Loss(train/val) 4.97896/5.02299. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 4.97927/5.02629. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.97676/5.02691. Took 1.00 sec\n",
      "Epoch 98, Loss(train/val) 4.98030/5.02061. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 4.97825/5.03011. Took 1.01 sec\n",
      "ACC: 0.4609375, MCC: -0.07930515857181442\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.04585/5.00527. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.99247/5.00722. Took 1.03 sec\n",
      "Epoch 2, Loss(train/val) 4.99385/4.98812. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 4.98860/4.98552. Took 1.02 sec\n",
      "Epoch 4, Loss(train/val) 4.98714/4.98858. Took 1.02 sec\n",
      "Epoch 5, Loss(train/val) 4.98680/4.99311. Took 1.00 sec\n",
      "Epoch 6, Loss(train/val) 4.98875/4.99555. Took 1.02 sec\n",
      "Epoch 7, Loss(train/val) 4.98772/4.99586. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 4.98898/4.99410. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.98736/4.99277. Took 1.00 sec\n",
      "Epoch 10, Loss(train/val) 4.98683/4.99412. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.98693/4.99599. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 4.98714/4.99721. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.98609/4.99773. Took 1.02 sec\n",
      "Epoch 14, Loss(train/val) 4.98731/4.99824. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.98713/4.99642. Took 1.02 sec\n",
      "Epoch 16, Loss(train/val) 4.98594/4.99441. Took 1.02 sec\n",
      "Epoch 17, Loss(train/val) 4.98523/4.99480. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 4.98446/4.99757. Took 1.00 sec\n",
      "Epoch 19, Loss(train/val) 4.98472/4.99613. Took 1.02 sec\n",
      "Epoch 20, Loss(train/val) 4.98563/4.99716. Took 1.08 sec\n",
      "Epoch 21, Loss(train/val) 4.98390/4.99768. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.98286/5.00151. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.98454/5.00201. Took 1.02 sec\n",
      "Epoch 24, Loss(train/val) 4.98404/4.99835. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 4.98237/4.99943. Took 1.02 sec\n",
      "Epoch 26, Loss(train/val) 4.98263/4.99946. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 4.98395/4.99720. Took 1.02 sec\n",
      "Epoch 28, Loss(train/val) 4.98372/4.99474. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.98176/4.99349. Took 1.02 sec\n",
      "Epoch 30, Loss(train/val) 4.98113/4.99739. Took 1.00 sec\n",
      "Epoch 31, Loss(train/val) 4.98205/4.99525. Took 1.00 sec\n",
      "Epoch 32, Loss(train/val) 4.98057/4.99608. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.98096/5.00128. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.98154/4.99728. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 4.98128/4.99145. Took 1.02 sec\n",
      "Epoch 36, Loss(train/val) 4.98021/4.99467. Took 1.00 sec\n",
      "Epoch 37, Loss(train/val) 4.98267/4.99258. Took 1.02 sec\n",
      "Epoch 38, Loss(train/val) 4.97842/4.99588. Took 1.02 sec\n",
      "Epoch 39, Loss(train/val) 4.98032/4.99126. Took 1.02 sec\n",
      "Epoch 40, Loss(train/val) 4.97924/4.98771. Took 1.00 sec\n",
      "Epoch 41, Loss(train/val) 4.97994/4.98689. Took 1.02 sec\n",
      "Epoch 42, Loss(train/val) 4.97915/4.99069. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.98004/4.99618. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.97717/4.99500. Took 1.00 sec\n",
      "Epoch 45, Loss(train/val) 4.97981/4.99436. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 4.97991/4.99288. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.97811/4.98874. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 4.97732/4.99383. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.97811/4.98872. Took 1.00 sec\n",
      "Epoch 50, Loss(train/val) 4.97953/4.98663. Took 1.00 sec\n",
      "Epoch 51, Loss(train/val) 4.97683/4.98902. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 4.97684/4.98928. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 4.97770/4.99293. Took 1.00 sec\n",
      "Epoch 54, Loss(train/val) 4.97571/4.98970. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 4.97954/4.98784. Took 1.00 sec\n",
      "Epoch 56, Loss(train/val) 4.98289/4.99454. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 4.97972/4.98760. Took 1.03 sec\n",
      "Epoch 58, Loss(train/val) 4.98083/4.99055. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.97757/4.98403. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.97567/4.99161. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 4.97482/4.98452. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 4.97920/4.98391. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.97839/4.99062. Took 1.00 sec\n",
      "Epoch 64, Loss(train/val) 4.97601/4.98465. Took 1.00 sec\n",
      "Epoch 65, Loss(train/val) 4.97466/4.98718. Took 1.02 sec\n",
      "Epoch 66, Loss(train/val) 4.97364/4.98480. Took 1.02 sec\n",
      "Epoch 67, Loss(train/val) 4.97467/4.98895. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 4.97343/4.99019. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 4.97722/4.98756. Took 1.00 sec\n",
      "Epoch 70, Loss(train/val) 4.97712/4.98443. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 4.97625/4.98540. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 4.97624/4.98249. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.97603/4.98035. Took 1.00 sec\n",
      "Epoch 74, Loss(train/val) 4.97492/4.97979. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.97773/4.97666. Took 1.09 sec\n",
      "Epoch 76, Loss(train/val) 4.97479/4.97985. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 4.97664/4.97736. Took 1.00 sec\n",
      "Epoch 78, Loss(train/val) 4.97577/4.97937. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 4.97543/4.97667. Took 1.02 sec\n",
      "Epoch 80, Loss(train/val) 4.97451/4.98023. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 4.97370/4.97925. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.97412/4.97888. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 4.97656/4.97475. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.97204/4.98119. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 4.97501/4.97970. Took 1.00 sec\n",
      "Epoch 86, Loss(train/val) 4.97144/4.98548. Took 1.00 sec\n",
      "Epoch 87, Loss(train/val) 4.97680/4.98594. Took 1.00 sec\n",
      "Epoch 88, Loss(train/val) 4.97453/4.98248. Took 1.00 sec\n",
      "Epoch 89, Loss(train/val) 4.97090/4.98761. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 4.97496/4.98258. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 4.97226/4.98143. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 4.97549/4.97812. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 4.97507/4.97831. Took 1.02 sec\n",
      "Epoch 94, Loss(train/val) 4.97096/4.98315. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 4.97512/4.97406. Took 1.02 sec\n",
      "Epoch 96, Loss(train/val) 4.97291/4.97680. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.97166/4.98372. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 4.97888/4.99375. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 4.97724/4.98379. Took 1.02 sec\n",
      "ACC: 0.5546875, MCC: 0.10828429210168992\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.04350/5.04106. Took 1.01 sec\n",
      "Epoch 1, Loss(train/val) 5.03870/5.02961. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 5.03594/5.02810. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 5.03434/5.02967. Took 1.00 sec\n",
      "Epoch 4, Loss(train/val) 5.03531/5.03090. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 5.03339/5.03207. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 5.03305/5.03451. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 5.03350/5.03744. Took 1.03 sec\n",
      "Epoch 8, Loss(train/val) 5.03330/5.03906. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 5.03256/5.03877. Took 1.02 sec\n",
      "Epoch 10, Loss(train/val) 5.03315/5.03765. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 5.03319/5.03881. Took 1.03 sec\n",
      "Epoch 12, Loss(train/val) 5.03248/5.04050. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 5.03253/5.04142. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 5.03303/5.04091. Took 1.02 sec\n",
      "Epoch 15, Loss(train/val) 5.03194/5.04045. Took 1.00 sec\n",
      "Epoch 16, Loss(train/val) 5.03175/5.04271. Took 1.00 sec\n",
      "Epoch 17, Loss(train/val) 5.03284/5.03951. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 5.03310/5.03878. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 5.03198/5.04099. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 5.03143/5.03988. Took 1.02 sec\n",
      "Epoch 21, Loss(train/val) 5.03053/5.03881. Took 1.02 sec\n",
      "Epoch 22, Loss(train/val) 5.03056/5.04145. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 5.03196/5.04210. Took 1.02 sec\n",
      "Epoch 24, Loss(train/val) 5.03207/5.04194. Took 1.09 sec\n",
      "Epoch 25, Loss(train/val) 5.02905/5.04321. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 5.03017/5.04247. Took 1.00 sec\n",
      "Epoch 27, Loss(train/val) 5.03011/5.04417. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 5.03080/5.04327. Took 1.03 sec\n",
      "Epoch 29, Loss(train/val) 5.02842/5.04524. Took 1.00 sec\n",
      "Epoch 30, Loss(train/val) 5.03252/5.04169. Took 1.00 sec\n",
      "Epoch 31, Loss(train/val) 5.03113/5.04091. Took 1.00 sec\n",
      "Epoch 32, Loss(train/val) 5.02958/5.04464. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 5.03009/5.04201. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 5.02919/5.04764. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 5.02889/5.04712. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 5.02898/5.04369. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 5.02963/5.04415. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 5.02995/5.04713. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 5.02911/5.04248. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 5.02891/5.04311. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 5.02844/5.04407. Took 1.02 sec\n",
      "Epoch 42, Loss(train/val) 5.02758/5.04582. Took 1.02 sec\n",
      "Epoch 43, Loss(train/val) 5.02950/5.03910. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 5.02825/5.04402. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 5.02802/5.05064. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 5.03020/5.04083. Took 1.02 sec\n",
      "Epoch 47, Loss(train/val) 5.02637/5.04518. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 5.02708/5.04912. Took 1.00 sec\n",
      "Epoch 49, Loss(train/val) 5.02971/5.04324. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 5.02796/5.04647. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 5.02753/5.05019. Took 1.00 sec\n",
      "Epoch 52, Loss(train/val) 5.02820/5.04579. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 5.02837/5.04398. Took 1.00 sec\n",
      "Epoch 54, Loss(train/val) 5.02561/5.04528. Took 1.00 sec\n",
      "Epoch 55, Loss(train/val) 5.02613/5.04690. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 5.02758/5.05033. Took 1.00 sec\n",
      "Epoch 57, Loss(train/val) 5.02760/5.04506. Took 1.00 sec\n",
      "Epoch 58, Loss(train/val) 5.02686/5.04786. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 5.02608/5.04555. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 5.02485/5.04701. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 5.02381/5.03942. Took 1.00 sec\n",
      "Epoch 62, Loss(train/val) 5.02678/5.04803. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 5.02464/5.05323. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 5.02692/5.04540. Took 1.04 sec\n",
      "Epoch 65, Loss(train/val) 5.02602/5.04820. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 5.02438/5.04829. Took 1.00 sec\n",
      "Epoch 67, Loss(train/val) 5.02749/5.04082. Took 1.00 sec\n",
      "Epoch 68, Loss(train/val) 5.02314/5.05458. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 5.02564/5.03870. Took 1.00 sec\n",
      "Epoch 70, Loss(train/val) 5.02403/5.05134. Took 1.02 sec\n",
      "Epoch 71, Loss(train/val) 5.02633/5.04011. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 5.02290/5.05245. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 5.02540/5.03852. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 5.02466/5.04874. Took 1.02 sec\n",
      "Epoch 75, Loss(train/val) 5.02594/5.04377. Took 1.02 sec\n",
      "Epoch 76, Loss(train/val) 5.02332/5.04579. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 5.02223/5.05018. Took 1.02 sec\n",
      "Epoch 78, Loss(train/val) 5.02484/5.04575. Took 1.00 sec\n",
      "Epoch 79, Loss(train/val) 5.02431/5.05023. Took 1.02 sec\n",
      "Epoch 80, Loss(train/val) 5.02275/5.04768. Took 1.09 sec\n",
      "Epoch 81, Loss(train/val) 5.02224/5.04864. Took 1.02 sec\n",
      "Epoch 82, Loss(train/val) 5.02334/5.04041. Took 1.00 sec\n",
      "Epoch 83, Loss(train/val) 5.02147/5.04626. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 5.02135/5.04815. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 5.02074/5.05015. Took 1.00 sec\n",
      "Epoch 86, Loss(train/val) 5.01718/5.05909. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 5.02311/5.03147. Took 1.02 sec\n",
      "Epoch 88, Loss(train/val) 5.02278/5.04209. Took 1.02 sec\n",
      "Epoch 89, Loss(train/val) 5.02792/5.04002. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 5.03038/5.04611. Took 1.00 sec\n",
      "Epoch 91, Loss(train/val) 5.02753/5.05026. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 5.02641/5.04398. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 5.02692/5.04673. Took 1.00 sec\n",
      "Epoch 94, Loss(train/val) 5.02479/5.04627. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 5.02369/5.04227. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 5.02279/5.04390. Took 1.00 sec\n",
      "Epoch 97, Loss(train/val) 5.02378/5.03638. Took 1.02 sec\n",
      "Epoch 98, Loss(train/val) 5.02299/5.03676. Took 1.02 sec\n",
      "Epoch 99, Loss(train/val) 5.01708/5.04412. Took 1.00 sec\n",
      "ACC: 0.484375, MCC: -0.02936860728761487\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.01810/4.98691. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.99208/4.98481. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 5.00005/4.98212. Took 1.04 sec\n",
      "Epoch 3, Loss(train/val) 5.00154/4.99054. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 4.99337/4.98872. Took 1.00 sec\n",
      "Epoch 5, Loss(train/val) 4.98938/4.98450. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.99070/4.98487. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.99123/4.98605. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 4.99061/4.98575. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.98887/4.98415. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 4.98808/4.98300. Took 1.00 sec\n",
      "Epoch 11, Loss(train/val) 4.98749/4.97969. Took 1.02 sec\n",
      "Epoch 12, Loss(train/val) 4.98708/4.97549. Took 1.00 sec\n",
      "Epoch 13, Loss(train/val) 4.98812/4.97866. Took 1.00 sec\n",
      "Epoch 14, Loss(train/val) 4.98387/4.97422. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.98362/4.97387. Took 1.00 sec\n",
      "Epoch 16, Loss(train/val) 4.98373/4.97575. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 4.98360/4.97444. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 4.98317/4.97309. Took 1.00 sec\n",
      "Epoch 19, Loss(train/val) 4.98516/4.97465. Took 1.00 sec\n",
      "Epoch 20, Loss(train/val) 4.98129/4.97527. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.97940/4.97234. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.98371/4.97568. Took 1.00 sec\n",
      "Epoch 23, Loss(train/val) 4.98236/4.97439. Took 1.00 sec\n",
      "Epoch 24, Loss(train/val) 4.98097/4.97586. Took 1.02 sec\n",
      "Epoch 25, Loss(train/val) 4.98005/4.97517. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 4.98015/4.97681. Took 1.02 sec\n",
      "Epoch 27, Loss(train/val) 4.98103/4.97531. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 4.98025/4.97528. Took 1.02 sec\n",
      "Epoch 29, Loss(train/val) 4.97868/4.97383. Took 1.02 sec\n",
      "Epoch 30, Loss(train/val) 4.97882/4.97544. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.98134/4.97656. Took 1.09 sec\n",
      "Epoch 32, Loss(train/val) 4.98096/4.97775. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.98213/4.97919. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.97989/4.97703. Took 1.00 sec\n",
      "Epoch 35, Loss(train/val) 4.98052/4.97900. Took 1.00 sec\n",
      "Epoch 36, Loss(train/val) 4.97877/4.97935. Took 1.02 sec\n",
      "Epoch 37, Loss(train/val) 4.97900/4.97796. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 4.98020/4.98154. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.98011/4.98127. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 4.97856/4.98617. Took 1.02 sec\n",
      "Epoch 41, Loss(train/val) 4.97942/4.98741. Took 1.03 sec\n",
      "Epoch 42, Loss(train/val) 4.97887/4.98878. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.97959/4.98911. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.97885/4.98777. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.97819/4.98906. Took 1.02 sec\n",
      "Epoch 46, Loss(train/val) 4.97767/4.98894. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.97623/4.99037. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 4.97879/4.98814. Took 1.02 sec\n",
      "Epoch 49, Loss(train/val) 4.97528/4.98964. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 4.97914/4.98898. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.97674/4.99117. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 4.97493/4.99307. Took 1.02 sec\n",
      "Epoch 53, Loss(train/val) 4.98121/4.98592. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 4.97917/4.98542. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 4.98438/4.98605. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 4.98337/4.97692. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 4.98169/4.97617. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 4.98096/4.97773. Took 1.02 sec\n",
      "Epoch 59, Loss(train/val) 4.97879/4.97674. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.97887/4.97788. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 4.97849/4.98063. Took 1.02 sec\n",
      "Epoch 62, Loss(train/val) 4.97775/4.98141. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.97912/4.98211. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 4.98012/4.98373. Took 1.00 sec\n",
      "Epoch 65, Loss(train/val) 4.97800/4.98276. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 4.97963/4.98210. Took 1.02 sec\n",
      "Epoch 67, Loss(train/val) 4.97699/4.98765. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 4.97746/4.98756. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 4.97691/4.98477. Took 1.02 sec\n",
      "Epoch 70, Loss(train/val) 4.97784/4.98843. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 4.97550/4.98555. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 4.97548/4.98865. Took 1.03 sec\n",
      "Epoch 73, Loss(train/val) 4.97667/4.98694. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.97659/4.98813. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.97731/4.99563. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 4.97685/4.99198. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 4.97594/4.99093. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.97572/4.98886. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 4.97711/4.99528. Took 1.02 sec\n",
      "Epoch 80, Loss(train/val) 4.97631/4.98979. Took 1.03 sec\n",
      "Epoch 81, Loss(train/val) 4.97571/4.99330. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.97491/4.99385. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 4.97559/4.98905. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.97571/4.98901. Took 1.03 sec\n",
      "Epoch 85, Loss(train/val) 4.97408/4.99323. Took 1.00 sec\n",
      "Epoch 86, Loss(train/val) 4.97796/4.99273. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 4.97547/4.98355. Took 1.08 sec\n",
      "Epoch 88, Loss(train/val) 4.97849/4.98591. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.97471/4.98901. Took 1.00 sec\n",
      "Epoch 90, Loss(train/val) 4.97374/4.99458. Took 1.02 sec\n",
      "Epoch 91, Loss(train/val) 4.97591/4.99297. Took 1.02 sec\n",
      "Epoch 92, Loss(train/val) 4.97853/4.98309. Took 1.02 sec\n",
      "Epoch 93, Loss(train/val) 4.97560/4.98487. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.97654/4.98984. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 4.97492/4.98759. Took 1.02 sec\n",
      "Epoch 96, Loss(train/val) 4.98108/4.98894. Took 1.03 sec\n",
      "Epoch 97, Loss(train/val) 4.97888/4.97765. Took 1.00 sec\n",
      "Epoch 98, Loss(train/val) 4.97997/4.97585. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 4.97938/4.97815. Took 1.00 sec\n",
      "ACC: 0.4609375, MCC: -0.08085289526061018\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.00922/4.99717. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.97288/4.98150. Took 1.03 sec\n",
      "Epoch 2, Loss(train/val) 4.97171/4.98101. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 4.97128/4.98237. Took 1.02 sec\n",
      "Epoch 4, Loss(train/val) 4.97180/4.98157. Took 1.00 sec\n",
      "Epoch 5, Loss(train/val) 4.97120/4.98134. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.97143/4.98175. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.97017/4.98071. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 4.96953/4.98027. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.97063/4.98059. Took 1.00 sec\n",
      "Epoch 10, Loss(train/val) 4.97160/4.97925. Took 1.00 sec\n",
      "Epoch 11, Loss(train/val) 4.97054/4.97815. Took 1.00 sec\n",
      "Epoch 12, Loss(train/val) 4.96772/4.97902. Took 1.00 sec\n",
      "Epoch 13, Loss(train/val) 4.96786/4.97903. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 4.96948/4.97798. Took 1.00 sec\n",
      "Epoch 15, Loss(train/val) 4.96811/4.97767. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 4.96896/4.97534. Took 1.00 sec\n",
      "Epoch 17, Loss(train/val) 4.96738/4.97494. Took 1.00 sec\n",
      "Epoch 18, Loss(train/val) 4.96881/4.97353. Took 1.00 sec\n",
      "Epoch 19, Loss(train/val) 4.97090/4.97228. Took 1.00 sec\n",
      "Epoch 20, Loss(train/val) 4.96922/4.96930. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.97128/4.97825. Took 1.00 sec\n",
      "Epoch 22, Loss(train/val) 4.97075/4.97593. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.96729/4.97880. Took 1.00 sec\n",
      "Epoch 24, Loss(train/val) 4.96710/4.97131. Took 1.00 sec\n",
      "Epoch 25, Loss(train/val) 4.96571/4.97991. Took 1.00 sec\n",
      "Epoch 26, Loss(train/val) 4.96870/4.97434. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 4.96642/4.97373. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 4.96487/4.97319. Took 1.00 sec\n",
      "Epoch 29, Loss(train/val) 4.96420/4.97456. Took 1.00 sec\n",
      "Epoch 30, Loss(train/val) 4.96637/4.97588. Took 1.02 sec\n",
      "Epoch 31, Loss(train/val) 4.96586/4.97546. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 4.96635/4.98398. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.96644/4.97706. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.96698/4.97660. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 4.96431/4.97499. Took 1.03 sec\n",
      "Epoch 36, Loss(train/val) 4.96573/4.97208. Took 1.05 sec\n",
      "Epoch 37, Loss(train/val) 4.96506/4.97497. Took 1.03 sec\n",
      "Epoch 38, Loss(train/val) 4.96354/4.97253. Took 1.02 sec\n",
      "Epoch 39, Loss(train/val) 4.96630/4.97615. Took 1.10 sec\n",
      "Epoch 40, Loss(train/val) 4.96293/4.97613. Took 1.02 sec\n",
      "Epoch 41, Loss(train/val) 4.96590/4.97265. Took 1.02 sec\n",
      "Epoch 42, Loss(train/val) 4.96405/4.97247. Took 1.02 sec\n",
      "Epoch 43, Loss(train/val) 4.96165/4.97244. Took 1.02 sec\n",
      "Epoch 44, Loss(train/val) 4.96423/4.97313. Took 1.02 sec\n",
      "Epoch 45, Loss(train/val) 4.96384/4.97168. Took 1.02 sec\n",
      "Epoch 46, Loss(train/val) 4.96181/4.97250. Took 1.02 sec\n",
      "Epoch 47, Loss(train/val) 4.96049/4.97220. Took 1.02 sec\n",
      "Epoch 48, Loss(train/val) 4.96248/4.97158. Took 1.02 sec\n",
      "Epoch 49, Loss(train/val) 4.96353/4.97495. Took 1.03 sec\n",
      "Epoch 50, Loss(train/val) 4.96289/4.97286. Took 1.02 sec\n",
      "Epoch 51, Loss(train/val) 4.96133/4.97116. Took 1.02 sec\n",
      "Epoch 52, Loss(train/val) 4.96158/4.97076. Took 1.02 sec\n",
      "Epoch 53, Loss(train/val) 4.96250/4.96878. Took 1.02 sec\n",
      "Epoch 54, Loss(train/val) 4.96165/4.97090. Took 1.02 sec\n",
      "Epoch 55, Loss(train/val) 4.96176/4.96960. Took 1.03 sec\n",
      "Epoch 56, Loss(train/val) 4.95919/4.96908. Took 1.02 sec\n",
      "Epoch 57, Loss(train/val) 4.95982/4.97481. Took 1.02 sec\n",
      "Epoch 58, Loss(train/val) 4.96060/4.97345. Took 1.02 sec\n",
      "Epoch 59, Loss(train/val) 4.96108/4.97187. Took 1.02 sec\n",
      "Epoch 60, Loss(train/val) 4.96242/4.96896. Took 1.03 sec\n",
      "Epoch 61, Loss(train/val) 4.95925/4.97077. Took 1.02 sec\n",
      "Epoch 62, Loss(train/val) 4.95888/4.97284. Took 1.03 sec\n",
      "Epoch 63, Loss(train/val) 4.96109/4.96843. Took 1.02 sec\n",
      "Epoch 64, Loss(train/val) 4.95982/4.97342. Took 1.02 sec\n",
      "Epoch 65, Loss(train/val) 4.95815/4.96482. Took 1.02 sec\n",
      "Epoch 66, Loss(train/val) 4.95756/4.96235. Took 1.02 sec\n",
      "Epoch 67, Loss(train/val) 4.95967/4.97392. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 4.96180/4.97270. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 4.95793/4.97037. Took 1.02 sec\n",
      "Epoch 70, Loss(train/val) 4.95625/4.97727. Took 1.02 sec\n",
      "Epoch 71, Loss(train/val) 4.96302/4.96996. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 4.95597/4.97430. Took 1.02 sec\n",
      "Epoch 73, Loss(train/val) 4.95812/4.97650. Took 1.02 sec\n",
      "Epoch 74, Loss(train/val) 4.95799/4.97182. Took 1.02 sec\n",
      "Epoch 75, Loss(train/val) 4.95832/4.97190. Took 1.02 sec\n",
      "Epoch 76, Loss(train/val) 4.95697/4.96612. Took 1.02 sec\n",
      "Epoch 77, Loss(train/val) 4.95815/4.96649. Took 1.02 sec\n",
      "Epoch 78, Loss(train/val) 4.95544/4.96698. Took 1.02 sec\n",
      "Epoch 79, Loss(train/val) 4.95861/4.96340. Took 1.02 sec\n",
      "Epoch 80, Loss(train/val) 4.95838/4.96986. Took 1.02 sec\n",
      "Epoch 81, Loss(train/val) 4.95699/4.97563. Took 1.02 sec\n",
      "Epoch 82, Loss(train/val) 4.95663/4.96978. Took 1.03 sec\n",
      "Epoch 83, Loss(train/val) 4.96081/4.97183. Took 1.02 sec\n",
      "Epoch 84, Loss(train/val) 4.95551/4.96481. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 4.95945/4.96890. Took 1.02 sec\n",
      "Epoch 86, Loss(train/val) 4.95938/4.97366. Took 1.02 sec\n",
      "Epoch 87, Loss(train/val) 4.95806/4.97154. Took 1.02 sec\n",
      "Epoch 88, Loss(train/val) 4.95535/4.96739. Took 1.02 sec\n",
      "Epoch 89, Loss(train/val) 4.95513/4.97881. Took 1.02 sec\n",
      "Epoch 90, Loss(train/val) 4.95436/4.96851. Took 1.02 sec\n",
      "Epoch 91, Loss(train/val) 4.95722/4.97359. Took 1.02 sec\n",
      "Epoch 92, Loss(train/val) 4.95622/4.97195. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 4.95226/4.96961. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.95925/4.97034. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 4.95417/4.97435. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 4.95625/4.97091. Took 1.02 sec\n",
      "Epoch 97, Loss(train/val) 4.95239/4.97310. Took 1.09 sec\n",
      "Epoch 98, Loss(train/val) 4.95307/4.97130. Took 1.00 sec\n",
      "Epoch 99, Loss(train/val) 4.95449/4.96920. Took 1.01 sec\n",
      "ACC: 0.484375, MCC: -0.03126526997403612\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.03405/5.02347. Took 1.01 sec\n",
      "Epoch 1, Loss(train/val) 5.00287/4.99909. Took 1.00 sec\n",
      "Epoch 2, Loss(train/val) 5.00138/4.99899. Took 1.00 sec\n",
      "Epoch 3, Loss(train/val) 5.00101/5.00092. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 5.00157/5.00138. Took 1.02 sec\n",
      "Epoch 5, Loss(train/val) 5.00239/4.99981. Took 1.00 sec\n",
      "Epoch 6, Loss(train/val) 5.00241/4.99870. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 5.00093/4.99899. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 4.99978/4.99929. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.99843/4.99924. Took 1.00 sec\n",
      "Epoch 10, Loss(train/val) 4.99864/4.99877. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.99958/4.99841. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 4.99914/4.99937. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.99987/4.99979. Took 1.00 sec\n",
      "Epoch 14, Loss(train/val) 4.99898/4.99985. Took 1.05 sec\n",
      "Epoch 15, Loss(train/val) 4.99816/5.00042. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 4.99852/5.00069. Took 1.02 sec\n",
      "Epoch 17, Loss(train/val) 4.99803/5.00115. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 4.99766/5.00141. Took 1.02 sec\n",
      "Epoch 19, Loss(train/val) 4.99750/5.00336. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 4.99682/5.00443. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.99764/5.00610. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.99592/5.00352. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.99521/5.00577. Took 1.03 sec\n",
      "Epoch 24, Loss(train/val) 4.99537/5.00643. Took 1.02 sec\n",
      "Epoch 25, Loss(train/val) 4.99563/5.00653. Took 1.00 sec\n",
      "Epoch 26, Loss(train/val) 4.99757/5.00486. Took 1.02 sec\n",
      "Epoch 27, Loss(train/val) 4.99619/5.00440. Took 1.00 sec\n",
      "Epoch 28, Loss(train/val) 4.99359/5.00767. Took 1.00 sec\n",
      "Epoch 29, Loss(train/val) 4.99445/5.00938. Took 1.00 sec\n",
      "Epoch 30, Loss(train/val) 4.99402/5.00871. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.99288/5.01261. Took 1.00 sec\n",
      "Epoch 32, Loss(train/val) 4.99343/5.01414. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.99176/5.01195. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.99287/5.01381. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 4.99075/5.01754. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 4.99144/5.01310. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.99143/5.01848. Took 1.00 sec\n",
      "Epoch 38, Loss(train/val) 4.99119/5.01607. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.98829/5.02094. Took 1.03 sec\n",
      "Epoch 40, Loss(train/val) 4.99046/5.01901. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.98844/5.01959. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 4.98718/5.01945. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.98927/5.01933. Took 1.00 sec\n",
      "Epoch 44, Loss(train/val) 4.98933/5.01860. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.98912/5.01762. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 4.98802/5.01643. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.98811/5.02242. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 4.98805/5.01120. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.98723/5.02059. Took 1.00 sec\n",
      "Epoch 50, Loss(train/val) 4.98781/5.02013. Took 1.09 sec\n",
      "Epoch 51, Loss(train/val) 4.98673/5.01715. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 4.98315/5.02029. Took 1.00 sec\n",
      "Epoch 53, Loss(train/val) 4.98716/5.01757. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 4.98631/5.02234. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 4.98530/5.02352. Took 1.02 sec\n",
      "Epoch 56, Loss(train/val) 4.98609/5.01917. Took 1.02 sec\n",
      "Epoch 57, Loss(train/val) 4.98648/5.02011. Took 1.02 sec\n",
      "Epoch 58, Loss(train/val) 4.98699/5.01855. Took 1.00 sec\n",
      "Epoch 59, Loss(train/val) 4.98468/5.01943. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.98588/5.01798. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 4.98465/5.01459. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 4.98154/5.02060. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.98429/5.01578. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 4.98284/5.01716. Took 1.00 sec\n",
      "Epoch 65, Loss(train/val) 4.98668/5.01327. Took 1.00 sec\n",
      "Epoch 66, Loss(train/val) 4.98388/5.00572. Took 1.00 sec\n",
      "Epoch 67, Loss(train/val) 4.98455/5.01703. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 4.98690/5.01293. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 4.98384/5.01937. Took 1.00 sec\n",
      "Epoch 70, Loss(train/val) 4.98348/5.00945. Took 1.00 sec\n",
      "Epoch 71, Loss(train/val) 4.98344/5.01545. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 4.98445/5.00855. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.98514/5.01258. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.98175/5.01433. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.98556/5.01631. Took 1.00 sec\n",
      "Epoch 76, Loss(train/val) 4.98452/5.01176. Took 1.02 sec\n",
      "Epoch 77, Loss(train/val) 4.98229/5.02215. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.98250/5.01186. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 4.98258/5.01494. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.98197/5.01182. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 4.98099/5.01876. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.98382/5.02090. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 4.98028/5.01540. Took 1.03 sec\n",
      "Epoch 84, Loss(train/val) 4.98410/5.01043. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 4.97954/5.01118. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 4.98112/5.01707. Took 1.02 sec\n",
      "Epoch 87, Loss(train/val) 4.98151/5.01098. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.98221/5.01410. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.98302/5.01386. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 4.98077/5.01029. Took 1.00 sec\n",
      "Epoch 91, Loss(train/val) 4.98218/5.01513. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 4.97822/5.01190. Took 1.02 sec\n",
      "Epoch 93, Loss(train/val) 4.98199/5.01154. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.98320/5.00782. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 4.98002/5.02128. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 4.98435/5.00784. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.98159/5.00531. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 4.97866/5.01134. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 4.98429/5.01423. Took 1.01 sec\n",
      "ACC: 0.53125, MCC: 0.03253000243161777\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 4.97985/4.97854. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.97054/4.97477. Took 1.04 sec\n",
      "Epoch 2, Loss(train/val) 4.96694/4.97612. Took 1.00 sec\n",
      "Epoch 3, Loss(train/val) 4.96550/4.96886. Took 1.13 sec\n",
      "Epoch 4, Loss(train/val) 4.96214/4.96537. Took 1.02 sec\n",
      "Epoch 5, Loss(train/val) 4.96122/4.96697. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.96212/4.96852. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.96155/4.96824. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 4.96086/4.96724. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.96020/4.96764. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 4.96051/4.96776. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.96028/4.96750. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 4.95953/4.96831. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.95923/4.96810. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 4.95818/4.96874. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.95874/4.97025. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 4.95905/4.97029. Took 1.02 sec\n",
      "Epoch 17, Loss(train/val) 4.95833/4.96815. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 4.95666/4.96870. Took 1.02 sec\n",
      "Epoch 19, Loss(train/val) 4.95849/4.97131. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 4.95718/4.96763. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.95694/4.97026. Took 1.03 sec\n",
      "Epoch 22, Loss(train/val) 4.95691/4.97365. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.95587/4.97269. Took 1.00 sec\n",
      "Epoch 24, Loss(train/val) 4.95524/4.97458. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 4.95448/4.97735. Took 1.03 sec\n",
      "Epoch 26, Loss(train/val) 4.95454/4.97599. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 4.95570/4.97918. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 4.95284/4.97567. Took 1.02 sec\n",
      "Epoch 29, Loss(train/val) 4.95317/4.98061. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 4.95528/4.97848. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.95178/4.98217. Took 1.00 sec\n",
      "Epoch 32, Loss(train/val) 4.95378/4.97753. Took 1.02 sec\n",
      "Epoch 33, Loss(train/val) 4.95131/4.98153. Took 1.02 sec\n",
      "Epoch 34, Loss(train/val) 4.95421/4.98209. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 4.94794/4.99152. Took 1.00 sec\n",
      "Epoch 36, Loss(train/val) 4.95124/4.98342. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.95268/4.97788. Took 1.02 sec\n",
      "Epoch 38, Loss(train/val) 4.95023/4.98162. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.95627/4.97258. Took 1.02 sec\n",
      "Epoch 40, Loss(train/val) 4.95342/4.97857. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.96085/4.96376. Took 1.02 sec\n",
      "Epoch 42, Loss(train/val) 4.95941/4.96323. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.95824/4.96279. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.95707/4.96314. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.95711/4.96504. Took 1.02 sec\n",
      "Epoch 46, Loss(train/val) 4.95578/4.96507. Took 1.02 sec\n",
      "Epoch 47, Loss(train/val) 4.95597/4.96499. Took 1.02 sec\n",
      "Epoch 48, Loss(train/val) 4.95583/4.96467. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.95570/4.97016. Took 1.02 sec\n",
      "Epoch 50, Loss(train/val) 4.95508/4.96964. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.95425/4.96785. Took 1.02 sec\n",
      "Epoch 52, Loss(train/val) 4.95452/4.97206. Took 1.00 sec\n",
      "Epoch 53, Loss(train/val) 4.95616/4.97273. Took 1.00 sec\n",
      "Epoch 54, Loss(train/val) 4.95599/4.96824. Took 1.02 sec\n",
      "Epoch 55, Loss(train/val) 4.95479/4.97162. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 4.95370/4.97616. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 4.95348/4.97449. Took 1.03 sec\n",
      "Epoch 58, Loss(train/val) 4.95160/4.97670. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.95272/4.96855. Took 1.02 sec\n",
      "Epoch 60, Loss(train/val) 4.94954/4.96671. Took 1.00 sec\n",
      "Epoch 61, Loss(train/val) 4.95303/4.97038. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 4.94967/4.97014. Took 1.02 sec\n",
      "Epoch 63, Loss(train/val) 4.95095/4.97416. Took 1.09 sec\n",
      "Epoch 64, Loss(train/val) 4.94891/4.96893. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 4.95210/4.97542. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 4.95074/4.97348. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 4.95014/4.97608. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 4.94836/4.98136. Took 1.03 sec\n",
      "Epoch 69, Loss(train/val) 4.94904/4.98598. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 4.94977/4.97986. Took 1.02 sec\n",
      "Epoch 71, Loss(train/val) 4.94760/4.97842. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 4.94740/4.98953. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.94631/4.98329. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.94606/4.98716. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.95002/4.98070. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 4.94630/4.98456. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 4.94667/4.99367. Took 1.02 sec\n",
      "Epoch 78, Loss(train/val) 4.94666/4.98616. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 4.94591/4.98973. Took 1.02 sec\n",
      "Epoch 80, Loss(train/val) 4.94477/4.99166. Took 1.00 sec\n",
      "Epoch 81, Loss(train/val) 4.94616/4.98396. Took 1.00 sec\n",
      "Epoch 82, Loss(train/val) 4.94428/4.98582. Took 1.03 sec\n",
      "Epoch 83, Loss(train/val) 4.94526/4.98717. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.94515/4.98413. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 4.94436/4.99155. Took 1.00 sec\n",
      "Epoch 86, Loss(train/val) 4.94320/4.99062. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 4.94462/4.99000. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.94292/5.00219. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.94652/4.98469. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 4.94101/4.99743. Took 1.02 sec\n",
      "Epoch 91, Loss(train/val) 4.94523/5.00082. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 4.94400/4.98972. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 4.94379/4.98940. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.94366/4.98733. Took 1.02 sec\n",
      "Epoch 95, Loss(train/val) 4.94453/4.99069. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 4.94351/4.99120. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.94191/4.98555. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 4.93879/5.00068. Took 1.00 sec\n",
      "Epoch 99, Loss(train/val) 4.94304/4.99230. Took 1.02 sec\n",
      "ACC: 0.5234375, MCC: 0.0469265834355506\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.02130/4.99741. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.98745/4.98432. Took 1.02 sec\n",
      "Epoch 2, Loss(train/val) 4.98565/4.98466. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 4.98607/4.98502. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 4.98582/4.98519. Took 1.02 sec\n",
      "Epoch 5, Loss(train/val) 4.98569/4.98588. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.98635/4.98567. Took 1.02 sec\n",
      "Epoch 7, Loss(train/val) 4.98559/4.98526. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 4.98584/4.98449. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.98509/4.98460. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 4.98575/4.98446. Took 1.03 sec\n",
      "Epoch 11, Loss(train/val) 4.98523/4.98471. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 4.98390/4.98514. Took 1.00 sec\n",
      "Epoch 13, Loss(train/val) 4.98488/4.98474. Took 1.00 sec\n",
      "Epoch 14, Loss(train/val) 4.98423/4.98522. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.98429/4.98520. Took 1.02 sec\n",
      "Epoch 16, Loss(train/val) 4.98398/4.98527. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 4.98390/4.98489. Took 1.11 sec\n",
      "Epoch 18, Loss(train/val) 4.98396/4.98459. Took 1.02 sec\n",
      "Epoch 19, Loss(train/val) 4.98457/4.98532. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 4.98413/4.98587. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.98363/4.98560. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.98213/4.98545. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.98352/4.98519. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 4.98317/4.98576. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 4.98365/4.98683. Took 1.02 sec\n",
      "Epoch 26, Loss(train/val) 4.98355/4.98712. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 4.98273/4.98806. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 4.98294/4.98787. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.98293/4.98777. Took 1.02 sec\n",
      "Epoch 30, Loss(train/val) 4.98283/4.98981. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.98258/4.98888. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 4.98171/4.98928. Took 1.02 sec\n",
      "Epoch 33, Loss(train/val) 4.98276/4.98917. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.98103/4.98864. Took 1.02 sec\n",
      "Epoch 35, Loss(train/val) 4.98082/4.98998. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 4.98183/4.98886. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.98114/4.98935. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 4.98201/4.99037. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.98061/4.99084. Took 1.04 sec\n",
      "Epoch 40, Loss(train/val) 4.98124/4.99263. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.98026/4.98996. Took 1.02 sec\n",
      "Epoch 42, Loss(train/val) 4.98134/4.98840. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.98024/4.98705. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.98119/4.98604. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.98117/4.98967. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 4.97898/4.99095. Took 1.02 sec\n",
      "Epoch 47, Loss(train/val) 4.97953/4.99090. Took 1.02 sec\n",
      "Epoch 48, Loss(train/val) 4.97794/4.99060. Took 1.02 sec\n",
      "Epoch 49, Loss(train/val) 4.97843/4.98908. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 4.98002/4.98523. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.97880/4.98681. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 4.97833/4.99288. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 4.97872/4.99056. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 4.98042/4.99153. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 4.97645/4.99964. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 4.97826/5.00075. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 4.97848/4.99713. Took 1.02 sec\n",
      "Epoch 58, Loss(train/val) 4.97665/5.00128. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.97991/4.99317. Took 1.03 sec\n",
      "Epoch 60, Loss(train/val) 4.97705/4.99448. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 4.97675/4.99719. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 4.98027/4.98994. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.97547/5.00597. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 4.97711/4.99560. Took 1.03 sec\n",
      "Epoch 65, Loss(train/val) 4.97684/4.99587. Took 1.02 sec\n",
      "Epoch 66, Loss(train/val) 4.97479/4.99631. Took 1.02 sec\n",
      "Epoch 67, Loss(train/val) 4.97674/4.99153. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 4.97388/5.01516. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 4.97705/4.99316. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 4.97593/5.00055. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 4.97404/4.99948. Took 1.04 sec\n",
      "Epoch 72, Loss(train/val) 4.97655/4.98846. Took 1.02 sec\n",
      "Epoch 73, Loss(train/val) 4.97328/4.99889. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.97257/4.99303. Took 1.04 sec\n",
      "Epoch 75, Loss(train/val) 4.97493/4.99679. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 4.97602/5.00335. Took 1.02 sec\n",
      "Epoch 77, Loss(train/val) 4.97496/5.00395. Took 1.09 sec\n",
      "Epoch 78, Loss(train/val) 4.97585/4.99158. Took 1.03 sec\n",
      "Epoch 79, Loss(train/val) 4.97268/5.00878. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.97324/4.99712. Took 1.02 sec\n",
      "Epoch 81, Loss(train/val) 4.97384/4.99587. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.97457/4.99228. Took 1.02 sec\n",
      "Epoch 83, Loss(train/val) 4.97529/4.99076. Took 1.02 sec\n",
      "Epoch 84, Loss(train/val) 4.97146/4.99485. Took 1.00 sec\n",
      "Epoch 85, Loss(train/val) 4.97711/4.98621. Took 1.02 sec\n",
      "Epoch 86, Loss(train/val) 4.97502/4.99027. Took 1.02 sec\n",
      "Epoch 87, Loss(train/val) 4.97452/4.99341. Took 1.02 sec\n",
      "Epoch 88, Loss(train/val) 4.97183/4.99163. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.97295/4.99268. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 4.97370/5.00594. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 4.97544/4.99457. Took 1.02 sec\n",
      "Epoch 92, Loss(train/val) 4.97555/5.00293. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 4.97262/5.00456. Took 1.00 sec\n",
      "Epoch 94, Loss(train/val) 4.97390/5.00134. Took 1.02 sec\n",
      "Epoch 95, Loss(train/val) 4.97354/4.99530. Took 1.00 sec\n",
      "Epoch 96, Loss(train/val) 4.97341/5.00424. Took 1.02 sec\n",
      "Epoch 97, Loss(train/val) 4.97366/4.99742. Took 1.02 sec\n",
      "Epoch 98, Loss(train/val) 4.97034/5.00190. Took 1.03 sec\n",
      "Epoch 99, Loss(train/val) 4.97157/5.00258. Took 1.03 sec\n",
      "ACC: 0.5078125, MCC: 0.017117010167502247\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.02538/4.97974. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.97389/4.96084. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 4.96353/4.96002. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 4.95943/4.96084. Took 1.00 sec\n",
      "Epoch 4, Loss(train/val) 4.95934/4.96128. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 4.96055/4.96127. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.96083/4.96163. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.95986/4.96169. Took 1.02 sec\n",
      "Epoch 8, Loss(train/val) 4.95919/4.96111. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.95882/4.96285. Took 1.00 sec\n",
      "Epoch 10, Loss(train/val) 4.95913/4.96331. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.95762/4.96278. Took 1.02 sec\n",
      "Epoch 12, Loss(train/val) 4.95764/4.96426. Took 1.00 sec\n",
      "Epoch 13, Loss(train/val) 4.95721/4.96520. Took 1.02 sec\n",
      "Epoch 14, Loss(train/val) 4.95623/4.96576. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.95617/4.96612. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 4.95640/4.96560. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 4.95619/4.96649. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 4.95244/4.96927. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 4.95267/4.96830. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 4.95234/4.96752. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.95167/4.96714. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.95192/4.96804. Took 1.02 sec\n",
      "Epoch 23, Loss(train/val) 4.95056/4.96702. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 4.95159/4.96747. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 4.95055/4.96683. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 4.95101/4.96599. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 4.94862/4.96791. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 4.94790/4.96837. Took 1.00 sec\n",
      "Epoch 29, Loss(train/val) 4.94838/4.97001. Took 1.00 sec\n",
      "Epoch 30, Loss(train/val) 4.94739/4.96957. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.94858/4.97043. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 4.94691/4.97112. Took 1.00 sec\n",
      "Epoch 33, Loss(train/val) 4.94684/4.97022. Took 1.10 sec\n",
      "Epoch 34, Loss(train/val) 4.94596/4.97193. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 4.94764/4.97171. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 4.94768/4.97164. Took 1.02 sec\n",
      "Epoch 37, Loss(train/val) 4.94585/4.96933. Took 1.00 sec\n",
      "Epoch 38, Loss(train/val) 4.94365/4.97065. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.94605/4.96857. Took 1.00 sec\n",
      "Epoch 40, Loss(train/val) 4.94499/4.97184. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.94503/4.96540. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 4.94295/4.97234. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.94428/4.97018. Took 1.02 sec\n",
      "Epoch 44, Loss(train/val) 4.94497/4.96419. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.94310/4.96658. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 4.94334/4.96795. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.94271/4.96873. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 4.94039/4.97218. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.94227/4.96991. Took 1.02 sec\n",
      "Epoch 50, Loss(train/val) 4.94069/4.97268. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.94254/4.97084. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 4.94145/4.97143. Took 1.02 sec\n",
      "Epoch 53, Loss(train/val) 4.93979/4.97170. Took 1.02 sec\n",
      "Epoch 54, Loss(train/val) 4.94212/4.97064. Took 1.02 sec\n",
      "Epoch 55, Loss(train/val) 4.94294/4.96767. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 4.93906/4.97580. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 4.93946/4.97121. Took 1.02 sec\n",
      "Epoch 58, Loss(train/val) 4.93819/4.95782. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.95519/4.95421. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.95198/4.95509. Took 1.02 sec\n",
      "Epoch 61, Loss(train/val) 4.94631/4.95737. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 4.94377/4.95864. Took 1.00 sec\n",
      "Epoch 63, Loss(train/val) 4.94462/4.95683. Took 1.00 sec\n",
      "Epoch 64, Loss(train/val) 4.93995/4.96358. Took 1.00 sec\n",
      "Epoch 65, Loss(train/val) 4.94138/4.96202. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 4.94095/4.96837. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 4.94507/4.95450. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 4.93980/4.96252. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 4.94113/4.96482. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 4.93950/4.96772. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 4.93876/4.96763. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 4.93576/4.96411. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.93753/4.96225. Took 1.03 sec\n",
      "Epoch 74, Loss(train/val) 4.93811/4.96710. Took 1.02 sec\n",
      "Epoch 75, Loss(train/val) 4.93482/4.96632. Took 1.02 sec\n",
      "Epoch 76, Loss(train/val) 4.93764/4.97039. Took 1.02 sec\n",
      "Epoch 77, Loss(train/val) 4.93903/4.96416. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.93597/4.96437. Took 1.02 sec\n",
      "Epoch 79, Loss(train/val) 4.93832/4.96949. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.94156/4.96286. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 4.93313/4.97104. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.93830/4.96323. Took 1.02 sec\n",
      "Epoch 83, Loss(train/val) 4.93547/4.97004. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.93576/4.97649. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 4.93112/4.98242. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 4.93716/4.96696. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 4.93550/4.97834. Took 1.00 sec\n",
      "Epoch 88, Loss(train/val) 4.93526/4.97152. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.93548/4.97594. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 4.93583/4.98187. Took 1.00 sec\n",
      "Epoch 91, Loss(train/val) 4.93387/4.98053. Took 1.00 sec\n",
      "Epoch 92, Loss(train/val) 4.93341/4.98403. Took 1.00 sec\n",
      "Epoch 93, Loss(train/val) 4.93394/4.97110. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.93417/4.98267. Took 1.10 sec\n",
      "Epoch 95, Loss(train/val) 4.93603/4.97547. Took 1.02 sec\n",
      "Epoch 96, Loss(train/val) 4.93250/4.98027. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.93795/4.97990. Took 0.99 sec\n",
      "Epoch 98, Loss(train/val) 4.93099/4.97709. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 4.93463/4.97507. Took 1.01 sec\n",
      "ACC: 0.640625, MCC: 0.2711108394996237\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.00715/4.98192. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.98280/4.98059. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 4.98522/4.97848. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 4.98210/4.97800. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 4.98251/4.97974. Took 1.02 sec\n",
      "Epoch 5, Loss(train/val) 4.98085/4.97935. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.97964/4.97947. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.97767/4.97992. Took 1.02 sec\n",
      "Epoch 8, Loss(train/val) 4.97832/4.97948. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.97816/4.97820. Took 1.02 sec\n",
      "Epoch 10, Loss(train/val) 4.97680/4.97925. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.97775/4.97810. Took 1.02 sec\n",
      "Epoch 12, Loss(train/val) 4.97738/4.97643. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.97655/4.97657. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 4.97622/4.97640. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.97546/4.97511. Took 1.00 sec\n",
      "Epoch 16, Loss(train/val) 4.97454/4.97443. Took 1.02 sec\n",
      "Epoch 17, Loss(train/val) 4.97336/4.97423. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 4.97483/4.97124. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 4.97260/4.97242. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 4.97275/4.97281. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.97272/4.97235. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.97239/4.97021. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.97190/4.96984. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 4.97161/4.97074. Took 1.02 sec\n",
      "Epoch 25, Loss(train/val) 4.97014/4.97257. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 4.97042/4.97383. Took 1.02 sec\n",
      "Epoch 27, Loss(train/val) 4.96806/4.97694. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 4.97098/4.97577. Took 1.00 sec\n",
      "Epoch 29, Loss(train/val) 4.96764/4.98019. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 4.96846/4.97747. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.96756/4.97474. Took 1.00 sec\n",
      "Epoch 32, Loss(train/val) 4.96848/4.97738. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.96580/4.97968. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.96724/4.97693. Took 1.02 sec\n",
      "Epoch 35, Loss(train/val) 4.96463/4.97934. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 4.96533/4.97968. Took 1.02 sec\n",
      "Epoch 37, Loss(train/val) 4.96574/4.97654. Took 1.03 sec\n",
      "Epoch 38, Loss(train/val) 4.96678/4.98002. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.96447/4.98280. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 4.96565/4.97663. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.96519/4.97988. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 4.96399/4.98272. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.96494/4.97624. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.96298/4.98003. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.96285/4.98387. Took 1.00 sec\n",
      "Epoch 46, Loss(train/val) 4.96486/4.98247. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.96177/4.98081. Took 1.00 sec\n",
      "Epoch 48, Loss(train/val) 4.96103/4.98978. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.96374/4.98401. Took 1.03 sec\n",
      "Epoch 50, Loss(train/val) 4.96104/4.98394. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.96086/4.98942. Took 1.10 sec\n",
      "Epoch 52, Loss(train/val) 4.96059/4.98389. Took 1.02 sec\n",
      "Epoch 53, Loss(train/val) 4.96091/4.98368. Took 1.03 sec\n",
      "Epoch 54, Loss(train/val) 4.96214/4.98838. Took 1.02 sec\n",
      "Epoch 55, Loss(train/val) 4.96155/4.98576. Took 1.02 sec\n",
      "Epoch 56, Loss(train/val) 4.96120/4.98745. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 4.95974/4.99290. Took 1.03 sec\n",
      "Epoch 58, Loss(train/val) 4.96048/4.98403. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.95647/4.98993. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.95965/4.98726. Took 1.04 sec\n",
      "Epoch 61, Loss(train/val) 4.95757/4.98885. Took 1.02 sec\n",
      "Epoch 62, Loss(train/val) 4.95806/4.99613. Took 1.03 sec\n",
      "Epoch 63, Loss(train/val) 4.96257/4.98227. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 4.95926/4.99638. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 4.95831/4.99466. Took 1.02 sec\n",
      "Epoch 66, Loss(train/val) 4.95848/4.98891. Took 1.03 sec\n",
      "Epoch 67, Loss(train/val) 4.96234/4.98887. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 4.95829/4.99167. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 4.95933/4.99380. Took 1.02 sec\n",
      "Epoch 70, Loss(train/val) 4.96092/4.98935. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 4.95795/4.98667. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 4.95682/4.98823. Took 1.02 sec\n",
      "Epoch 73, Loss(train/val) 4.95924/4.99243. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.95816/4.99325. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.95896/4.98823. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 4.95783/4.98865. Took 1.02 sec\n",
      "Epoch 77, Loss(train/val) 4.95831/4.99403. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.95699/4.98811. Took 1.03 sec\n",
      "Epoch 79, Loss(train/val) 4.95709/4.99356. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.95755/4.99841. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 4.95590/5.00258. Took 1.02 sec\n",
      "Epoch 82, Loss(train/val) 4.95692/4.99001. Took 1.03 sec\n",
      "Epoch 83, Loss(train/val) 4.95809/4.99387. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.95779/5.00090. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 4.95637/4.99266. Took 1.02 sec\n",
      "Epoch 86, Loss(train/val) 4.95768/4.99178. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 4.95639/4.98926. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.95697/4.99699. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.95512/4.99656. Took 1.02 sec\n",
      "Epoch 90, Loss(train/val) 4.95628/4.99456. Took 1.02 sec\n",
      "Epoch 91, Loss(train/val) 4.95616/4.99514. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 4.95737/4.99684. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 4.95485/4.99258. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.95534/4.99205. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 4.95653/4.99657. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 4.95490/4.98685. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.95418/4.99609. Took 1.02 sec\n",
      "Epoch 98, Loss(train/val) 4.95342/4.99223. Took 1.02 sec\n",
      "Epoch 99, Loss(train/val) 4.95511/4.99921. Took 1.01 sec\n",
      "ACC: 0.53125, MCC: 0.07845331985520247\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 4.88857/4.88669. Took 1.01 sec\n",
      "Epoch 1, Loss(train/val) 4.88209/4.89161. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 4.88140/4.89371. Took 1.00 sec\n",
      "Epoch 3, Loss(train/val) 4.88066/4.87228. Took 1.02 sec\n",
      "Epoch 4, Loss(train/val) 4.87167/4.87170. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 4.87017/4.87558. Took 1.00 sec\n",
      "Epoch 6, Loss(train/val) 4.87044/4.87744. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.87161/4.87659. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 4.87038/4.87677. Took 1.09 sec\n",
      "Epoch 9, Loss(train/val) 4.87081/4.87570. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 4.86896/4.87574. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.86929/4.87623. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 4.86922/4.87672. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.86941/4.87682. Took 1.03 sec\n",
      "Epoch 14, Loss(train/val) 4.86984/4.87510. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.86697/4.87636. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 4.86790/4.87627. Took 1.02 sec\n",
      "Epoch 17, Loss(train/val) 4.86791/4.87723. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 4.86766/4.87611. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 4.86766/4.87556. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 4.86663/4.87653. Took 1.02 sec\n",
      "Epoch 21, Loss(train/val) 4.86663/4.87751. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.86662/4.87794. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.86566/4.87824. Took 1.04 sec\n",
      "Epoch 24, Loss(train/val) 4.86559/4.87789. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 4.86467/4.87747. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 4.86415/4.87634. Took 1.02 sec\n",
      "Epoch 27, Loss(train/val) 4.86346/4.87857. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 4.86411/4.87798. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.86287/4.87899. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 4.86474/4.87575. Took 1.02 sec\n",
      "Epoch 31, Loss(train/val) 4.86314/4.87836. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 4.86182/4.87632. Took 1.02 sec\n",
      "Epoch 33, Loss(train/val) 4.86128/4.87587. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.86111/4.87142. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 4.86064/4.87442. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 4.86041/4.87436. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.86033/4.88038. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 4.86216/4.87293. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.85974/4.87216. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 4.86021/4.87348. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.85976/4.88085. Took 1.02 sec\n",
      "Epoch 42, Loss(train/val) 4.85994/4.87414. Took 1.02 sec\n",
      "Epoch 43, Loss(train/val) 4.86177/4.87356. Took 1.02 sec\n",
      "Epoch 44, Loss(train/val) 4.86047/4.87126. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.85710/4.87419. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 4.86016/4.87283. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.85991/4.87338. Took 1.00 sec\n",
      "Epoch 48, Loss(train/val) 4.85837/4.87353. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.85942/4.87264. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 4.85731/4.87116. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.85817/4.86930. Took 1.02 sec\n",
      "Epoch 52, Loss(train/val) 4.85699/4.87354. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 4.85853/4.86771. Took 1.02 sec\n",
      "Epoch 54, Loss(train/val) 4.85642/4.87548. Took 1.02 sec\n",
      "Epoch 55, Loss(train/val) 4.85896/4.86533. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 4.85731/4.86917. Took 1.00 sec\n",
      "Epoch 57, Loss(train/val) 4.85779/4.86994. Took 1.02 sec\n",
      "Epoch 58, Loss(train/val) 4.85915/4.86775. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.85588/4.86461. Took 1.00 sec\n",
      "Epoch 60, Loss(train/val) 4.85782/4.87213. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 4.85939/4.86624. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 4.85549/4.86423. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.85866/4.86310. Took 1.00 sec\n",
      "Epoch 64, Loss(train/val) 4.85467/4.86780. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 4.85705/4.86044. Took 1.05 sec\n",
      "Epoch 66, Loss(train/val) 4.85635/4.86665. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 4.85699/4.86201. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 4.85679/4.86173. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 4.85492/4.86921. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 4.85437/4.86742. Took 1.10 sec\n",
      "Epoch 71, Loss(train/val) 4.85806/4.87022. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 4.86449/4.86408. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.86081/4.86515. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.85753/4.86197. Took 1.02 sec\n",
      "Epoch 75, Loss(train/val) 4.85673/4.86908. Took 1.02 sec\n",
      "Epoch 76, Loss(train/val) 4.85727/4.87192. Took 1.02 sec\n",
      "Epoch 77, Loss(train/val) 4.85875/4.86507. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.85753/4.86979. Took 1.03 sec\n",
      "Epoch 79, Loss(train/val) 4.85611/4.86978. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.85660/4.86466. Took 1.00 sec\n",
      "Epoch 81, Loss(train/val) 4.85685/4.86621. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.85552/4.86645. Took 1.02 sec\n",
      "Epoch 83, Loss(train/val) 4.85674/4.86708. Took 1.02 sec\n",
      "Epoch 84, Loss(train/val) 4.85671/4.86819. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 4.85502/4.86706. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 4.85734/4.86656. Took 1.02 sec\n",
      "Epoch 87, Loss(train/val) 4.86335/4.86953. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.86147/4.87268. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.85839/4.87359. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 4.85744/4.87126. Took 1.00 sec\n",
      "Epoch 91, Loss(train/val) 4.85711/4.86740. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 4.85798/4.87180. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 4.85624/4.87253. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.85530/4.87152. Took 1.02 sec\n",
      "Epoch 95, Loss(train/val) 4.85358/4.87832. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 4.85506/4.87733. Took 1.02 sec\n",
      "Epoch 97, Loss(train/val) 4.85862/4.87562. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 4.85350/4.87336. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 4.85642/4.86483. Took 1.01 sec\n",
      "ACC: 0.453125, MCC: -0.0944911182523068\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.10544/5.08158. Took 1.01 sec\n",
      "Epoch 1, Loss(train/val) 5.08979/5.08389. Took 1.02 sec\n",
      "Epoch 2, Loss(train/val) 5.08559/5.08249. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 5.08567/5.08237. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 5.08458/5.08283. Took 1.00 sec\n",
      "Epoch 5, Loss(train/val) 5.08454/5.08421. Took 1.03 sec\n",
      "Epoch 6, Loss(train/val) 5.08349/5.08526. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 5.08217/5.08717. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 5.08141/5.08824. Took 1.03 sec\n",
      "Epoch 9, Loss(train/val) 5.08222/5.08929. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 5.08126/5.08758. Took 1.02 sec\n",
      "Epoch 11, Loss(train/val) 5.08091/5.08663. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 5.08103/5.08851. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 5.07904/5.08947. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 5.07871/5.09093. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 5.07939/5.09176. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 5.07928/5.09052. Took 1.00 sec\n",
      "Epoch 17, Loss(train/val) 5.07843/5.08907. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 5.07706/5.09369. Took 1.03 sec\n",
      "Epoch 19, Loss(train/val) 5.07830/5.09033. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 5.07931/5.08987. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 5.07815/5.09304. Took 1.02 sec\n",
      "Epoch 22, Loss(train/val) 5.07841/5.08877. Took 1.03 sec\n",
      "Epoch 23, Loss(train/val) 5.07822/5.09280. Took 1.00 sec\n",
      "Epoch 24, Loss(train/val) 5.07762/5.08970. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 5.07695/5.08484. Took 1.00 sec\n",
      "Epoch 26, Loss(train/val) 5.07725/5.08613. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 5.07537/5.08757. Took 1.10 sec\n",
      "Epoch 28, Loss(train/val) 5.07588/5.08642. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 5.07564/5.09446. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 5.07719/5.08258. Took 1.02 sec\n",
      "Epoch 31, Loss(train/val) 5.07477/5.08361. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 5.07284/5.08806. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 5.07401/5.07929. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 5.07438/5.07226. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 5.07407/5.08808. Took 1.02 sec\n",
      "Epoch 36, Loss(train/val) 5.07613/5.08422. Took 1.03 sec\n",
      "Epoch 37, Loss(train/val) 5.07410/5.08262. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 5.07214/5.08641. Took 1.02 sec\n",
      "Epoch 39, Loss(train/val) 5.06993/5.08005. Took 1.03 sec\n",
      "Epoch 40, Loss(train/val) 5.07337/5.08312. Took 1.03 sec\n",
      "Epoch 41, Loss(train/val) 5.07423/5.07277. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 5.07219/5.08871. Took 1.00 sec\n",
      "Epoch 43, Loss(train/val) 5.07495/5.08614. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 5.06997/5.08675. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 5.07461/5.07981. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 5.06862/5.08664. Took 1.02 sec\n",
      "Epoch 47, Loss(train/val) 5.06933/5.08500. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 5.07243/5.07501. Took 1.02 sec\n",
      "Epoch 49, Loss(train/val) 5.07087/5.07672. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 5.07061/5.07362. Took 1.00 sec\n",
      "Epoch 51, Loss(train/val) 5.06684/5.06832. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 5.07726/5.06200. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 5.07596/5.07643. Took 1.00 sec\n",
      "Epoch 54, Loss(train/val) 5.07310/5.07912. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 5.06955/5.07232. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 5.06910/5.08175. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 5.07330/5.07973. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 5.07024/5.07278. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 5.07257/5.06350. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 5.06990/5.07451. Took 1.02 sec\n",
      "Epoch 61, Loss(train/val) 5.06513/5.07433. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 5.07243/5.07018. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 5.07063/5.06710. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 5.06676/5.07041. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 5.06713/5.06965. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 5.06950/5.07364. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 5.06819/5.07471. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 5.06821/5.06474. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 5.06655/5.06849. Took 1.03 sec\n",
      "Epoch 70, Loss(train/val) 5.06798/5.06597. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 5.06702/5.06542. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 5.06464/5.07231. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 5.06571/5.07717. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 5.06676/5.08191. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 5.06625/5.07671. Took 1.00 sec\n",
      "Epoch 76, Loss(train/val) 5.06514/5.06586. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 5.06567/5.06541. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 5.06201/5.07028. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 5.06518/5.06995. Took 1.02 sec\n",
      "Epoch 80, Loss(train/val) 5.06468/5.07558. Took 1.00 sec\n",
      "Epoch 81, Loss(train/val) 5.06473/5.06684. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 5.06034/5.07795. Took 1.02 sec\n",
      "Epoch 83, Loss(train/val) 5.06409/5.05683. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 5.06579/5.06558. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 5.06572/5.08361. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 5.06479/5.06669. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 5.06444/5.07661. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 5.06358/5.07351. Took 1.00 sec\n",
      "Epoch 89, Loss(train/val) 5.06350/5.07076. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 5.06192/5.07384. Took 1.12 sec\n",
      "Epoch 91, Loss(train/val) 5.06399/5.07295. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 5.06207/5.07980. Took 1.02 sec\n",
      "Epoch 93, Loss(train/val) 5.06148/5.06579. Took 1.00 sec\n",
      "Epoch 94, Loss(train/val) 5.06179/5.07999. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 5.06242/5.06747. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 5.06050/5.06767. Took 1.02 sec\n",
      "Epoch 97, Loss(train/val) 5.06026/5.07031. Took 1.00 sec\n",
      "Epoch 98, Loss(train/val) 5.06409/5.06669. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 5.06422/5.07405. Took 1.01 sec\n",
      "ACC: 0.4375, MCC: -0.10560513668479576\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.05662/5.01760. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 5.02075/5.01573. Took 1.02 sec\n",
      "Epoch 2, Loss(train/val) 5.03585/5.02388. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 5.03868/5.04934. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 5.02733/5.03399. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 5.02236/5.02675. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 5.02295/5.03212. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 5.02192/5.03196. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 5.02299/5.03110. Took 1.02 sec\n",
      "Epoch 9, Loss(train/val) 5.02242/5.03043. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 5.02358/5.03568. Took 1.02 sec\n",
      "Epoch 11, Loss(train/val) 5.02234/5.02819. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 5.02177/5.03218. Took 1.02 sec\n",
      "Epoch 13, Loss(train/val) 5.02125/5.03095. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 5.02153/5.02919. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 5.02159/5.03221. Took 1.02 sec\n",
      "Epoch 16, Loss(train/val) 5.02135/5.02883. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 5.02014/5.03015. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 5.01996/5.03087. Took 1.04 sec\n",
      "Epoch 19, Loss(train/val) 5.02085/5.02930. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 5.02020/5.03042. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 5.01989/5.02897. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 5.01988/5.03027. Took 1.02 sec\n",
      "Epoch 23, Loss(train/val) 5.01961/5.02941. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 5.02019/5.03119. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 5.01910/5.03291. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 5.01981/5.03024. Took 1.00 sec\n",
      "Epoch 27, Loss(train/val) 5.01904/5.02972. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 5.01934/5.03157. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 5.01833/5.03250. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 5.01766/5.03324. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 5.01827/5.03391. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 5.01831/5.03734. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 5.01844/5.03439. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 5.01763/5.03686. Took 1.00 sec\n",
      "Epoch 35, Loss(train/val) 5.01714/5.03486. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 5.01895/5.03241. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 5.01710/5.03583. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 5.01695/5.03556. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 5.01517/5.04087. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 5.01697/5.04579. Took 1.00 sec\n",
      "Epoch 41, Loss(train/val) 5.01569/5.03926. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 5.01512/5.04379. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 5.01633/5.04583. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 5.01329/5.04885. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 5.01498/5.03886. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 5.01319/5.04318. Took 1.02 sec\n",
      "Epoch 47, Loss(train/val) 5.01535/5.03873. Took 1.00 sec\n",
      "Epoch 48, Loss(train/val) 5.01522/5.04595. Took 1.03 sec\n",
      "Epoch 49, Loss(train/val) 5.01228/5.04357. Took 1.10 sec\n",
      "Epoch 50, Loss(train/val) 5.01249/5.05763. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 5.01297/5.04820. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 5.01220/5.04630. Took 1.02 sec\n",
      "Epoch 53, Loss(train/val) 5.01105/5.05108. Took 1.03 sec\n",
      "Epoch 54, Loss(train/val) 5.01198/5.04320. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 5.01317/5.04409. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 5.01451/5.04375. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 5.01108/5.05039. Took 1.02 sec\n",
      "Epoch 58, Loss(train/val) 5.01225/5.04471. Took 1.02 sec\n",
      "Epoch 59, Loss(train/val) 5.01028/5.05478. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 5.01071/5.04881. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 5.01336/5.05265. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 5.00934/5.04347. Took 1.02 sec\n",
      "Epoch 63, Loss(train/val) 5.01212/5.04983. Took 1.04 sec\n",
      "Epoch 64, Loss(train/val) 5.00940/5.05683. Took 1.02 sec\n",
      "Epoch 65, Loss(train/val) 5.01904/5.03615. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 5.01913/5.03122. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 5.01912/5.02939. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 5.01823/5.02926. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 5.01799/5.02964. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 5.01813/5.03129. Took 1.02 sec\n",
      "Epoch 71, Loss(train/val) 5.01783/5.03192. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 5.01720/5.03579. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 5.01825/5.03535. Took 1.02 sec\n",
      "Epoch 74, Loss(train/val) 5.01844/5.03470. Took 1.02 sec\n",
      "Epoch 75, Loss(train/val) 5.01670/5.03450. Took 1.02 sec\n",
      "Epoch 76, Loss(train/val) 5.01656/5.03543. Took 1.02 sec\n",
      "Epoch 77, Loss(train/val) 5.01694/5.03795. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 5.01701/5.03501. Took 1.02 sec\n",
      "Epoch 79, Loss(train/val) 5.01802/5.03618. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 5.01680/5.03428. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 5.01530/5.03639. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 5.01570/5.03544. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 5.01775/5.03785. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 5.01618/5.03626. Took 1.00 sec\n",
      "Epoch 85, Loss(train/val) 5.01630/5.03342. Took 1.00 sec\n",
      "Epoch 86, Loss(train/val) 5.01539/5.03808. Took 1.02 sec\n",
      "Epoch 87, Loss(train/val) 5.01542/5.03790. Took 1.03 sec\n",
      "Epoch 88, Loss(train/val) 5.01597/5.04066. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 5.01574/5.03686. Took 1.02 sec\n",
      "Epoch 90, Loss(train/val) 5.01587/5.03867. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 5.01449/5.03291. Took 1.02 sec\n",
      "Epoch 92, Loss(train/val) 5.01775/5.03468. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 5.01541/5.03207. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 5.01605/5.03704. Took 1.02 sec\n",
      "Epoch 95, Loss(train/val) 5.01546/5.03496. Took 1.03 sec\n",
      "Epoch 96, Loss(train/val) 5.01426/5.03995. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 5.01348/5.03881. Took 1.02 sec\n",
      "Epoch 98, Loss(train/val) 5.01460/5.03874. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 5.01351/5.03483. Took 1.02 sec\n",
      "ACC: 0.5234375, MCC: 0.04795115105550837\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.09825/5.12472. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 5.08186/5.10439. Took 1.00 sec\n",
      "Epoch 2, Loss(train/val) 5.07994/5.08724. Took 1.02 sec\n",
      "Epoch 3, Loss(train/val) 5.07745/5.08097. Took 1.00 sec\n",
      "Epoch 4, Loss(train/val) 5.07416/5.08100. Took 1.02 sec\n",
      "Epoch 5, Loss(train/val) 5.07478/5.08605. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 5.07446/5.08695. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 5.07396/5.08498. Took 1.00 sec\n",
      "Epoch 8, Loss(train/val) 5.07398/5.08762. Took 1.02 sec\n",
      "Epoch 9, Loss(train/val) 5.07428/5.08778. Took 1.10 sec\n",
      "Epoch 10, Loss(train/val) 5.07511/5.08393. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 5.07321/5.08304. Took 1.02 sec\n",
      "Epoch 12, Loss(train/val) 5.07346/5.08372. Took 1.00 sec\n",
      "Epoch 13, Loss(train/val) 5.07310/5.08296. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 5.07259/5.08546. Took 1.04 sec\n",
      "Epoch 15, Loss(train/val) 5.07221/5.08433. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 5.07208/5.08597. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 5.07191/5.08944. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 5.07376/5.08385. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 5.07271/5.08055. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 5.07160/5.08586. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 5.07132/5.08822. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 5.07130/5.08811. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 5.07104/5.08877. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 5.07087/5.08723. Took 1.02 sec\n",
      "Epoch 25, Loss(train/val) 5.06978/5.09028. Took 1.02 sec\n",
      "Epoch 26, Loss(train/val) 5.06968/5.09053. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 5.06940/5.09346. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 5.06876/5.09289. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 5.06871/5.09932. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 5.06883/5.09691. Took 1.03 sec\n",
      "Epoch 31, Loss(train/val) 5.06868/5.09561. Took 1.03 sec\n",
      "Epoch 32, Loss(train/val) 5.06977/5.09236. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 5.06877/5.09316. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 5.06778/5.09408. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 5.06680/5.10375. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 5.06744/5.09948. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 5.07002/5.08662. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 5.06744/5.09840. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 5.06525/5.09668. Took 1.02 sec\n",
      "Epoch 40, Loss(train/val) 5.06676/5.10236. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 5.06738/5.10112. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 5.06602/5.09650. Took 1.02 sec\n",
      "Epoch 43, Loss(train/val) 5.06749/5.08725. Took 1.02 sec\n",
      "Epoch 44, Loss(train/val) 5.07331/5.08309. Took 1.03 sec\n",
      "Epoch 45, Loss(train/val) 5.07089/5.08560. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 5.06903/5.08050. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 5.06867/5.08725. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 5.06822/5.08428. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 5.06864/5.09475. Took 1.02 sec\n",
      "Epoch 50, Loss(train/val) 5.06761/5.08825. Took 1.02 sec\n",
      "Epoch 51, Loss(train/val) 5.06714/5.09384. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 5.06712/5.09134. Took 1.02 sec\n",
      "Epoch 53, Loss(train/val) 5.06587/5.10614. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 5.06597/5.11279. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 5.06637/5.09965. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 5.06692/5.09634. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 5.06420/5.10345. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 5.06538/5.09250. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 5.06692/5.10422. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 5.06439/5.09920. Took 1.02 sec\n",
      "Epoch 61, Loss(train/val) 5.06498/5.10839. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 5.06268/5.10050. Took 1.00 sec\n",
      "Epoch 63, Loss(train/val) 5.06528/5.09674. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 5.06322/5.10175. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 5.06316/5.09873. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 5.06353/5.10416. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 5.06522/5.10522. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 5.06421/5.09144. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 5.06486/5.11000. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 5.06356/5.10510. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 5.06324/5.09944. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 5.06297/5.11225. Took 1.02 sec\n",
      "Epoch 73, Loss(train/val) 5.06149/5.10254. Took 1.02 sec\n",
      "Epoch 74, Loss(train/val) 5.06398/5.10321. Took 1.10 sec\n",
      "Epoch 75, Loss(train/val) 5.06439/5.10707. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 5.06178/5.10360. Took 1.02 sec\n",
      "Epoch 77, Loss(train/val) 5.06257/5.10565. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 5.06161/5.09889. Took 1.02 sec\n",
      "Epoch 79, Loss(train/val) 5.06014/5.10882. Took 1.02 sec\n",
      "Epoch 80, Loss(train/val) 5.06127/5.11532. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 5.06181/5.10914. Took 1.02 sec\n",
      "Epoch 82, Loss(train/val) 5.06251/5.10646. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 5.05762/5.11978. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 5.06076/5.11804. Took 1.00 sec\n",
      "Epoch 85, Loss(train/val) 5.06296/5.10216. Took 1.00 sec\n",
      "Epoch 86, Loss(train/val) 5.05779/5.11411. Took 1.03 sec\n",
      "Epoch 87, Loss(train/val) 5.06126/5.12698. Took 1.02 sec\n",
      "Epoch 88, Loss(train/val) 5.06191/5.10787. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 5.06098/5.11778. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 5.06012/5.10991. Took 1.00 sec\n",
      "Epoch 91, Loss(train/val) 5.06124/5.12255. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 5.05985/5.11253. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 5.06082/5.11454. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 5.05736/5.12647. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 5.05969/5.12053. Took 1.00 sec\n",
      "Epoch 96, Loss(train/val) 5.05836/5.11478. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 5.05987/5.10817. Took 1.02 sec\n",
      "Epoch 98, Loss(train/val) 5.05682/5.12215. Took 1.00 sec\n",
      "Epoch 99, Loss(train/val) 5.05936/5.11927. Took 1.01 sec\n",
      "ACC: 0.5234375, MCC: 0.06901349345708524\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 4.97462/4.96165. Took 1.01 sec\n",
      "Epoch 1, Loss(train/val) 4.96263/4.95980. Took 1.00 sec\n",
      "Epoch 2, Loss(train/val) 4.96258/4.95955. Took 1.02 sec\n",
      "Epoch 3, Loss(train/val) 4.96235/4.96030. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 4.96141/4.96051. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 4.96063/4.96304. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.96065/4.96536. Took 1.02 sec\n",
      "Epoch 7, Loss(train/val) 4.96004/4.96570. Took 1.00 sec\n",
      "Epoch 8, Loss(train/val) 4.95967/4.96687. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.96251/4.96654. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 4.96251/4.96823. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.95937/4.97137. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 4.95745/4.96803. Took 1.02 sec\n",
      "Epoch 13, Loss(train/val) 4.95866/4.96707. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 4.95740/4.96881. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.95896/4.96955. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 4.95824/4.96829. Took 1.02 sec\n",
      "Epoch 17, Loss(train/val) 4.95702/4.96867. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 4.95665/4.97293. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 4.95622/4.97372. Took 1.00 sec\n",
      "Epoch 20, Loss(train/val) 4.95683/4.97094. Took 1.02 sec\n",
      "Epoch 21, Loss(train/val) 4.95628/4.96963. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.95601/4.97153. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.95624/4.97687. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 4.95516/4.97110. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 4.95824/4.96694. Took 1.02 sec\n",
      "Epoch 26, Loss(train/val) 4.95734/4.96922. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 4.95521/4.97570. Took 1.02 sec\n",
      "Epoch 28, Loss(train/val) 4.95576/4.96880. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.95624/4.96999. Took 1.00 sec\n",
      "Epoch 30, Loss(train/val) 4.95606/4.97634. Took 1.00 sec\n",
      "Epoch 31, Loss(train/val) 4.95417/4.97321. Took 1.00 sec\n",
      "Epoch 32, Loss(train/val) 4.95417/4.97050. Took 1.00 sec\n",
      "Epoch 33, Loss(train/val) 4.95566/4.96900. Took 1.00 sec\n",
      "Epoch 34, Loss(train/val) 4.95534/4.96860. Took 1.14 sec\n",
      "Epoch 35, Loss(train/val) 4.95431/4.96736. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 4.95439/4.96794. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.95132/4.97363. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 4.95508/4.96473. Took 1.04 sec\n",
      "Epoch 39, Loss(train/val) 4.95446/4.96120. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 4.95372/4.96999. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.95366/4.97372. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 4.95360/4.97368. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.95604/4.96281. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.95430/4.97351. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.94687/4.98420. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 4.95300/4.96845. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.95265/4.97192. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 4.95238/4.96959. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.95322/4.96323. Took 1.02 sec\n",
      "Epoch 50, Loss(train/val) 4.94998/4.97342. Took 1.00 sec\n",
      "Epoch 51, Loss(train/val) 4.95657/4.95930. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 4.95549/4.96185. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 4.95392/4.95930. Took 1.02 sec\n",
      "Epoch 54, Loss(train/val) 4.95436/4.97033. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 4.94986/4.97544. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 4.95241/4.97096. Took 1.00 sec\n",
      "Epoch 57, Loss(train/val) 4.95077/4.97328. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 4.94876/4.96918. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.94808/4.96948. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.94912/4.97754. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 4.94674/4.97257. Took 1.00 sec\n",
      "Epoch 62, Loss(train/val) 4.94391/4.97982. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.95022/4.96536. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 4.94990/4.96696. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 4.94748/4.97665. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 4.94677/4.96915. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 4.94554/4.97029. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 4.94900/4.96838. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 4.94699/4.97578. Took 1.00 sec\n",
      "Epoch 70, Loss(train/val) 4.94733/4.96953. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 4.95850/4.97433. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 4.95958/4.96620. Took 1.00 sec\n",
      "Epoch 73, Loss(train/val) 4.95551/4.96786. Took 1.00 sec\n",
      "Epoch 74, Loss(train/val) 4.95066/4.97068. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.95025/4.96705. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 4.94755/4.97177. Took 1.00 sec\n",
      "Epoch 77, Loss(train/val) 4.94709/4.96886. Took 1.03 sec\n",
      "Epoch 78, Loss(train/val) 4.94679/4.97321. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 4.94506/4.97301. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.94333/4.97938. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 4.94958/4.97395. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.94425/4.97537. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 4.94798/4.98209. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.94759/4.97617. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 4.94378/4.97902. Took 1.02 sec\n",
      "Epoch 86, Loss(train/val) 4.94478/4.97984. Took 1.02 sec\n",
      "Epoch 87, Loss(train/val) 4.94744/4.97989. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.94835/4.97504. Took 1.03 sec\n",
      "Epoch 89, Loss(train/val) 4.94579/4.97395. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 4.94293/4.98436. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 4.94368/4.97634. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 4.94582/4.97383. Took 1.02 sec\n",
      "Epoch 93, Loss(train/val) 4.94259/4.97170. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.94209/4.98120. Took 1.02 sec\n",
      "Epoch 95, Loss(train/val) 4.94022/4.98421. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 4.94379/4.97653. Took 1.03 sec\n",
      "Epoch 97, Loss(train/val) 4.94334/4.97805. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 4.93952/4.98757. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 4.94404/4.97707. Took 1.01 sec\n",
      "ACC: 0.4296875, MCC: -0.14905929271921117\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.01074/5.04028. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 5.00832/5.00746. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 4.99966/5.01819. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 5.00219/5.02114. Took 1.02 sec\n",
      "Epoch 4, Loss(train/val) 5.00565/5.00742. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 5.00816/4.99954. Took 1.03 sec\n",
      "Epoch 6, Loss(train/val) 5.00082/4.99950. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.99700/5.00069. Took 1.03 sec\n",
      "Epoch 8, Loss(train/val) 4.99816/5.00128. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.99859/5.00043. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 4.99910/5.00007. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.99767/5.00047. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 4.99638/5.00180. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.99668/5.00262. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 4.99706/5.00251. Took 1.02 sec\n",
      "Epoch 15, Loss(train/val) 4.99698/5.00274. Took 1.00 sec\n",
      "Epoch 16, Loss(train/val) 4.99709/5.00237. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 4.99628/5.00115. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 4.99481/5.00269. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 4.99549/5.00374. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 4.99504/5.00393. Took 1.00 sec\n",
      "Epoch 21, Loss(train/val) 4.99491/5.00511. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.99600/5.00211. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.99369/5.00463. Took 1.00 sec\n",
      "Epoch 24, Loss(train/val) 4.99504/5.00138. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 4.99254/5.00618. Took 1.00 sec\n",
      "Epoch 26, Loss(train/val) 4.99303/5.00559. Took 1.00 sec\n",
      "Epoch 27, Loss(train/val) 4.99446/5.00354. Took 1.03 sec\n",
      "Epoch 28, Loss(train/val) 4.99098/5.00485. Took 1.03 sec\n",
      "Epoch 29, Loss(train/val) 4.99333/5.00340. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 4.99267/5.00593. Took 1.02 sec\n",
      "Epoch 31, Loss(train/val) 4.99132/5.00897. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 4.99262/5.00638. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.99022/5.00900. Took 1.03 sec\n",
      "Epoch 34, Loss(train/val) 4.99138/5.00385. Took 1.02 sec\n",
      "Epoch 35, Loss(train/val) 4.99056/5.00852. Took 1.00 sec\n",
      "Epoch 36, Loss(train/val) 4.99198/5.00894. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.99075/5.01288. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 4.98975/5.00696. Took 1.02 sec\n",
      "Epoch 39, Loss(train/val) 4.98959/5.00856. Took 1.02 sec\n",
      "Epoch 40, Loss(train/val) 4.98867/5.00766. Took 1.02 sec\n",
      "Epoch 41, Loss(train/val) 4.98850/5.00867. Took 1.02 sec\n",
      "Epoch 42, Loss(train/val) 4.98943/5.01414. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.98883/5.01100. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.98895/5.01818. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.98894/5.00908. Took 1.00 sec\n",
      "Epoch 46, Loss(train/val) 4.98760/5.01134. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.98928/5.01257. Took 1.02 sec\n",
      "Epoch 48, Loss(train/val) 4.98611/5.01723. Took 1.02 sec\n",
      "Epoch 49, Loss(train/val) 4.98601/5.00721. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 4.98628/5.01356. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.98955/5.00886. Took 1.02 sec\n",
      "Epoch 52, Loss(train/val) 4.98605/5.02024. Took 1.03 sec\n",
      "Epoch 53, Loss(train/val) 4.98584/5.00960. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 4.98535/5.00904. Took 1.02 sec\n",
      "Epoch 55, Loss(train/val) 4.98942/5.01149. Took 1.00 sec\n",
      "Epoch 56, Loss(train/val) 4.98310/5.01482. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 4.98458/5.01588. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 4.98482/5.01757. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.98651/5.01114. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.98592/5.01782. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 4.98509/5.01193. Took 1.00 sec\n",
      "Epoch 62, Loss(train/val) 4.98451/5.01466. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.98185/5.01766. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 4.98345/5.01715. Took 1.00 sec\n",
      "Epoch 65, Loss(train/val) 4.98272/5.01463. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 4.98590/5.01453. Took 1.10 sec\n",
      "Epoch 67, Loss(train/val) 4.98100/5.01438. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 4.98348/5.02134. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 4.98449/5.01860. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 4.98136/5.01865. Took 1.00 sec\n",
      "Epoch 71, Loss(train/val) 4.98338/5.01828. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 4.98305/5.01939. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.99235/5.00283. Took 1.00 sec\n",
      "Epoch 74, Loss(train/val) 4.99523/5.00309. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.99533/5.00007. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 4.99098/5.00642. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 4.99056/5.00957. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.98912/5.01184. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 4.98961/5.01173. Took 1.02 sec\n",
      "Epoch 80, Loss(train/val) 4.98890/5.01416. Took 1.02 sec\n",
      "Epoch 81, Loss(train/val) 4.98559/5.02174. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.98772/5.01125. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 4.98863/5.01465. Took 1.00 sec\n",
      "Epoch 84, Loss(train/val) 4.99107/5.02045. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 4.99010/5.02289. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 4.98651/5.03063. Took 1.03 sec\n",
      "Epoch 87, Loss(train/val) 4.98776/5.02491. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.98780/5.02903. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.98618/5.03981. Took 1.02 sec\n",
      "Epoch 90, Loss(train/val) 4.98723/5.02988. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 4.98519/5.03838. Took 1.02 sec\n",
      "Epoch 92, Loss(train/val) 4.98478/5.04874. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 4.98373/5.04056. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.98488/5.04252. Took 1.02 sec\n",
      "Epoch 95, Loss(train/val) 4.98144/5.04753. Took 1.02 sec\n",
      "Epoch 96, Loss(train/val) 4.98354/5.04665. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.98312/5.04670. Took 1.02 sec\n",
      "Epoch 98, Loss(train/val) 4.98451/5.03990. Took 1.02 sec\n",
      "Epoch 99, Loss(train/val) 4.98287/5.04044. Took 1.02 sec\n",
      "ACC: 0.515625, MCC: 0.03126526997403612\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.02527/5.00993. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.99383/5.00155. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 4.99080/4.98725. Took 1.02 sec\n",
      "Epoch 3, Loss(train/val) 4.98915/4.98352. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 4.98662/4.98396. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 4.98766/4.98668. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.98732/4.98794. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.98741/4.98733. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 4.98805/4.98670. Took 1.00 sec\n",
      "Epoch 9, Loss(train/val) 4.98708/4.98786. Took 1.02 sec\n",
      "Epoch 10, Loss(train/val) 4.98635/4.98977. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.98483/4.99186. Took 1.02 sec\n",
      "Epoch 12, Loss(train/val) 4.98555/4.99361. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.98539/4.99248. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 4.98511/4.99284. Took 1.00 sec\n",
      "Epoch 15, Loss(train/val) 4.98460/4.99499. Took 1.00 sec\n",
      "Epoch 16, Loss(train/val) 4.98451/4.99469. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 4.98309/4.99408. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 4.98319/4.99668. Took 1.00 sec\n",
      "Epoch 19, Loss(train/val) 4.98214/4.99571. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 4.98284/4.99561. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.98238/4.99614. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.98287/4.99801. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.98084/5.00150. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 4.98114/5.00022. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 4.98108/5.00098. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 4.98006/5.00432. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 4.97965/5.00852. Took 1.10 sec\n",
      "Epoch 28, Loss(train/val) 4.97790/5.00808. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.97974/5.00510. Took 1.02 sec\n",
      "Epoch 30, Loss(train/val) 4.97780/5.00979. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.97945/5.00433. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 4.97813/5.01116. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.97910/5.00582. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.97724/5.01286. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 4.97803/5.01000. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 4.97570/5.00982. Took 1.00 sec\n",
      "Epoch 37, Loss(train/val) 4.97708/5.01464. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 4.97509/5.01233. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.97420/5.01079. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 4.97508/5.01311. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.97441/5.01277. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 4.97519/5.00924. Took 1.02 sec\n",
      "Epoch 43, Loss(train/val) 4.97292/5.02037. Took 1.03 sec\n",
      "Epoch 44, Loss(train/val) 4.97387/5.01627. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.97453/5.01726. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 4.97303/5.01795. Took 1.00 sec\n",
      "Epoch 47, Loss(train/val) 4.97169/5.02121. Took 1.00 sec\n",
      "Epoch 48, Loss(train/val) 4.97347/5.01530. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.97337/5.01961. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 4.97206/5.01281. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.97146/5.01886. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 4.97067/5.02006. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 4.97158/5.02951. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 4.97167/5.02474. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 4.97054/5.02442. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 4.96901/5.03256. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 4.97096/5.03228. Took 1.02 sec\n",
      "Epoch 58, Loss(train/val) 4.97226/5.00937. Took 1.03 sec\n",
      "Epoch 59, Loss(train/val) 4.96947/5.02334. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.96681/5.02377. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 4.96852/5.02974. Took 1.02 sec\n",
      "Epoch 62, Loss(train/val) 4.96972/5.01760. Took 1.03 sec\n",
      "Epoch 63, Loss(train/val) 4.96855/5.01939. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 4.96883/5.03412. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 4.96578/5.03779. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 4.96826/5.02392. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 4.96651/5.03631. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 4.96820/5.02874. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 4.96533/5.02619. Took 1.00 sec\n",
      "Epoch 70, Loss(train/val) 4.96638/5.01894. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 4.96841/5.03766. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 4.96729/5.03093. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.96988/5.03448. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.96592/5.02273. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.96841/5.02868. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 4.96656/5.03807. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 4.96505/5.02968. Took 1.03 sec\n",
      "Epoch 78, Loss(train/val) 4.96615/5.03209. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 4.96509/5.03083. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.96754/5.03579. Took 1.02 sec\n",
      "Epoch 81, Loss(train/val) 4.96425/5.04766. Took 1.02 sec\n",
      "Epoch 82, Loss(train/val) 4.96558/5.03031. Took 1.02 sec\n",
      "Epoch 83, Loss(train/val) 4.96789/5.03441. Took 1.02 sec\n",
      "Epoch 84, Loss(train/val) 4.96589/5.02867. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 4.96470/5.03645. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 4.96497/5.02955. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 4.96594/5.02972. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.96670/5.04291. Took 1.00 sec\n",
      "Epoch 89, Loss(train/val) 4.96402/5.04475. Took 1.03 sec\n",
      "Epoch 90, Loss(train/val) 4.96569/5.03893. Took 1.03 sec\n",
      "Epoch 91, Loss(train/val) 4.96569/5.03660. Took 1.02 sec\n",
      "Epoch 92, Loss(train/val) 4.96484/5.04459. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 4.96650/5.03266. Took 1.02 sec\n",
      "Epoch 94, Loss(train/val) 4.96369/5.04327. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 4.96405/5.02978. Took 1.11 sec\n",
      "Epoch 96, Loss(train/val) 4.96155/5.04437. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.96261/5.03770. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 4.96263/5.02552. Took 1.02 sec\n",
      "Epoch 99, Loss(train/val) 4.96455/5.03537. Took 1.02 sec\n",
      "ACC: 0.5625, MCC: 0.12848960542144847\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.08625/5.02951. Took 1.01 sec\n",
      "Epoch 1, Loss(train/val) 5.04339/5.03045. Took 1.00 sec\n",
      "Epoch 2, Loss(train/val) 5.03849/5.02963. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 5.03779/5.03454. Took 1.00 sec\n",
      "Epoch 4, Loss(train/val) 5.03842/5.03770. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 5.03826/5.04013. Took 1.00 sec\n",
      "Epoch 6, Loss(train/val) 5.03944/5.03784. Took 1.00 sec\n",
      "Epoch 7, Loss(train/val) 5.03926/5.03508. Took 1.03 sec\n",
      "Epoch 8, Loss(train/val) 5.03962/5.03370. Took 1.02 sec\n",
      "Epoch 9, Loss(train/val) 5.03805/5.03454. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 5.03879/5.03356. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 5.03754/5.03396. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 5.03758/5.03355. Took 1.00 sec\n",
      "Epoch 13, Loss(train/val) 5.03807/5.03420. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 5.03626/5.03344. Took 1.00 sec\n",
      "Epoch 15, Loss(train/val) 5.03735/5.03146. Took 1.00 sec\n",
      "Epoch 16, Loss(train/val) 5.03588/5.03691. Took 1.00 sec\n",
      "Epoch 17, Loss(train/val) 5.03579/5.03510. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 5.03689/5.03989. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 5.03528/5.03169. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 5.03600/5.03559. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 5.03437/5.03268. Took 1.00 sec\n",
      "Epoch 22, Loss(train/val) 5.03493/5.03563. Took 1.00 sec\n",
      "Epoch 23, Loss(train/val) 5.03523/5.03448. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 5.03424/5.03551. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 5.03475/5.03446. Took 1.00 sec\n",
      "Epoch 26, Loss(train/val) 5.03155/5.03504. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 5.03293/5.04099. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 5.03271/5.03583. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 5.03356/5.03524. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 5.03232/5.03460. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 5.03269/5.04061. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 5.03128/5.03997. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 5.03067/5.03961. Took 1.03 sec\n",
      "Epoch 34, Loss(train/val) 5.03050/5.03732. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 5.03151/5.03653. Took 1.02 sec\n",
      "Epoch 36, Loss(train/val) 5.03041/5.03973. Took 1.00 sec\n",
      "Epoch 37, Loss(train/val) 5.02886/5.03551. Took 1.02 sec\n",
      "Epoch 38, Loss(train/val) 5.03062/5.04415. Took 1.02 sec\n",
      "Epoch 39, Loss(train/val) 5.02843/5.04980. Took 1.02 sec\n",
      "Epoch 40, Loss(train/val) 5.03031/5.03746. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 5.03123/5.03301. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 5.02935/5.03951. Took 1.02 sec\n",
      "Epoch 43, Loss(train/val) 5.02712/5.03618. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 5.02825/5.03593. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 5.02806/5.03516. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 5.02682/5.03494. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 5.02835/5.04488. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 5.02418/5.05372. Took 1.00 sec\n",
      "Epoch 49, Loss(train/val) 5.02815/5.03754. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 5.03115/5.04073. Took 1.00 sec\n",
      "Epoch 51, Loss(train/val) 5.02370/5.03884. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 5.02540/5.03731. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 5.02568/5.03815. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 5.02494/5.04410. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 5.02788/5.03996. Took 1.00 sec\n",
      "Epoch 56, Loss(train/val) 5.02366/5.04053. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 5.02701/5.04213. Took 1.10 sec\n",
      "Epoch 58, Loss(train/val) 5.02380/5.03353. Took 1.02 sec\n",
      "Epoch 59, Loss(train/val) 5.02298/5.03993. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 5.02154/5.04491. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 5.02360/5.05023. Took 1.02 sec\n",
      "Epoch 62, Loss(train/val) 5.02234/5.04920. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 5.02312/5.05044. Took 1.02 sec\n",
      "Epoch 64, Loss(train/val) 5.02699/5.03665. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 5.02551/5.03015. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 5.02088/5.04188. Took 1.00 sec\n",
      "Epoch 67, Loss(train/val) 5.02395/5.04221. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 5.02488/5.03557. Took 1.00 sec\n",
      "Epoch 69, Loss(train/val) 5.02395/5.03334. Took 1.02 sec\n",
      "Epoch 70, Loss(train/val) 5.02137/5.04288. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 5.02360/5.05685. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 5.02076/5.04884. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 5.02418/5.03258. Took 1.00 sec\n",
      "Epoch 74, Loss(train/val) 5.02838/5.01942. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 5.03183/5.02861. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 5.02816/5.03407. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 5.02821/5.03468. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 5.02599/5.03147. Took 1.00 sec\n",
      "Epoch 79, Loss(train/val) 5.02564/5.03564. Took 1.03 sec\n",
      "Epoch 80, Loss(train/val) 5.02339/5.03833. Took 1.02 sec\n",
      "Epoch 81, Loss(train/val) 5.02411/5.03475. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 5.02635/5.03476. Took 1.02 sec\n",
      "Epoch 83, Loss(train/val) 5.02219/5.04291. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 5.02212/5.05112. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 5.02151/5.03736. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 5.02501/5.03705. Took 1.00 sec\n",
      "Epoch 87, Loss(train/val) 5.02157/5.04139. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 5.02133/5.04381. Took 1.00 sec\n",
      "Epoch 89, Loss(train/val) 5.01993/5.03847. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 5.01949/5.04044. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 5.02037/5.03969. Took 1.00 sec\n",
      "Epoch 92, Loss(train/val) 5.02020/5.04002. Took 1.00 sec\n",
      "Epoch 93, Loss(train/val) 5.01974/5.04873. Took 1.00 sec\n",
      "Epoch 94, Loss(train/val) 5.01943/5.04179. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 5.02099/5.03257. Took 1.00 sec\n",
      "Epoch 96, Loss(train/val) 5.02147/5.04648. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 5.01788/5.03534. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 5.01984/5.04397. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 5.02094/5.03960. Took 1.01 sec\n",
      "ACC: 0.4921875, MCC: -0.0024768186456053313\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 4.99686/4.96776. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.96698/4.96302. Took 1.02 sec\n",
      "Epoch 2, Loss(train/val) 4.96508/4.96336. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 4.96495/4.96300. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 4.96392/4.96275. Took 1.03 sec\n",
      "Epoch 5, Loss(train/val) 4.96420/4.96174. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.96662/4.96126. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.96449/4.96173. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 4.96554/4.96317. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.96386/4.96288. Took 1.00 sec\n",
      "Epoch 10, Loss(train/val) 4.96324/4.96331. Took 1.02 sec\n",
      "Epoch 11, Loss(train/val) 4.96350/4.96378. Took 1.02 sec\n",
      "Epoch 12, Loss(train/val) 4.96392/4.96478. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.96106/4.96493. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 4.96137/4.96692. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.96169/4.96982. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 4.96354/4.96790. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 4.96076/4.97135. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 4.95976/4.97283. Took 1.02 sec\n",
      "Epoch 19, Loss(train/val) 4.96059/4.97073. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 4.95984/4.97535. Took 1.02 sec\n",
      "Epoch 21, Loss(train/val) 4.95815/4.97746. Took 1.10 sec\n",
      "Epoch 22, Loss(train/val) 4.95866/4.97604. Took 1.02 sec\n",
      "Epoch 23, Loss(train/val) 4.96085/4.96772. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 4.96114/4.97186. Took 1.02 sec\n",
      "Epoch 25, Loss(train/val) 4.95983/4.97144. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 4.95947/4.97919. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 4.95904/4.97447. Took 1.02 sec\n",
      "Epoch 28, Loss(train/val) 4.95881/4.97735. Took 1.03 sec\n",
      "Epoch 29, Loss(train/val) 4.95536/4.98361. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 4.95784/4.98281. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.95741/4.98012. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 4.95670/4.98269. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.95800/4.97885. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.95831/4.98021. Took 1.03 sec\n",
      "Epoch 35, Loss(train/val) 4.95882/4.98350. Took 1.00 sec\n",
      "Epoch 36, Loss(train/val) 4.95429/4.98802. Took 1.02 sec\n",
      "Epoch 37, Loss(train/val) 4.95619/4.98147. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 4.95554/4.98654. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.95528/4.98724. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 4.95628/4.98380. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.95566/4.98371. Took 1.03 sec\n",
      "Epoch 42, Loss(train/val) 4.95333/4.98736. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.95296/4.98715. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.95386/4.98169. Took 1.03 sec\n",
      "Epoch 45, Loss(train/val) 4.95642/4.98498. Took 1.02 sec\n",
      "Epoch 46, Loss(train/val) 4.95503/4.98412. Took 1.02 sec\n",
      "Epoch 47, Loss(train/val) 4.95207/4.98915. Took 1.02 sec\n",
      "Epoch 48, Loss(train/val) 4.95298/4.98881. Took 1.00 sec\n",
      "Epoch 49, Loss(train/val) 4.95552/4.98000. Took 1.03 sec\n",
      "Epoch 50, Loss(train/val) 4.95465/4.98410. Took 1.02 sec\n",
      "Epoch 51, Loss(train/val) 4.95205/4.98609. Took 1.02 sec\n",
      "Epoch 52, Loss(train/val) 4.95534/4.97889. Took 1.03 sec\n",
      "Epoch 53, Loss(train/val) 4.95282/4.98157. Took 1.02 sec\n",
      "Epoch 54, Loss(train/val) 4.95119/4.99063. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 4.95332/4.99440. Took 1.02 sec\n",
      "Epoch 56, Loss(train/val) 4.95560/4.97869. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 4.95486/4.97675. Took 1.02 sec\n",
      "Epoch 58, Loss(train/val) 4.95393/4.98433. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.95189/4.98751. Took 1.00 sec\n",
      "Epoch 60, Loss(train/val) 4.95477/4.98706. Took 1.02 sec\n",
      "Epoch 61, Loss(train/val) 4.95393/4.98728. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 4.95518/4.98736. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.95471/4.98178. Took 1.02 sec\n",
      "Epoch 64, Loss(train/val) 4.95257/4.98296. Took 1.00 sec\n",
      "Epoch 65, Loss(train/val) 4.95257/4.99067. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 4.95216/4.98981. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 4.95302/4.99311. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 4.95423/4.98564. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 4.95385/4.98150. Took 1.02 sec\n",
      "Epoch 70, Loss(train/val) 4.95179/4.99116. Took 1.02 sec\n",
      "Epoch 71, Loss(train/val) 4.95233/4.99108. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 4.94978/4.98367. Took 1.02 sec\n",
      "Epoch 73, Loss(train/val) 4.95254/4.98197. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.95496/4.98616. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.95477/4.98222. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 4.95303/4.97950. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 4.95329/4.98619. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.95330/4.98540. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 4.95351/4.98349. Took 1.00 sec\n",
      "Epoch 80, Loss(train/val) 4.95273/4.98860. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 4.95342/4.98622. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.95224/4.98229. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 4.95290/4.97851. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.95087/4.99039. Took 1.00 sec\n",
      "Epoch 85, Loss(train/val) 4.95237/4.99140. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 4.95131/4.99062. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 4.95262/4.98222. Took 1.00 sec\n",
      "Epoch 88, Loss(train/val) 4.95218/4.98517. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.95053/4.98708. Took 1.03 sec\n",
      "Epoch 90, Loss(train/val) 4.95269/4.98310. Took 1.11 sec\n",
      "Epoch 91, Loss(train/val) 4.95310/4.98064. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 4.95136/4.98946. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 4.94808/4.99468. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.95183/4.98762. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 4.95446/4.98441. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 4.94958/4.98977. Took 1.02 sec\n",
      "Epoch 97, Loss(train/val) 4.95199/4.98767. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 4.94885/4.99050. Took 1.03 sec\n",
      "Epoch 99, Loss(train/val) 4.95364/4.99029. Took 1.01 sec\n",
      "ACC: 0.46875, MCC: -0.04776968367223478\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.01242/5.02119. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.99271/5.00941. Took 1.00 sec\n",
      "Epoch 2, Loss(train/val) 4.99192/5.03085. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 4.99327/5.02443. Took 1.00 sec\n",
      "Epoch 4, Loss(train/val) 4.99479/5.02237. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 4.99723/5.00659. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.99694/5.00058. Took 1.00 sec\n",
      "Epoch 7, Loss(train/val) 4.99335/5.00106. Took 1.02 sec\n",
      "Epoch 8, Loss(train/val) 4.99019/5.00454. Took 1.02 sec\n",
      "Epoch 9, Loss(train/val) 4.99073/5.00619. Took 1.03 sec\n",
      "Epoch 10, Loss(train/val) 4.99210/5.00423. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.99062/5.00589. Took 1.00 sec\n",
      "Epoch 12, Loss(train/val) 4.99034/5.00608. Took 1.00 sec\n",
      "Epoch 13, Loss(train/val) 4.99080/5.00587. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 4.99006/5.00530. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.98924/5.00525. Took 1.00 sec\n",
      "Epoch 16, Loss(train/val) 4.99033/5.00657. Took 1.00 sec\n",
      "Epoch 17, Loss(train/val) 4.98974/5.00640. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 4.98977/5.00481. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 4.98763/5.00730. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 4.98761/5.00889. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.98865/5.00662. Took 1.02 sec\n",
      "Epoch 22, Loss(train/val) 4.98656/5.00948. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.98689/5.00996. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 4.98756/5.00732. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 4.98741/5.00504. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 4.98562/5.00752. Took 1.00 sec\n",
      "Epoch 27, Loss(train/val) 4.98599/5.01127. Took 1.00 sec\n",
      "Epoch 28, Loss(train/val) 4.98556/5.00724. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.98486/5.00913. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 4.98572/5.00641. Took 1.00 sec\n",
      "Epoch 31, Loss(train/val) 4.98431/5.00773. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 4.98504/5.00462. Took 1.00 sec\n",
      "Epoch 33, Loss(train/val) 4.98356/5.01006. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.98382/5.00742. Took 1.00 sec\n",
      "Epoch 35, Loss(train/val) 4.98528/5.01132. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 4.98285/5.00999. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.98336/5.01276. Took 1.00 sec\n",
      "Epoch 38, Loss(train/val) 4.98369/5.01201. Took 1.00 sec\n",
      "Epoch 39, Loss(train/val) 4.98352/5.01437. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 4.98057/5.01619. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.98528/5.00864. Took 1.03 sec\n",
      "Epoch 42, Loss(train/val) 4.98310/5.01502. Took 1.00 sec\n",
      "Epoch 43, Loss(train/val) 4.97735/5.02274. Took 1.02 sec\n",
      "Epoch 44, Loss(train/val) 4.98210/5.01322. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.98200/5.01467. Took 1.02 sec\n",
      "Epoch 46, Loss(train/val) 4.98031/5.01734. Took 1.02 sec\n",
      "Epoch 47, Loss(train/val) 4.98178/5.01602. Took 1.02 sec\n",
      "Epoch 48, Loss(train/val) 4.97786/5.01598. Took 1.04 sec\n",
      "Epoch 49, Loss(train/val) 4.97921/5.01609. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 4.97912/5.01717. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.98058/5.01827. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 4.97986/5.01938. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 4.98017/5.02676. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 4.97723/5.02469. Took 1.12 sec\n",
      "Epoch 55, Loss(train/val) 4.98531/5.01424. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 4.98416/5.00519. Took 1.04 sec\n",
      "Epoch 57, Loss(train/val) 4.98381/5.01560. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 4.98263/5.02527. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.97998/5.01941. Took 1.02 sec\n",
      "Epoch 60, Loss(train/val) 4.98046/5.02149. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 4.97815/5.02161. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 4.97706/5.02313. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.98050/5.02087. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 4.97559/5.03301. Took 1.02 sec\n",
      "Epoch 65, Loss(train/val) 4.97869/5.02496. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 4.97710/5.02810. Took 1.03 sec\n",
      "Epoch 67, Loss(train/val) 4.97691/5.02400. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 4.97985/5.01811. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 4.97343/5.03614. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 4.97429/5.02746. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 4.97383/5.03708. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 4.97670/5.02320. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.97688/5.02099. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.97393/5.03234. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.97399/5.02834. Took 1.03 sec\n",
      "Epoch 76, Loss(train/val) 4.97331/5.03650. Took 1.00 sec\n",
      "Epoch 77, Loss(train/val) 4.97669/5.02989. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.97560/5.02800. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 4.97589/5.02301. Took 1.02 sec\n",
      "Epoch 80, Loss(train/val) 4.97443/5.02835. Took 1.02 sec\n",
      "Epoch 81, Loss(train/val) 4.97231/5.04260. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.97718/5.03134. Took 1.02 sec\n",
      "Epoch 83, Loss(train/val) 4.97571/5.03715. Took 1.04 sec\n",
      "Epoch 84, Loss(train/val) 4.97274/5.04642. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 4.97510/5.02723. Took 1.00 sec\n",
      "Epoch 86, Loss(train/val) 4.97180/5.04232. Took 1.00 sec\n",
      "Epoch 87, Loss(train/val) 4.97520/5.02983. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.97485/5.03017. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.97301/5.03661. Took 1.00 sec\n",
      "Epoch 90, Loss(train/val) 4.97425/5.02871. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 4.96750/5.04436. Took 1.00 sec\n",
      "Epoch 92, Loss(train/val) 4.97295/5.03535. Took 1.00 sec\n",
      "Epoch 93, Loss(train/val) 4.97480/5.02866. Took 1.00 sec\n",
      "Epoch 94, Loss(train/val) 4.97237/5.04089. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 4.97321/5.03078. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 4.97104/5.04234. Took 1.00 sec\n",
      "Epoch 97, Loss(train/val) 4.97245/5.04152. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 4.97411/5.02705. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 4.97210/5.03946. Took 1.01 sec\n",
      "ACC: 0.5234375, MCC: 0.03854355264048933\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 4.96837/4.94260. Took 1.03 sec\n",
      "Epoch 1, Loss(train/val) 4.94244/4.92837. Took 1.04 sec\n",
      "Epoch 2, Loss(train/val) 4.94016/4.92943. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 4.93797/4.93085. Took 1.00 sec\n",
      "Epoch 4, Loss(train/val) 4.93662/4.93260. Took 0.99 sec\n",
      "Epoch 5, Loss(train/val) 4.93556/4.93235. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.93464/4.93139. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.93581/4.93097. Took 1.00 sec\n",
      "Epoch 8, Loss(train/val) 4.93589/4.93159. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.93509/4.93181. Took 1.00 sec\n",
      "Epoch 10, Loss(train/val) 4.93474/4.93189. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.93480/4.93331. Took 1.02 sec\n",
      "Epoch 12, Loss(train/val) 4.93492/4.93313. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.93543/4.93189. Took 1.00 sec\n",
      "Epoch 14, Loss(train/val) 4.93397/4.93313. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.93321/4.93384. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 4.93353/4.93491. Took 1.03 sec\n",
      "Epoch 17, Loss(train/val) 4.93327/4.93557. Took 1.00 sec\n",
      "Epoch 18, Loss(train/val) 4.93176/4.93542. Took 1.11 sec\n",
      "Epoch 19, Loss(train/val) 4.93020/4.93839. Took 1.00 sec\n",
      "Epoch 20, Loss(train/val) 4.93046/4.93458. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.93039/4.93520. Took 1.02 sec\n",
      "Epoch 22, Loss(train/val) 4.92931/4.93798. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.92822/4.93837. Took 1.02 sec\n",
      "Epoch 24, Loss(train/val) 4.92946/4.93388. Took 1.02 sec\n",
      "Epoch 25, Loss(train/val) 4.93016/4.93788. Took 1.03 sec\n",
      "Epoch 26, Loss(train/val) 4.92821/4.93833. Took 1.02 sec\n",
      "Epoch 27, Loss(train/val) 4.92794/4.94196. Took 1.02 sec\n",
      "Epoch 28, Loss(train/val) 4.92841/4.93283. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.92675/4.93731. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 4.92737/4.93861. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.92466/4.93793. Took 1.02 sec\n",
      "Epoch 32, Loss(train/val) 4.92683/4.93654. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.92650/4.93737. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.92669/4.94054. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 4.92199/4.94954. Took 1.02 sec\n",
      "Epoch 36, Loss(train/val) 4.92521/4.93342. Took 1.02 sec\n",
      "Epoch 37, Loss(train/val) 4.92389/4.94461. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 4.92221/4.94116. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.92424/4.93779. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 4.92152/4.94832. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.92116/4.93795. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 4.92133/4.93731. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.92159/4.93673. Took 1.02 sec\n",
      "Epoch 44, Loss(train/val) 4.92072/4.93379. Took 1.02 sec\n",
      "Epoch 45, Loss(train/val) 4.91982/4.94479. Took 1.04 sec\n",
      "Epoch 46, Loss(train/val) 4.91970/4.94315. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.92031/4.94028. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 4.91849/4.94871. Took 1.02 sec\n",
      "Epoch 49, Loss(train/val) 4.91864/4.94537. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 4.91861/4.94502. Took 1.02 sec\n",
      "Epoch 51, Loss(train/val) 4.92042/4.94409. Took 1.03 sec\n",
      "Epoch 52, Loss(train/val) 4.91764/4.95889. Took 1.03 sec\n",
      "Epoch 53, Loss(train/val) 4.91912/4.94539. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 4.91952/4.93820. Took 1.02 sec\n",
      "Epoch 55, Loss(train/val) 4.91581/4.94418. Took 1.02 sec\n",
      "Epoch 56, Loss(train/val) 4.91809/4.94158. Took 1.04 sec\n",
      "Epoch 57, Loss(train/val) 4.91693/4.94476. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 4.92357/4.93053. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.92274/4.93849. Took 1.02 sec\n",
      "Epoch 60, Loss(train/val) 4.91944/4.94541. Took 1.02 sec\n",
      "Epoch 61, Loss(train/val) 4.91931/4.94528. Took 1.02 sec\n",
      "Epoch 62, Loss(train/val) 4.91812/4.95294. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.92012/4.94288. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 4.92001/4.94071. Took 1.04 sec\n",
      "Epoch 65, Loss(train/val) 4.91776/4.94486. Took 1.03 sec\n",
      "Epoch 66, Loss(train/val) 4.91555/4.94189. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 4.91803/4.93928. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 4.91687/4.93551. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 4.91715/4.93938. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 4.91655/4.94039. Took 1.03 sec\n",
      "Epoch 71, Loss(train/val) 4.91823/4.94482. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 4.92501/4.94483. Took 1.00 sec\n",
      "Epoch 73, Loss(train/val) 4.92505/4.94487. Took 1.02 sec\n",
      "Epoch 74, Loss(train/val) 4.92011/4.94429. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.91855/4.95076. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 4.92081/4.93883. Took 1.00 sec\n",
      "Epoch 77, Loss(train/val) 4.91840/4.94904. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.91874/4.93798. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 4.91655/4.95573. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.91653/4.95675. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 4.91613/4.94038. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.91659/4.94196. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 4.91380/4.94969. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.91769/4.94257. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 4.91535/4.94272. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 4.91645/4.94479. Took 1.02 sec\n",
      "Epoch 87, Loss(train/val) 4.91801/4.94716. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.91436/4.94511. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.91728/4.94360. Took 1.12 sec\n",
      "Epoch 90, Loss(train/val) 4.92099/4.94854. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 4.91882/4.93400. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 4.91642/4.94373. Took 1.02 sec\n",
      "Epoch 93, Loss(train/val) 4.91331/4.94593. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.91258/4.94841. Took 1.02 sec\n",
      "Epoch 95, Loss(train/val) 4.91780/4.94452. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 4.91433/4.94662. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.91641/4.94401. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 4.91311/4.94851. Took 1.04 sec\n",
      "Epoch 99, Loss(train/val) 4.91621/4.95100. Took 1.03 sec\n",
      "ACC: 0.484375, MCC: -0.03138824102871723\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.08052/5.04728. Took 1.01 sec\n",
      "Epoch 1, Loss(train/val) 5.02514/5.03430. Took 1.03 sec\n",
      "Epoch 2, Loss(train/val) 5.02142/5.02030. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 5.02034/5.02108. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 5.02035/5.02020. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 5.02209/5.02045. Took 1.03 sec\n",
      "Epoch 6, Loss(train/val) 5.02197/5.01811. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 5.02128/5.01734. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 5.02296/5.01812. Took 1.02 sec\n",
      "Epoch 9, Loss(train/val) 5.02069/5.02080. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 5.02126/5.01796. Took 1.03 sec\n",
      "Epoch 11, Loss(train/val) 5.02142/5.01985. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 5.02012/5.01763. Took 1.02 sec\n",
      "Epoch 13, Loss(train/val) 5.01933/5.01257. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 5.01878/5.01566. Took 1.02 sec\n",
      "Epoch 15, Loss(train/val) 5.01927/5.01770. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 5.01872/5.01778. Took 1.02 sec\n",
      "Epoch 17, Loss(train/val) 5.01959/5.02044. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 5.01802/5.01921. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 5.01722/5.01694. Took 1.02 sec\n",
      "Epoch 20, Loss(train/val) 5.01774/5.01464. Took 1.02 sec\n",
      "Epoch 21, Loss(train/val) 5.01802/5.01640. Took 1.02 sec\n",
      "Epoch 22, Loss(train/val) 5.01701/5.01911. Took 1.02 sec\n",
      "Epoch 23, Loss(train/val) 5.01693/5.01918. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 5.01588/5.01921. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 5.01588/5.01335. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 5.01691/5.01453. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 5.01752/5.01283. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 5.01757/5.01966. Took 1.02 sec\n",
      "Epoch 29, Loss(train/val) 5.01661/5.02250. Took 1.02 sec\n",
      "Epoch 30, Loss(train/val) 5.01560/5.02386. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 5.01606/5.02283. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 5.01493/5.01978. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 5.01481/5.01944. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 5.01288/5.01797. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 5.01529/5.02005. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 5.01494/5.02079. Took 1.03 sec\n",
      "Epoch 37, Loss(train/val) 5.01288/5.02031. Took 1.02 sec\n",
      "Epoch 38, Loss(train/val) 5.01415/5.02125. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 5.01388/5.02298. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 5.01219/5.02406. Took 1.02 sec\n",
      "Epoch 41, Loss(train/val) 5.01360/5.01948. Took 1.03 sec\n",
      "Epoch 42, Loss(train/val) 5.01362/5.02398. Took 1.03 sec\n",
      "Epoch 43, Loss(train/val) 5.01360/5.02092. Took 1.02 sec\n",
      "Epoch 44, Loss(train/val) 5.01149/5.02272. Took 1.04 sec\n",
      "Epoch 45, Loss(train/val) 5.01171/5.02260. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 5.01397/5.01995. Took 1.02 sec\n",
      "Epoch 47, Loss(train/val) 5.01317/5.02117. Took 1.02 sec\n",
      "Epoch 48, Loss(train/val) 5.01091/5.02485. Took 1.02 sec\n",
      "Epoch 49, Loss(train/val) 5.01185/5.02176. Took 1.02 sec\n",
      "Epoch 50, Loss(train/val) 5.01191/5.02537. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 5.01165/5.02222. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 5.01238/5.02627. Took 1.02 sec\n",
      "Epoch 53, Loss(train/val) 5.01217/5.02499. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 5.01133/5.02656. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 5.00985/5.02938. Took 1.12 sec\n",
      "Epoch 56, Loss(train/val) 5.01237/5.03043. Took 1.02 sec\n",
      "Epoch 57, Loss(train/val) 5.00973/5.02969. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 5.00982/5.02912. Took 1.02 sec\n",
      "Epoch 59, Loss(train/val) 5.01003/5.02028. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 5.01033/5.02821. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 5.00908/5.03507. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 5.00914/5.02874. Took 1.02 sec\n",
      "Epoch 63, Loss(train/val) 5.01044/5.03310. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 5.00971/5.03093. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 5.00967/5.03067. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 5.00537/5.02735. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 5.01017/5.03360. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 5.00905/5.03167. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 5.00796/5.04093. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 5.01115/5.02140. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 5.00802/5.02771. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 5.00814/5.03245. Took 1.03 sec\n",
      "Epoch 73, Loss(train/val) 5.00635/5.03464. Took 1.02 sec\n",
      "Epoch 74, Loss(train/val) 5.00772/5.02668. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 5.00610/5.01769. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 5.00802/5.02710. Took 1.02 sec\n",
      "Epoch 77, Loss(train/val) 5.00687/5.03267. Took 1.02 sec\n",
      "Epoch 78, Loss(train/val) 5.00805/5.02391. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 5.00367/5.03051. Took 1.02 sec\n",
      "Epoch 80, Loss(train/val) 5.00800/5.03608. Took 1.02 sec\n",
      "Epoch 81, Loss(train/val) 5.00528/5.03129. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 5.00518/5.02853. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 5.00468/5.02675. Took 1.02 sec\n",
      "Epoch 84, Loss(train/val) 5.00590/5.03418. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 5.00589/5.03942. Took 1.03 sec\n",
      "Epoch 86, Loss(train/val) 5.00602/5.03031. Took 1.02 sec\n",
      "Epoch 87, Loss(train/val) 5.00542/5.03922. Took 1.02 sec\n",
      "Epoch 88, Loss(train/val) 5.00840/5.02832. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 5.00266/5.04799. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 5.00790/5.01867. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 5.00320/5.04047. Took 1.03 sec\n",
      "Epoch 92, Loss(train/val) 5.00747/5.02753. Took 1.02 sec\n",
      "Epoch 93, Loss(train/val) 5.00471/5.03595. Took 1.02 sec\n",
      "Epoch 94, Loss(train/val) 5.00584/5.02621. Took 1.03 sec\n",
      "Epoch 95, Loss(train/val) 5.00243/5.03939. Took 1.02 sec\n",
      "Epoch 96, Loss(train/val) 5.00431/5.02742. Took 1.03 sec\n",
      "Epoch 97, Loss(train/val) 5.00084/5.04616. Took 1.02 sec\n",
      "Epoch 98, Loss(train/val) 5.00386/5.02611. Took 1.02 sec\n",
      "Epoch 99, Loss(train/val) 5.00187/5.03821. Took 1.01 sec\n",
      "ACC: 0.5703125, MCC: 0.1376650229838827\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.02916/5.00843. Took 1.01 sec\n",
      "Epoch 1, Loss(train/val) 5.00216/4.99814. Took 1.02 sec\n",
      "Epoch 2, Loss(train/val) 5.00372/4.99977. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 5.00369/5.00758. Took 1.00 sec\n",
      "Epoch 4, Loss(train/val) 5.00164/5.00852. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 4.99842/5.00375. Took 1.00 sec\n",
      "Epoch 6, Loss(train/val) 4.99662/5.00348. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.99733/5.00435. Took 1.00 sec\n",
      "Epoch 8, Loss(train/val) 4.99839/5.00612. Took 1.00 sec\n",
      "Epoch 9, Loss(train/val) 4.99601/5.00407. Took 1.02 sec\n",
      "Epoch 10, Loss(train/val) 4.99574/5.00454. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.99537/5.00457. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 4.99552/5.00491. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.99601/5.00629. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 4.99470/5.00525. Took 1.02 sec\n",
      "Epoch 15, Loss(train/val) 4.99431/5.00502. Took 1.02 sec\n",
      "Epoch 16, Loss(train/val) 4.99510/5.00832. Took 1.00 sec\n",
      "Epoch 17, Loss(train/val) 4.99468/5.00497. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 4.99378/5.00496. Took 1.00 sec\n",
      "Epoch 19, Loss(train/val) 4.99469/5.00700. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 4.99451/5.00489. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.99319/5.00560. Took 1.02 sec\n",
      "Epoch 22, Loss(train/val) 4.99333/5.00705. Took 1.11 sec\n",
      "Epoch 23, Loss(train/val) 4.99341/5.00969. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 4.99305/5.00711. Took 1.02 sec\n",
      "Epoch 25, Loss(train/val) 4.99398/5.01002. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 4.99311/5.00624. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 4.99327/5.00766. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 4.99399/5.00646. Took 1.02 sec\n",
      "Epoch 29, Loss(train/val) 4.99372/5.00806. Took 1.03 sec\n",
      "Epoch 30, Loss(train/val) 4.99300/5.00866. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.99254/5.00976. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 4.99193/5.00918. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.99214/5.00767. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.99268/5.00798. Took 1.03 sec\n",
      "Epoch 35, Loss(train/val) 4.99352/5.00992. Took 1.02 sec\n",
      "Epoch 36, Loss(train/val) 4.99243/5.00814. Took 1.00 sec\n",
      "Epoch 37, Loss(train/val) 4.99078/5.01047. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 4.99179/5.01233. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.99109/5.01155. Took 1.02 sec\n",
      "Epoch 40, Loss(train/val) 4.99162/5.01270. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.99042/5.01248. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 4.98956/5.01680. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.99172/5.01013. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.99192/5.00996. Took 1.03 sec\n",
      "Epoch 45, Loss(train/val) 4.99011/5.01535. Took 1.03 sec\n",
      "Epoch 46, Loss(train/val) 4.98735/5.01572. Took 1.03 sec\n",
      "Epoch 47, Loss(train/val) 4.98923/5.01511. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 4.99111/5.01202. Took 1.03 sec\n",
      "Epoch 49, Loss(train/val) 4.98896/5.01208. Took 1.02 sec\n",
      "Epoch 50, Loss(train/val) 4.98905/5.01787. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.99034/5.01425. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 4.98716/5.01843. Took 1.02 sec\n",
      "Epoch 53, Loss(train/val) 4.99160/5.01067. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 4.99217/5.00775. Took 1.02 sec\n",
      "Epoch 55, Loss(train/val) 4.98838/5.00452. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 4.98892/5.01038. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 4.98665/5.01796. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 4.98975/5.01322. Took 1.02 sec\n",
      "Epoch 59, Loss(train/val) 4.98755/5.02007. Took 1.02 sec\n",
      "Epoch 60, Loss(train/val) 4.98709/5.01246. Took 1.03 sec\n",
      "Epoch 61, Loss(train/val) 4.98733/5.01635. Took 1.02 sec\n",
      "Epoch 62, Loss(train/val) 4.98753/5.02244. Took 1.02 sec\n",
      "Epoch 63, Loss(train/val) 4.98855/5.01283. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 4.98748/5.01775. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 4.98821/5.01460. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 4.98464/5.02191. Took 1.02 sec\n",
      "Epoch 67, Loss(train/val) 4.98541/5.01383. Took 1.03 sec\n",
      "Epoch 68, Loss(train/val) 4.98594/5.01906. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 4.98366/5.01892. Took 1.02 sec\n",
      "Epoch 70, Loss(train/val) 4.98717/5.01551. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 4.98612/5.01743. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 4.98495/5.01743. Took 1.02 sec\n",
      "Epoch 73, Loss(train/val) 4.98467/5.01466. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.98383/5.01956. Took 1.00 sec\n",
      "Epoch 75, Loss(train/val) 4.98534/5.01284. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 4.98644/5.01621. Took 1.02 sec\n",
      "Epoch 77, Loss(train/val) 4.98463/5.01475. Took 1.02 sec\n",
      "Epoch 78, Loss(train/val) 4.98307/5.01911. Took 1.02 sec\n",
      "Epoch 79, Loss(train/val) 4.98564/5.01185. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.98358/5.02258. Took 1.02 sec\n",
      "Epoch 81, Loss(train/val) 4.98644/5.01425. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.98343/5.02287. Took 1.02 sec\n",
      "Epoch 83, Loss(train/val) 4.98368/5.02082. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.98353/5.01936. Took 1.03 sec\n",
      "Epoch 85, Loss(train/val) 4.98316/5.02676. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 4.98134/5.02532. Took 1.03 sec\n",
      "Epoch 87, Loss(train/val) 4.98176/5.01860. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.98540/5.01684. Took 1.02 sec\n",
      "Epoch 89, Loss(train/val) 4.98430/5.01634. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 4.98386/5.01924. Took 1.02 sec\n",
      "Epoch 91, Loss(train/val) 4.98246/5.02264. Took 1.02 sec\n",
      "Epoch 92, Loss(train/val) 4.98352/5.01807. Took 1.00 sec\n",
      "Epoch 93, Loss(train/val) 4.98290/5.02707. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.98339/5.02458. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 4.98309/5.02410. Took 1.12 sec\n",
      "Epoch 96, Loss(train/val) 4.98442/5.02290. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.98187/5.02152. Took 1.02 sec\n",
      "Epoch 98, Loss(train/val) 4.97897/5.02418. Took 1.00 sec\n",
      "Epoch 99, Loss(train/val) 4.98047/5.02758. Took 1.02 sec\n",
      "ACC: 0.546875, MCC: 0.10344827586206896\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.02499/4.97212. Took 1.01 sec\n",
      "Epoch 1, Loss(train/val) 4.97343/4.96964. Took 1.02 sec\n",
      "Epoch 2, Loss(train/val) 4.97572/4.97061. Took 1.02 sec\n",
      "Epoch 3, Loss(train/val) 4.97866/4.97236. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 4.97826/4.97540. Took 1.00 sec\n",
      "Epoch 5, Loss(train/val) 4.97591/4.97427. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.97503/4.97230. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.97317/4.97249. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 4.97226/4.97305. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.97388/4.97391. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 4.97374/4.97438. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.97163/4.97524. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 4.97142/4.97541. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.97152/4.97692. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 4.97168/4.97951. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.97048/4.98148. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 4.96990/4.97966. Took 1.00 sec\n",
      "Epoch 17, Loss(train/val) 4.97056/4.98001. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 4.96966/4.98064. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 4.96853/4.98136. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 4.97045/4.98034. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.96918/4.98092. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.96950/4.98153. Took 1.03 sec\n",
      "Epoch 23, Loss(train/val) 4.96817/4.98112. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 4.96889/4.97957. Took 1.00 sec\n",
      "Epoch 25, Loss(train/val) 4.96826/4.97613. Took 1.02 sec\n",
      "Epoch 26, Loss(train/val) 4.96799/4.97892. Took 1.00 sec\n",
      "Epoch 27, Loss(train/val) 4.96774/4.97676. Took 1.02 sec\n",
      "Epoch 28, Loss(train/val) 4.96788/4.97693. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.96831/4.97247. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 4.96751/4.97361. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.96790/4.97658. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 4.96637/4.97620. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.96681/4.97571. Took 1.05 sec\n",
      "Epoch 34, Loss(train/val) 4.96594/4.97694. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 4.96547/4.97701. Took 1.02 sec\n",
      "Epoch 36, Loss(train/val) 4.96772/4.97386. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.96608/4.97801. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 4.96559/4.97585. Took 1.00 sec\n",
      "Epoch 39, Loss(train/val) 4.96487/4.97730. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 4.96415/4.97982. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.96653/4.97917. Took 1.02 sec\n",
      "Epoch 42, Loss(train/val) 4.96473/4.98134. Took 1.00 sec\n",
      "Epoch 43, Loss(train/val) 4.96445/4.98154. Took 1.02 sec\n",
      "Epoch 44, Loss(train/val) 4.96474/4.97989. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.96313/4.98148. Took 1.03 sec\n",
      "Epoch 46, Loss(train/val) 4.96362/4.98532. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.96478/4.98393. Took 1.00 sec\n",
      "Epoch 48, Loss(train/val) 4.96317/4.98296. Took 1.02 sec\n",
      "Epoch 49, Loss(train/val) 4.96224/4.98975. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 4.96522/4.98085. Took 1.02 sec\n",
      "Epoch 51, Loss(train/val) 4.96349/4.98337. Took 1.02 sec\n",
      "Epoch 52, Loss(train/val) 4.96246/4.98286. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 4.96183/4.98393. Took 1.02 sec\n",
      "Epoch 54, Loss(train/val) 4.96301/4.98632. Took 1.02 sec\n",
      "Epoch 55, Loss(train/val) 4.96209/4.99144. Took 1.02 sec\n",
      "Epoch 56, Loss(train/val) 4.96307/4.98301. Took 1.02 sec\n",
      "Epoch 57, Loss(train/val) 4.96030/4.99178. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 4.96266/4.98670. Took 1.03 sec\n",
      "Epoch 59, Loss(train/val) 4.96197/4.99072. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.96463/4.98211. Took 1.02 sec\n",
      "Epoch 61, Loss(train/val) 4.96070/4.98884. Took 1.02 sec\n",
      "Epoch 62, Loss(train/val) 4.96265/4.98605. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.96159/4.98880. Took 1.11 sec\n",
      "Epoch 64, Loss(train/val) 4.96024/4.98744. Took 1.03 sec\n",
      "Epoch 65, Loss(train/val) 4.95934/4.99311. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 4.96090/4.98430. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 4.96266/4.98373. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 4.96195/4.98437. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 4.96027/4.98910. Took 1.02 sec\n",
      "Epoch 70, Loss(train/val) 4.96198/4.98356. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 4.95973/4.98941. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 4.96024/4.99198. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.96063/4.98950. Took 1.04 sec\n",
      "Epoch 74, Loss(train/val) 4.96237/4.98345. Took 1.02 sec\n",
      "Epoch 75, Loss(train/val) 4.96094/4.98892. Took 1.03 sec\n",
      "Epoch 76, Loss(train/val) 4.95929/4.99029. Took 1.02 sec\n",
      "Epoch 77, Loss(train/val) 4.96064/4.98485. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.95868/4.98741. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 4.96006/4.99052. Took 1.03 sec\n",
      "Epoch 80, Loss(train/val) 4.96156/4.98549. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 4.96037/4.98895. Took 1.02 sec\n",
      "Epoch 82, Loss(train/val) 4.95865/4.98814. Took 1.02 sec\n",
      "Epoch 83, Loss(train/val) 4.96035/4.98951. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.96137/4.98681. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 4.96101/4.98898. Took 1.02 sec\n",
      "Epoch 86, Loss(train/val) 4.95708/4.99088. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 4.96008/4.98521. Took 1.00 sec\n",
      "Epoch 88, Loss(train/val) 4.95918/4.98838. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.95987/4.98891. Took 1.00 sec\n",
      "Epoch 90, Loss(train/val) 4.95799/4.98647. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 4.95727/4.99263. Took 1.00 sec\n",
      "Epoch 92, Loss(train/val) 4.95996/4.98607. Took 1.02 sec\n",
      "Epoch 93, Loss(train/val) 4.95690/4.99176. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.96146/4.98307. Took 1.02 sec\n",
      "Epoch 95, Loss(train/val) 4.95989/4.98548. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 4.96103/4.98801. Took 1.02 sec\n",
      "Epoch 97, Loss(train/val) 4.95942/4.99120. Took 1.00 sec\n",
      "Epoch 98, Loss(train/val) 4.96026/4.98438. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 4.96133/4.98252. Took 1.02 sec\n",
      "ACC: 0.4375, MCC: -0.10560513668479576\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.03055/4.98234. Took 1.01 sec\n",
      "Epoch 1, Loss(train/val) 4.98516/4.98157. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 4.98531/4.98014. Took 1.02 sec\n",
      "Epoch 3, Loss(train/val) 4.98303/4.97572. Took 1.02 sec\n",
      "Epoch 4, Loss(train/val) 4.98055/4.97470. Took 1.00 sec\n",
      "Epoch 5, Loss(train/val) 4.98142/4.97449. Took 1.03 sec\n",
      "Epoch 6, Loss(train/val) 4.98094/4.97455. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.98182/4.97401. Took 1.03 sec\n",
      "Epoch 8, Loss(train/val) 4.98105/4.97255. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.97996/4.97212. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 4.98111/4.97165. Took 1.00 sec\n",
      "Epoch 11, Loss(train/val) 4.97989/4.97236. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 4.97880/4.97157. Took 1.00 sec\n",
      "Epoch 13, Loss(train/val) 4.97960/4.97066. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 4.97906/4.97022. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.97878/4.96967. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 4.97913/4.96769. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 4.97755/4.96698. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 4.97801/4.96725. Took 1.02 sec\n",
      "Epoch 19, Loss(train/val) 4.97818/4.96754. Took 1.02 sec\n",
      "Epoch 20, Loss(train/val) 4.97805/4.96860. Took 1.02 sec\n",
      "Epoch 21, Loss(train/val) 4.97808/4.96767. Took 1.02 sec\n",
      "Epoch 22, Loss(train/val) 4.97744/4.96607. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.97716/4.96282. Took 1.02 sec\n",
      "Epoch 24, Loss(train/val) 4.97568/4.96502. Took 1.02 sec\n",
      "Epoch 25, Loss(train/val) 4.97647/4.96722. Took 1.02 sec\n",
      "Epoch 26, Loss(train/val) 4.97637/4.96598. Took 1.02 sec\n",
      "Epoch 27, Loss(train/val) 4.97535/4.96391. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 4.97693/4.96760. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.97530/4.96527. Took 1.02 sec\n",
      "Epoch 30, Loss(train/val) 4.97560/4.96644. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.97575/4.96493. Took 1.12 sec\n",
      "Epoch 32, Loss(train/val) 4.97662/4.96910. Took 1.04 sec\n",
      "Epoch 33, Loss(train/val) 4.97457/4.96815. Took 1.03 sec\n",
      "Epoch 34, Loss(train/val) 4.97464/4.96689. Took 1.02 sec\n",
      "Epoch 35, Loss(train/val) 4.97469/4.97160. Took 1.02 sec\n",
      "Epoch 36, Loss(train/val) 4.97449/4.97053. Took 1.02 sec\n",
      "Epoch 37, Loss(train/val) 4.97612/4.97382. Took 1.02 sec\n",
      "Epoch 38, Loss(train/val) 4.97547/4.96826. Took 1.02 sec\n",
      "Epoch 39, Loss(train/val) 4.97464/4.96779. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 4.97210/4.96680. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 4.97490/4.97167. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 4.97372/4.96978. Took 1.02 sec\n",
      "Epoch 43, Loss(train/val) 4.97404/4.96799. Took 1.03 sec\n",
      "Epoch 44, Loss(train/val) 4.97364/4.97010. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.96933/4.96873. Took 1.02 sec\n",
      "Epoch 46, Loss(train/val) 4.97444/4.96654. Took 1.03 sec\n",
      "Epoch 47, Loss(train/val) 4.97267/4.96686. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 4.97280/4.96671. Took 1.02 sec\n",
      "Epoch 49, Loss(train/val) 4.97198/4.96610. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 4.97270/4.96489. Took 1.02 sec\n",
      "Epoch 51, Loss(train/val) 4.97540/4.96659. Took 1.02 sec\n",
      "Epoch 52, Loss(train/val) 4.97485/4.96603. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 4.97231/4.96620. Took 1.03 sec\n",
      "Epoch 54, Loss(train/val) 4.97324/4.96760. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 4.97228/4.96948. Took 1.02 sec\n",
      "Epoch 56, Loss(train/val) 4.97420/4.97493. Took 1.02 sec\n",
      "Epoch 57, Loss(train/val) 4.97166/4.97225. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 4.97300/4.97192. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.97188/4.97187. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.97094/4.97347. Took 1.02 sec\n",
      "Epoch 61, Loss(train/val) 4.97127/4.97435. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 4.97145/4.97383. Took 1.02 sec\n",
      "Epoch 63, Loss(train/val) 4.97127/4.97118. Took 1.03 sec\n",
      "Epoch 64, Loss(train/val) 4.96989/4.97528. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 4.96981/4.97095. Took 1.02 sec\n",
      "Epoch 66, Loss(train/val) 4.96949/4.97348. Took 1.02 sec\n",
      "Epoch 67, Loss(train/val) 4.96814/4.97829. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 4.97027/4.97680. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 4.97045/4.97089. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 4.96836/4.97069. Took 1.03 sec\n",
      "Epoch 71, Loss(train/val) 4.96828/4.97145. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 4.96802/4.97107. Took 1.02 sec\n",
      "Epoch 73, Loss(train/val) 4.97330/4.97100. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.97369/4.96936. Took 1.04 sec\n",
      "Epoch 75, Loss(train/val) 4.97302/4.97299. Took 1.03 sec\n",
      "Epoch 76, Loss(train/val) 4.97337/4.96943. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 4.96980/4.97014. Took 1.03 sec\n",
      "Epoch 78, Loss(train/val) 4.97135/4.97144. Took 1.02 sec\n",
      "Epoch 79, Loss(train/val) 4.96846/4.97182. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.97065/4.97507. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 4.96726/4.97041. Took 1.02 sec\n",
      "Epoch 82, Loss(train/val) 4.96746/4.96880. Took 1.03 sec\n",
      "Epoch 83, Loss(train/val) 4.97105/4.98125. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.96938/4.97347. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 4.97013/4.97375. Took 1.02 sec\n",
      "Epoch 86, Loss(train/val) 4.96461/4.97419. Took 1.02 sec\n",
      "Epoch 87, Loss(train/val) 4.96466/4.98152. Took 1.02 sec\n",
      "Epoch 88, Loss(train/val) 4.96672/4.97627. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.96621/4.97516. Took 1.02 sec\n",
      "Epoch 90, Loss(train/val) 4.96707/4.97701. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 4.96321/4.98105. Took 1.02 sec\n",
      "Epoch 92, Loss(train/val) 4.96721/4.97851. Took 1.03 sec\n",
      "Epoch 93, Loss(train/val) 4.96410/4.97341. Took 1.03 sec\n",
      "Epoch 94, Loss(train/val) 4.96365/4.97911. Took 1.03 sec\n",
      "Epoch 95, Loss(train/val) 4.96595/4.98547. Took 1.03 sec\n",
      "Epoch 96, Loss(train/val) 4.96399/4.98215. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.96375/4.97608. Took 1.02 sec\n",
      "Epoch 98, Loss(train/val) 4.96648/4.97755. Took 1.03 sec\n",
      "Epoch 99, Loss(train/val) 4.96097/4.97903. Took 1.01 sec\n",
      "ACC: 0.453125, MCC: -0.08385063815555238\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.09676/5.10440. Took 1.13 sec\n",
      "Epoch 1, Loss(train/val) 5.09355/5.07352. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 5.08784/5.09500. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 5.08904/5.10222. Took 1.00 sec\n",
      "Epoch 4, Loss(train/val) 5.08757/5.10325. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 5.08736/5.10354. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 5.08757/5.10394. Took 1.02 sec\n",
      "Epoch 7, Loss(train/val) 5.08814/5.10082. Took 1.02 sec\n",
      "Epoch 8, Loss(train/val) 5.08870/5.09255. Took 1.02 sec\n",
      "Epoch 9, Loss(train/val) 5.08847/5.08307. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 5.08636/5.08068. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 5.08602/5.08259. Took 1.00 sec\n",
      "Epoch 12, Loss(train/val) 5.08673/5.08370. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 5.08571/5.08386. Took 1.02 sec\n",
      "Epoch 14, Loss(train/val) 5.08700/5.08236. Took 1.02 sec\n",
      "Epoch 15, Loss(train/val) 5.08613/5.08387. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 5.08566/5.08464. Took 1.00 sec\n",
      "Epoch 17, Loss(train/val) 5.08509/5.08516. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 5.08420/5.08538. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 5.08637/5.08423. Took 1.03 sec\n",
      "Epoch 20, Loss(train/val) 5.08511/5.08409. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 5.08495/5.08574. Took 1.02 sec\n",
      "Epoch 22, Loss(train/val) 5.08460/5.08447. Took 1.02 sec\n",
      "Epoch 23, Loss(train/val) 5.08495/5.08446. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 5.08444/5.08439. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 5.08413/5.08320. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 5.08241/5.08535. Took 1.02 sec\n",
      "Epoch 27, Loss(train/val) 5.08370/5.08621. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 5.08287/5.08736. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 5.08364/5.08732. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 5.08303/5.08419. Took 1.02 sec\n",
      "Epoch 31, Loss(train/val) 5.08205/5.08508. Took 1.02 sec\n",
      "Epoch 32, Loss(train/val) 5.08336/5.07947. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 5.08102/5.08236. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 5.08135/5.08434. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 5.08243/5.08596. Took 1.00 sec\n",
      "Epoch 36, Loss(train/val) 5.07912/5.08467. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 5.08052/5.08610. Took 1.00 sec\n",
      "Epoch 38, Loss(train/val) 5.08006/5.08903. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 5.08000/5.09138. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 5.08172/5.08878. Took 1.02 sec\n",
      "Epoch 41, Loss(train/val) 5.07935/5.08749. Took 1.03 sec\n",
      "Epoch 42, Loss(train/val) 5.07834/5.09089. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 5.08009/5.09256. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 5.07618/5.09295. Took 1.03 sec\n",
      "Epoch 45, Loss(train/val) 5.08060/5.08239. Took 1.02 sec\n",
      "Epoch 46, Loss(train/val) 5.07679/5.08920. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 5.07815/5.09601. Took 1.02 sec\n",
      "Epoch 48, Loss(train/val) 5.07838/5.09663. Took 1.03 sec\n",
      "Epoch 49, Loss(train/val) 5.07653/5.09487. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 5.07502/5.10385. Took 1.02 sec\n",
      "Epoch 51, Loss(train/val) 5.07611/5.09464. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 5.07675/5.09607. Took 1.02 sec\n",
      "Epoch 53, Loss(train/val) 5.07589/5.09922. Took 1.02 sec\n",
      "Epoch 54, Loss(train/val) 5.07459/5.10350. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 5.07390/5.10209. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 5.07647/5.10597. Took 1.02 sec\n",
      "Epoch 57, Loss(train/val) 5.07531/5.09974. Took 1.02 sec\n",
      "Epoch 58, Loss(train/val) 5.07504/5.10541. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 5.07600/5.09844. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 5.07629/5.10068. Took 1.03 sec\n",
      "Epoch 61, Loss(train/val) 5.07204/5.10035. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 5.07268/5.10722. Took 1.02 sec\n",
      "Epoch 63, Loss(train/val) 5.07331/5.10501. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 5.07552/5.10039. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 5.07312/5.11686. Took 1.02 sec\n",
      "Epoch 66, Loss(train/val) 5.07311/5.10543. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 5.07039/5.11131. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 5.07284/5.11002. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 5.07482/5.10638. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 5.07104/5.10818. Took 1.03 sec\n",
      "Epoch 71, Loss(train/val) 5.07259/5.10388. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 5.07125/5.11009. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 5.07230/5.12387. Took 1.02 sec\n",
      "Epoch 74, Loss(train/val) 5.07072/5.12002. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 5.07084/5.11046. Took 1.12 sec\n",
      "Epoch 76, Loss(train/val) 5.06991/5.11032. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 5.07295/5.11913. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 5.06959/5.11793. Took 1.02 sec\n",
      "Epoch 79, Loss(train/val) 5.07113/5.10972. Took 1.03 sec\n",
      "Epoch 80, Loss(train/val) 5.07345/5.10157. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 5.07243/5.11133. Took 1.02 sec\n",
      "Epoch 82, Loss(train/val) 5.06970/5.12431. Took 1.02 sec\n",
      "Epoch 83, Loss(train/val) 5.07068/5.11199. Took 1.02 sec\n",
      "Epoch 84, Loss(train/val) 5.07124/5.11327. Took 1.03 sec\n",
      "Epoch 85, Loss(train/val) 5.06758/5.11990. Took 1.02 sec\n",
      "Epoch 86, Loss(train/val) 5.07192/5.11650. Took 1.03 sec\n",
      "Epoch 87, Loss(train/val) 5.07254/5.09745. Took 1.02 sec\n",
      "Epoch 88, Loss(train/val) 5.07011/5.11956. Took 1.03 sec\n",
      "Epoch 89, Loss(train/val) 5.06630/5.11135. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 5.07018/5.12783. Took 1.01 sec\n",
      "Epoch 91, Loss(train/val) 5.07330/5.11271. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 5.07050/5.10748. Took 1.02 sec\n",
      "Epoch 93, Loss(train/val) 5.06927/5.10495. Took 1.02 sec\n",
      "Epoch 94, Loss(train/val) 5.07165/5.11963. Took 1.02 sec\n",
      "Epoch 95, Loss(train/val) 5.07081/5.11214. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 5.06991/5.11718. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 5.07068/5.10871. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 5.06986/5.11636. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 5.07192/5.12221. Took 1.01 sec\n",
      "ACC: 0.5625, MCC: 0.12747736726205042\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 4.97255/4.94369. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.94363/4.94464. Took 1.00 sec\n",
      "Epoch 2, Loss(train/val) 4.94534/4.94568. Took 1.00 sec\n",
      "Epoch 3, Loss(train/val) 4.94542/4.94448. Took 1.00 sec\n",
      "Epoch 4, Loss(train/val) 4.94320/4.94391. Took 1.00 sec\n",
      "Epoch 5, Loss(train/val) 4.94474/4.94454. Took 1.00 sec\n",
      "Epoch 6, Loss(train/val) 4.94471/4.94577. Took 1.00 sec\n",
      "Epoch 7, Loss(train/val) 4.94424/4.94675. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 4.94538/4.94866. Took 1.00 sec\n",
      "Epoch 9, Loss(train/val) 4.94437/4.95026. Took 1.02 sec\n",
      "Epoch 10, Loss(train/val) 4.94434/4.94912. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.94420/4.94844. Took 1.00 sec\n",
      "Epoch 12, Loss(train/val) 4.94320/4.94766. Took 1.00 sec\n",
      "Epoch 13, Loss(train/val) 4.94261/4.94873. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 4.94218/4.95090. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.94138/4.95275. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 4.94143/4.95375. Took 1.02 sec\n",
      "Epoch 17, Loss(train/val) 4.94050/4.95587. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 4.94085/4.95734. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 4.93985/4.95950. Took 1.02 sec\n",
      "Epoch 20, Loss(train/val) 4.93910/4.96164. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.93905/4.96406. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.93741/4.96725. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.93806/4.96730. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 4.93766/4.96920. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 4.93624/4.97287. Took 1.00 sec\n",
      "Epoch 26, Loss(train/val) 4.93754/4.96521. Took 1.00 sec\n",
      "Epoch 27, Loss(train/val) 4.93561/4.96322. Took 1.02 sec\n",
      "Epoch 28, Loss(train/val) 4.93771/4.95201. Took 1.02 sec\n",
      "Epoch 29, Loss(train/val) 4.93349/4.97015. Took 1.02 sec\n",
      "Epoch 30, Loss(train/val) 4.93481/4.94939. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.93295/4.96354. Took 1.02 sec\n",
      "Epoch 32, Loss(train/val) 4.93508/4.95588. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.93297/4.96091. Took 1.02 sec\n",
      "Epoch 34, Loss(train/val) 4.93273/4.95601. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 4.92991/4.95819. Took 1.02 sec\n",
      "Epoch 36, Loss(train/val) 4.92958/4.96233. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.93643/4.94612. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 4.93612/4.95229. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.93196/4.95055. Took 1.00 sec\n",
      "Epoch 40, Loss(train/val) 4.93165/4.95378. Took 1.00 sec\n",
      "Epoch 41, Loss(train/val) 4.93023/4.95603. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 4.93025/4.95175. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.92813/4.95777. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.93209/4.95043. Took 1.00 sec\n",
      "Epoch 45, Loss(train/val) 4.93068/4.95547. Took 1.13 sec\n",
      "Epoch 46, Loss(train/val) 4.93123/4.94853. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.92944/4.95088. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 4.92966/4.95190. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.92827/4.94851. Took 1.02 sec\n",
      "Epoch 50, Loss(train/val) 4.92829/4.94871. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.92883/4.95219. Took 1.00 sec\n",
      "Epoch 52, Loss(train/val) 4.92904/4.94619. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 4.92938/4.94848. Took 1.02 sec\n",
      "Epoch 54, Loss(train/val) 4.92615/4.95246. Took 1.04 sec\n",
      "Epoch 55, Loss(train/val) 4.92745/4.95687. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 4.92870/4.94999. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 4.92876/4.94738. Took 1.00 sec\n",
      "Epoch 58, Loss(train/val) 4.92871/4.94832. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.92820/4.94834. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.92913/4.94736. Took 1.02 sec\n",
      "Epoch 61, Loss(train/val) 4.92903/4.95391. Took 1.02 sec\n",
      "Epoch 62, Loss(train/val) 4.93005/4.94974. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.92724/4.94733. Took 1.00 sec\n",
      "Epoch 64, Loss(train/val) 4.92856/4.95003. Took 1.03 sec\n",
      "Epoch 65, Loss(train/val) 4.92380/4.94976. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 4.92517/4.95266. Took 1.03 sec\n",
      "Epoch 67, Loss(train/val) 4.92605/4.95427. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 4.92366/4.95472. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 4.92915/4.95441. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 4.92561/4.95995. Took 1.02 sec\n",
      "Epoch 71, Loss(train/val) 4.92645/4.95382. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 4.92593/4.95341. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.92557/4.95213. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.92785/4.94733. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.92518/4.95090. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 4.92590/4.94596. Took 1.02 sec\n",
      "Epoch 77, Loss(train/val) 4.92532/4.94531. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.92593/4.95171. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 4.92303/4.95896. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.92735/4.95735. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 4.92560/4.95680. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.92638/4.94793. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 4.92598/4.95353. Took 1.02 sec\n",
      "Epoch 84, Loss(train/val) 4.92187/4.95787. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 4.92459/4.95829. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 4.93185/4.94947. Took 1.04 sec\n",
      "Epoch 87, Loss(train/val) 4.92964/4.94477. Took 1.03 sec\n",
      "Epoch 88, Loss(train/val) 4.92930/4.94838. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.93948/4.95273. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 4.93783/4.94735. Took 1.03 sec\n",
      "Epoch 91, Loss(train/val) 4.93758/4.94763. Took 1.00 sec\n",
      "Epoch 92, Loss(train/val) 4.93577/4.95149. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 4.93339/4.95467. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.93410/4.94451. Took 1.00 sec\n",
      "Epoch 95, Loss(train/val) 4.93237/4.95628. Took 1.03 sec\n",
      "Epoch 96, Loss(train/val) 4.93098/4.95861. Took 1.02 sec\n",
      "Epoch 97, Loss(train/val) 4.93253/4.95269. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 4.93091/4.95515. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 4.92935/4.94895. Took 1.02 sec\n",
      "ACC: 0.5703125, MCC: 0.14864156009117363\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.13073/5.09086. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 5.08657/5.08098. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 5.08286/5.07813. Took 1.02 sec\n",
      "Epoch 3, Loss(train/val) 5.08182/5.07951. Took 1.02 sec\n",
      "Epoch 4, Loss(train/val) 5.08119/5.08097. Took 1.02 sec\n",
      "Epoch 5, Loss(train/val) 5.08186/5.08195. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 5.08043/5.08288. Took 1.02 sec\n",
      "Epoch 7, Loss(train/val) 5.08181/5.08395. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 5.08041/5.08140. Took 1.00 sec\n",
      "Epoch 9, Loss(train/val) 5.08174/5.07912. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 5.08034/5.07679. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 5.07872/5.07399. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 5.07882/5.07341. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 5.07803/5.07175. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 5.07888/5.07091. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 5.07786/5.06943. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 5.07825/5.06694. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 5.07678/5.06339. Took 1.12 sec\n",
      "Epoch 18, Loss(train/val) 5.07660/5.06580. Took 1.02 sec\n",
      "Epoch 19, Loss(train/val) 5.07613/5.06010. Took 1.02 sec\n",
      "Epoch 20, Loss(train/val) 5.07488/5.05451. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 5.07289/5.05723. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 5.07439/5.05934. Took 1.02 sec\n",
      "Epoch 23, Loss(train/val) 5.07393/5.05689. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 5.07351/5.06018. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 5.07282/5.05566. Took 1.02 sec\n",
      "Epoch 26, Loss(train/val) 5.07151/5.05729. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 5.07436/5.05748. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 5.07293/5.05807. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 5.07018/5.05285. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 5.07107/5.05499. Took 1.02 sec\n",
      "Epoch 31, Loss(train/val) 5.07142/5.05501. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 5.06777/5.05157. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 5.07082/5.04956. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 5.07091/5.05589. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 5.07150/5.05371. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 5.06172/5.04161. Took 1.00 sec\n",
      "Epoch 37, Loss(train/val) 5.06986/5.05622. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 5.07013/5.05009. Took 1.02 sec\n",
      "Epoch 39, Loss(train/val) 5.06759/5.05159. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 5.06834/5.04893. Took 1.02 sec\n",
      "Epoch 41, Loss(train/val) 5.06938/5.05596. Took 1.00 sec\n",
      "Epoch 42, Loss(train/val) 5.07082/5.05788. Took 1.03 sec\n",
      "Epoch 43, Loss(train/val) 5.06874/5.04758. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 5.06596/5.04531. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 5.06438/5.05053. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 5.06790/5.05634. Took 1.00 sec\n",
      "Epoch 47, Loss(train/val) 5.06598/5.05761. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 5.06762/5.05173. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 5.06746/5.05111. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 5.06711/5.05508. Took 1.00 sec\n",
      "Epoch 51, Loss(train/val) 5.06629/5.05240. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 5.06651/5.04875. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 5.06440/5.05515. Took 1.02 sec\n",
      "Epoch 54, Loss(train/val) 5.06608/5.04681. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 5.06421/5.04170. Took 1.00 sec\n",
      "Epoch 56, Loss(train/val) 5.06439/5.04528. Took 1.03 sec\n",
      "Epoch 57, Loss(train/val) 5.06667/5.05272. Took 1.00 sec\n",
      "Epoch 58, Loss(train/val) 5.06466/5.04651. Took 1.02 sec\n",
      "Epoch 59, Loss(train/val) 5.06406/5.04702. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 5.06534/5.04820. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 5.06402/5.04930. Took 1.02 sec\n",
      "Epoch 62, Loss(train/val) 5.06349/5.04666. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 5.06177/5.04932. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 5.06140/5.04278. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 5.06063/5.04274. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 5.06415/5.04288. Took 1.02 sec\n",
      "Epoch 67, Loss(train/val) 5.06563/5.04749. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 5.06258/5.04424. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 5.06399/5.04340. Took 1.02 sec\n",
      "Epoch 70, Loss(train/val) 5.06294/5.04815. Took 1.03 sec\n",
      "Epoch 71, Loss(train/val) 5.06361/5.04597. Took 1.00 sec\n",
      "Epoch 72, Loss(train/val) 5.06138/5.04397. Took 1.02 sec\n",
      "Epoch 73, Loss(train/val) 5.06376/5.04857. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 5.06425/5.04939. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 5.06345/5.04828. Took 1.02 sec\n",
      "Epoch 76, Loss(train/val) 5.06369/5.05304. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 5.06474/5.05367. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 5.06462/5.04948. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 5.06325/5.05101. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 5.06470/5.04925. Took 1.02 sec\n",
      "Epoch 81, Loss(train/val) 5.06337/5.04374. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 5.06059/5.04972. Took 1.00 sec\n",
      "Epoch 83, Loss(train/val) 5.06465/5.04303. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 5.06342/5.04641. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 5.06055/5.04581. Took 1.03 sec\n",
      "Epoch 86, Loss(train/val) 5.06056/5.04424. Took 1.02 sec\n",
      "Epoch 87, Loss(train/val) 5.06172/5.04330. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 5.06454/5.05890. Took 1.00 sec\n",
      "Epoch 89, Loss(train/val) 5.06357/5.04718. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 5.06297/5.04303. Took 1.02 sec\n",
      "Epoch 91, Loss(train/val) 5.06257/5.04895. Took 1.02 sec\n",
      "Epoch 92, Loss(train/val) 5.06055/5.03976. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 5.06163/5.05117. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 5.06355/5.04523. Took 1.13 sec\n",
      "Epoch 95, Loss(train/val) 5.06386/5.05035. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 5.06013/5.04270. Took 1.00 sec\n",
      "Epoch 97, Loss(train/val) 5.06170/5.04401. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 5.05882/5.04368. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 5.06262/5.04861. Took 1.01 sec\n",
      "ACC: 0.4375, MCC: -0.13156218570112832\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.13782/5.10717. Took 1.01 sec\n",
      "Epoch 1, Loss(train/val) 5.10825/5.11704. Took 1.00 sec\n",
      "Epoch 2, Loss(train/val) 5.10676/5.12438. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 5.10505/5.12682. Took 1.00 sec\n",
      "Epoch 4, Loss(train/val) 5.10422/5.12735. Took 1.02 sec\n",
      "Epoch 5, Loss(train/val) 5.10550/5.12678. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 5.10379/5.12912. Took 1.00 sec\n",
      "Epoch 7, Loss(train/val) 5.10392/5.12927. Took 1.00 sec\n",
      "Epoch 8, Loss(train/val) 5.10411/5.13167. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 5.10308/5.13033. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 5.10138/5.13063. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 5.10227/5.13050. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 5.10231/5.12983. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 5.10242/5.12998. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 5.10079/5.13286. Took 1.00 sec\n",
      "Epoch 15, Loss(train/val) 5.10116/5.12834. Took 1.02 sec\n",
      "Epoch 16, Loss(train/val) 5.09825/5.12877. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 5.10020/5.12867. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 5.09631/5.12722. Took 1.02 sec\n",
      "Epoch 19, Loss(train/val) 5.09956/5.12701. Took 1.02 sec\n",
      "Epoch 20, Loss(train/val) 5.09891/5.12456. Took 1.00 sec\n",
      "Epoch 21, Loss(train/val) 5.09868/5.12233. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 5.09843/5.12419. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 5.09842/5.11902. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 5.09960/5.11680. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 5.09554/5.12388. Took 1.02 sec\n",
      "Epoch 26, Loss(train/val) 5.09535/5.12485. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 5.09762/5.12350. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 5.09697/5.12314. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 5.09898/5.11702. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 5.09494/5.12231. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 5.09960/5.11048. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 5.09667/5.11745. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 5.09391/5.11801. Took 1.00 sec\n",
      "Epoch 34, Loss(train/val) 5.09676/5.11464. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 5.09360/5.11781. Took 1.00 sec\n",
      "Epoch 36, Loss(train/val) 5.09450/5.11663. Took 1.00 sec\n",
      "Epoch 37, Loss(train/val) 5.09419/5.11820. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 5.09642/5.12101. Took 1.00 sec\n",
      "Epoch 39, Loss(train/val) 5.09469/5.11934. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 5.09353/5.12081. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 5.09281/5.11656. Took 1.02 sec\n",
      "Epoch 42, Loss(train/val) 5.09291/5.11684. Took 1.00 sec\n",
      "Epoch 43, Loss(train/val) 5.09374/5.11990. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 5.09219/5.11376. Took 1.02 sec\n",
      "Epoch 45, Loss(train/val) 5.09387/5.11353. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 5.09298/5.11704. Took 1.00 sec\n",
      "Epoch 47, Loss(train/val) 5.09240/5.11798. Took 1.02 sec\n",
      "Epoch 48, Loss(train/val) 5.09126/5.11231. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 5.09351/5.11573. Took 1.00 sec\n",
      "Epoch 50, Loss(train/val) 5.09226/5.11962. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 5.09392/5.11560. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 5.09189/5.11328. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 5.09176/5.11690. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 5.09186/5.11682. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 5.09347/5.11031. Took 1.03 sec\n",
      "Epoch 56, Loss(train/val) 5.09168/5.12418. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 5.09058/5.11312. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 5.09009/5.11982. Took 1.02 sec\n",
      "Epoch 59, Loss(train/val) 5.09048/5.12160. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 5.08868/5.12330. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 5.09023/5.11500. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 5.09162/5.11855. Took 1.00 sec\n",
      "Epoch 63, Loss(train/val) 5.09189/5.11608. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 5.09033/5.11554. Took 1.02 sec\n",
      "Epoch 65, Loss(train/val) 5.08941/5.11419. Took 1.00 sec\n",
      "Epoch 66, Loss(train/val) 5.09097/5.11830. Took 1.12 sec\n",
      "Epoch 67, Loss(train/val) 5.09043/5.11755. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 5.08915/5.11331. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 5.08834/5.12089. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 5.08886/5.11306. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 5.09996/5.11632. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 5.09736/5.11859. Took 1.02 sec\n",
      "Epoch 73, Loss(train/val) 5.09595/5.10967. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 5.09536/5.11101. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 5.09055/5.12146. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 5.10787/5.11237. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 5.10035/5.11974. Took 1.04 sec\n",
      "Epoch 78, Loss(train/val) 5.09742/5.12059. Took 1.02 sec\n",
      "Epoch 79, Loss(train/val) 5.09797/5.11870. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 5.09228/5.12567. Took 1.02 sec\n",
      "Epoch 81, Loss(train/val) 5.09181/5.12021. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 5.09082/5.11179. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 5.09117/5.11145. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 5.09141/5.11757. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 5.09234/5.11145. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 5.08982/5.11640. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 5.09109/5.11228. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 5.09482/5.12016. Took 1.05 sec\n",
      "Epoch 89, Loss(train/val) 5.09495/5.12635. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 5.09072/5.12162. Took 1.02 sec\n",
      "Epoch 91, Loss(train/val) 5.08912/5.12173. Took 1.00 sec\n",
      "Epoch 92, Loss(train/val) 5.09135/5.11228. Took 1.02 sec\n",
      "Epoch 93, Loss(train/val) 5.08903/5.11651. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 5.08667/5.11696. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 5.08596/5.11194. Took 1.02 sec\n",
      "Epoch 96, Loss(train/val) 5.08821/5.11372. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 5.08767/5.10681. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 5.09193/5.10963. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 5.08767/5.10701. Took 1.01 sec\n",
      "ACC: 0.453125, MCC: -0.07654259938766833\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 4.93636/4.88955. Took 1.03 sec\n",
      "Epoch 1, Loss(train/val) 4.90311/4.89522. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 4.90011/4.89523. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 4.89860/4.89572. Took 1.02 sec\n",
      "Epoch 4, Loss(train/val) 4.89898/4.89394. Took 1.02 sec\n",
      "Epoch 5, Loss(train/val) 4.89818/4.89173. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.89860/4.89075. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.89883/4.89109. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 4.89949/4.89175. Took 1.02 sec\n",
      "Epoch 9, Loss(train/val) 4.89806/4.89105. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 4.89876/4.89222. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.89693/4.89143. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 4.89821/4.89036. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.89746/4.89251. Took 1.01 sec\n",
      "Epoch 14, Loss(train/val) 4.89817/4.89391. Took 1.02 sec\n",
      "Epoch 15, Loss(train/val) 4.89774/4.89373. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 4.89759/4.89369. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 4.89707/4.89427. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 4.89698/4.89533. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 4.89720/4.89535. Took 1.02 sec\n",
      "Epoch 20, Loss(train/val) 4.89733/4.89561. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.89658/4.89353. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.89665/4.89454. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.89681/4.89428. Took 1.02 sec\n",
      "Epoch 24, Loss(train/val) 4.89634/4.89408. Took 1.02 sec\n",
      "Epoch 25, Loss(train/val) 4.89543/4.89374. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 4.89679/4.89472. Took 1.02 sec\n",
      "Epoch 27, Loss(train/val) 4.89555/4.89549. Took 1.02 sec\n",
      "Epoch 28, Loss(train/val) 4.89751/4.89564. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.89644/4.89531. Took 1.01 sec\n",
      "Epoch 30, Loss(train/val) 4.89670/4.89464. Took 1.02 sec\n",
      "Epoch 31, Loss(train/val) 4.89533/4.89237. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 4.89521/4.89211. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.89561/4.89208. Took 1.02 sec\n",
      "Epoch 34, Loss(train/val) 4.89578/4.89256. Took 1.02 sec\n",
      "Epoch 35, Loss(train/val) 4.89584/4.89320. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 4.89629/4.89292. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.89534/4.89147. Took 1.02 sec\n",
      "Epoch 38, Loss(train/val) 4.89522/4.88899. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.89857/4.88775. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 4.89756/4.89180. Took 1.13 sec\n",
      "Epoch 41, Loss(train/val) 4.89685/4.88967. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 4.89709/4.88982. Took 1.02 sec\n",
      "Epoch 43, Loss(train/val) 4.89671/4.88937. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.89575/4.88882. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.89511/4.88840. Took 1.02 sec\n",
      "Epoch 46, Loss(train/val) 4.89515/4.88762. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.89519/4.89155. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 4.89589/4.88894. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.89533/4.89097. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 4.89581/4.89308. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.89478/4.89233. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 4.89561/4.89197. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 4.89417/4.89240. Took 1.06 sec\n",
      "Epoch 54, Loss(train/val) 4.89319/4.89372. Took 1.02 sec\n",
      "Epoch 55, Loss(train/val) 4.89419/4.89313. Took 1.03 sec\n",
      "Epoch 56, Loss(train/val) 4.89423/4.89385. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 4.89443/4.89528. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 4.89388/4.89625. Took 1.03 sec\n",
      "Epoch 59, Loss(train/val) 4.89251/4.89682. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.89272/4.89714. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 4.89344/4.89739. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 4.89428/4.89612. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.89286/4.89598. Took 1.02 sec\n",
      "Epoch 64, Loss(train/val) 4.89183/4.89781. Took 1.03 sec\n",
      "Epoch 65, Loss(train/val) 4.89255/4.89743. Took 1.02 sec\n",
      "Epoch 66, Loss(train/val) 4.89183/4.89580. Took 1.00 sec\n",
      "Epoch 67, Loss(train/val) 4.89288/4.89973. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 4.89052/4.89762. Took 1.02 sec\n",
      "Epoch 69, Loss(train/val) 4.89185/4.89863. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 4.89095/4.89986. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 4.89023/4.89690. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 4.89219/4.89603. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.89010/4.89561. Took 1.02 sec\n",
      "Epoch 74, Loss(train/val) 4.89341/4.89810. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.89221/4.89903. Took 1.02 sec\n",
      "Epoch 76, Loss(train/val) 4.89120/4.89827. Took 1.00 sec\n",
      "Epoch 77, Loss(train/val) 4.89165/4.90078. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.88999/4.89915. Took 1.01 sec\n",
      "Epoch 79, Loss(train/val) 4.89241/4.89970. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.89146/4.89723. Took 1.03 sec\n",
      "Epoch 81, Loss(train/val) 4.89250/4.89749. Took 1.02 sec\n",
      "Epoch 82, Loss(train/val) 4.89151/4.90095. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 4.88861/4.89915. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.89019/4.90125. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 4.88949/4.89759. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 4.88925/4.90097. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 4.88895/4.90538. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.89170/4.89643. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.88928/4.90356. Took 1.02 sec\n",
      "Epoch 90, Loss(train/val) 4.89044/4.90093. Took 1.02 sec\n",
      "Epoch 91, Loss(train/val) 4.88902/4.90576. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 4.88967/4.90168. Took 1.00 sec\n",
      "Epoch 93, Loss(train/val) 4.89069/4.89886. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.88867/4.90421. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 4.89039/4.90406. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 4.88819/4.90420. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.89050/4.89999. Took 1.01 sec\n",
      "Epoch 98, Loss(train/val) 4.89037/4.89931. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 4.88901/4.90515. Took 1.01 sec\n",
      "ACC: 0.4609375, MCC: -0.08479880400338806\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 4.98877/4.95778. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.96806/4.95976. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 4.96695/4.96558. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 4.96679/4.97065. Took 1.02 sec\n",
      "Epoch 4, Loss(train/val) 4.96383/4.97001. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 4.96246/4.96835. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.96302/4.96721. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.96219/4.96764. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 4.96129/4.96993. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.96110/4.96973. Took 1.02 sec\n",
      "Epoch 10, Loss(train/val) 4.96181/4.97055. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 4.96132/4.96979. Took 1.02 sec\n",
      "Epoch 12, Loss(train/val) 4.96202/4.96952. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.96127/4.97157. Took 1.13 sec\n",
      "Epoch 14, Loss(train/val) 4.96171/4.97152. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.96074/4.97264. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 4.96071/4.97188. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 4.95984/4.97258. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 4.95974/4.97051. Took 1.04 sec\n",
      "Epoch 19, Loss(train/val) 4.95925/4.96913. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 4.95989/4.96855. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.95931/4.96915. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.95808/4.96880. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.95949/4.96904. Took 1.02 sec\n",
      "Epoch 24, Loss(train/val) 4.95854/4.96603. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 4.95738/4.96736. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 4.95921/4.96601. Took 1.02 sec\n",
      "Epoch 27, Loss(train/val) 4.95794/4.96713. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 4.95789/4.96577. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.95811/4.96562. Took 1.02 sec\n",
      "Epoch 30, Loss(train/val) 4.95772/4.96456. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.95766/4.96394. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 4.95672/4.96517. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.95588/4.96727. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.95736/4.96311. Took 1.03 sec\n",
      "Epoch 35, Loss(train/val) 4.95468/4.96891. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 4.95631/4.96217. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.95543/4.96618. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 4.95611/4.96080. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 4.95511/4.97005. Took 1.00 sec\n",
      "Epoch 40, Loss(train/val) 4.95724/4.96334. Took 1.02 sec\n",
      "Epoch 41, Loss(train/val) 4.95482/4.97321. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 4.95550/4.96117. Took 1.02 sec\n",
      "Epoch 43, Loss(train/val) 4.95432/4.96776. Took 1.02 sec\n",
      "Epoch 44, Loss(train/val) 4.95433/4.96237. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.95486/4.97442. Took 1.00 sec\n",
      "Epoch 46, Loss(train/val) 4.95716/4.96534. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.95503/4.96920. Took 1.01 sec\n",
      "Epoch 48, Loss(train/val) 4.95458/4.96807. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.95547/4.96700. Took 1.02 sec\n",
      "Epoch 50, Loss(train/val) 4.95505/4.96547. Took 1.03 sec\n",
      "Epoch 51, Loss(train/val) 4.95331/4.97085. Took 1.00 sec\n",
      "Epoch 52, Loss(train/val) 4.95358/4.97008. Took 1.01 sec\n",
      "Epoch 53, Loss(train/val) 4.95438/4.96849. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 4.95316/4.96498. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 4.95654/4.96636. Took 1.03 sec\n",
      "Epoch 56, Loss(train/val) 4.95334/4.97092. Took 1.02 sec\n",
      "Epoch 57, Loss(train/val) 4.95188/4.96818. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 4.95478/4.96527. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.95255/4.96967. Took 1.02 sec\n",
      "Epoch 60, Loss(train/val) 4.95511/4.96430. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 4.95299/4.97115. Took 1.02 sec\n",
      "Epoch 62, Loss(train/val) 4.95168/4.97223. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.95492/4.96975. Took 1.02 sec\n",
      "Epoch 64, Loss(train/val) 4.95420/4.97145. Took 1.03 sec\n",
      "Epoch 65, Loss(train/val) 4.95441/4.96886. Took 1.02 sec\n",
      "Epoch 66, Loss(train/val) 4.95201/4.96892. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 4.95303/4.97039. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 4.95201/4.97218. Took 1.03 sec\n",
      "Epoch 69, Loss(train/val) 4.95305/4.97603. Took 1.02 sec\n",
      "Epoch 70, Loss(train/val) 4.95188/4.97797. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 4.95256/4.97356. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 4.95356/4.97241. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.95052/4.97821. Took 1.02 sec\n",
      "Epoch 74, Loss(train/val) 4.95158/4.97956. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.95084/4.97385. Took 1.02 sec\n",
      "Epoch 76, Loss(train/val) 4.94945/4.98210. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 4.95154/4.98450. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.95367/4.97543. Took 1.02 sec\n",
      "Epoch 79, Loss(train/val) 4.95201/4.97899. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.95091/4.97650. Took 1.02 sec\n",
      "Epoch 81, Loss(train/val) 4.94977/4.97779. Took 1.01 sec\n",
      "Epoch 82, Loss(train/val) 4.95022/4.97631. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 4.95082/4.97216. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.95043/4.97472. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 4.94854/4.98328. Took 1.02 sec\n",
      "Epoch 86, Loss(train/val) 4.95174/4.97418. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 4.95076/4.98249. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.94800/4.98564. Took 1.03 sec\n",
      "Epoch 89, Loss(train/val) 4.94968/4.97624. Took 1.02 sec\n",
      "Epoch 90, Loss(train/val) 4.94947/4.98511. Took 1.02 sec\n",
      "Epoch 91, Loss(train/val) 4.94862/4.97789. Took 1.02 sec\n",
      "Epoch 92, Loss(train/val) 4.94946/4.98274. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 4.95006/4.97865. Took 1.12 sec\n",
      "Epoch 94, Loss(train/val) 4.94829/4.97741. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 4.94857/4.98201. Took 1.02 sec\n",
      "Epoch 96, Loss(train/val) 4.94855/4.99314. Took 1.04 sec\n",
      "Epoch 97, Loss(train/val) 4.95068/4.97303. Took 1.03 sec\n",
      "Epoch 98, Loss(train/val) 4.94799/4.98157. Took 1.03 sec\n",
      "Epoch 99, Loss(train/val) 4.94921/4.98722. Took 1.03 sec\n",
      "ACC: 0.453125, MCC: -0.10046671579464506\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 4.95384/4.91397. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.93663/4.91875. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 4.93481/4.92530. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 4.93440/4.93137. Took 1.00 sec\n",
      "Epoch 4, Loss(train/val) 4.93287/4.93269. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 4.93402/4.93204. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 4.93362/4.93076. Took 1.00 sec\n",
      "Epoch 7, Loss(train/val) 4.93268/4.93125. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 4.93140/4.93007. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 4.93165/4.92955. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 4.93246/4.93096. Took 1.00 sec\n",
      "Epoch 11, Loss(train/val) 4.93081/4.93244. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 4.93163/4.92981. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 4.93040/4.92745. Took 1.00 sec\n",
      "Epoch 14, Loss(train/val) 4.93177/4.92845. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.93124/4.92915. Took 1.02 sec\n",
      "Epoch 16, Loss(train/val) 4.92980/4.93011. Took 1.02 sec\n",
      "Epoch 17, Loss(train/val) 4.92973/4.93272. Took 1.00 sec\n",
      "Epoch 18, Loss(train/val) 4.92768/4.92958. Took 1.01 sec\n",
      "Epoch 19, Loss(train/val) 4.92817/4.92974. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 4.93013/4.93537. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.92906/4.93569. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.92895/4.93837. Took 1.01 sec\n",
      "Epoch 23, Loss(train/val) 4.92823/4.93842. Took 1.02 sec\n",
      "Epoch 24, Loss(train/val) 4.92625/4.94103. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 4.92493/4.93917. Took 1.02 sec\n",
      "Epoch 26, Loss(train/val) 4.92677/4.93149. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 4.92642/4.93780. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 4.92316/4.92809. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 4.92705/4.93228. Took 1.00 sec\n",
      "Epoch 30, Loss(train/val) 4.92491/4.93924. Took 1.00 sec\n",
      "Epoch 31, Loss(train/val) 4.92458/4.93646. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 4.92442/4.93500. Took 1.02 sec\n",
      "Epoch 33, Loss(train/val) 4.92417/4.93590. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.92725/4.94370. Took 1.03 sec\n",
      "Epoch 35, Loss(train/val) 4.92295/4.95092. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 4.92405/4.94929. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.93066/4.94600. Took 1.00 sec\n",
      "Epoch 38, Loss(train/val) 4.92696/4.93706. Took 1.00 sec\n",
      "Epoch 39, Loss(train/val) 4.92494/4.94151. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 4.92500/4.95020. Took 1.00 sec\n",
      "Epoch 41, Loss(train/val) 4.92123/4.94510. Took 1.02 sec\n",
      "Epoch 42, Loss(train/val) 4.92377/4.93838. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.92308/4.94065. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.92212/4.94351. Took 1.02 sec\n",
      "Epoch 45, Loss(train/val) 4.91918/4.94616. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 4.92197/4.93451. Took 1.01 sec\n",
      "Epoch 47, Loss(train/val) 4.92043/4.94169. Took 1.03 sec\n",
      "Epoch 48, Loss(train/val) 4.92293/4.95206. Took 1.01 sec\n",
      "Epoch 49, Loss(train/val) 4.92275/4.94491. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 4.91830/4.94521. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.91995/4.94976. Took 1.00 sec\n",
      "Epoch 52, Loss(train/val) 4.91666/4.95325. Took 1.02 sec\n",
      "Epoch 53, Loss(train/val) 4.92334/4.93375. Took 1.02 sec\n",
      "Epoch 54, Loss(train/val) 4.92080/4.94510. Took 1.02 sec\n",
      "Epoch 55, Loss(train/val) 4.92140/4.94196. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 4.91902/4.94211. Took 1.00 sec\n",
      "Epoch 57, Loss(train/val) 4.92148/4.94899. Took 1.01 sec\n",
      "Epoch 58, Loss(train/val) 4.92027/4.94554. Took 1.02 sec\n",
      "Epoch 59, Loss(train/val) 4.92139/4.94515. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 4.91918/4.94196. Took 1.02 sec\n",
      "Epoch 61, Loss(train/val) 4.91927/4.93505. Took 1.00 sec\n",
      "Epoch 62, Loss(train/val) 4.91929/4.95031. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.91889/4.94283. Took 1.00 sec\n",
      "Epoch 64, Loss(train/val) 4.91648/4.94147. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 4.91674/4.94217. Took 1.02 sec\n",
      "Epoch 66, Loss(train/val) 4.91789/4.94444. Took 1.01 sec\n",
      "Epoch 67, Loss(train/val) 4.91894/4.93819. Took 1.14 sec\n",
      "Epoch 68, Loss(train/val) 4.91583/4.93805. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 4.91600/4.94959. Took 1.03 sec\n",
      "Epoch 70, Loss(train/val) 4.91762/4.95253. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 4.92029/4.93935. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 4.91535/4.93577. Took 1.02 sec\n",
      "Epoch 73, Loss(train/val) 4.91772/4.94306. Took 1.01 sec\n",
      "Epoch 74, Loss(train/val) 4.91666/4.94133. Took 1.01 sec\n",
      "Epoch 75, Loss(train/val) 4.91803/4.94360. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 4.91835/4.93720. Took 1.03 sec\n",
      "Epoch 77, Loss(train/val) 4.91511/4.95004. Took 1.01 sec\n",
      "Epoch 78, Loss(train/val) 4.91788/4.94345. Took 1.00 sec\n",
      "Epoch 79, Loss(train/val) 4.91758/4.94234. Took 1.02 sec\n",
      "Epoch 80, Loss(train/val) 4.91759/4.94406. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 4.91969/4.94066. Took 1.03 sec\n",
      "Epoch 82, Loss(train/val) 4.91600/4.94732. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 4.91645/4.94323. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.91284/4.95267. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 4.91528/4.93960. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 4.91760/4.95472. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 4.91553/4.94733. Took 1.02 sec\n",
      "Epoch 88, Loss(train/val) 4.91573/4.94520. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 4.91423/4.93991. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 4.91475/4.93642. Took 1.02 sec\n",
      "Epoch 91, Loss(train/val) 4.91306/4.94739. Took 1.00 sec\n",
      "Epoch 92, Loss(train/val) 4.91183/4.94830. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 4.91066/4.95092. Took 1.02 sec\n",
      "Epoch 94, Loss(train/val) 4.91848/4.94910. Took 1.02 sec\n",
      "Epoch 95, Loss(train/val) 4.91452/4.94597. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 4.91401/4.94799. Took 1.02 sec\n",
      "Epoch 97, Loss(train/val) 4.91329/4.94628. Took 1.03 sec\n",
      "Epoch 98, Loss(train/val) 4.91307/4.95482. Took 1.03 sec\n",
      "Epoch 99, Loss(train/val) 4.91321/4.94262. Took 1.02 sec\n",
      "ACC: 0.4765625, MCC: -0.031364542844116654\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 5.12072/5.05341. Took 1.03 sec\n",
      "Epoch 1, Loss(train/val) 5.05047/5.05282. Took 1.01 sec\n",
      "Epoch 2, Loss(train/val) 5.04651/5.05226. Took 1.03 sec\n",
      "Epoch 3, Loss(train/val) 5.04470/5.05400. Took 1.01 sec\n",
      "Epoch 4, Loss(train/val) 5.04461/5.05425. Took 1.01 sec\n",
      "Epoch 5, Loss(train/val) 5.04556/5.05495. Took 1.01 sec\n",
      "Epoch 6, Loss(train/val) 5.04339/5.05730. Took 1.02 sec\n",
      "Epoch 7, Loss(train/val) 5.04474/5.05949. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 5.04388/5.06016. Took 1.01 sec\n",
      "Epoch 9, Loss(train/val) 5.04354/5.06238. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 5.04464/5.06299. Took 1.01 sec\n",
      "Epoch 11, Loss(train/val) 5.04287/5.06430. Took 1.01 sec\n",
      "Epoch 12, Loss(train/val) 5.04242/5.06461. Took 1.01 sec\n",
      "Epoch 13, Loss(train/val) 5.04537/5.06403. Took 1.03 sec\n",
      "Epoch 14, Loss(train/val) 5.04256/5.06542. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 5.04275/5.06618. Took 1.01 sec\n",
      "Epoch 16, Loss(train/val) 5.04462/5.06647. Took 1.02 sec\n",
      "Epoch 17, Loss(train/val) 5.04171/5.06825. Took 1.01 sec\n",
      "Epoch 18, Loss(train/val) 5.04183/5.06848. Took 1.02 sec\n",
      "Epoch 19, Loss(train/val) 5.04323/5.07011. Took 1.01 sec\n",
      "Epoch 20, Loss(train/val) 5.04247/5.06798. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 5.04254/5.07058. Took 1.02 sec\n",
      "Epoch 22, Loss(train/val) 5.04196/5.07045. Took 1.02 sec\n",
      "Epoch 23, Loss(train/val) 5.04203/5.06827. Took 1.01 sec\n",
      "Epoch 24, Loss(train/val) 5.04188/5.07067. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 5.04116/5.06745. Took 1.01 sec\n",
      "Epoch 26, Loss(train/val) 5.04145/5.07031. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 5.03965/5.07064. Took 1.02 sec\n",
      "Epoch 28, Loss(train/val) 5.04050/5.07154. Took 1.01 sec\n",
      "Epoch 29, Loss(train/val) 5.04042/5.07298. Took 1.02 sec\n",
      "Epoch 30, Loss(train/val) 5.04053/5.07102. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 5.03979/5.07326. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 5.03740/5.07472. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 5.03782/5.07268. Took 1.02 sec\n",
      "Epoch 34, Loss(train/val) 5.03959/5.07186. Took 1.02 sec\n",
      "Epoch 35, Loss(train/val) 5.04087/5.06993. Took 1.02 sec\n",
      "Epoch 36, Loss(train/val) 5.03847/5.07032. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 5.04035/5.06939. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 5.04040/5.07073. Took 1.01 sec\n",
      "Epoch 39, Loss(train/val) 5.04007/5.07309. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 5.03907/5.07221. Took 1.01 sec\n",
      "Epoch 41, Loss(train/val) 5.03858/5.07186. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 5.03787/5.07184. Took 1.13 sec\n",
      "Epoch 43, Loss(train/val) 5.03919/5.07305. Took 1.02 sec\n",
      "Epoch 44, Loss(train/val) 5.03715/5.07279. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 5.03775/5.07002. Took 1.02 sec\n",
      "Epoch 46, Loss(train/val) 5.04039/5.07070. Took 1.04 sec\n",
      "Epoch 47, Loss(train/val) 5.03806/5.07537. Took 1.03 sec\n",
      "Epoch 48, Loss(train/val) 5.03815/5.07232. Took 1.02 sec\n",
      "Epoch 49, Loss(train/val) 5.03636/5.07305. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 5.03542/5.07735. Took 1.02 sec\n",
      "Epoch 51, Loss(train/val) 5.03920/5.07777. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 5.03738/5.08172. Took 1.02 sec\n",
      "Epoch 53, Loss(train/val) 5.03421/5.08347. Took 1.01 sec\n",
      "Epoch 54, Loss(train/val) 5.03604/5.07858. Took 1.02 sec\n",
      "Epoch 55, Loss(train/val) 5.04391/5.07236. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 5.03726/5.07845. Took 1.01 sec\n",
      "Epoch 57, Loss(train/val) 5.03925/5.07963. Took 1.02 sec\n",
      "Epoch 58, Loss(train/val) 5.03840/5.08082. Took 1.02 sec\n",
      "Epoch 59, Loss(train/val) 5.03825/5.08088. Took 1.01 sec\n",
      "Epoch 60, Loss(train/val) 5.03471/5.08039. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 5.03876/5.08144. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 5.03687/5.07803. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 5.03510/5.07641. Took 1.02 sec\n",
      "Epoch 64, Loss(train/val) 5.03539/5.08045. Took 1.01 sec\n",
      "Epoch 65, Loss(train/val) 5.03359/5.09310. Took 1.01 sec\n",
      "Epoch 66, Loss(train/val) 5.03285/5.08335. Took 1.03 sec\n",
      "Epoch 67, Loss(train/val) 5.03602/5.08865. Took 1.01 sec\n",
      "Epoch 68, Loss(train/val) 5.03552/5.08301. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 5.03589/5.08743. Took 1.01 sec\n",
      "Epoch 70, Loss(train/val) 5.03575/5.08290. Took 1.01 sec\n",
      "Epoch 71, Loss(train/val) 5.03320/5.09432. Took 1.01 sec\n",
      "Epoch 72, Loss(train/val) 5.03574/5.09145. Took 1.00 sec\n",
      "Epoch 73, Loss(train/val) 5.03377/5.09062. Took 1.00 sec\n",
      "Epoch 74, Loss(train/val) 5.03602/5.08759. Took 1.02 sec\n",
      "Epoch 75, Loss(train/val) 5.03401/5.09153. Took 1.00 sec\n",
      "Epoch 76, Loss(train/val) 5.03933/5.07403. Took 1.02 sec\n",
      "Epoch 77, Loss(train/val) 5.03488/5.08694. Took 1.02 sec\n",
      "Epoch 78, Loss(train/val) 5.03445/5.09528. Took 1.02 sec\n",
      "Epoch 79, Loss(train/val) 5.03521/5.09133. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 5.03306/5.10272. Took 1.03 sec\n",
      "Epoch 81, Loss(train/val) 5.03168/5.10921. Took 1.03 sec\n",
      "Epoch 82, Loss(train/val) 5.03263/5.09956. Took 1.04 sec\n",
      "Epoch 83, Loss(train/val) 5.03317/5.10406. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 5.03586/5.08372. Took 1.02 sec\n",
      "Epoch 85, Loss(train/val) 5.03855/5.08530. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 5.03231/5.09753. Took 1.02 sec\n",
      "Epoch 87, Loss(train/val) 5.03193/5.09449. Took 1.02 sec\n",
      "Epoch 88, Loss(train/val) 5.03316/5.08773. Took 1.01 sec\n",
      "Epoch 89, Loss(train/val) 5.03325/5.09680. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 5.03115/5.09468. Took 1.03 sec\n",
      "Epoch 91, Loss(train/val) 5.03269/5.09964. Took 1.02 sec\n",
      "Epoch 92, Loss(train/val) 5.03292/5.09669. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 5.03113/5.10341. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 5.03010/5.10972. Took 1.02 sec\n",
      "Epoch 95, Loss(train/val) 5.03260/5.10222. Took 1.01 sec\n",
      "Epoch 96, Loss(train/val) 5.03467/5.10123. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 5.03235/5.09783. Took 1.02 sec\n",
      "Epoch 98, Loss(train/val) 5.02991/5.10452. Took 1.01 sec\n",
      "Epoch 99, Loss(train/val) 5.03260/5.10032. Took 1.02 sec\n",
      "ACC: 0.453125, MCC: -0.08925605834988286\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch 0, Loss(train/val) 4.93597/4.92487. Took 1.02 sec\n",
      "Epoch 1, Loss(train/val) 4.91942/4.92911. Took 1.02 sec\n",
      "Epoch 2, Loss(train/val) 4.91948/4.94408. Took 1.01 sec\n",
      "Epoch 3, Loss(train/val) 4.92295/4.94521. Took 1.02 sec\n",
      "Epoch 4, Loss(train/val) 4.92757/4.92109. Took 1.02 sec\n",
      "Epoch 5, Loss(train/val) 4.92310/4.91628. Took 1.02 sec\n",
      "Epoch 6, Loss(train/val) 4.91813/4.91828. Took 1.01 sec\n",
      "Epoch 7, Loss(train/val) 4.91759/4.91933. Took 1.01 sec\n",
      "Epoch 8, Loss(train/val) 4.91818/4.91847. Took 1.03 sec\n",
      "Epoch 9, Loss(train/val) 4.91773/4.91812. Took 1.01 sec\n",
      "Epoch 10, Loss(train/val) 4.91781/4.91853. Took 1.03 sec\n",
      "Epoch 11, Loss(train/val) 4.91763/4.91735. Took 1.02 sec\n",
      "Epoch 12, Loss(train/val) 4.91626/4.91797. Took 1.02 sec\n",
      "Epoch 13, Loss(train/val) 4.91751/4.91791. Took 1.03 sec\n",
      "Epoch 14, Loss(train/val) 4.91657/4.91784. Took 1.01 sec\n",
      "Epoch 15, Loss(train/val) 4.91623/4.91865. Took 1.02 sec\n",
      "Epoch 16, Loss(train/val) 4.91627/4.91797. Took 1.01 sec\n",
      "Epoch 17, Loss(train/val) 4.91527/4.91908. Took 1.02 sec\n",
      "Epoch 18, Loss(train/val) 4.91557/4.91884. Took 1.13 sec\n",
      "Epoch 19, Loss(train/val) 4.91592/4.91917. Took 1.02 sec\n",
      "Epoch 20, Loss(train/val) 4.91367/4.92105. Took 1.01 sec\n",
      "Epoch 21, Loss(train/val) 4.91462/4.92092. Took 1.01 sec\n",
      "Epoch 22, Loss(train/val) 4.91333/4.92106. Took 1.03 sec\n",
      "Epoch 23, Loss(train/val) 4.91481/4.92061. Took 1.02 sec\n",
      "Epoch 24, Loss(train/val) 4.91370/4.92083. Took 1.01 sec\n",
      "Epoch 25, Loss(train/val) 4.91294/4.92038. Took 1.02 sec\n",
      "Epoch 26, Loss(train/val) 4.91393/4.92044. Took 1.01 sec\n",
      "Epoch 27, Loss(train/val) 4.91374/4.91924. Took 1.01 sec\n",
      "Epoch 28, Loss(train/val) 4.91251/4.92135. Took 1.03 sec\n",
      "Epoch 29, Loss(train/val) 4.91320/4.91966. Took 1.02 sec\n",
      "Epoch 30, Loss(train/val) 4.91324/4.91936. Took 1.01 sec\n",
      "Epoch 31, Loss(train/val) 4.91323/4.91934. Took 1.01 sec\n",
      "Epoch 32, Loss(train/val) 4.91154/4.92150. Took 1.01 sec\n",
      "Epoch 33, Loss(train/val) 4.91393/4.91373. Took 1.01 sec\n",
      "Epoch 34, Loss(train/val) 4.91680/4.91424. Took 1.01 sec\n",
      "Epoch 35, Loss(train/val) 4.91340/4.92297. Took 1.01 sec\n",
      "Epoch 36, Loss(train/val) 4.91399/4.91831. Took 1.01 sec\n",
      "Epoch 37, Loss(train/val) 4.91246/4.92321. Took 1.01 sec\n",
      "Epoch 38, Loss(train/val) 4.91185/4.92093. Took 1.02 sec\n",
      "Epoch 39, Loss(train/val) 4.91040/4.92899. Took 1.01 sec\n",
      "Epoch 40, Loss(train/val) 4.91122/4.91833. Took 1.02 sec\n",
      "Epoch 41, Loss(train/val) 4.91016/4.91974. Took 1.01 sec\n",
      "Epoch 42, Loss(train/val) 4.90988/4.92308. Took 1.01 sec\n",
      "Epoch 43, Loss(train/val) 4.91095/4.92400. Took 1.01 sec\n",
      "Epoch 44, Loss(train/val) 4.90953/4.92377. Took 1.01 sec\n",
      "Epoch 45, Loss(train/val) 4.90855/4.92850. Took 1.01 sec\n",
      "Epoch 46, Loss(train/val) 4.90503/4.93051. Took 1.02 sec\n",
      "Epoch 47, Loss(train/val) 4.90734/4.92569. Took 1.02 sec\n",
      "Epoch 48, Loss(train/val) 4.90462/4.93375. Took 1.02 sec\n",
      "Epoch 49, Loss(train/val) 4.90713/4.91636. Took 1.01 sec\n",
      "Epoch 50, Loss(train/val) 4.90749/4.91970. Took 1.01 sec\n",
      "Epoch 51, Loss(train/val) 4.90700/4.92067. Took 1.01 sec\n",
      "Epoch 52, Loss(train/val) 4.90659/4.92440. Took 1.02 sec\n",
      "Epoch 53, Loss(train/val) 4.90640/4.92204. Took 1.02 sec\n",
      "Epoch 54, Loss(train/val) 4.90563/4.92597. Took 1.01 sec\n",
      "Epoch 55, Loss(train/val) 4.90490/4.92210. Took 1.01 sec\n",
      "Epoch 56, Loss(train/val) 4.90896/4.92068. Took 1.02 sec\n",
      "Epoch 57, Loss(train/val) 4.90368/4.92477. Took 1.02 sec\n",
      "Epoch 58, Loss(train/val) 4.90428/4.92036. Took 1.01 sec\n",
      "Epoch 59, Loss(train/val) 4.90501/4.92218. Took 1.02 sec\n",
      "Epoch 60, Loss(train/val) 4.90874/4.91373. Took 1.01 sec\n",
      "Epoch 61, Loss(train/val) 4.91477/4.90771. Took 1.01 sec\n",
      "Epoch 62, Loss(train/val) 4.91295/4.91556. Took 1.01 sec\n",
      "Epoch 63, Loss(train/val) 4.90990/4.92001. Took 1.01 sec\n",
      "Epoch 64, Loss(train/val) 4.91026/4.91859. Took 1.02 sec\n",
      "Epoch 65, Loss(train/val) 4.90888/4.91552. Took 1.03 sec\n",
      "Epoch 66, Loss(train/val) 4.90770/4.91689. Took 1.02 sec\n",
      "Epoch 67, Loss(train/val) 4.90819/4.91065. Took 1.02 sec\n",
      "Epoch 68, Loss(train/val) 4.90654/4.91576. Took 1.01 sec\n",
      "Epoch 69, Loss(train/val) 4.90640/4.91911. Took 1.02 sec\n",
      "Epoch 70, Loss(train/val) 4.90674/4.91068. Took 1.02 sec\n",
      "Epoch 71, Loss(train/val) 4.90818/4.91336. Took 1.02 sec\n",
      "Epoch 72, Loss(train/val) 4.90580/4.91455. Took 1.01 sec\n",
      "Epoch 73, Loss(train/val) 4.90393/4.91284. Took 1.03 sec\n",
      "Epoch 74, Loss(train/val) 4.90362/4.91221. Took 1.02 sec\n",
      "Epoch 75, Loss(train/val) 4.90203/4.92087. Took 1.01 sec\n",
      "Epoch 76, Loss(train/val) 4.90624/4.91226. Took 1.01 sec\n",
      "Epoch 77, Loss(train/val) 4.90569/4.91391. Took 1.00 sec\n",
      "Epoch 78, Loss(train/val) 4.90486/4.91789. Took 1.02 sec\n",
      "Epoch 79, Loss(train/val) 4.90612/4.91498. Took 1.01 sec\n",
      "Epoch 80, Loss(train/val) 4.90574/4.91478. Took 1.01 sec\n",
      "Epoch 81, Loss(train/val) 4.90205/4.91492. Took 1.02 sec\n",
      "Epoch 82, Loss(train/val) 4.90236/4.91653. Took 1.01 sec\n",
      "Epoch 83, Loss(train/val) 4.90266/4.91937. Took 1.01 sec\n",
      "Epoch 84, Loss(train/val) 4.90243/4.91386. Took 1.01 sec\n",
      "Epoch 85, Loss(train/val) 4.90553/4.91392. Took 1.01 sec\n",
      "Epoch 86, Loss(train/val) 4.90400/4.90975. Took 1.01 sec\n",
      "Epoch 87, Loss(train/val) 4.90315/4.90691. Took 1.01 sec\n",
      "Epoch 88, Loss(train/val) 4.90194/4.90962. Took 1.02 sec\n",
      "Epoch 89, Loss(train/val) 4.90375/4.90802. Took 1.01 sec\n",
      "Epoch 90, Loss(train/val) 4.90250/4.91535. Took 1.03 sec\n",
      "Epoch 91, Loss(train/val) 4.89927/4.92007. Took 1.01 sec\n",
      "Epoch 92, Loss(train/val) 4.90721/4.92739. Took 1.01 sec\n",
      "Epoch 93, Loss(train/val) 4.91187/4.92190. Took 1.01 sec\n",
      "Epoch 94, Loss(train/val) 4.91023/4.92484. Took 1.01 sec\n",
      "Epoch 95, Loss(train/val) 4.90545/4.93422. Took 1.02 sec\n",
      "Epoch 96, Loss(train/val) 4.90637/4.92692. Took 1.01 sec\n",
      "Epoch 97, Loss(train/val) 4.90549/4.92691. Took 1.02 sec\n",
      "Epoch 98, Loss(train/val) 4.90359/4.92826. Took 1.02 sec\n",
      "Epoch 99, Loss(train/val) 4.90455/4.92012. Took 1.01 sec\n",
      "ACC: 0.4140625, MCC: -0.16636042155899566\n"
     ]
    }
   ],
   "source": [
    "with open(args.save_file_path + '\\\\' + 'DTML_result_t.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"model\", \"stock\", \"entire_exp_time\",  \"avg_test_ACC\", \"avg_test_MCC\"])\n",
    "\n",
    "    for stock in args.stock_list:\n",
    "        est = time.time()\n",
    "        setattr(args, 'symbol', stock)\n",
    "        args.new_file_path = args.save_file_path + '\\\\' + \"DTML_\" + args.symbol\n",
    "        os.makedirs(args.new_file_path)\n",
    "\n",
    "        args.entire_datalist = args.index + [stock]\n",
    "        # 0번째에 index 1번째에 stock 1개가 input으로 들어감\n",
    "        trainset = StockDataset(args.entire_datalist, args.x_frames, args.y_frames,\n",
    "                                args.train_start, args.train_end)\n",
    "        valset = StockDataset(args.entire_datalist, args.x_frames, args.y_frames,\n",
    "                              args.validation_start,args.validation_end)\n",
    "        testset = StockDataset(args.entire_datalist, args.x_frames, args.y_frames,\n",
    "                               args.test_start, args.test_end)\n",
    "        partition = {'train': trainset, 'val': valset, 'test': testset}\n",
    "\n",
    "\n",
    "        setting, result = experiment(partition, args)\n",
    "        eet = time.time()\n",
    "        entire_exp_time = eet - est\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.plot(result['train_losses'])\n",
    "        plt.plot(result['val_losses'])\n",
    "        plt.legend(['train_losses', 'val_losses'], fontsize=15)\n",
    "        plt.xlabel('epoch', fontsize=15)\n",
    "        plt.ylabel('loss', fontsize=15)\n",
    "        plt.grid()\n",
    "        plt.savefig(args.new_file_path + '\\\\' + str(args.symbol) + '_fig' + '.png')\n",
    "        plt.close(fig)\n",
    "\n",
    "        # csv파일에 기록하기\n",
    "        wr.writerow([\"DTML\", args.symbol,entire_exp_time, result['ACC'], result['MCC']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4bb3f-2033-4b97-8031-19370a89d75b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_64",
   "language": "python",
   "name": "py38_64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
