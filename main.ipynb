{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db823349-554f-4e1c-8b9f-47a4503d8fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Stock_Dataset.ipynb\n",
      "importing Jupyter notebook from AttLSTM.ipynb\n",
      "importing Jupyter notebook from attention.ipynb\n",
      "importing Jupyter notebook from Transformer_Encoder.ipynb\n",
      "importing Jupyter notebook from metric.ipynb\n",
      "importing Jupyter notebook from loss_fn1.ipynb\n",
      "importing Jupyter notebook from Stock_datasets_csv.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\USER\\\\JupyterProjects\\\\DTML')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from Stock_Dataset import StockDataset\n",
    "import argparse\n",
    "from AttLSTM import att_LSTM\n",
    "from Transformer_Encoder import Transformer\n",
    "import numpy as np\n",
    "import time\n",
    "from metric import metric_acc as ACC\n",
    "from metric import metric_mcc as MCC\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "from loss_fn1 import Selective_Regularization\n",
    "from Stock_datasets_csv import stock_csv_read\n",
    "\n",
    "def train(att_LSTM,transformer,                                  ## Model\n",
    "          att_LSTM_optimizer, transformer_optimizer,   ## Optimizer\n",
    "          Partition, args):                                      ## Data, loss function, argument\n",
    "    trainloader = DataLoader(Partition['train'],\n",
    "                             batch_size = args.batch_size,\n",
    "                             shuffle=False, drop_last=True)\n",
    "\n",
    "    att_LSTM.train()\n",
    "    transformer.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for j, data in enumerate(trainloader):\n",
    "        data_out_list = []\n",
    "        for i in range(len(data)):\n",
    "\n",
    "            att_LSTM_optimizer.zero_grad()\n",
    "            transformer_optimizer.zero_grad()\n",
    "            \n",
    "            x_input = data[i][0].to(args.device)\n",
    "            \n",
    "            if i == 1:\n",
    "                true_y = data[i][1].squeeze().float().to(args.device)\n",
    "            \n",
    "            att_LSTM.hidden = [hidden.to(args.device) for hidden in att_LSTM.init_hidden()]\n",
    "            \n",
    "            # 'list' object has no attribute 'float'\n",
    "\n",
    "            hidden_context, attn_weights = att_LSTM(x_input)\n",
    "\n",
    "            data_out_list.append(hidden_context)\n",
    "\n",
    "        index_output = data_out_list[0]  # torch.Size([128, 10])\n",
    "        stock_output = data_out_list[1] # torch.Size([128, 10])\n",
    "        \n",
    "\n",
    "        Norm_ = nn.LayerNorm(10, device=args.device)\n",
    "        \n",
    "        index_output = args.r * Norm_(index_output) + args.b\n",
    "        stock_output = args.r * Norm_(stock_output) + args.b\n",
    "\n",
    "\n",
    "        Transformer_input = index_output* args.market_beta + stock_output\n",
    "        \n",
    "\n",
    "        output1,wp,bp = transformer(Transformer_input.transpose(0,1).unsqueeze(2))\n",
    "\n",
    "\n",
    "        # output_ = torch.where(output1 >= 0.5, 1.0, 0.0)\n",
    "        # output_.requires_grad=True\n",
    "        \n",
    "        loss = args.loss_fn(output1, true_y, wp.squeeze(), bp)\n",
    "        loss.backward()\n",
    "\n",
    "        att_LSTM_optimizer.step() ## parameter 갱신\n",
    "        transformer_optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    return att_LSTM, transformer, train_loss\n",
    "\n",
    "\n",
    "def validation(att_LSTM,transformer,\n",
    "               partition, args):\n",
    "    valloader = DataLoader(partition['val'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    att_LSTM.eval()\n",
    "    transformer.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, data in enumerate(valloader):\n",
    "\n",
    "            data_out_list = []\n",
    "            for i in range(len(data)):\n",
    "                \n",
    "                x_input = data[i][0].to(args.device)\n",
    "\n",
    "                if i == 1:\n",
    "                    true_y = data[i][1].squeeze().float().to(args.device)\n",
    "                    \n",
    "                att_LSTM.hidden = [hidden.to(args.device) for hidden in att_LSTM.init_hidden()]\n",
    "\n",
    "\n",
    "                hidden_context, attn_weights = att_LSTM(x_input)\n",
    "\n",
    "                data_out_list.append(hidden_context)\n",
    "\n",
    "            index_output = data_out_list[0]  # torch.Size([128, 10])\n",
    "            stock_output = data_out_list[1]  # torch.Size([128, 10])\n",
    "\n",
    "            Norm_ = nn.LayerNorm(10, device=args.device)\n",
    "            index_output = args.r * Norm_(index_output) + args.b\n",
    "            stock_output = args.r * Norm_(stock_output) + args.b\n",
    "\n",
    "            Transformer_input = index_output* args.market_beta  + stock_output\n",
    "\n",
    "            output1,wp,bp = transformer(Transformer_input.transpose(0,1).unsqueeze(2))\n",
    "\n",
    "            loss = args.loss_fn(output1, true_y, wp.squeeze(), bp)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "        return att_LSTM, transformer, val_loss\n",
    "\n",
    "\n",
    "def test(att_LSTM, transformer,\n",
    "               partition, args):\n",
    "    testloader = DataLoader(partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    att_LSTM.eval()\n",
    "    transformer.eval()\n",
    "\n",
    "    ACC_metric = 0.0\n",
    "    MCC_metric = 0.0\n",
    "    with torch.no_grad():\n",
    "        for j, data in enumerate(testloader):\n",
    "\n",
    "            data_out_list = []\n",
    "            for i in range(len(data)):\n",
    "                \n",
    "                x_input = data[i][0].to(args.device)\n",
    "\n",
    "                if i == 1:\n",
    "                    true_y = data[i][1].squeeze().float().to(args.device)\n",
    "\n",
    "                att_LSTM.hidden = [hidden.to(args.device) for hidden in att_LSTM.init_hidden()]\n",
    "\n",
    "\n",
    "                hidden_context, attn_weights = att_LSTM(x_input)\n",
    "\n",
    "                data_out_list.append(hidden_context)\n",
    "\n",
    "            index_output = data_out_list[0]  # torch.Size([128, 10])\n",
    "            stock_output = data_out_list[1]  # torch.Size([128, 10])\n",
    "\n",
    "            Norm_ = nn.LayerNorm(10, device=args.device)\n",
    "            index_output = args.r * Norm_(index_output) + args.b\n",
    "            stock_output = args.r * Norm_(stock_output) + args.b\n",
    "\n",
    "            Transformer_input = index_output * args.market_beta + stock_output\n",
    "\n",
    "            output1,wp,bp = transformer(Transformer_input.transpose(0,1).unsqueeze(2))\n",
    "\n",
    "            output_ = torch.where(output1 >= 0.5, 1.0, 0.0)\n",
    "\n",
    "            output_.requires_grad = True\n",
    "\n",
    "            ACC_metric += ACC(output_, true_y)\n",
    "            MCC_metric += MCC(output_, true_y)\n",
    "\n",
    "        ACC_metric = ACC_metric / len(testloader)\n",
    "        MCC_metric = MCC_metric / len(testloader)\n",
    "\n",
    "        return ACC_metric, MCC_metric\n",
    "\n",
    "\n",
    "\n",
    "def experiment(partition, args):\n",
    "    att_LSTM = args.att_LSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size,\n",
    "                             args.dropout, args.use_bn, args.attention_head, args.attn_size, activation=\"ReLU\")\n",
    "    transformer = args.transformer(args.trans_feature_size, args.trans_num_laysers,args.dropout, args.batch_size, args.x_frames, args.trans_nhead)\n",
    "    \n",
    "    \n",
    "    args.r = nn.init.xavier_normal_(torch.empty(args.batch_size, args.hid_dim)).to(args.device) \n",
    "    args.b = torch.zeros(1).to(args.device)\n",
    "    \n",
    "    att_LSTM.to(args.device)\n",
    "    transformer.to(args.device)\n",
    "\n",
    "    if args.optim == 'SGD':\n",
    "        att_LSTM_optimizer = optim.SGD(att_LSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "        transformer_optimizer = optim.SGD(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        att_LSTM_optimizer = optim.RMSprop(att_LSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "        transformer_optimizer = optim.RMSprop(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        att_LSTM_optimizer = optim.Adam(att_LSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "        transformer_optimizer = optim.Adam(transformer.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "\n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    # ===================================== #\n",
    "    for epoch in range(args.epoch):\n",
    "        ts = time.time()\n",
    "        att_LSTM, transformer, train_loss = train(att_LSTM, transformer,\n",
    "                                                  att_LSTM_optimizer, transformer_optimizer,\n",
    "                                                  partition, args)\n",
    "\n",
    "        att_LSTM, transformer, val_loss = validation(att_LSTM, transformer, partition, args)\n",
    "\n",
    "        te = time.time()\n",
    "\n",
    "        ## 각 에폭마다 모델을 저장하기 위한 코드\n",
    "        torch.save(att_LSTM.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_epoch' +'_att_LSTM' +'.pt')\n",
    "        torch.save(transformer.state_dict(), args.split_file_path + '\\\\' + str(epoch) +'_epoch' +'_transformer' +'.pt')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "              .format(epoch, train_loss, val_loss, te - ts))\n",
    "\n",
    "    ## val_losses에서 가장 값이 최소인 위치를 저장함\n",
    "    site_val_losses = val_losses.index(min(val_losses)) ## 10 epoch일 경우 0번째~9번째 까지로 나옴\n",
    "    att_LSTM = args.att_LSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size,\n",
    "                             args.dropout, args.use_bn, args.attention_head, args.attn_size,\n",
    "                             activation=\"ReLU\")\n",
    "    transformer = args.transformer(args.trans_feature_size, args.trans_num_laysers,\n",
    "                                   args.dropout, args.batch_size, args.x_frames, args.trans_nhead)\n",
    "    att_LSTM.to(args.device)\n",
    "    transformer.to(args.device)\n",
    "\n",
    "    att_LSTM.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(site_val_losses) +'_epoch' +'_att_LSTM'+ '.pt'))\n",
    "    transformer.load_state_dict(torch.load(args.split_file_path + '\\\\' + str(site_val_losses) + '_epoch' + '_transformer' + '.pt'))\n",
    "\n",
    "    ACC,MCC = test(att_LSTM, transformer, partition, args)\n",
    "    print('ACC: {}, MCC: {}'.format(ACC, MCC))\n",
    "\n",
    "    with open(args.split_file_path + '\\\\'+ str(site_val_losses)+'Epoch_test_metric' +'.csv', 'w') as fd:\n",
    "        print('ACC: {}, MCC: {}'.format(ACC, MCC), file=fd)\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['ACC'] = ACC\n",
    "    result['MCC'] = MCC\n",
    "\n",
    "    return vars(args), result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd115972-567d-475b-ac08-15dedfc7d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# ========= experiment setting ========== #\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.save_file_path = \"D:\\\\dtml_results\"\n",
    "\n",
    "\n",
    "# ====== hyperparameter ======= #\n",
    "args.batch_size = 64\n",
    "args.x_frames = 10\n",
    "args.y_frames = 1\n",
    "args.input_dim = 10\n",
    "args.output_dim = 1\n",
    "args.dropout = 0.2\n",
    "args.use_bn = True\n",
    "args.loss_fn = Selective_Regularization  ## loss function for classification : cross entropy\n",
    "args.optim = 'Adam'\n",
    "args.lr = 0.0005\n",
    "args.l2 = 0.00001 #?\n",
    "args.epoch = 100\n",
    "# ============= model ================== #\n",
    "args.att_LSTM = att_LSTM\n",
    "args.transformer = Transformer\n",
    "\n",
    "# ====== att_lstm hyperparameter ======= #\n",
    "args.hid_dim = 10\n",
    "args.attention_head = 1\n",
    "args.attn_size = 10\n",
    "args.num_layers = 1\n",
    "args.decoder_x_frames = 1\n",
    "\n",
    "# ====== transformer hyperparameter ======= #\n",
    "args.trans_feature_size = 250\n",
    "args.trans_num_laysers = 1\n",
    "args.trans_nhead = 10\n",
    "args.market_beta = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb0bdf-b811-4e2c-a9a4-1289b194c7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss(train/val) 5.01212/4.99966. Took 0.44 sec\n",
      "Epoch 1, Loss(train/val) 4.97541/4.95292. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.97606/4.95682. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.97408/4.96102. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 4.97289/4.95803. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.97375/4.96182. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.97403/4.96500. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.97088/4.96536. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.97389/4.98538. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.97275/5.00028. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.97544/5.01324. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.97262/5.02048. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.96995/5.00011. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.96867/4.99284. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.96808/4.98872. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.96582/4.99629. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.96474/5.01039. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.96601/5.01444. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.96459/5.01226. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.96341/5.01249. Took 0.22 sec\n",
      "Epoch 20, Loss(train/val) 4.96434/5.01876. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.96067/5.02218. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.96063/5.03305. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.96214/5.02655. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.95812/5.01550. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.95870/5.02933. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.95350/5.04367. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.95829/5.04648. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.95540/5.05238. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.96309/5.00929. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.95740/5.02363. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.95436/5.03637. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.95605/5.03868. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.95341/5.03101. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.95067/5.04340. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.95339/5.02803. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.94914/5.05442. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.95124/5.06108. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.95210/5.05266. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.95159/5.04219. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.94698/5.04775. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.94727/5.04204. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.95225/5.03753. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.94954/5.04424. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.95023/5.04213. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.94986/5.04171. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.95062/5.03900. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.94707/5.04522. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.94575/5.04340. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.94698/5.04490. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.94994/5.04388. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.94949/5.03390. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.94653/5.04945. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.94849/5.04176. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.94183/5.05428. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.94200/5.08127. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.94598/5.06534. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.94271/5.02883. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.94887/5.02685. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.94114/5.03496. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.94401/5.04847. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.95022/5.02943. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.93505/5.07602. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.94550/5.02572. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.93804/5.05504. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.93826/5.05470. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.93997/5.05445. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.94189/5.05961. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.93918/5.04197. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.93582/5.05080. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.94497/5.04923. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.93932/5.05648. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.93893/5.05894. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.93333/5.07190. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.94294/5.02659. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.93876/5.03574. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.93778/5.04404. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.92842/5.05132. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.93593/5.07381. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.93801/5.05341. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.93611/5.07685. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.93219/5.04979. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.93255/5.04227. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.94228/5.05312. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.93426/5.05531. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.93347/5.05670. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.93959/5.05317. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.93010/5.04884. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.92758/5.06531. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.92372/5.03920. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.93462/5.04283. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.93627/5.05147. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.93293/5.05611. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.93492/5.05844. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.92613/5.12147. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.92932/5.04501. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.93384/5.04720. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.93182/5.03642. Took 0.22 sec\n",
      "Epoch 98, Loss(train/val) 4.92490/5.10590. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.93393/5.05182. Took 0.21 sec\n",
      "ACC: 0.5625, MCC: 0.09759000729485331\n",
      "Epoch 0, Loss(train/val) 4.84350/4.82127. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.82122/4.80282. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.82052/4.80165. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.81730/4.80699. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.81385/4.80618. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.81154/4.80969. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.81174/4.80731. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.80850/4.80987. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.81153/4.80780. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.81360/4.81252. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.81128/4.81315. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.80977/4.81305. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.80649/4.81393. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.81060/4.82518. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.81099/4.82189. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.80655/4.82764. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.81085/4.82453. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.80944/4.82386. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.80655/4.82071. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 4.80691/4.82616. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.80344/4.82655. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.80490/4.82896. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.80238/4.83712. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.80389/4.83357. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.80012/4.84202. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.80213/4.82397. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.80491/4.83054. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.79753/4.83252. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.79896/4.82510. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.79720/4.84066. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.80222/4.82692. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.79819/4.83913. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.80190/4.82925. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.79679/4.84455. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.79691/4.83699. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.79696/4.83064. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.80173/4.83619. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.80040/4.83047. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.79768/4.85836. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.79384/4.85725. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.79888/4.83831. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.79562/4.85234. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.79558/4.85238. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.79373/4.85275. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.79472/4.85384. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.79595/4.85271. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.79513/4.85418. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.79295/4.85329. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.79438/4.87552. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.79543/4.84690. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.78703/4.89225. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.78939/4.85791. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.78950/4.88250. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.79295/4.85860. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.79212/4.88221. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.78969/4.85777. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.79493/4.87699. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.79671/4.83467. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 4.79330/4.86725. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.79040/4.87806. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.79491/4.82873. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.79367/4.86705. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.79363/4.84449. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.79112/4.86958. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.79335/4.83938. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.79240/4.87489. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.78775/4.85112. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.78629/4.86699. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.79019/4.85351. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.79103/4.85120. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.78893/4.87531. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.78953/4.85385. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.79763/4.84855. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.78862/4.88703. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.78728/4.85734. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.78842/4.87939. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.79060/4.86076. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.78340/4.89723. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.78624/4.88896. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.78579/4.89129. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.78811/4.88420. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.78839/4.86458. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.78623/4.90629. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.78840/4.86218. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.78465/4.89150. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.78376/4.88083. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.79585/4.84130. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.79306/4.85949. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.78551/4.89757. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.78807/4.87427. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.78535/4.89580. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.79035/4.87268. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.78673/4.87242. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.78391/4.90441. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77719/4.89467. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79388/4.85516. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.78501/4.89168. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.78431/4.87293. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.78681/4.88175. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.78039/4.87823. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.002944085489318884\n",
      "Epoch 0, Loss(train/val) 4.92094/4.86104. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86299/4.89687. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.86377/4.89994. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.86311/4.89868. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.86376/4.89561. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.86249/4.89598. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.86498/4.89150. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.86390/4.88747. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85773/4.89598. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.85597/4.89788. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.85885/4.89014. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.85858/4.89150. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85505/4.89970. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85597/4.90150. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85610/4.90490. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85257/4.90190. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.85671/4.90510. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85838/4.90226. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85565/4.89143. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.85608/4.88586. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.85077/4.88840. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.84974/4.89254. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85234/4.89983. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84862/4.90233. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85328/4.89867. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.84600/4.91669. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.85159/4.90038. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.85443/4.89880. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.85291/4.89439. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.85153/4.89263. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.85390/4.89135. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.85074/4.89428. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.84808/4.89223. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84760/4.89791. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84955/4.88964. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84581/4.89356. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.85389/4.85394. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.85675/4.86869. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.85344/4.85271. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.85309/4.87083. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.85120/4.87547. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.85332/4.87170. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.84984/4.87425. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.85282/4.88203. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84789/4.88059. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84749/4.88397. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.85192/4.88130. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84762/4.89128. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.85104/4.88211. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.84296/4.89531. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.84222/4.88868. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.84655/4.90697. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.84393/4.91020. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.83964/4.90694. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.84975/4.89106. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.83916/4.88055. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.84640/4.86615. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.84925/4.88602. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.84892/4.91572. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.84507/4.90938. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.84635/4.89033. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84181/4.90218. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.84234/4.90530. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.85008/4.88589. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.84620/4.89269. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.84447/4.90862. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.84591/4.90356. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.83709/4.95658. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.84088/4.96104. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84144/4.93731. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.84208/4.91892. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83633/4.93778. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83265/4.96253. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.84638/4.93389. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.85327/4.89667. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.85206/4.90931. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.84533/4.89045. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.84851/4.91757. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.84299/4.91929. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.84214/4.92331. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.84538/4.90836. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83656/4.90147. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.86170/4.89894. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.85173/4.91537. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.84989/4.90482. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.84204/4.91437. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.84046/4.93730. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83992/4.93138. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83374/4.95271. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.84760/4.93048. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.84047/4.92496. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.83944/4.92962. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.83110/4.95222. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83593/4.93289. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83264/4.96208. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.83427/4.94157. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83773/4.94563. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83332/4.94631. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82706/4.96282. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.83157/4.94905. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.09607689228305229\n",
      "Epoch 0, Loss(train/val) 5.00594/4.93063. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.92291/4.91925. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.91824/4.90907. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.91659/4.90639. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.91733/4.90711. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.91574/4.90513. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.91668/4.90644. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.91642/4.90808. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.91284/4.91222. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.91331/4.91185. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.91211/4.91580. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.91291/4.91630. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.91004/4.91144. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.91180/4.91366. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.91092/4.91422. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.91008/4.91374. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.91000/4.90930. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.90643/4.91982. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.91443/4.92558. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.90832/4.92192. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.90699/4.92345. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.90547/4.92431. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.90580/4.92029. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.90703/4.92316. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.90476/4.92422. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.90733/4.93176. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.90561/4.93432. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.90580/4.92966. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90133/4.92683. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.90127/4.92330. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90110/4.92446. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.90027/4.93346. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.90477/4.93093. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.90462/4.93444. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.90149/4.92410. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.89953/4.92666. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.90034/4.93165. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 4.90043/4.91925. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.90260/4.92648. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.89793/4.93175. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.89410/4.92941. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.89948/4.93476. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.90001/4.94835. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.89681/4.94849. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.89290/4.94588. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.89722/4.93268. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.89379/4.92695. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89601/4.94057. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89315/4.94291. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.90013/4.94313. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89340/4.92819. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.89000/4.94763. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89296/4.93941. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.88782/4.94808. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.89349/4.93752. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.89774/4.92637. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.89323/4.94141. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.88860/4.94101. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.89455/4.94410. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.88943/4.91935. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.88918/4.93944. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.88890/4.93731. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.88268/4.92632. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.89178/4.96384. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.89269/4.92845. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.88925/4.93224. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.88352/4.91968. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.88055/4.93916. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.89435/4.93973. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.88757/4.93308. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.89216/4.95555. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.88256/4.94836. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.88312/4.93788. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.87543/4.92870. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.88426/4.94993. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.88695/4.95158. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.88536/4.92448. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.88531/4.95825. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.87946/4.94009. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.88384/4.94298. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88041/4.94696. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.88117/4.93694. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.88086/4.91801. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.88401/4.93544. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.87924/4.96490. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.88034/4.99117. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.87575/4.94420. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.87612/4.93851. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.87520/4.95942. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.87352/4.95997. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.87607/4.98096. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.87823/4.97405. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.86649/4.95298. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.87910/4.96786. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88097/4.93856. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.87145/4.92765. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.87190/4.94561. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.87108/4.97467. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.86864/4.94861. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88589/4.97463. Took 0.19 sec\n",
      "ACC: 0.625, MCC: 0.24305875451990117\n",
      "Epoch 0, Loss(train/val) 4.75145/4.84139. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.74766/4.81291. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.74555/4.78660. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.74785/4.72995. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.73403/4.72471. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.72870/4.73586. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.73194/4.73842. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.73409/4.73393. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.72881/4.73090. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.73137/4.73144. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.72895/4.73144. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.73085/4.72764. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.72980/4.72674. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.72902/4.72849. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.72712/4.72849. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.72796/4.72793. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.72483/4.72778. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.72617/4.73113. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.72784/4.72845. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.72497/4.72923. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.72923/4.71750. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.72290/4.72657. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.72626/4.72324. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.72432/4.72538. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.72289/4.72571. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.72277/4.72905. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.72172/4.73502. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.72225/4.72541. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.72266/4.73327. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.72060/4.72267. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.72116/4.72735. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.71829/4.73074. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.72285/4.72468. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.71259/4.73087. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.72010/4.72898. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.71859/4.72880. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.71594/4.73223. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.71385/4.72357. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.71007/4.73167. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.72087/4.71809. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.71411/4.70466. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.71447/4.72385. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.70667/4.72993. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.71355/4.73536. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.72052/4.70781. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.71350/4.70395. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.71115/4.70613. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.70653/4.70783. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.70928/4.72422. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.70029/4.69869. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.70726/4.72005. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.70048/4.71754. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.70474/4.73366. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.70715/4.74194. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.71103/4.70686. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.70736/4.71076. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.70297/4.72401. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.70669/4.69473. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.70313/4.71705. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.69633/4.72658. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.70764/4.71471. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.69942/4.71737. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.70188/4.70367. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.69496/4.71886. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.69703/4.74552. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.69553/4.72137. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.71125/4.70123. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.71105/4.73038. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.70275/4.72221. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.69626/4.71542. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.69365/4.72983. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.70236/4.71058. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.70515/4.70549. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.70152/4.73970. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.69723/4.72654. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.70268/4.75878. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.69542/4.72911. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.69672/4.72864. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.69415/4.72408. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.69227/4.73588. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.69575/4.74916. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.70051/4.73572. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.69448/4.69983. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.69038/4.73026. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.69042/4.71039. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.69160/4.76255. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.69615/4.74004. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.68279/4.73313. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.68725/4.74405. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.69392/4.71003. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.68838/4.75451. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.68665/4.73691. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.69157/4.72692. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.69136/4.74624. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.69416/4.73560. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.69320/4.74774. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.69400/4.73034. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.69419/4.73988. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.69215/4.74204. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.68646/4.75233. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.17004550636718183\n",
      "Epoch 0, Loss(train/val) 5.05493/4.98263. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.98594/4.99163. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.99130/4.99661. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.99408/5.00533. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.99035/5.00647. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.98783/5.00209. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.98549/5.00037. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.98132/5.00878. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.97842/4.99205. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.97755/4.98417. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.97476/4.97713. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.97348/4.98322. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.97296/4.98022. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.97953/4.97786. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.97333/4.98231. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.96762/4.98721. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.97082/4.99198. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.97097/4.99423. Took 0.22 sec\n",
      "Epoch 18, Loss(train/val) 4.96783/4.98982. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 4.96697/4.99348. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.97082/4.98835. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.96717/4.98638. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.96273/4.98516. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.97561/4.98478. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.97790/4.97412. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.97168/4.97423. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.96920/4.97367. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.97101/4.97658. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.96746/4.97281. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.96616/4.97301. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.96793/4.97076. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.96711/4.97385. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.96694/4.96813. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 4.96629/4.97239. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.96536/4.97489. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.96349/4.97504. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.96328/4.97581. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.96439/4.96969. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.96269/4.97487. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.96224/4.97195. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.96049/4.96526. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.96012/4.97432. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.96321/4.97333. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.97821/4.99344. Took 0.22 sec\n",
      "Epoch 44, Loss(train/val) 4.98025/4.98019. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.97426/4.98465. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.97353/4.98416. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.97283/4.98136. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.96771/4.98808. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.97030/4.98338. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.96575/4.97199. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.96896/4.97751. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.96867/4.98390. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.96415/4.98428. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.96818/4.97606. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.97092/4.98430. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.96219/4.97847. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.96320/4.98020. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.96581/4.98233. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.96232/4.97408. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.96073/4.98250. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.96445/4.98537. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.96782/5.00139. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.95939/5.00066. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.96075/4.98641. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.97606/4.98597. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.97235/4.97975. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.96933/4.97549. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.96882/4.97917. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.96985/4.98271. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.96846/4.98861. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.96290/4.99395. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.95926/4.99666. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.96297/4.98960. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.96227/4.99170. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.95969/4.99328. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.95947/4.99767. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.95448/5.00831. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.96119/5.00633. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.96191/4.98086. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.95905/4.97894. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.95654/4.98847. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.95812/4.99448. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.95531/5.00654. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.95806/4.99431. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.95103/5.01803. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.96067/4.99615. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.95765/4.99144. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.95395/5.03095. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.95969/4.99509. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.96018/4.99958. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.95060/5.00758. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.94934/5.00923. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.95836/5.01358. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.95314/5.01293. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.95454/4.98908. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.95244/4.99689. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.95413/5.01202. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.95360/4.99308. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.94964/4.99596. Took 0.19 sec\n",
      "ACC: 0.65625, MCC: 0.2966080766503938\n",
      "Epoch 0, Loss(train/val) 4.92037/4.81017. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 4.83043/4.82147. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.83373/4.80726. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.82852/4.80455. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.82530/4.80708. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.82280/4.80732. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.82530/4.80913. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.82277/4.81110. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.82408/4.81115. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.81983/4.81341. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.82125/4.81516. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.81881/4.81378. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.81830/4.81487. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.81773/4.81819. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.81664/4.81765. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.81534/4.81147. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.81674/4.81755. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.81584/4.81947. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81370/4.81778. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.81149/4.81877. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.81247/4.81731. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.81531/4.82210. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.81320/4.81741. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.81152/4.82139. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.81108/4.81559. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.81077/4.81741. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.80694/4.82185. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.80881/4.81872. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.81107/4.81884. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.80686/4.82108. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.80598/4.82050. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.80948/4.82120. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.80354/4.82601. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.80493/4.81940. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.80348/4.80832. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.80491/4.83288. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.80560/4.82917. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.80125/4.82760. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.80597/4.81780. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.80189/4.82551. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.80119/4.81875. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.80020/4.82478. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.80301/4.81316. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.80191/4.82719. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.80460/4.81738. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.80401/4.82452. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.80384/4.81696. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.79928/4.83100. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.79861/4.82360. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.80204/4.81126. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.80009/4.83482. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.79741/4.82777. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.80040/4.82611. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.79610/4.83425. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.79686/4.82140. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.79648/4.83043. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.80614/4.82426. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.79904/4.83771. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.79621/4.82075. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.79776/4.83382. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.79638/4.82776. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.80045/4.82333. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.79897/4.82004. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.79535/4.83018. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.78681/4.82728. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.79541/4.82219. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.79365/4.83848. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.79437/4.83730. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.79225/4.82453. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.79773/4.83340. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.79737/4.82838. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.79216/4.82465. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.78485/4.83693. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.79096/4.83670. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.79095/4.83982. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.79442/4.82145. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.78961/4.84973. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.78944/4.84366. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.78955/4.84560. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.78714/4.84309. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.78968/4.82098. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.78305/4.85986. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.78652/4.84570. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.79362/4.85773. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.77953/4.86375. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.79515/4.82667. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.79323/4.82501. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.79215/4.83605. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.79198/4.85344. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.78735/4.83273. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.78619/4.86268. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.78634/4.84983. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.78741/4.85088. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.79000/4.86870. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.79383/4.82463. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.78213/4.87802. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.78915/4.84474. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.78229/4.85804. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.78182/4.86266. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.78749/4.84173. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.1211560890116727\n",
      "Epoch 0, Loss(train/val) 5.03261/4.96946. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.93431/4.96123. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.93635/4.99387. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.94531/4.98797. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.94297/4.96714. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.93349/4.96563. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.92990/4.96414. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.92426/4.96855. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.92746/4.96948. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.92658/4.97172. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.92999/4.96434. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.92345/4.97018. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.92603/4.97119. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.92661/4.96543. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.92106/4.97032. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.92182/4.97069. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.92163/4.96841. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.92327/4.96879. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.92407/4.96789. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.91999/4.96963. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.92309/4.96814. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.91901/4.96922. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.91973/4.97096. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.92133/4.96522. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.91851/4.96389. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.92122/4.96935. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.92210/4.97081. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.91684/4.97550. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.92255/4.96805. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.92573/4.96050. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.92117/4.96948. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.91930/4.97531. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.91804/4.97702. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.92031/4.97363. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.91977/4.97669. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.91888/4.96326. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.91848/4.97576. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.91819/4.97350. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.91858/4.96571. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.91706/4.96760. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 4.91840/4.97208. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.91580/4.98800. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 4.91461/4.98461. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.91414/4.98918. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.91553/4.98647. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.91680/4.99174. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.91765/5.00590. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.91484/4.97176. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.91369/4.99425. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.91180/5.00792. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.91408/5.00433. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.91078/5.01567. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.91198/5.01823. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.90985/5.01187. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.91190/5.01231. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.91126/5.01488. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.92353/4.95687. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.91347/4.99765. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.92221/4.97434. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.91277/4.99498. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.91166/4.99642. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.91691/5.00542. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.91125/5.00532. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.91295/5.00733. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.91057/5.00436. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.90981/5.01689. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.91510/4.99815. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.90995/5.01213. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.90898/5.01689. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.90761/5.02588. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.90814/5.02040. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.90862/5.03627. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.90802/5.02006. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.91024/5.01676. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.90467/5.02680. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.91392/4.97636. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.90963/5.02584. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.90934/4.99336. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.90636/5.02812. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.91181/5.01814. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.90867/4.99364. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.90596/5.00688. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.90451/5.02329. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.90526/5.00972. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.90014/5.01980. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.90704/5.01959. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.90407/5.01379. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.90690/5.02473. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.90780/5.00016. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.91775/4.96627. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.91374/4.96386. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.90582/4.99480. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.90556/5.02120. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.90722/5.00512. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.90649/5.01481. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.90990/4.97862. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.90178/5.02608. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.90042/5.01619. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.90319/5.02010. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.90310/5.01092. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.1513518081969605\n",
      "Epoch 0, Loss(train/val) 5.03873/5.04529. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.01312/4.97618. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.99926/4.97067. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.99825/4.97414. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.00295/4.97993. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.99572/4.98755. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.99938/4.99586. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.99762/5.01116. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.99675/5.01429. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.99681/5.01925. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.99127/5.00957. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.99315/5.00562. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.99133/5.01612. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.99117/5.00643. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.98880/4.99589. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.99114/5.00337. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.98852/5.00455. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.98584/5.00086. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.98923/5.00860. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.98723/5.00685. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.98403/5.00808. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.98933/5.00896. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.99002/5.00319. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.98508/4.99959. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.98853/5.00439. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.98692/5.00537. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.98036/5.01118. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.98506/5.00886. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.98600/5.01226. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.98549/5.01496. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.97514/5.03156. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.98668/5.01168. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.98541/5.00030. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.98341/4.99704. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.97764/5.01399. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.98330/5.02493. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.98452/5.02353. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.98187/5.01591. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.97939/5.00473. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.98420/5.00569. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.99062/5.00599. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.98301/5.00312. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.98353/5.00402. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.97239/5.02906. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.98217/5.00993. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.98388/5.00801. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.98521/5.00036. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.97902/4.99601. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.97896/5.00508. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.97569/5.02335. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.97776/5.02553. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.97494/5.01592. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.97362/5.03149. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.97329/5.02297. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.98137/5.00410. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.98236/4.99779. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.97555/5.01778. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.97737/5.01887. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.97549/5.00545. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.97592/4.99967. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.98899/5.01610. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.98874/5.01550. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.98012/5.01431. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.98330/5.01975. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.97741/5.01913. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.97842/5.02142. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.98107/5.01548. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.97836/5.01614. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.98307/5.01047. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.97536/5.02884. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.97875/5.01602. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.97478/5.03382. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.97518/5.01380. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.97339/5.01730. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.97962/5.02426. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.99899/4.98526. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.98734/5.01585. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.98652/4.99037. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.98585/5.00134. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.98849/5.00116. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.98805/4.98981. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.98528/5.00328. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.98263/4.98088. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.98426/5.00532. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.98661/4.98263. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.97864/5.01831. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.98481/4.98499. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.97870/5.01684. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.98633/4.99306. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.99577/4.97693. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.99224/4.96740. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.98937/4.96658. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.99113/4.96566. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.98649/4.96392. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.98818/4.96912. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.98883/4.96306. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.98714/4.97019. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.98396/4.97526. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.98996/4.97227. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.98665/4.97078. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.019772733298509516\n",
      "Epoch 0, Loss(train/val) 4.71259/4.68649. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.67953/4.67102. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.67675/4.67282. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.67378/4.67396. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.67783/4.67577. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.67817/4.67645. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.67336/4.68178. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.67501/4.68106. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.67288/4.68277. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.67378/4.68202. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.67329/4.67781. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.67325/4.67959. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.67109/4.68041. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.67079/4.67993. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.66990/4.68223. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.66834/4.67127. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.67665/4.67241. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.67465/4.67432. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.67381/4.67367. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.67270/4.67597. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.67287/4.67486. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.67041/4.67659. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.67168/4.67733. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.67198/4.67677. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.67241/4.67689. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.66871/4.67283. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.66737/4.67348. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.66831/4.67189. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.66803/4.67291. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.66471/4.67510. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.66476/4.67472. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.65864/4.67995. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.66506/4.67603. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.66284/4.67529. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.65603/4.68492. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.66198/4.67188. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.65751/4.68396. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.65343/4.67410. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.65482/4.68281. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.65697/4.67948. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.65352/4.67674. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.65199/4.69012. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.65794/4.69728. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.66978/4.67863. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.66288/4.68095. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.65654/4.67492. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.65139/4.68360. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.64875/4.68665. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.65070/4.68921. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.65534/4.68537. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.65314/4.69453. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.65110/4.67841. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.65827/4.68681. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.65301/4.69723. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.65603/4.68334. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.65717/4.69493. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.65474/4.69753. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.64879/4.69009. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.65854/4.70690. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.65597/4.70428. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.65317/4.70129. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.65269/4.69489. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.65134/4.72251. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.66005/4.68580. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.65573/4.69806. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.64465/4.70939. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.64998/4.70359. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.64769/4.72040. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.65031/4.70472. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.64844/4.70500. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.64598/4.70229. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.64081/4.73079. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.65433/4.71333. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.64361/4.71590. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.65125/4.70733. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.64293/4.72601. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.65416/4.70544. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.64577/4.71551. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.64443/4.75072. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.64147/4.73651. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.64336/4.73175. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.64147/4.73786. Took 0.21 sec\n",
      "Epoch 82, Loss(train/val) 4.64424/4.72963. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.64399/4.74211. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.64129/4.75463. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.64511/4.72496. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.64382/4.75699. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.64141/4.73981. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.64060/4.75628. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.64394/4.73133. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.63908/4.75095. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.63989/4.72914. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.63920/4.78014. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.63951/4.75553. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.64577/4.73153. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.63698/4.74989. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.63465/4.75255. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.64602/4.73988. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.63714/4.76419. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.63903/4.76304. Took 0.20 sec\n",
      "ACC: 0.515625, MCC: 0.025861699363244256\n",
      "Epoch 0, Loss(train/val) 4.91364/4.86296. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.88780/4.86191. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.86056/4.87077. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.84116/4.84323. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.84449/4.85350. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.84382/4.84890. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.84467/4.85031. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.84203/4.85407. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.84279/4.85184. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.83995/4.85357. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84185/4.85506. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.84054/4.85105. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.83825/4.85309. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.83772/4.85368. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.83521/4.85686. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.83898/4.84709. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.83856/4.86078. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.83205/4.85941. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83344/4.85901. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.83339/4.86475. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.83585/4.85834. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.83036/4.86030. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.83415/4.85760. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.83323/4.85851. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.82853/4.84718. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.83124/4.85740. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82791/4.87099. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.83094/4.85575. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.82830/4.86251. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.82648/4.86347. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.82565/4.86815. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.82467/4.87624. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.82630/4.85337. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.82443/4.85862. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 4.82824/4.86078. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82802/4.85258. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.82394/4.84908. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.82038/4.85063. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.82685/4.87514. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.82619/4.87275. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.81906/4.86663. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.82221/4.86080. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.82303/4.86041. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.82112/4.87648. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.82274/4.84432. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.81793/4.85005. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.82207/4.87745. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.82579/4.87261. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.82677/4.86645. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.82504/4.86086. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.82111/4.86871. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.82551/4.86922. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.82007/4.89159. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.82582/4.88835. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.82055/4.88166. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.82037/4.88382. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.81853/4.90058. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.81946/4.88552. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81721/4.91440. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.82183/4.90143. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.82100/4.87933. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81590/4.89795. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.81747/4.89491. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.81283/4.89639. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.81468/4.89217. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81479/4.90054. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.81444/4.91662. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.81775/4.89967. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.82273/4.90076. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.82109/4.87493. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.82072/4.89729. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81421/4.88462. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.81704/4.89993. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.81741/4.91551. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81925/4.88007. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.81198/4.88727. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.81682/4.89290. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.82609/4.89199. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.81644/4.87224. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.81449/4.89088. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.81282/4.89835. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.81081/4.91369. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.81865/4.89830. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80638/4.90509. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.81014/4.90230. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.81812/4.88230. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.81355/4.88957. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.81305/4.90700. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.81080/4.90921. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.80318/4.90797. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.81887/4.89940. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.81438/4.88595. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.81361/4.88853. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.80892/4.91115. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.80579/4.90477. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.81183/4.89332. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.80597/4.89222. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81193/4.91195. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.80758/4.90537. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.80820/4.90709. Took 0.20 sec\n",
      "ACC: 0.578125, MCC: 0.1793740008335438\n",
      "Epoch 0, Loss(train/val) 5.02739/4.97146. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.95160/4.97968. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.95051/4.92306. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.92221/4.92441. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.92374/4.93130. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.93152/4.93002. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.93100/4.92865. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.92837/4.92477. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.92457/4.92826. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.92693/4.92873. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.92435/4.92686. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.92235/4.92830. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.92367/4.92868. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.92196/4.92772. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.92264/4.92935. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.92080/4.92862. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.91747/4.93568. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.91958/4.93184. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.92036/4.93276. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.92136/4.92516. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.91684/4.92424. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.91764/4.92355. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.91482/4.92273. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.91752/4.90413. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.92404/4.91973. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.92060/4.92119. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.92048/4.92146. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.91679/4.92259. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.91841/4.92403. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.91534/4.92304. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.91493/4.92788. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.91407/4.92689. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.91420/4.93052. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.91472/4.93357. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.91224/4.93814. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90908/4.94447. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.90935/4.94911. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.90563/4.96385. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.90413/4.96169. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90734/4.97078. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90585/4.96677. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.90380/4.99022. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.90354/4.97364. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.90038/4.98699. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.89870/4.98330. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.89947/5.00282. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.89975/4.97023. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89836/4.97635. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89295/5.00777. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.89290/4.97916. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.90038/4.97401. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.89751/4.97823. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.89413/4.99432. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.89325/5.01583. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.89619/4.99272. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.89225/4.98138. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.89301/4.99411. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.89294/4.98964. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.89346/4.98257. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.88890/4.99752. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.89424/4.99432. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.88669/4.99344. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.89293/4.98230. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.88478/5.01548. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.88605/5.00412. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.89759/4.96977. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.88627/4.99949. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.89222/5.00298. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.88538/4.99739. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.88393/5.00443. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.89404/4.98387. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.88917/5.01761. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.88885/4.98442. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88228/5.01795. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.89053/4.98387. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.88479/4.98546. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.88904/4.97762. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.88793/4.97699. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.88267/5.01352. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.88234/4.99241. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88666/4.99761. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.88325/4.98510. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.88203/4.98407. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.88620/4.99682. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.87892/5.01602. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.88173/5.00747. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.88623/4.99610. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.88645/5.00064. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 4.88245/5.02120. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.88270/4.96941. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.87567/5.01222. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.88184/4.97887. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.87763/5.00366. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.87882/5.00796. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88038/4.99223. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.89011/5.00697. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.89977/4.97596. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.90660/4.95724. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.89672/4.98877. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88860/4.96970. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.08879923794435518\n",
      "Epoch 0, Loss(train/val) 4.88739/4.81286. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.85008/4.83287. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.85316/4.82900. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.86320/4.84616. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.86347/4.89586. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.85240/4.86878. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.84818/4.86377. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.84941/4.87296. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85075/4.87278. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.84975/4.86844. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84952/4.87113. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.84965/4.86938. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.84764/4.87121. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.84894/4.87452. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.84764/4.87745. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85076/4.87652. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.84709/4.87627. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 4.84643/4.88035. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.84602/4.88406. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.84428/4.88335. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84472/4.88603. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.84375/4.88681. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.84069/4.89242. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84117/4.89636. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.83967/4.88876. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.83761/4.89550. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.83569/4.87896. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.84018/4.89073. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.83842/4.89715. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.83525/4.90560. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.83114/4.90365. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.83324/4.90064. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.83601/4.91853. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.83263/4.93013. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.83318/4.89790. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82955/4.89976. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.83067/4.91849. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.82631/4.92113. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.82580/4.90948. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.83284/4.91141. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.83039/4.94197. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.82555/4.92969. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83006/4.91649. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.82005/4.98462. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.83172/4.89564. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.82255/4.94575. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.82676/4.91484. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.82352/4.91439. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.82533/4.93286. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.82283/4.92269. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.81884/4.91616. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.82488/4.95115. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.82429/4.93678. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.81600/4.91808. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.82531/4.90368. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.81586/4.92127. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.82349/4.91038. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.82413/4.93250. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81827/4.92857. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.82411/4.91467. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.81907/4.93134. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81665/4.91633. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.81857/4.92912. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.81777/4.92906. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81698/4.94301. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81723/4.98869. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.81278/4.92263. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.81463/4.90953. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.81557/4.92564. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.81135/4.95807. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.81141/4.92641. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81060/4.95271. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.81890/4.94385. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.81197/4.92843. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.80782/4.96605. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.80970/4.95855. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.80887/4.94014. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80523/4.98525. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.81176/4.93917. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.81817/4.92967. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.83376/4.90282. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83268/4.91566. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.82965/4.91378. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.82504/4.91769. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.82412/4.95373. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.81910/4.93712. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.81810/4.93646. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.81743/4.94292. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.81596/4.94444. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82128/4.94628. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.81227/4.95431. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.81325/4.94534. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.81426/4.96572. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.81348/4.97180. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.80517/4.95635. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.81530/4.93444. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.81460/4.93040. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.80898/4.97867. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.80879/4.95345. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.80936/5.00766. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.18786728732554486\n",
      "Epoch 0, Loss(train/val) 4.81618/4.80441. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.80627/4.81867. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.79493/4.80527. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 4.79934/4.79598. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.80469/4.79989. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79997/4.79364. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.79265/4.79010. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79097/4.78671. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79408/4.78837. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79168/4.79011. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79151/4.78789. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.79171/4.78948. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.78995/4.78704. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 4.79075/4.79082. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.79071/4.78956. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.78722/4.78877. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.78827/4.79267. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.79014/4.79326. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78770/4.79218. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78689/4.79543. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.78668/4.79698. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78408/4.79733. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78830/4.79945. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78981/4.80014. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78745/4.80009. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78742/4.79618. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78453/4.80058. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.78757/4.79470. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78890/4.79763. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78762/4.79265. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.78554/4.78501. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.78370/4.79489. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.78295/4.79257. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.78398/4.79618. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.78116/4.79681. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.78153/4.79794. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78016/4.79983. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.78207/4.80293. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.78102/4.81839. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.78413/4.80599. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.78178/4.80757. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.78103/4.80486. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.78126/4.80391. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.77876/4.80909. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.78183/4.81105. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.78020/4.81071. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77829/4.80450. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.77759/4.81923. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.78654/4.80834. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.78536/4.80537. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.77956/4.81295. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.77969/4.81234. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.78176/4.81026. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.78203/4.81519. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.78300/4.80982. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.78219/4.80505. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.77514/4.80511. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.77525/4.81656. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.77658/4.81853. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.77180/4.83445. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.78924/4.81318. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.77819/4.80560. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.77964/4.81535. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.78579/4.81109. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.78824/4.81820. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.78534/4.81333. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.78670/4.81707. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.78540/4.81404. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.78558/4.81510. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.78240/4.81512. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.78300/4.81409. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.77967/4.82163. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.78111/4.81516. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.78407/4.81378. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.78052/4.81294. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.77913/4.81295. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.77843/4.80769. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77735/4.80366. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.77413/4.81038. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.78033/4.81142. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.77730/4.81704. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.77554/4.81997. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.77432/4.81505. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.77725/4.81474. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.77615/4.82331. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.77472/4.81696. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.77193/4.82194. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.76762/4.82015. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.77466/4.81895. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.77605/4.82927. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.77430/4.81953. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.76840/4.81427. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.76864/4.81717. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77235/4.82348. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77327/4.84906. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.76908/4.83582. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.77043/4.81181. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.77017/4.81442. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.76854/4.81528. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.76580/4.82416. Took 0.20 sec\n",
      "ACC: 0.5, MCC: -0.02404930351219409\n",
      "Epoch 0, Loss(train/val) 4.86609/4.83911. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.84974/4.84115. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.84433/4.84306. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.84141/4.84621. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.84108/4.84223. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.84279/4.83308. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.83818/4.83120. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.83648/4.83185. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.83592/4.83331. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.83711/4.83300. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.83554/4.83269. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.83471/4.83393. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.83579/4.83418. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.83410/4.83261. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.83371/4.83140. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.83069/4.83233. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.83263/4.83000. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.83304/4.82609. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83167/4.82599. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.82870/4.82316. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.82814/4.82559. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.82720/4.83106. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.82897/4.82288. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.82842/4.82183. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82377/4.82262. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82222/4.82593. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82761/4.82132. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.82431/4.81254. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.81909/4.81712. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.82426/4.82044. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.81988/4.82032. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.81755/4.81648. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.82076/4.81698. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.81846/4.82023. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.82490/4.84162. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82369/4.83568. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.81943/4.83772. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.82400/4.83427. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.81884/4.84124. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.82521/4.82326. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.81872/4.82564. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.82052/4.82598. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.81125/4.82516. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.81663/4.81765. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 4.82087/4.82094. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.81556/4.82033. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.81701/4.81331. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.81803/4.81426. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.81764/4.80991. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.81786/4.81579. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.81486/4.81338. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.81140/4.81216. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81540/4.83383. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.82557/4.82416. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81322/4.83110. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.81706/4.80787. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.81573/4.81545. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.81526/4.82109. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81816/4.81468. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.81285/4.82108. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.81261/4.82319. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81506/4.83321. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.80723/4.82283. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 4.81446/4.81600. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81074/4.82744. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81293/4.82594. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.80942/4.83141. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.80959/4.83731. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.81498/4.82155. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.81197/4.83563. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.81076/4.82028. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.80875/4.81666. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.80843/4.81588. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.80934/4.81104. Took 0.21 sec\n",
      "Epoch 74, Loss(train/val) 4.80685/4.83601. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.81121/4.82156. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.80198/4.81978. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.80807/4.83191. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.81174/4.82475. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.79995/4.83182. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.80792/4.81893. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.80822/4.84163. Took 0.21 sec\n",
      "Epoch 82, Loss(train/val) 4.80074/4.82349. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80613/4.83478. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.80688/4.83208. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.79883/4.84507. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.80692/4.82958. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.80148/4.82554. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.80029/4.83441. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.80914/4.82664. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.80479/4.82111. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.80696/4.82977. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.81107/4.82465. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.80282/4.82686. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.80226/4.83225. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79879/4.83271. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.80284/4.83703. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.80805/4.83822. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.80062/4.82621. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.80040/4.83724. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.1259881576697424\n",
      "Epoch 0, Loss(train/val) 4.93498/4.87910. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.90837/4.90635. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.90082/4.88519. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.90520/4.87813. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.90825/4.87271. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.91934/4.88692. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.91185/4.92183. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.89759/4.90361. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.89999/4.90372. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.90168/4.90653. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.89733/4.90308. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.90261/4.90228. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.90005/4.90500. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.89806/4.90118. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.89815/4.90142. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.89888/4.90345. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.89832/4.90826. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.89558/4.90414. Took 0.22 sec\n",
      "Epoch 18, Loss(train/val) 4.89433/4.90411. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.90149/4.89600. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.89548/4.91468. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.89074/4.90898. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.89451/4.91063. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.89266/4.91882. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.89057/4.90959. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.88905/4.91884. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.89416/4.91259. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.88498/4.92784. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.88869/4.91353. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.88616/4.91851. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.88547/4.91566. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.88950/4.91152. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.88490/4.91534. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.88937/4.90789. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.88416/4.92167. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.88489/4.90572. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.88332/4.92623. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.88565/4.90771. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.88194/4.91852. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.88388/4.91199. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.87845/4.91237. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.88823/4.91106. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.88455/4.92356. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.87769/4.91544. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.88200/4.91760. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.88047/4.92341. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.88106/4.90682. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.88010/4.91546. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.88056/4.91531. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.88002/4.90969. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.88538/4.91218. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 4.87654/4.92890. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.87888/4.91994. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.87705/4.90710. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.87931/4.91678. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.87246/4.93033. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.87220/4.91212. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.87204/4.90936. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.87195/4.92084. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.87399/4.91716. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.86980/4.91063. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.87751/4.91168. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.87383/4.91883. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.87111/4.92822. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.87121/4.90954. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.86504/4.93414. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.87070/4.92722. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.86592/4.93134. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.86175/4.89650. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.87727/4.93482. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.87892/4.92611. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.87094/4.90623. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.86992/4.92871. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.87303/4.92305. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.86445/4.92626. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.86469/4.91845. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.85681/4.92375. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.87404/4.94117. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.86844/4.92756. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.86855/4.92553. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.86094/4.91166. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.86826/4.93424. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.86109/4.93811. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.86450/4.93692. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.86005/4.93033. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.85890/4.96219. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.86714/4.91913. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.86234/4.93242. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.86188/4.92656. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.86359/4.94351. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.85570/4.93221. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.86690/4.93477. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.85986/4.92743. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.85819/4.92793. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.86517/4.90538. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.85685/4.92799. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.85382/4.92525. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.86531/4.94074. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.85931/4.91355. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.85718/4.94935. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.03643724696356275\n",
      "Epoch 0, Loss(train/val) 5.02519/4.93625. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.96175/4.94415. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.96161/4.94390. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.96504/4.93939. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.96731/4.93801. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.96302/4.93950. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.96011/4.94047. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.95798/4.94499. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.95811/4.94279. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.95944/4.94271. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.95699/4.94632. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.95537/4.94892. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.95578/4.94811. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.95825/4.95096. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.95752/4.95287. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.95425/4.95415. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.95581/4.95784. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.95255/4.95680. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.95007/4.96164. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.95162/4.96208. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.95193/4.96428. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.94955/4.95460. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.95245/4.95365. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.94761/4.95730. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.94369/4.95764. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.95128/4.95100. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.95073/4.95204. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.94694/4.95522. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.94646/4.95266. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.94289/4.95317. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.94186/4.95359. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.94343/4.95329. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.93836/4.96385. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.94362/4.95948. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.93182/4.96221. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.94008/4.95038. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.93363/4.96536. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.93338/4.95633. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.93810/4.96122. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.92659/4.97111. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.94406/4.95010. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.93786/4.95542. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.93499/4.95740. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.93729/4.95488. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.93128/4.96069. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.93881/4.96100. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.93612/4.95879. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.93711/4.96123. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.93147/4.95188. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.93650/4.96258. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.93289/4.95405. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.93754/4.94048. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.94223/4.94023. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.93448/4.94671. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.93545/4.93993. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.93158/4.95125. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.92855/4.94976. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.93688/4.94892. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.93069/4.93630. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.93044/4.94783. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.92907/4.95142. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.93347/4.94281. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.93970/4.95659. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.92877/4.97950. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.93238/4.95995. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.92437/4.97123. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.93327/4.96224. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.93009/4.95542. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.92631/4.97516. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.92943/4.95248. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.92325/4.96470. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.92731/4.96253. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.92377/4.96425. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.91895/4.94671. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.92345/4.95198. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.92471/4.95650. Took 0.21 sec\n",
      "Epoch 76, Loss(train/val) 4.92232/4.95126. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.92288/4.94753. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.92472/4.95929. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.92167/4.95795. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.92569/4.95314. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.92797/4.96284. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.92892/4.97053. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.92924/4.94609. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.92673/4.96625. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.92481/4.96632. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.92328/4.95397. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.92090/4.96956. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.92228/4.95116. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.92270/4.94307. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.91983/4.93083. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.92593/4.94152. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.92374/4.94315. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.92264/4.95872. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.92358/4.96151. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.92590/4.95095. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.91781/4.95316. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.91756/4.95671. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.92175/4.95497. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.92374/4.96000. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.09607689228305229\n",
      "Epoch 0, Loss(train/val) 4.82305/4.77316. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.75045/4.75217. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.74745/4.75383. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.74445/4.75854. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.74393/4.76041. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.74742/4.76282. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.74694/4.75910. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.74369/4.76338. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.74243/4.76806. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.74152/4.77175. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.74082/4.75993. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.74084/4.76699. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.74041/4.76450. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.73694/4.76453. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.73914/4.76506. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.73685/4.77808. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.73332/4.76049. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.73329/4.77277. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.73359/4.77053. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.73733/4.77239. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.73331/4.78076. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.73340/4.77823. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.72973/4.79000. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.73134/4.78052. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.72910/4.77100. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.73693/4.75774. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.73020/4.76207. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.72242/4.78164. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.72823/4.78832. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.72514/4.78436. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.73269/4.77273. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.72447/4.79187. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.72015/4.80286. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.72981/4.77542. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.73038/4.76461. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.72446/4.78934. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 4.72468/4.79472. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.71778/4.80024. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.73092/4.77332. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.72091/4.79460. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.72383/4.79122. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.73158/4.78652. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.72504/4.77265. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.72185/4.79172. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.74399/4.74631. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.73416/4.75735. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.73451/4.74399. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.72846/4.74995. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.72833/4.75516. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.73325/4.75188. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.72557/4.74792. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.72472/4.75074. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.72731/4.76929. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.72949/4.76607. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.72251/4.77897. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.72407/4.75625. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.72097/4.76771. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.72547/4.77765. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.72684/4.77733. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.72635/4.76256. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.72851/4.76915. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.72193/4.79287. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.72713/4.75696. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.72641/4.76074. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.72423/4.77152. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.72625/4.76219. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.71931/4.76689. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.72298/4.77986. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.72329/4.76228. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.71686/4.77670. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.72358/4.77112. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.70895/4.79241. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.71783/4.77942. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.71344/4.78649. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.71813/4.78109. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.71438/4.76551. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.71687/4.77553. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.70736/4.77770. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.71194/4.76093. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.71638/4.76923. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.71916/4.76154. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.70702/4.77677. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.71976/4.78029. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.71616/4.76951. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.72025/4.76938. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.71105/4.77042. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.70800/4.78918. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.71769/4.76767. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.71302/4.76873. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.71618/4.77348. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.70983/4.78333. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.71302/4.78614. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.71248/4.77234. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.71261/4.77579. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.70515/4.75445. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.70659/4.74527. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.70903/4.77650. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.70110/4.78702. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.71080/4.76872. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.70837/4.78480. Took 0.19 sec\n",
      "ACC: 0.40625, MCC: -0.1686533906277161\n",
      "Epoch 0, Loss(train/val) 5.04434/4.99422. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.00654/4.98036. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.00217/4.97098. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.00575/4.98021. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.00389/4.98466. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.99842/4.98658. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.00086/4.98713. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.99798/4.99725. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.99896/4.99526. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.99523/4.99565. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.99749/4.99742. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.99418/5.00057. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.99202/5.00022. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.99350/4.99871. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.99110/4.99926. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.99387/4.99852. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.99227/4.99633. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.99168/4.99915. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.99129/4.99694. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.99282/4.99977. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.99196/4.99355. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.99060/4.99455. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.98830/4.99745. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.99161/4.99884. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.99022/5.00111. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.98763/4.99619. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.98904/5.00041. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.98524/4.99907. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.98832/5.00257. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.98347/5.01190. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.98810/5.01558. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.98650/5.00328. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 4.98625/5.00473. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.98205/5.00362. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.98358/5.00711. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.98290/5.01091. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.98370/5.02083. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.97948/5.02140. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.98389/5.00254. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.98275/5.00872. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.98229/5.01955. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.97770/5.01667. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.97508/5.01850. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.98447/5.02149. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.97948/5.02225. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.97531/5.02489. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.98235/5.01825. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.97975/5.02594. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.97896/5.02392. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.97908/5.02381. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.97260/5.02170. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.98235/5.02325. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.97673/5.01958. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.97588/5.04543. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.97703/5.03956. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.97811/5.03582. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.97942/5.03142. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.97547/5.03607. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.97108/5.02757. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.97702/5.04343. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.98284/5.01946. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.97376/5.04223. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.96535/5.03239. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.98177/5.02662. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.97514/5.06790. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.98184/5.02446. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.97884/5.02036. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.97551/5.02825. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.97436/5.03678. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.97357/5.02801. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.97202/5.04038. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.96943/5.04674. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.97509/5.03295. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.97260/5.03616. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.96574/5.02585. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.97029/5.03594. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.97369/5.04126. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.96794/5.05918. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.97189/5.02878. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.97414/5.04045. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.97314/5.04805. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.96647/5.04552. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.97537/5.03610. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.97185/5.04735. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.96858/5.03385. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.97307/5.04834. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.96640/5.05120. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.97180/5.02994. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.97355/5.05078. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.96529/5.03089. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.95732/5.05771. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.96727/5.04034. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.96214/5.03962. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.96599/5.05040. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.96904/5.03521. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.96609/5.03932. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.96257/5.04048. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.96279/5.03811. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.97168/5.03727. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.96276/5.04966. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.03138824102871723\n",
      "Epoch 0, Loss(train/val) 5.07626/5.05666. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.04432/5.05139. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.03768/5.05218. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.03787/5.06733. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.03941/5.07631. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.03942/5.06444. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 5.03738/5.05297. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.03289/5.04305. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.02836/5.04325. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.02884/5.04389. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.02880/5.03710. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 5.02498/5.04061. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.02520/5.04156. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.02537/5.04430. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.02529/5.04928. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.02489/5.04860. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.02501/5.04515. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.02305/5.04731. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 5.01971/5.05127. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.01958/5.05440. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.02502/5.04839. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.02141/5.04886. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.01770/5.05297. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.01895/5.05094. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.01537/5.04969. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.02022/5.04959. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.01655/5.05863. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.01401/5.07255. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.01907/5.06882. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.01624/5.07156. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.01754/5.06132. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.01192/5.07779. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.01401/5.06605. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.01608/5.05489. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.01422/5.06855. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 5.01126/5.07619. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.01356/5.07176. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.01370/5.06692. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.01263/5.07276. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.01271/5.06832. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.01440/5.07093. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.01028/5.06868. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.01280/5.08002. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.01180/5.07551. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.01273/5.07015. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.00876/5.07330. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.01540/5.06907. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.01187/5.08947. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.01266/5.05536. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.01676/5.06733. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.00989/5.06729. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.01111/5.06814. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.01016/5.05646. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 5.00690/5.06206. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.00667/5.06298. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.00916/5.06907. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.00774/5.07145. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.00697/5.07197. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.01074/5.06777. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.00474/5.07159. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 5.00460/5.06377. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.00691/5.06360. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.00791/5.06971. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.01051/5.08106. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.00689/5.07587. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.00574/5.10054. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.00643/5.09318. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.00835/5.09015. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.01031/5.07531. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.01004/5.06210. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.00464/5.06645. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.00911/5.06388. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.00414/5.06508. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.00462/5.06668. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.00141/5.04822. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.99964/5.07631. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.99706/5.10711. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.01051/5.08932. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 5.00996/5.08571. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.00806/5.10026. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.00074/5.10761. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.00522/5.07720. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.00449/5.05794. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 5.03083/5.04499. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.02389/5.02270. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.02376/5.02680. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.01768/5.05967. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.00126/5.04872. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.99758/5.09714. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.99821/5.08750. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.00099/5.09675. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.99856/5.10996. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.00670/5.09499. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.99961/5.08689. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.99965/5.09204. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.99652/5.06460. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.99874/5.10195. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.00099/5.05900. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.01784/5.02363. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.00564/5.06724. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 4.93182/4.87427. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86049/4.87388. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.86070/4.86690. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.86462/4.86839. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.86360/4.86895. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.86315/4.87496. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.86188/4.87580. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.86145/4.87811. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.86043/4.87686. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.85516/4.87421. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.85801/4.87254. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.85279/4.87786. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85231/4.87580. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.85339/4.88223. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85091/4.89234. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.84598/4.89037. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.84677/4.88266. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.84614/4.89165. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.84437/4.89874. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.84971/4.90014. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84364/4.89563. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.84572/4.89304. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.84243/4.89237. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84689/4.88138. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.84668/4.88226. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.84646/4.88499. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84562/4.88369. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84299/4.88317. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.84536/4.88641. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84295/4.90119. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84573/4.89386. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.83997/4.90244. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.83915/4.89746. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84452/4.89330. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.83937/4.91672. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.83826/4.90260. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.83647/4.89951. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84164/4.88524. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.83529/4.91863. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.83791/4.88759. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.83813/4.90740. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.83196/4.91998. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83746/4.89874. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.83402/4.91076. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.83264/4.92210. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84165/4.89353. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.83604/4.90887. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83659/4.90371. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.83634/4.89047. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.83118/4.90771. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.83714/4.90485. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.83684/4.90514. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.82816/4.92849. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.83668/4.90150. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.83423/4.91299. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.83187/4.91378. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.83875/4.91114. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.83649/4.90466. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.82775/4.92571. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.83092/4.91392. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.82836/4.91537. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.83231/4.90797. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.83304/4.93194. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.83145/4.89496. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.83630/4.92334. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.83012/4.91843. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.83014/4.94197. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83113/4.91551. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.82936/4.92156. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.82784/4.93155. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.83258/4.91117. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.82672/4.93591. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.82649/4.93277. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.82784/4.92681. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.83530/4.90545. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.82343/4.92606. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.82453/4.92914. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.82354/4.95598. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.82311/4.91286. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.82825/4.92517. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.82532/4.91030. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.82222/4.91947. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.82605/4.91367. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.82443/4.95065. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.82592/4.92763. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.82369/4.91101. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.82250/4.93178. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.82456/4.94881. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.82983/4.92966. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.81441/4.97696. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82781/4.90267. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82518/4.92750. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82899/4.89990. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82554/4.93606. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82957/4.92556. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.81931/4.90347. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82165/4.93302. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82324/4.91494. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82098/4.93795. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.82714/4.92005. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.0757356892408296\n",
      "Epoch 0, Loss(train/val) 4.91477/4.87444. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86073/4.88398. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.85539/4.87519. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.85299/4.87412. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.84993/4.87557. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.85337/4.87598. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.85334/4.87593. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.85147/4.88038. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.84988/4.88319. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.85147/4.88107. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84988/4.88311. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.85168/4.88826. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.84639/4.89330. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.84567/4.88972. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85252/4.87726. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.84959/4.88108. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.84771/4.88671. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.85139/4.88786. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85348/4.89743. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.85115/4.89961. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84734/4.89921. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.84558/4.91021. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.84359/4.90581. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84365/4.90259. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.84866/4.89507. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.84485/4.90294. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84908/4.89254. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84209/4.89942. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 4.83965/4.90286. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.83877/4.91003. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.83789/4.90443. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84268/4.90399. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.84023/4.90104. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84224/4.89878. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84210/4.89454. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.85171/4.85444. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.84687/4.87953. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84745/4.87938. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84277/4.88981. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.83983/4.89806. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.83837/4.90017. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.84343/4.89577. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.83798/4.90996. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.83045/4.90924. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.83931/4.91114. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84008/4.90040. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.83578/4.91426. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83336/4.91568. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.83667/4.91459. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.83763/4.90619. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.82947/4.89493. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.83884/4.90285. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.83728/4.89995. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.83329/4.90705. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.83870/4.90308. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.83883/4.89715. Took 0.21 sec\n",
      "Epoch 56, Loss(train/val) 4.82689/4.90887. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.83042/4.92006. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.83986/4.90020. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.83492/4.89353. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.83290/4.90835. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.82975/4.90688. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.83126/4.89895. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.83381/4.91703. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.83132/4.91583. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.83379/4.91253. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.83330/4.90193. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.82869/4.91742. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.83426/4.90555. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.82909/4.91302. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.83224/4.91427. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83033/4.91435. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.82977/4.91296. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.82671/4.91616. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.82803/4.92897. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.83266/4.92177. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.82571/4.94609. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.83532/4.91954. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.82692/4.92502. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.82300/4.93364. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.83115/4.92046. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.82729/4.93562. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.83004/4.90885. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.82873/4.91572. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.82820/4.91662. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.82191/4.92566. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.82677/4.92874. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.82557/4.94095. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.82480/4.93045. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82209/4.94874. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82516/4.92808. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.83182/4.91875. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82821/4.93300. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82195/4.92974. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82391/4.93954. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82362/4.95295. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.82485/4.93069. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82533/4.93064. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82008/4.96590. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.82871/4.95687. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.06825406626599889\n",
      "Epoch 0, Loss(train/val) 5.07262/5.04349. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.00380/5.00389. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.99108/5.00264. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.98916/5.00383. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.98934/5.00586. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.98981/5.00568. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.98551/5.00604. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.98840/5.00666. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.98545/5.01274. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.98655/5.02569. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.98174/5.01452. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.97968/5.00942. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.97947/5.01051. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.98316/5.01145. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.97683/5.01906. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.98126/5.01409. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.97915/5.01594. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.97801/5.02039. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.97615/5.02256. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.97915/5.02531. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.97906/5.02218. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.97493/5.02488. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.97406/5.02987. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.97655/5.02533. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.97473/5.02646. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.97042/5.03239. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.97680/5.02288. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.97242/5.03052. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.97283/5.02699. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.97430/5.03284. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.97112/5.03413. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.96911/5.04451. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.97579/5.02552. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.97206/5.03976. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.97219/5.03906. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.96847/5.04202. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.97005/5.02904. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.96931/5.03588. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.96680/5.03389. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.96337/5.04631. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.96856/5.01850. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.97012/5.02880. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.96903/5.02862. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.96778/5.04846. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.97120/5.02046. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.96425/5.05921. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.97000/5.03015. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.96462/5.04993. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.96786/5.04341. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.96251/5.07046. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.97085/5.02131. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.95955/5.07898. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.96839/5.02067. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.96278/5.08055. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.96763/5.03301. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.96633/5.04592. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.96429/5.05010. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.96469/5.01225. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.95754/5.07345. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.96358/5.03571. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.96332/5.05278. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.96332/5.05887. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.96285/5.04925. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.96285/5.06702. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.95886/5.08083. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.95662/5.03602. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.96015/5.05337. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.95580/5.07395. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 4.95578/5.05994. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.95660/5.02278. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.96277/5.04645. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.95807/5.03532. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.95294/5.08200. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.95668/5.04010. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.95459/5.07713. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.95903/5.02796. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.95700/5.05659. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.94645/5.04015. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.95082/5.06889. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.95235/5.05406. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.94869/5.06472. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.95600/5.03191. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.94934/5.03959. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.95663/5.03932. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.94835/5.07625. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.94055/5.04275. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.94370/5.05890. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.95541/5.02880. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.94636/5.08544. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.93995/5.06092. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.93627/5.04339. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.94174/5.03912. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.94953/5.03807. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.93967/5.06067. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.94717/5.03695. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.93692/5.04544. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.93626/5.03566. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.94773/5.04363. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.93952/5.09115. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.94524/5.07987. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.03138824102871723\n",
      "Epoch 0, Loss(train/val) 4.91541/4.86694. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.86575/4.86030. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.86243/4.85985. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.86384/4.86014. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.86179/4.86222. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.86115/4.85891. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.86266/4.85964. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.86291/4.85991. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.86083/4.86000. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.86128/4.85930. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.85715/4.86013. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.86012/4.85966. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85862/4.85938. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.86158/4.86308. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85708/4.86490. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85508/4.86591. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85641/4.86714. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85325/4.86774. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85566/4.86822. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.85523/4.87248. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.85573/4.86888. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85440/4.87423. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85249/4.87640. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85351/4.87841. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85260/4.87733. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.85176/4.87915. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.85360/4.88084. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.85099/4.88318. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.85300/4.88468. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84636/4.88761. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84913/4.88309. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84516/4.90947. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.85069/4.89073. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.85062/4.89923. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.84880/4.89524. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.85087/4.90274. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84735/4.90415. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84643/4.92174. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84165/4.91133. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84202/4.90612. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.85967/4.89510. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.85067/4.90353. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.84448/4.90667. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.84408/4.91775. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84417/4.92814. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84630/4.92082. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.84431/4.91983. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84449/4.91639. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.84364/4.92699. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.84485/4.91850. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.84636/4.92819. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.83864/4.92866. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.84314/4.94658. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.84687/4.92165. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.84462/4.93916. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.84152/4.93605. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.83734/4.95148. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.83698/4.94754. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.84276/4.93971. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.84077/4.93968. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.84068/4.93995. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84176/4.95110. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.83968/4.93131. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.83751/4.94257. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.83984/4.95668. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.83823/4.94784. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.83211/4.97457. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83756/4.95040. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.83168/4.95900. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.83742/4.95894. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.83477/4.96395. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.83276/4.95544. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83873/4.94332. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83618/4.96934. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.83602/4.95879. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.83795/4.95792. Took 0.22 sec\n",
      "Epoch 76, Loss(train/val) 4.83795/4.96861. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.83233/4.95513. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.83097/4.97693. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.83480/4.95147. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.83259/4.95882. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83550/4.94813. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.83135/4.97517. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.83518/4.94983. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.82821/4.97413. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.83235/4.95494. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.82964/4.97985. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.83215/4.95898. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.82883/4.96987. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82900/4.95793. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.82898/4.95613. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82736/4.97323. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82135/4.99390. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83026/4.96062. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83094/4.96053. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82957/4.97658. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82582/4.98254. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.82415/4.98329. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82970/4.99039. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.84002/4.91326. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.0821440096816907\n",
      "Epoch 0, Loss(train/val) 4.93782/4.86979. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.83268/4.83549. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.83457/4.82906. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.83516/4.82998. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.83647/4.83222. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.83536/4.83086. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.83278/4.82889. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.83531/4.82933. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.83534/4.82747. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.83615/4.82770. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.83294/4.82611. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.83104/4.82721. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.83391/4.82761. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.82896/4.82765. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.83115/4.82542. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.83096/4.82833. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.83166/4.82787. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.83223/4.83219. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.82795/4.82624. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.83225/4.83689. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.82776/4.84383. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.82561/4.83838. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.82629/4.84492. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.82796/4.83782. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82805/4.84630. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82306/4.84022. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82198/4.85217. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.81915/4.83765. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.82063/4.84993. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.81943/4.83667. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.81573/4.85758. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.82563/4.84663. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.81861/4.85891. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.81658/4.85324. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.81993/4.84835. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.81875/4.85644. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.81772/4.85023. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.81292/4.85180. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.80999/4.85712. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.81175/4.86236. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.80894/4.84070. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.81815/4.85250. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.81883/4.84490. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.81777/4.85489. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.81272/4.85284. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.81374/4.84042. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.81163/4.84001. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.80576/4.84657. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.81203/4.84577. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.80252/4.85133. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.80994/4.85109. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.80844/4.84282. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81140/4.84136. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.80711/4.84478. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.80441/4.85667. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.80739/4.84154. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.80971/4.84125. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.81414/4.85561. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.80615/4.86762. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.80086/4.86552. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.81073/4.85328. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.80756/4.85567. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.79734/4.86643. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.79512/4.90843. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.80909/4.84047. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.80445/4.86176. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.80486/4.83934. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.80502/4.84774. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.79111/4.87973. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.81214/4.83393. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.80455/4.85253. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.80313/4.85667. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.80804/4.84354. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.79709/4.84976. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.80516/4.85145. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.79824/4.84032. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.79783/4.86883. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80053/4.85696. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.79747/4.86136. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.79793/4.86797. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.81845/4.82376. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.82490/4.81192. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.81259/4.81071. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80821/4.82045. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 4.80234/4.83976. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.79670/4.86463. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.79327/4.88956. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.79731/4.88044. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.79495/4.87813. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.80682/4.85346. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 4.79171/4.88486. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.79560/4.88217. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.79187/4.88984. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.78851/4.90744. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.79178/4.87415. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79384/4.87263. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.79409/4.89064. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.78305/4.89239. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.79462/4.88901. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.78710/4.89245. Took 0.19 sec\n",
      "ACC: 0.40625, MCC: -0.1746031746031746\n",
      "Epoch 0, Loss(train/val) 4.96576/4.99311. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.95700/4.93014. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.94210/4.94454. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.94490/4.94497. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.94253/4.94475. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.94404/4.95040. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.94278/4.95349. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.94459/4.95950. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.94518/4.95906. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.94269/4.95278. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.94205/4.95249. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.94224/4.94874. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.94045/4.94817. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.93652/4.94666. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.93981/4.95063. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.94224/4.94811. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.94179/4.94538. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.93474/4.94902. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.94285/4.94388. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.93675/4.94095. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.93663/4.94334. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.93673/4.94576. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.93926/4.93568. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.93443/4.94332. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.93506/4.94196. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.93497/4.94051. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.93328/4.93619. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.93599/4.93809. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.93233/4.93745. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.93226/4.93574. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.92866/4.92702. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.92810/4.94043. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.92593/4.92546. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.92821/4.93210. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.92771/4.93113. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.92485/4.93270. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.92521/4.92938. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.92572/4.92138. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.92321/4.94167. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.91923/4.92208. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.91747/4.92030. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.92119/4.92135. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.92322/4.92551. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.92199/4.93031. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.91874/4.91584. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.92054/4.91559. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.92053/4.91000. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.92283/4.91009. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.91574/4.90900. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.92319/4.91726. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.91662/4.91984. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.92146/4.90957. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.91973/4.92605. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.91822/4.92247. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.92212/4.90358. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.91517/4.91440. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.91526/4.92033. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.91675/4.91269. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.91726/4.92304. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.91328/4.91420. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.91263/4.91823. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.91209/4.92601. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.91292/4.91060. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.91931/4.90720. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.91788/4.91029. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.91514/4.92777. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.91725/4.91276. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.90863/4.91769. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.91763/4.91034. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.91146/4.90854. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.90707/4.92116. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.91301/4.91035. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.90929/4.90782. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.90999/4.91316. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.91448/4.91466. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.91032/4.91212. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.90953/4.90984. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.91924/4.90537. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.91475/4.91674. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.90984/4.91476. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.91471/4.92232. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.91308/4.91764. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.91049/4.91939. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.91526/4.90729. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 4.90920/4.90455. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.91465/4.93027. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.91972/4.93571. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.91503/4.95304. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.92313/4.92133. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.91134/4.93046. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.91287/4.93432. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.91671/4.91588. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.91188/4.92354. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.91075/4.91337. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.91259/4.92021. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.90758/4.91227. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.91155/4.91194. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.90894/4.92240. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.91112/4.91391. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.91177/4.91878. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.10421364855284106\n",
      "Epoch 0, Loss(train/val) 5.03526/5.00033. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.00396/4.99752. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.99953/5.00436. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.00093/5.00515. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.99669/5.00762. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.99691/5.00921. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.99583/5.00955. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.99581/5.00937. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.99606/5.00594. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.99484/5.00487. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.99201/5.00351. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.99324/5.00752. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.99116/5.00206. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.98882/5.00571. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.98644/5.00451. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.98659/5.00491. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.98819/5.00347. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.98912/5.00325. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.98484/5.00640. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.98440/5.00942. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.98571/4.99909. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.98513/5.00361. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.98228/5.00357. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.97915/5.00276. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.98096/5.00726. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.98009/5.00099. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.98370/4.99517. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.98074/4.99565. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.98025/4.99698. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.98127/4.99577. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.97909/4.99648. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.97826/4.99295. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.97340/4.99925. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.97969/4.98313. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.97645/4.98892. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.97384/4.98932. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.97431/4.98632. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.97922/4.98591. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.97185/4.99602. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.97695/4.99466. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.97140/4.99580. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.96942/4.99856. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.97259/4.99539. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.96962/4.98840. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.96566/4.98765. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.97167/4.99257. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.96885/4.99995. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.96771/4.99749. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.96901/4.99649. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.96617/4.99516. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.96831/5.00087. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.96598/4.99816. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.95863/5.00747. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.97755/5.00564. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.97015/5.00381. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.96967/4.99858. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.96383/5.00811. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.96492/5.00456. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.97051/4.99311. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.96819/4.99527. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.96321/4.99065. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.96451/4.99862. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.96483/4.99273. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.95882/4.99865. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.96784/4.99250. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.96831/5.00441. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.96701/4.99943. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.96713/4.99481. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.96131/4.99742. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.95736/4.99384. Took 0.21 sec\n",
      "Epoch 70, Loss(train/val) 4.96196/4.99306. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.96182/5.00270. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.96933/4.99139. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.95911/5.00370. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.95870/4.99143. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.95882/5.00068. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.96418/4.99479. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.95986/5.00372. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.95727/4.99153. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.96208/4.99243. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.95435/4.99825. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.95900/4.99788. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.96209/4.99145. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.95969/4.99091. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.95624/4.98960. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.95721/4.98772. Took 0.21 sec\n",
      "Epoch 86, Loss(train/val) 4.95731/5.00042. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.96068/4.99579. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.95689/5.00120. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.96240/4.99058. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.95498/4.99906. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.95796/4.99320. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.95648/4.99002. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.95678/5.00231. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.95392/5.00165. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.95254/5.00344. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.95550/4.98966. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.95615/4.99741. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.95317/4.99485. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.95904/4.98630. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.043224813349211175\n",
      "Epoch 0, Loss(train/val) 5.08391/5.05986. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.03818/5.05677. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.03843/5.06666. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.04077/5.06206. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.04572/5.05497. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.04736/5.05827. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.04717/5.07255. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 5.04215/5.06706. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.03796/5.06592. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.03706/5.06506. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.03693/5.06446. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.03573/5.06243. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.03678/5.06331. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.03471/5.06267. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.03425/5.06397. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.03531/5.07011. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 5.03590/5.07010. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.03476/5.07117. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.03449/5.07312. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.03433/5.07357. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.03320/5.07317. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.03200/5.06804. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.03648/5.07203. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.03476/5.06867. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.03506/5.06625. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.03073/5.07421. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 5.03190/5.07229. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.03354/5.05842. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.03201/5.05797. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.03081/5.05798. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.03386/5.06922. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.03198/5.06515. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.03069/5.06874. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.03014/5.06551. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.02828/5.06666. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 5.02904/5.07153. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.02994/5.07976. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 5.02987/5.07739. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.02932/5.07870. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.02966/5.07908. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.03064/5.07692. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.02763/5.07856. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.02832/5.07272. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 5.02299/5.07582. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.02483/5.06307. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.02416/5.07847. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.02383/5.07198. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.02892/5.07799. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 5.02522/5.06682. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.01791/5.07505. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.02545/5.06431. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.01831/5.08423. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.02295/5.07903. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.02097/5.07363. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.02277/5.07832. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.01797/5.08013. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.01556/5.08617. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.02409/5.07745. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.01606/5.08364. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.01667/5.08156. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.01681/5.07346. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.01137/5.09597. Took 0.22 sec\n",
      "Epoch 62, Loss(train/val) 5.02371/5.08323. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.02823/5.05975. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.02334/5.06491. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.03710/5.05301. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.03239/5.06199. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.02345/5.08631. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.02556/5.08095. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.01811/5.09332. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.01450/5.08139. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.01734/5.09444. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.01364/5.07754. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.01672/5.08331. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.01607/5.08444. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 5.01639/5.07879. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 5.00461/5.09351. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.00927/5.10329. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 5.04039/5.03297. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.02820/5.03555. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.02339/5.03822. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 5.02826/5.04191. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 5.02850/5.04485. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.02064/5.05486. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.02165/5.05376. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.02450/5.05292. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 5.01946/5.06388. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.03295/5.05251. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.03506/5.05878. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 5.02927/5.05490. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.02704/5.05691. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 5.02248/5.06459. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.02229/5.05899. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.01424/5.06128. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 5.01794/5.07671. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 5.01954/5.07293. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.01584/5.06296. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 5.01531/5.06502. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.01571/5.08624. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 5.01450/5.07723. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.03734655007457467\n",
      "Epoch 0, Loss(train/val) 4.80204/4.78811. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.77490/4.78068. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.77227/4.78491. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.77433/4.77848. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.77886/4.78283. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.77209/4.78568. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.76687/4.78948. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.77414/4.78633. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.77005/4.78366. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.77121/4.78953. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.76864/4.79038. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.76691/4.79929. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.76759/4.79885. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.76637/4.80290. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.76682/4.79859. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.76266/4.80327. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.76137/4.80347. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.75673/4.80653. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.75647/4.80008. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.77032/4.78337. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.76634/4.80684. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.77128/4.81398. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.76949/4.80158. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.76438/4.78841. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.76565/4.79137. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.76195/4.79752. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.75793/4.80227. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.75876/4.80306. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.76224/4.80411. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.76021/4.79390. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.76357/4.78882. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.76451/4.77829. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.75831/4.78030. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.75709/4.78012. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.75688/4.78327. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.76136/4.78628. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.75446/4.79932. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.75582/4.79524. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.75458/4.79234. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.75817/4.78716. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.75176/4.81490. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.75151/4.80150. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.74995/4.80127. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.75383/4.79047. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.75574/4.77563. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.75375/4.79284. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.74971/4.81236. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.74585/4.83829. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.75541/4.79816. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.75044/4.82523. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.75126/4.80464. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.74745/4.81965. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.74577/4.81648. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.74592/4.82210. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.75322/4.82091. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.75515/4.79307. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.76097/4.80742. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.75433/4.80401. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.75232/4.82269. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.75759/4.78431. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.74902/4.82544. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.74953/4.81551. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.75497/4.80962. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.74999/4.81630. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.74880/4.80801. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.75940/4.78474. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75538/4.78580. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.75806/4.79269. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.75503/4.78267. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.74468/4.80590. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.75141/4.81338. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.75472/4.82320. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.74610/4.81038. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.74841/4.82968. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.75058/4.81098. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 4.74810/4.83203. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.74999/4.80771. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.74464/4.85573. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.74574/4.82200. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.74973/4.80574. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.75390/4.80221. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.75039/4.79064. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.76046/4.78845. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.75340/4.80686. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.74569/4.82480. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.74250/4.82908. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.74927/4.81125. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.74606/4.82066. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.74252/4.79927. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.74576/4.81857. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.74743/4.82432. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.74327/4.83799. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.75071/4.81131. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.74877/4.81699. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.74321/4.83678. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.73920/4.81886. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.73893/4.82928. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.74441/4.82155. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.73903/4.84532. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.73947/4.83136. Took 0.21 sec\n",
      "ACC: 0.5, MCC: -0.008866995073891626\n",
      "Epoch 0, Loss(train/val) 4.73031/4.76148. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.72139/4.74645. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.73168/4.70753. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.73028/4.71049. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.71942/4.70486. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.71296/4.70477. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.71369/4.70498. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.71406/4.70495. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.71271/4.70470. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.71507/4.70409. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.71195/4.70359. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.71133/4.70524. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.71382/4.71126. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.71131/4.71583. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.70928/4.72186. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.70926/4.72488. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.70805/4.72259. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.71024/4.71436. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.71260/4.71117. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.70648/4.71207. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.70916/4.71843. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.70682/4.72807. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.70369/4.74260. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.71009/4.73692. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.70881/4.72898. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.70713/4.73541. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.70406/4.74472. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.70420/4.75715. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.70583/4.74680. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.70415/4.74589. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.69922/4.76247. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.70274/4.74604. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.70399/4.74540. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.69967/4.74929. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.69429/4.76417. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.69776/4.76382. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.69860/4.75264. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.69727/4.76851. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.69908/4.76267. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.69636/4.77136. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.69508/4.76873. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.69211/4.76896. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.69497/4.77202. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.69292/4.77718. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.69365/4.77184. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.69390/4.77529. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.69009/4.78038. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.69353/4.77382. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.69439/4.78610. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.68927/4.77701. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.69064/4.78965. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.68836/4.79416. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.69062/4.77643. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.68506/4.80102. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.68827/4.78519. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.68757/4.78333. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.69064/4.78939. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.68835/4.80268. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.68835/4.78499. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.68539/4.80417. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.69370/4.77402. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.69203/4.76118. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.68495/4.78437. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 4.68810/4.78577. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.68849/4.78928. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.68261/4.79450. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.68524/4.79347. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.68555/4.79234. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.68641/4.78819. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.68674/4.77836. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.68333/4.77661. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.68371/4.77699. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.68630/4.79459. Took 0.18 sec\n",
      "Epoch 73, Loss(train/val) 4.69024/4.78107. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.68548/4.78364. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.68079/4.78616. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.68474/4.78221. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.68499/4.78132. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.68736/4.77611. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.68331/4.78549. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.68443/4.80066. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.68457/4.80793. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.68094/4.80912. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.68071/4.77626. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.67851/4.80530. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.67799/4.78210. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.68722/4.77611. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.68086/4.80282. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.68175/4.78030. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.68028/4.78096. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.67770/4.79656. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.67813/4.79847. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.67845/4.76063. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.67919/4.80239. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.68023/4.75996. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.67999/4.79377. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.68005/4.78855. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.67562/4.80838. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.67169/4.79210. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.68041/4.76608. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.03779786830012622\n",
      "Epoch 0, Loss(train/val) 4.94275/4.81962. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.82494/4.82495. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.81957/4.83071. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.81817/4.82901. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.81826/4.82957. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.81775/4.82613. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.81507/4.82928. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.81606/4.83547. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.81740/4.84283. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.81696/4.83901. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.81413/4.83952. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.81358/4.84577. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.81538/4.84282. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.81082/4.85021. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.81315/4.84015. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.81322/4.83490. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.81282/4.84240. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.81346/4.84074. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81005/4.84368. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.81122/4.84080. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.81313/4.84314. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.80718/4.84749. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.80720/4.85257. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.81034/4.84410. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.80781/4.84555. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.80732/4.84877. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.80581/4.84395. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.80503/4.84682. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.80876/4.84674. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.80531/4.84493. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.80505/4.85301. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.81022/4.85468. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.80338/4.85753. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.80333/4.86293. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.80318/4.85933. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.80325/4.86865. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.80433/4.86083. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.79506/4.87691. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.79608/4.88059. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.80070/4.86995. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.80519/4.85918. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.79598/4.88748. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.79821/4.87304. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.79760/4.87159. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.79656/4.89301. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.80026/4.86904. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.79370/4.87703. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.79815/4.88546. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.79484/4.88859. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.79762/4.87876. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.79227/4.87186. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.79373/4.88988. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.79743/4.85222. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.79189/4.87901. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.78812/4.89447. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.78980/4.87369. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.79376/4.88905. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.78766/4.87929. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.79232/4.87016. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.78642/4.87417. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.78949/4.88294. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.78668/4.89735. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.78848/4.87380. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.78674/4.90159. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.79023/4.91161. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.78559/4.87051. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.79168/4.88675. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.79006/4.89027. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77757/4.92767. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.78693/4.88931. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.79384/4.88006. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.78359/4.89692. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.78367/4.92067. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.78909/4.90973. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.78605/4.90048. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.78140/4.91828. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.78425/4.89580. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.78206/4.86532. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.77823/4.90131. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.78362/4.88699. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.78228/4.89098. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.77683/4.89952. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.78052/4.89384. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.77698/4.89155. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.78535/4.89084. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.77960/4.90226. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.77675/4.89952. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.78074/4.88450. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.77433/4.92055. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.77306/4.93248. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.77955/4.91649. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.77818/4.90546. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.77259/4.91379. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77899/4.87840. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77760/4.89583. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.77782/4.90873. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.77199/4.91608. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.77971/4.88810. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.77257/4.93396. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.78019/4.89704. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.007889684472185849\n",
      "Epoch 0, Loss(train/val) 4.83463/4.82134. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.78460/4.81027. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.78288/4.81536. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.78316/4.81640. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.78127/4.81714. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.78371/4.81634. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.78366/4.82336. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.78469/4.81712. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.78543/4.81146. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79162/4.81014. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.78932/4.80820. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78755/4.81219. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.78425/4.81469. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.77879/4.81832. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78173/4.81782. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78322/4.82050. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78257/4.81676. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.77815/4.82241. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.77844/4.82127. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.77842/4.81587. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.77845/4.82441. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.77592/4.82343. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.77820/4.82578. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78046/4.82288. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.77568/4.82749. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.77639/4.83159. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.77544/4.82371. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.77700/4.83087. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.77701/4.82042. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.77410/4.82804. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.77251/4.83256. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77129/4.82756. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77462/4.83264. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.76944/4.83327. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.77309/4.83540. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77490/4.82250. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77110/4.82605. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.76951/4.83157. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77200/4.81641. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77225/4.84539. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.76965/4.82635. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.76926/4.82368. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.77086/4.82942. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.77087/4.82050. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77104/4.82354. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.76519/4.81467. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.76572/4.81851. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.77145/4.81405. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.76427/4.82203. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.76369/4.82504. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76474/4.81281. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.75998/4.82337. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.76751/4.80272. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 4.76640/4.80206. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76410/4.81375. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.76189/4.81690. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.76125/4.80774. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.75813/4.81448. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.76383/4.80906. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.75920/4.82120. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.75909/4.81409. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.75980/4.82073. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.76066/4.82586. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.76516/4.81748. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.75970/4.81657. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.75612/4.82417. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75548/4.81648. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.76069/4.81418. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.76241/4.81976. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.75752/4.82200. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.75390/4.82227. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.75905/4.82824. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.74971/4.82679. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.75435/4.81014. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.75582/4.81864. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.75486/4.81639. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.74768/4.81886. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.75366/4.84932. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.75828/4.83834. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.76030/4.82381. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.75587/4.83430. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.75685/4.83564. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.74898/4.83155. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.75564/4.83572. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.75037/4.83224. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.74990/4.84133. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.75544/4.81107. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.75223/4.83691. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.74881/4.82986. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.74795/4.83413. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.75512/4.82820. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.74807/4.83611. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.74820/4.83104. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.74434/4.82765. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.74433/4.84364. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.74601/4.83051. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.75110/4.84834. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.75035/4.82500. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.74301/4.85232. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75007/4.84152. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.19136555680572745\n",
      "Epoch 0, Loss(train/val) 4.92762/4.90934. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.87483/4.88862. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.87180/4.90131. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87055/4.90444. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.86740/4.91090. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.87085/4.91596. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.86512/4.91253. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.86188/4.92084. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.86344/4.92138. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.86248/4.92816. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.86313/4.92732. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.86406/4.93122. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.86145/4.92408. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85913/4.92018. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.86281/4.91999. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.86127/4.92337. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 4.85923/4.93636. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85683/4.93705. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85657/4.94113. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.85612/4.94791. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.85666/4.94406. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85922/4.92989. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.85577/4.94790. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85689/4.93959. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85158/4.95167. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.85418/4.94994. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.85284/4.94826. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.85384/4.94349. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.85383/4.94988. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.85388/4.94562. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.85323/4.96579. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.85114/4.95280. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.85359/4.95490. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.85116/4.94866. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85303/4.95113. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84635/4.95423. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84982/4.94950. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.85301/4.95806. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84925/4.96443. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84597/4.97928. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84761/4.93881. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84898/4.96680. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.84641/4.98905. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.85014/4.93358. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84915/4.97412. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84812/4.96248. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84489/5.00928. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84605/4.95811. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.84551/4.99433. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.84373/4.97937. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.84594/4.96974. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.84721/4.96795. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.84753/4.97468. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.84274/4.97995. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.84547/4.97166. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.84105/4.99186. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84226/4.97615. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.84172/4.98638. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.84439/4.99895. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.83792/4.99028. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.83994/4.97401. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84292/4.99331. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.84228/5.00525. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 4.84589/4.96538. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.84463/4.98719. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.84092/4.97002. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.84158/4.99537. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.83541/4.99990. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.83867/4.97257. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.83719/4.96076. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.83989/4.96192. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83895/4.99922. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.84515/4.97399. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84758/4.95735. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.84295/5.00899. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.83628/4.96681. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83760/5.01537. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.83616/4.98546. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.84189/4.99267. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.83341/5.01524. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.83514/4.98868. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83615/5.00382. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.84397/4.97507. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84209/4.96550. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83894/4.98888. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83755/5.00087. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.83611/4.98009. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.83551/5.00603. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83414/4.98400. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.83537/5.02581. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.84176/4.96569. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.83897/4.97016. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.83176/5.00418. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83867/4.98796. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83615/5.00000. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.83377/4.99641. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83695/5.00240. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82991/5.01424. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.83797/5.01423. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.83621/5.00267. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.94823/5.00622. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 4.92907/4.93129. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.92276/4.91872. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.92785/4.91354. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.93253/4.91848. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.93095/4.91429. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.92278/4.91685. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.91998/4.91525. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.92200/4.91514. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.92296/4.91502. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.92216/4.91531. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.92102/4.91576. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.92217/4.91607. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.92046/4.91634. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.92019/4.92014. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.91933/4.91944. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.92110/4.92460. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.91818/4.92392. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.91927/4.92519. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.91777/4.93416. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.91766/4.93665. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.91736/4.94035. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.91428/4.94419. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.91340/4.94621. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.91182/4.95259. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.91286/4.95431. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.90965/4.96279. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.90852/4.97237. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.91548/4.96163. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.91172/4.96479. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90915/4.98194. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.90168/4.99024. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.90598/4.98392. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.90427/4.99628. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.91325/4.95857. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90682/4.96384. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.90302/4.99061. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.90400/5.00060. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.90962/4.97183. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90353/4.98131. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90198/4.99524. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.90095/4.96956. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.90466/4.97406. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.90277/4.98921. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.90730/4.98003. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.90385/4.98211. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.89783/5.00456. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.90486/4.94735. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.90168/4.98053. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.90393/4.96714. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.90109/4.97312. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.89752/4.99653. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89929/4.98562. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.90136/4.97862. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.89432/4.98110. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.89708/5.00019. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.89993/4.96325. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.89475/4.98388. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.89868/4.97863. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.90010/4.97587. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.89996/4.99415. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.89707/4.98840. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.89433/4.98828. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.89585/4.97729. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.89756/4.97632. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.89577/4.99315. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.89079/4.98680. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.89451/4.98838. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.89865/4.96466. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.89339/4.99177. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.89464/4.98473. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.88940/4.99598. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.89131/4.97943. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.89201/4.99173. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.89152/4.99381. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.89308/5.00047. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.89224/4.98059. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.89130/4.96951. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.89219/5.00298. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.88773/4.99374. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.89124/4.98084. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.89436/4.99163. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.89272/4.99351. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.88585/4.99524. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88939/4.97860. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.89259/4.97555. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.88925/5.00367. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.88619/4.98660. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.89138/5.00373. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.88805/4.98237. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.88608/4.99994. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.88751/4.98926. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.88386/4.99135. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.89013/5.00884. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.89289/4.98381. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.88370/5.02106. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.88890/4.97434. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.88497/5.02877. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.89101/5.00143. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.88363/5.00265. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.08818077954471167\n",
      "Epoch 0, Loss(train/val) 4.82922/4.77428. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.76443/4.75269. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.75826/4.75733. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.75644/4.75880. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.75417/4.75808. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.75362/4.75372. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.75267/4.75458. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.75127/4.74708. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.74989/4.73975. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.75243/4.74466. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.74901/4.74094. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.74832/4.73976. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.74851/4.74026. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.74478/4.74087. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.74426/4.74495. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.74402/4.74560. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.74105/4.75002. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.74401/4.74726. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.74324/4.74643. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.74245/4.74632. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.74061/4.75237. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.74109/4.75216. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.73791/4.75654. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.73882/4.75043. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.73748/4.75172. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.74049/4.74396. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.73730/4.74097. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.73768/4.74009. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.73337/4.74180. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.74037/4.74566. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.73199/4.74587. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.73296/4.75067. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.72999/4.75114. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.73519/4.75253. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.73024/4.75231. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.72548/4.76056. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.72800/4.76929. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.72961/4.76380. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.72979/4.75258. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.72588/4.76077. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.72483/4.76462. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.72582/4.76557. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.72862/4.76513. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.72078/4.77912. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.72774/4.77408. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.71865/4.78521. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.71987/4.77543. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.71123/4.78340. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.72415/4.77585. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.72103/4.78186. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.72105/4.78800. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.71816/4.78644. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.71907/4.76780. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.72205/4.77782. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.72099/4.77695. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.72898/4.75694. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.72018/4.78051. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.71763/4.78100. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.71373/4.81265. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.71765/4.78233. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.71533/4.79281. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.71657/4.78854. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.71150/4.80094. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.71641/4.80072. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.70805/4.77459. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.71447/4.80296. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.71850/4.79950. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.70948/4.79247. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.71359/4.79999. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.71935/4.77282. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.70858/4.80696. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.71590/4.79078. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.70878/4.79864. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.70385/4.80521. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.70708/4.81592. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.70558/4.80480. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.72830/4.75024. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.72181/4.76026. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.71049/4.78722. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.70800/4.79165. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.71147/4.80456. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.72007/4.79621. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.70362/4.80193. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.70800/4.80732. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 4.70609/4.78704. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.71463/4.79060. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.71215/4.79807. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.70589/4.81529. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.70682/4.80371. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.71568/4.79828. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.71606/4.80281. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.70744/4.82351. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.72240/4.80976. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.72147/4.79427. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.71952/4.80558. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.71335/4.80829. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.71399/4.80695. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.71576/4.79526. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.71126/4.80102. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.70798/4.80731. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.0629940788348712\n",
      "Epoch 0, Loss(train/val) 4.73978/4.71366. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.74363/4.70300. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.73134/4.70357. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.72797/4.70793. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.72639/4.70932. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.72707/4.71138. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.72908/4.72097. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.72905/4.73782. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.72632/4.74393. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.72618/4.74095. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.72498/4.73944. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.72530/4.73678. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.72445/4.73771. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.72307/4.74017. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.72402/4.73981. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.72241/4.73774. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.72398/4.73732. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.72368/4.74126. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.71986/4.73638. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.71776/4.73655. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.72542/4.74335. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.72425/4.74356. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.71949/4.74872. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.72111/4.74758. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.72013/4.74156. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.71821/4.75037. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.71687/4.74474. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.71952/4.74001. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.72383/4.73900. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.71524/4.75350. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.71790/4.74928. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.71414/4.76159. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.71806/4.75408. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.71490/4.75637. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.71281/4.75558. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.71634/4.75176. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.71341/4.75507. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.71168/4.75239. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.71263/4.76007. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.71304/4.75323. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.71061/4.76676. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.70896/4.76093. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 4.71833/4.74558. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.70982/4.75900. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.71161/4.75351. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.70884/4.76123. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.70699/4.77112. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.71066/4.75056. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.70617/4.76741. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.70836/4.77639. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.71287/4.75216. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.71023/4.76077. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.70735/4.76690. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.70845/4.76904. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.70803/4.76706. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.71336/4.76326. Took 0.21 sec\n",
      "Epoch 56, Loss(train/val) 4.70564/4.77455. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.70845/4.76468. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.70666/4.76483. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.70744/4.75529. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.70733/4.75987. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.70633/4.76544. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.70573/4.76641. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.70876/4.76809. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.70404/4.76668. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.70530/4.75882. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.70335/4.77305. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.70792/4.76449. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.70457/4.77489. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.70677/4.76454. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.70277/4.76866. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.70682/4.76153. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.70464/4.77107. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.70339/4.77585. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.70291/4.77442. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.70334/4.77930. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.70388/4.76575. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.70319/4.77188. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.70457/4.76830. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.70594/4.76904. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.69981/4.77177. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.69774/4.77445. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.70271/4.79374. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.70406/4.76719. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.70279/4.77264. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.70514/4.75264. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.69606/4.76017. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.69872/4.76281. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.69787/4.77045. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.70088/4.77626. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.69895/4.76819. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.69586/4.77730. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.70287/4.77553. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.69845/4.77623. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.69879/4.75838. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.69442/4.75950. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.69240/4.77112. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.69847/4.79980. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.70432/4.75837. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.69749/4.74295. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.72440/4.71491. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.69982/4.69373. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.68847/4.69863. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.69268/4.69857. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.69274/4.69365. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.69439/4.69331. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.69066/4.69785. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.68654/4.70419. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.68208/4.70954. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.68521/4.69893. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.68605/4.70422. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.68247/4.71023. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.68230/4.71319. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.68206/4.71027. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.67711/4.70915. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.67695/4.71376. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.67063/4.70697. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.67734/4.70577. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.67889/4.70068. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.67540/4.71083. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.67305/4.71139. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.67169/4.71546. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.67231/4.71377. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.66639/4.71734. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.66621/4.71417. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.66704/4.71418. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.67064/4.70689. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.66406/4.70585. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.67063/4.71484. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.66229/4.71881. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.66508/4.71250. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.66010/4.70383. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.66032/4.71604. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.65656/4.71243. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.65799/4.70995. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.66420/4.70561. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.65777/4.71564. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.65842/4.70523. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.66308/4.69912. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.65444/4.72017. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.66013/4.71031. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.65659/4.70505. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.67249/4.70674. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.68489/4.69438. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.66453/4.71854. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.65887/4.71157. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.65774/4.71404. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.65386/4.71724. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.67471/4.69314. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.67819/4.69567. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.67116/4.70826. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.66456/4.71119. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.66269/4.70873. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.65604/4.70085. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.65481/4.70145. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.65597/4.70205. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.65966/4.71638. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.65104/4.71615. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.65652/4.70720. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.65019/4.69733. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.65504/4.72579. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.65333/4.70795. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.64969/4.72602. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.64831/4.71040. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.65132/4.71536. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.64200/4.71184. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.64555/4.72734. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.64010/4.72350. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.65434/4.70798. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.64952/4.71327. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.65153/4.70415. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.64536/4.70193. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.64993/4.70913. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.64948/4.70280. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.63822/4.71496. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.64232/4.71930. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.64476/4.70508. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.64794/4.69689. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.63539/4.71756. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.63684/4.71288. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.64062/4.74024. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.63930/4.73268. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.64021/4.70931. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.63718/4.71252. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.64760/4.71278. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.63679/4.72975. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.64449/4.70205. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.63792/4.69998. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.64799/4.77633. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.65990/4.69096. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.65043/4.70856. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.64055/4.72603. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.64041/4.70456. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.63953/4.70914. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.63368/4.73602. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.64580/4.70282. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.64884/4.70119. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.64663/4.71499. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.64241/4.69500. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.64989/4.70544. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.00700902994282404\n",
      "Epoch 0, Loss(train/val) 5.01686/4.96486. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.95865/4.95500. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.95482/4.95593. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.95057/4.96013. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.95383/4.95996. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.95018/4.96387. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.95216/4.96149. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.95105/4.96533. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.94895/4.96794. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.95001/4.96668. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.94867/4.97102. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.95334/4.97159. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.95424/4.96985. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.95513/4.96807. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.95243/4.97181. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.95076/4.97789. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.94864/4.97848. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.94676/4.98096. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.94753/4.97536. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.94656/4.98104. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.94690/4.98432. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.94893/4.98346. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.94491/4.98792. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.94797/4.98512. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.94616/5.00021. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.94680/4.99278. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.94399/4.98795. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.94324/5.00017. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.94682/4.98456. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.94065/5.00751. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.94311/4.98852. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.93895/5.00587. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.94275/4.99359. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.93742/4.99508. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 4.93873/4.98705. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.93876/5.01301. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.93897/5.00248. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.93569/5.01270. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.93464/5.03757. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.94076/4.99964. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.93721/5.02144. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.93754/5.01828. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.93769/5.00376. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.93385/5.02142. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.93277/5.01083. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.92997/5.04226. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.93941/4.99817. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.92612/5.02642. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.92899/5.02246. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.94450/4.97882. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.94602/4.97683. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.94753/4.99259. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.94545/4.98940. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.94339/4.98688. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.93840/4.99771. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.93870/4.98808. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.93310/5.01189. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.93332/5.00131. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.92928/5.02870. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.92588/5.01178. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.92765/5.01865. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.92651/5.00051. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.93857/4.99676. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.92906/5.01628. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.92510/5.00462. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.93042/4.99991. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.92525/4.98604. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.91991/5.05182. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.92714/5.00682. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.92066/5.02058. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.92328/5.01507. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.93181/5.01103. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.92591/5.01750. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.92595/5.03115. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.92815/4.99078. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.91982/5.03661. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.92716/5.00616. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.92082/5.02504. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.92438/5.03399. Took 0.18 sec\n",
      "Epoch 79, Loss(train/val) 4.92107/5.01637. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.92291/5.02441. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.91891/5.02702. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.91912/5.01407. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.92040/5.01671. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.91226/5.06676. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.93247/5.00208. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.91454/5.02270. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.91855/5.01062. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.91632/5.00572. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.91636/5.02473. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.91984/5.01922. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.91620/5.01950. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.91430/5.02856. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.92562/4.99122. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.91448/5.00703. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.91215/5.06284. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.91521/4.98602. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.91729/4.99886. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.91234/5.01979. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.91422/4.99678. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.0657512425947884\n",
      "Epoch 0, Loss(train/val) 4.74036/4.65854. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.67286/4.66177. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.68082/4.66416. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.68127/4.66331. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.68059/4.66998. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.68069/4.67708. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.67850/4.67272. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.67523/4.67138. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.67602/4.67196. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.67289/4.67249. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.67147/4.67829. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.66999/4.67693. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.67211/4.68622. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.67196/4.67978. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.67008/4.68716. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.66597/4.68480. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.66739/4.68719. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.66636/4.69484. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.66340/4.70248. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.66173/4.70557. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.66391/4.69893. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.66240/4.70412. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.66166/4.71492. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.66693/4.67704. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.66543/4.68656. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.66156/4.70790. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.66380/4.70119. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.65801/4.70485. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.65993/4.69794. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.66360/4.69458. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.66095/4.70084. Took 0.21 sec\n",
      "Epoch 31, Loss(train/val) 4.66302/4.70347. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.65799/4.71701. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.65933/4.71424. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.66090/4.71252. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.65612/4.72209. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.65825/4.72242. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.65902/4.71973. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.65540/4.73475. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.65527/4.72283. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.66207/4.70852. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.65481/4.72907. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.65699/4.70918. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.65617/4.72856. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.64966/4.74655. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.65601/4.71645. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.65122/4.73178. Took 0.21 sec\n",
      "Epoch 47, Loss(train/val) 4.65466/4.71691. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.65348/4.74131. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.65224/4.71716. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.65052/4.72003. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.65602/4.72535. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.65809/4.73247. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.65047/4.72790. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.65170/4.72314. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.64670/4.76773. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.64643/4.73211. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.64353/4.74237. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.64614/4.74384. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.65400/4.72500. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.65039/4.73574. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.65217/4.73232. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.64272/4.75141. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.64796/4.74302. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.64789/4.73808. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.64550/4.73407. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.64922/4.74198. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.64525/4.75492. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.64602/4.73782. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.64224/4.76566. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.64903/4.72287. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.64506/4.73815. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.64006/4.76047. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.64228/4.74385. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.64543/4.74753. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.64838/4.75775. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.65108/4.73849. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.63747/4.76123. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.64427/4.73592. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.63843/4.75054. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.63899/4.74909. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.64497/4.72700. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.64344/4.72781. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.64047/4.74116. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.64395/4.71083. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.63639/4.73674. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.64274/4.74916. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.64582/4.73575. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.63901/4.75819. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.64560/4.74828. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.63637/4.75127. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.63478/4.74269. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.64338/4.71850. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.63878/4.72818. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.63542/4.72230. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.64238/4.72260. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.63867/4.72994. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.64630/4.69623. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.63708/4.74766. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.63303/4.73360. Took 0.20 sec\n",
      "ACC: 0.53125, MCC: 0.07192118226600985\n",
      "Epoch 0, Loss(train/val) 5.24250/5.18903. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.19758/5.20245. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.19603/5.19610. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.19466/5.19138. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.19392/5.19410. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 5.19278/5.18983. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.19399/5.18881. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.18721/5.18970. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.18838/5.18660. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.18688/5.18793. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.18826/5.18706. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.18785/5.18038. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 5.18998/5.18886. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.18348/5.19103. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 5.18119/5.18357. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 5.18674/5.18660. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.18254/5.18024. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.17902/5.17667. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.18417/5.19165. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.18672/5.18532. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.18365/5.18641. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.18125/5.18780. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 5.17967/5.18973. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.17936/5.18524. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.17632/5.19636. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 5.17769/5.20456. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.17496/5.20640. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 5.17484/5.22635. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.17700/5.22735. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 5.16960/5.22374. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 5.17578/5.22506. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.16947/5.23476. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.17544/5.22964. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.17293/5.24286. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.17239/5.23817. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.16714/5.24320. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.16636/5.23765. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.16933/5.23600. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 5.17024/5.24094. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.17112/5.23972. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 5.16704/5.25441. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.16624/5.24906. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.16573/5.24213. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.16699/5.24606. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.17427/5.24625. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 5.17070/5.26105. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 5.16145/5.25720. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 5.16440/5.27419. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.16720/5.25665. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.16662/5.25885. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.16800/5.24929. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 5.16216/5.26268. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 5.16268/5.26962. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 5.16633/5.24828. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 5.15977/5.26229. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 5.15541/5.27339. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.16541/5.26281. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 5.16123/5.27116. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.16329/5.26756. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 5.15491/5.26985. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.15991/5.28931. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.16852/5.26966. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.16204/5.27645. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.16265/5.28591. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.15925/5.27152. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.16319/5.27345. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 5.16091/5.27309. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 5.16502/5.25387. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.16012/5.28869. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 5.16492/5.27392. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.15910/5.28051. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.16125/5.26819. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.16377/5.28274. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.16119/5.27764. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.16246/5.26824. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 5.16179/5.26658. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.16262/5.28286. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 5.15748/5.27980. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 5.16007/5.27451. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.16026/5.27645. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 5.15729/5.27323. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.15817/5.29138. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 5.15579/5.27343. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 5.16028/5.27536. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 5.16092/5.30662. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.15805/5.28429. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.15959/5.26758. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.15770/5.27581. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 5.15640/5.28230. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 5.16347/5.27634. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.15716/5.30575. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.16011/5.28508. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 5.15444/5.28522. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 5.15554/5.29480. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.16147/5.32697. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.15831/5.27287. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.15657/5.28907. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.15557/5.28541. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.15843/5.28163. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.15991/5.28452. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.1659919028340081\n",
      "Epoch 0, Loss(train/val) 4.99059/4.93768. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.93953/4.95486. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.93999/4.95303. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.93547/4.95555. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.93860/4.95181. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.93700/4.94602. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.93710/4.94642. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.93390/4.95341. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.92987/4.95493. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.93176/4.95890. Took 0.22 sec\n",
      "Epoch 10, Loss(train/val) 4.93116/4.95262. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.92755/4.96015. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.92988/4.95520. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.93170/4.95722. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.92503/4.95913. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.92809/4.95649. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.92810/4.95492. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.92663/4.95767. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.92441/4.95698. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.92637/4.95653. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.92364/4.96044. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 4.92297/4.97637. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.92431/4.94750. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.92243/4.94660. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.92335/5.00196. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.93553/4.94902. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.92680/4.95244. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.92973/4.95668. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.92885/4.95887. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.92830/4.96350. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.92581/4.97094. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.92714/4.97179. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.92904/4.96500. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.92702/4.96203. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.92374/4.96786. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.92451/4.96980. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.92720/4.95764. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.91861/4.97266. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.92278/4.96567. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.92101/4.97795. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.92324/4.95297. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.92304/4.96105. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.92503/4.96359. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.92291/4.96851. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.92515/4.96257. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.92304/4.96617. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.92090/4.94694. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.92151/4.96338. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.91674/4.95676. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.92422/4.95405. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.91673/4.95156. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.91745/4.98993. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.91808/4.95550. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.91962/4.96243. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.91596/4.98533. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.91890/4.94502. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.91649/4.97856. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.91974/4.95823. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.91958/4.96067. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.91585/4.99014. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.91922/4.96715. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.91372/4.96519. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.91545/4.97379. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.91335/4.97436. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.91200/4.97888. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.91190/4.99486. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.91092/4.98704. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.92454/4.95153. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.93153/4.93884. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.92538/4.93726. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.92541/4.94506. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.92162/4.94189. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.91903/4.94177. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 4.91725/4.93906. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.91092/4.95208. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.91326/4.95690. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.91059/4.96553. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.91329/4.95072. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.91357/4.94803. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.91631/4.95666. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.90809/4.99801. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.90701/5.00632. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.90627/5.00904. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.90671/4.99470. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.91490/4.99551. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.91499/5.01254. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.91883/4.96716. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.90880/4.99507. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.90958/5.00874. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.90842/4.97231. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.91241/5.01379. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.90466/5.00628. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.90635/5.01325. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.90356/5.02599. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.90512/5.00451. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.90428/4.99832. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.91077/4.99422. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.90837/5.01365. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.90696/5.01180. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.91001/5.00515. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.17004550636718183\n",
      "Epoch 0, Loss(train/val) 5.09655/5.01475. Took 0.32 sec\n",
      "Epoch 1, Loss(train/val) 4.98923/5.03612. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.99520/4.97911. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 4.97740/4.97812. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.97506/4.98232. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.97712/4.98291. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.97447/4.98719. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.97663/4.98434. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.97474/4.98518. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.96987/4.99084. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.97141/4.99648. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.97252/4.99147. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.97231/4.99155. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.96701/4.99705. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.96831/5.00132. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.96816/4.99411. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.96690/5.00051. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.96762/5.00448. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.96463/4.99522. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.96667/5.00454. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.96246/5.00119. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.96018/5.01040. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.96180/5.00725. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.96365/4.99365. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.96300/4.99274. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.96022/5.00856. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.95817/5.00102. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.95785/5.01609. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.95893/4.99578. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.95611/5.02053. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.96086/5.01327. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.95538/5.02843. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.95848/5.00039. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.95727/5.01757. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.95491/5.01455. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.95633/5.00676. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.95661/5.02454. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.95466/5.01606. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.94948/5.03719. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.96154/5.01293. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.95151/5.01659. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.95895/5.00928. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.95464/5.01749. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.95188/5.01002. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 4.95258/5.02036. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.94822/5.02653. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.94851/5.02718. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.95426/5.02356. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.94481/5.01367. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.95373/5.01693. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.94749/5.02412. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.94956/5.02446. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.94844/5.03457. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.94690/5.03177. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.94340/5.04226. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.94665/5.03622. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.94617/5.03786. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.94089/5.04006. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.94507/5.04553. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.94412/5.04117. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.94622/5.03288. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.93605/5.04035. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.94544/5.06619. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.94293/5.04265. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.94364/5.06486. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.94354/5.04020. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.94409/5.06047. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.94056/5.06123. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.93975/5.02847. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.93665/5.06864. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.93969/5.04417. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.93982/5.06426. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.93815/5.06174. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.93435/5.07265. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.93945/5.02962. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.93889/5.07168. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.94069/5.02894. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.94281/5.07618. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.92972/5.07308. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.93932/5.05391. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.94204/5.05863. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.93066/5.09230. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.93534/5.06841. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.93289/5.08874. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.93252/5.04710. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.93982/5.03346. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.93353/5.08556. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.93522/5.04961. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.93419/5.11454. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.93515/5.05629. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.93547/5.05694. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.93680/5.08753. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.92932/5.07202. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.92516/5.06780. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.93433/5.08603. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.92466/5.11057. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.93472/5.06978. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.93678/5.04918. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.93405/5.08772. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.92853/5.07983. Took 0.19 sec\n",
      "ACC: 0.4375, MCC: -0.12855839970025792\n",
      "Epoch 0, Loss(train/val) 5.04724/5.02663. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.99121/4.99237. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.98562/4.99008. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.98344/4.99581. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.98185/5.00418. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.98355/5.00264. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.97764/5.00556. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.97750/5.00074. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.98137/5.00364. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.97746/5.00185. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.97649/5.00771. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.97359/5.01646. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.97471/5.00804. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.97667/5.01484. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.97278/5.01348. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.96915/5.03012. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.96979/5.03192. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.96865/5.03070. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.97149/5.02558. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.96660/5.03023. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.97084/5.03105. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.96913/5.03932. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.97153/5.02542. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 4.96949/5.03217. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.96801/5.04017. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.96631/5.04124. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.96974/5.03666. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.96469/5.04895. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.96506/5.04842. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.96377/5.04769. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.96170/5.05280. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.96124/5.04679. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.96563/5.05761. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.95955/5.05910. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.96445/5.04997. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.96511/5.05734. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.96436/5.05075. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.96066/5.05054. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.96369/5.04920. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.95908/5.07765. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.95799/5.06661. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.96370/5.06539. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.96218/5.05882. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.95673/5.07653. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.95867/5.07176. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.96231/5.06313. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.96062/5.05702. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.95598/5.07594. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.95770/5.09655. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.95709/5.08243. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.95467/5.07366. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.95651/5.08226. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 4.95405/5.08382. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.95104/5.11026. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.94947/5.10396. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.95471/5.07174. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.94866/5.08801. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.95839/5.05510. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.96868/5.04949. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.96753/5.05754. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.95702/5.04620. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.96044/5.08266. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.95364/5.07109. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.95346/5.08328. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.95554/5.08108. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.95559/5.04916. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.95445/5.05938. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.95658/5.07495. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.95287/5.09754. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.95075/5.05944. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.95353/5.08808. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.94943/5.08615. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.95114/5.10995. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.94688/5.06409. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.95014/5.10438. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.95316/5.06104. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.94828/5.08635. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.94354/5.12522. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.94919/5.08992. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.94537/5.09473. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.94226/5.10750. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.94620/5.08899. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.94959/5.06682. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.94288/5.10402. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.93923/5.12342. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.93531/5.09662. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.95082/5.07429. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.94557/5.06704. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.95374/5.10274. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.94465/5.09261. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.93545/5.12081. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.95043/5.12035. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.94184/5.04922. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.94127/5.10140. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.94229/5.05840. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.94129/5.11023. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.94152/5.10276. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.94788/5.08913. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.93895/5.08686. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.94885/5.07661. Took 0.20 sec\n",
      "ACC: 0.46875, MCC: -0.06362847629757777\n",
      "Epoch 0, Loss(train/val) 4.90528/4.86968. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.88099/4.85311. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.88033/4.85669. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.88519/4.87701. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.87900/4.89013. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.87114/4.87284. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.86958/4.87119. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.87221/4.87830. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.87146/4.87793. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87184/4.87642. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.87089/4.87851. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.86932/4.87581. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.86930/4.87920. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.86907/4.88185. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.86990/4.88075. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.86801/4.87795. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.86835/4.88305. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.86827/4.89208. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.86753/4.88861. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.86663/4.90066. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.86460/4.89033. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.86585/4.89162. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.86500/4.89861. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.86524/4.90934. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.86362/4.88613. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.86240/4.90199. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.86134/4.91004. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.86560/4.89691. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.86113/4.91552. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.86128/4.92087. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.86206/4.88917. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86246/4.90730. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.85758/4.91397. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.85860/4.91165. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85604/4.92882. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.86157/4.90836. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.86031/4.91754. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.85817/4.91182. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.86117/4.93416. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.85683/4.93457. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.85790/4.93978. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.85584/4.91084. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.85817/4.91332. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.85760/4.91238. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.85302/4.93693. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.85760/4.90023. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.85700/4.91083. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.85789/4.91849. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.86191/4.88339. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.85867/4.94128. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.85622/4.92448. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.86260/4.88900. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.85675/4.89283. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.85549/4.90880. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.85439/4.92703. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.85542/4.90859. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.85004/4.93456. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.85339/4.91991. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.85605/4.91730. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.84912/4.93814. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85302/4.92196. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.84914/4.94289. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85532/4.91223. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.85230/4.88945. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.85640/4.90090. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.84869/4.95312. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.84974/4.92196. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.84992/4.94509. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.84939/4.91733. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84233/4.93394. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.84450/4.96579. Took 0.21 sec\n",
      "Epoch 71, Loss(train/val) 4.85201/4.93884. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.84544/4.94515. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84769/4.93627. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.84867/4.91108. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.84796/4.95905. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.84816/4.92432. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.84716/4.92060. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.84476/4.95843. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.84359/4.90588. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.84690/4.96829. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.84128/4.93769. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.85039/4.93837. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84378/4.95520. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.84719/4.96475. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83688/4.97081. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.84258/4.95567. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83937/4.95735. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.84510/4.92320. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83888/4.92990. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.83879/4.94326. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.83574/4.93539. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.83936/4.92399. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.84295/4.95094. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83940/4.90976. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.84254/4.94987. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.83687/4.94289. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83430/4.95772. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.83876/4.95613. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.84073/4.92152. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.011953709238683663\n",
      "Epoch 0, Loss(train/val) 4.89922/4.90646. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86700/4.89022. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.86813/4.84014. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 4.85943/4.82702. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.85213/4.82909. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.84982/4.82963. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.85052/4.83018. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.85230/4.83162. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85142/4.83192. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.84818/4.83857. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84966/4.83691. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.84618/4.84076. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.84481/4.84015. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.84771/4.83730. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.84407/4.83694. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.84115/4.84600. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.84322/4.84777. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.84100/4.84372. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.84403/4.84315. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.83737/4.84093. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84082/4.84922. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.83896/4.85164. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.83783/4.85276. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.83746/4.85093. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.83206/4.84898. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.83675/4.84665. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.83568/4.84776. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.83170/4.84120. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.83790/4.85198. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.83574/4.84710. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.83471/4.84630. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.83694/4.84731. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.83415/4.84782. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.83410/4.85169. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.83144/4.85664. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.83344/4.86354. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.83232/4.84274. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.82903/4.86166. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.83224/4.85597. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.83337/4.85247. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.83180/4.84660. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.82890/4.85405. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.82914/4.85257. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.83089/4.85555. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.83101/4.85663. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.83013/4.84800. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.82767/4.85590. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83232/4.85441. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.82852/4.85471. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.82639/4.86572. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.83160/4.85048. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.82894/4.85065. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.82914/4.85074. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.82888/4.85761. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.82423/4.86083. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.82722/4.86008. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.82850/4.85412. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.82691/4.85107. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.82657/4.84338. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.82686/4.85725. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.83280/4.84107. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.82458/4.85633. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.82240/4.86313. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.82403/4.87165. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.82631/4.84512. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.82423/4.86227. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.82863/4.86047. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.82479/4.84787. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.82587/4.85783. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.82380/4.85980. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.82571/4.85505. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.82120/4.85421. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.82244/4.85635. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.82605/4.84909. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.81955/4.85932. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.82575/4.86087. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.82683/4.84718. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.82437/4.85185. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.82602/4.84295. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.82321/4.86819. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.81896/4.84781. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.81855/4.86195. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.82021/4.85196. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.82631/4.85493. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.81870/4.86292. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.82188/4.85815. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.81899/4.85595. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.82351/4.85962. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.81866/4.86659. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.81880/4.85766. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.81847/4.85683. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.81757/4.85959. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82412/4.87497. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.81757/4.87135. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.81910/4.85370. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82452/4.86956. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.82016/4.84961. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81988/4.88383. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82612/4.85045. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.82607/4.85025. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.02757337634153148\n",
      "Epoch 0, Loss(train/val) 4.85921/4.84067. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.79845/4.78751. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.79173/4.78268. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79441/4.77992. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79323/4.77774. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.78935/4.77730. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.79510/4.78177. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79442/4.78470. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79259/4.78411. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.79084/4.78415. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.79312/4.78739. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.79242/4.78964. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79306/4.78877. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.78846/4.78992. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.78824/4.79244. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78972/4.79404. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78731/4.79363. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78217/4.79597. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78217/4.79411. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78746/4.79563. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.78300/4.79921. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78422/4.79728. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78681/4.79605. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78401/4.79683. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78367/4.79472. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78357/4.79829. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78201/4.79808. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78083/4.79733. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78355/4.79477. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78519/4.79483. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.78197/4.79558. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.78144/4.79795. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.78421/4.79743. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.77885/4.80104. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.77887/4.80327. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.78360/4.79865. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77650/4.79917. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.77755/4.79812. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77677/4.80184. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.78130/4.80154. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77629/4.80454. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.77527/4.80585. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.77699/4.79262. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.78026/4.79621. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77548/4.80657. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.76866/4.81497. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77577/4.79913. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.77300/4.79733. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.77402/4.80501. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77463/4.81478. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.77416/4.79847. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.77660/4.79516. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.77352/4.80772. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76996/4.78734. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.77439/4.80294. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.77224/4.81108. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.77380/4.79211. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.77576/4.83078. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.77840/4.80048. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.77417/4.80379. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.77054/4.81622. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.77424/4.79720. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.77860/4.81244. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.77128/4.81434. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.77368/4.80786. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.76902/4.80320. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.76982/4.82865. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.77651/4.79688. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77257/4.79998. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77360/4.84197. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.77106/4.82196. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.76404/4.81923. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.77302/4.81156. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.76531/4.80404. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77303/4.82421. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.76899/4.82911. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.76660/4.82630. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.76576/4.83010. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.77238/4.82552. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.76499/4.79694. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.76726/4.80081. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.76957/4.81273. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.76989/4.82173. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.75906/4.82772. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.76685/4.79757. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.77285/4.82880. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.76976/4.82526. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.76328/4.81981. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.77077/4.82784. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.76414/4.82568. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.76540/4.83657. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.75808/4.83012. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.76162/4.83033. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.76283/4.82915. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.76593/4.78659. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.76075/4.84189. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.76768/4.83244. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.76418/4.81555. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.75956/4.84206. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.76430/4.84100. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.06643282473893375\n",
      "Epoch 0, Loss(train/val) 4.94851/4.92465. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.89256/4.89147. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.88367/4.88149. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87873/4.87723. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.87869/4.87765. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.87758/4.88008. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.87884/4.87817. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.87595/4.87404. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.87437/4.87364. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87501/4.87332. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.87406/4.87445. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.87245/4.87677. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.87742/4.87677. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.87487/4.87181. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87160/4.87075. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.87212/4.87455. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.87045/4.87936. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.87168/4.87785. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.87287/4.87774. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.87185/4.87742. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.86701/4.87846. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.86889/4.88131. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.87048/4.88271. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.86336/4.88829. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.86248/4.89218. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.86915/4.88631. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.86361/4.88395. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.86684/4.88402. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.86154/4.89597. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.85850/4.90208. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.86379/4.89453. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86040/4.89217. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.85948/4.89378. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.86061/4.89024. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.86474/4.88566. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.85554/4.89363. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.85906/4.89420. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.86009/4.88383. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.85445/4.89751. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.85620/4.90022. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.85518/4.89598. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.85059/4.90198. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.85572/4.88145. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.86270/4.88427. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.86041/4.88208. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.86077/4.89034. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.85609/4.90701. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.85487/4.88588. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.85529/4.89406. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.84662/4.90112. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.85016/4.89286. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.85630/4.87654. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.84922/4.89497. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.84963/4.88078. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.84997/4.87852. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.84602/4.89452. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.85264/4.87097. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.84783/4.88072. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.84752/4.89859. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.85449/4.89280. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.84555/4.90313. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84672/4.88416. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85260/4.88687. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.84905/4.87825. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.84093/4.88753. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.84712/4.86365. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.84400/4.89083. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.85091/4.87195. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.87100/4.86364. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.85732/4.86423. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.84988/4.86551. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.85131/4.87692. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.84447/4.87234. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.83989/4.88692. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.85076/4.87439. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.84025/4.87971. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.84637/4.87342. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.83971/4.89055. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.84033/4.87542. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 4.84392/4.86621. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.84113/4.86704. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.84334/4.88320. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.84456/4.87003. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84024/4.87319. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83819/4.87831. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.84184/4.86495. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.84393/4.87722. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83535/4.88972. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83923/4.88833. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83695/4.88721. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.84351/4.84834. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.84216/4.88248. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.84114/4.87143. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83500/4.87478. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83450/4.89315. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.83784/4.86494. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.84071/4.86848. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83331/4.88904. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.83460/4.85719. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.83635/4.86009. Took 0.19 sec\n",
      "ACC: 0.359375, MCC: -0.26226526415648105\n",
      "Epoch 0, Loss(train/val) 4.95687/4.92956. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.88834/4.91196. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.88458/4.90522. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.88174/4.89913. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.87863/4.89491. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.87871/4.89611. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.87756/4.89699. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.87363/4.90154. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.87453/4.90806. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87073/4.90795. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.87000/4.91987. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.86808/4.93433. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.86582/4.91767. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.86597/4.91712. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.86396/4.91948. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.86303/4.92774. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.86179/4.92354. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.86394/4.92718. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.86129/4.92821. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.85860/4.93214. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.85962/4.92139. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.86322/4.89882. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85833/4.91377. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85865/4.92566. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85957/4.91934. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.85848/4.91702. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.85685/4.91865. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.86015/4.91396. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.85496/4.92423. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.85609/4.91588. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.85076/4.91955. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.85065/4.91287. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.85248/4.91426. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.85427/4.90785. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85685/4.90581. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.85033/4.91282. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84653/4.90046. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.85098/4.91029. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84660/4.90437. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84675/4.90200. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.85646/4.93524. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.85791/4.90741. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.86185/4.91417. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.85799/4.90440. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.85081/4.89093. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84691/4.89106. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.85289/4.88218. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84818/4.89000. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.84807/4.88732. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.84867/4.88465. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.84417/4.89596. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.84685/4.88735. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.84615/4.88546. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.85044/4.89515. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.84343/4.89340. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 4.84603/4.89091. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84413/4.88927. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.84720/4.88638. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.84917/4.88174. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.84434/4.88168. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.84147/4.88872. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84554/4.88187. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.84261/4.89505. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83747/4.88973. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.84107/4.88484. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.84603/4.88481. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.84516/4.88420. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.84323/4.88467. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.83843/4.89590. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.83964/4.89740. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.83782/4.88552. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.84025/4.88642. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.84066/4.89061. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84145/4.88982. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.83998/4.89926. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.84125/4.88346. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83862/4.88481. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.83560/4.90449. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.84058/4.88372. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.83985/4.89096. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.84221/4.88416. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.84302/4.88831. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.84052/4.89742. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.83677/4.89606. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83874/4.89534. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83881/4.89285. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.83966/4.89791. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83808/4.89686. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83021/4.89999. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83787/4.89243. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.83187/4.90761. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.83670/4.88968. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.83156/4.90768. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83272/4.91486. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83624/4.90611. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.83568/4.91903. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83609/4.88789. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83061/4.91134. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82599/4.89949. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.84144/4.91370. Took 0.20 sec\n",
      "ACC: 0.515625, MCC: 0.049929648757959806\n",
      "Epoch 0, Loss(train/val) 4.92167/4.91732. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86587/4.86600. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.85544/4.86121. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.85449/4.86553. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.85189/4.87536. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.85114/4.88193. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.85031/4.88865. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.85151/4.88646. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85184/4.87745. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.84683/4.87443. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84770/4.86568. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.84732/4.86766. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.84790/4.86550. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.84374/4.85691. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.84139/4.86185. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.84384/4.85852. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.84224/4.86861. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.84193/4.87087. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.84196/4.86498. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.83872/4.87342. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.83724/4.86991. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.84025/4.86839. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.84078/4.87196. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.83981/4.86687. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.83721/4.87453. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.83783/4.86875. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.83556/4.87042. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.83528/4.87245. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.83542/4.89095. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.83784/4.87601. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.83303/4.88776. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.82792/4.89052. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.83344/4.88707. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.83416/4.89806. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.83532/4.90032. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.83432/4.88560. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.82989/4.87468. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.82754/4.88169. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84740/4.90888. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84431/4.88043. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.83688/4.90488. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.83622/4.89537. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83501/4.89751. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.84183/4.87504. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84441/4.90438. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.83934/4.89166. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.83201/4.90724. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83274/4.91651. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.83015/4.92101. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.82760/4.92668. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.84157/4.88290. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.83674/4.88946. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.83234/4.87521. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.82664/4.91488. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.82858/4.89621. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.82568/4.93542. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.82821/4.92601. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.82897/4.90321. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.82488/4.93604. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.84108/4.92379. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.83186/4.91915. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.82604/4.94429. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.82011/4.95674. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.81946/4.95215. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.82528/4.96108. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81986/4.96683. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.81873/4.95566. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.82036/4.93860. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.81772/4.95855. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.81875/4.96303. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.81742/4.95338. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.82948/4.90184. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.82496/4.93295. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 4.82155/4.94677. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81706/4.96707. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.84064/4.93949. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83579/4.91374. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.82373/4.92173. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.83024/4.92046. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.81955/4.93629. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.82022/4.92512. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.82373/4.92765. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.82280/4.96900. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.82154/4.94541. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83579/4.87048. Took 0.22 sec\n",
      "Epoch 85, Loss(train/val) 4.82784/4.90928. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.83373/4.88285. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.82730/4.90061. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83036/4.90822. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82977/4.91325. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82612/4.89047. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82296/4.88197. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.82245/4.89892. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.82606/4.89131. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82595/4.90042. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82137/4.90844. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82290/4.88374. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.82061/4.90839. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82263/4.89447. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.82070/4.90157. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.008866995073891626\n",
      "Epoch 0, Loss(train/val) 4.84534/4.82430. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.76617/4.78512. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.76243/4.77624. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.76103/4.77591. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.75925/4.78835. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.76364/4.78405. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.75822/4.78342. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.75803/4.78457. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.75742/4.79221. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.75992/4.78284. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.75829/4.77777. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.75684/4.79141. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.75675/4.77683. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.75621/4.78522. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.75904/4.77983. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.75595/4.78469. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.75478/4.77461. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.75718/4.77728. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.75567/4.78723. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.74992/4.79980. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.75140/4.79274. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.75560/4.78976. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.75443/4.80537. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.75113/4.79621. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.75291/4.80679. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.75243/4.78695. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.75457/4.80123. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.75171/4.82195. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.75080/4.79775. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.75006/4.80965. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.74679/4.81907. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.75528/4.78136. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.75377/4.79679. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.74912/4.79975. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.74979/4.79916. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.74449/4.80516. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.74768/4.81758. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.74517/4.81210. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.74977/4.83510. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.74407/4.82210. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.74762/4.81033. Took 0.22 sec\n",
      "Epoch 41, Loss(train/val) 4.74519/4.79548. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.75674/4.82570. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.75473/4.79339. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 4.75063/4.78518. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 4.75156/4.80217. Took 0.22 sec\n",
      "Epoch 46, Loss(train/val) 4.75791/4.78205. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.75816/4.79126. Took 0.21 sec\n",
      "Epoch 48, Loss(train/val) 4.75121/4.79696. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.75274/4.82948. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.76035/4.77903. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.75296/4.78574. Took 0.23 sec\n",
      "Epoch 52, Loss(train/val) 4.75236/4.78548. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 4.74919/4.79073. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.74824/4.79145. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.74470/4.80287. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.74830/4.81567. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.74445/4.80951. Took 0.22 sec\n",
      "Epoch 58, Loss(train/val) 4.75102/4.81290. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.74515/4.82085. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.74541/4.80618. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 4.74764/4.82274. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 4.74285/4.82879. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 4.74479/4.83457. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 4.74446/4.81381. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.74633/4.79681. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 4.75727/4.81116. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 4.75614/4.79116. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.75512/4.78381. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.75491/4.79021. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.75480/4.79493. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.75328/4.79713. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.75143/4.79892. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.75084/4.80130. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.75431/4.80594. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.75286/4.79416. Took 0.21 sec\n",
      "Epoch 76, Loss(train/val) 4.74918/4.80250. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.74888/4.81166. Took 0.21 sec\n",
      "Epoch 78, Loss(train/val) 4.74627/4.81104. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.74395/4.81830. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.74569/4.80856. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.74583/4.82703. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.75501/4.78229. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.74806/4.79550. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.74697/4.80979. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.74432/4.80171. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.74279/4.80099. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.74991/4.79722. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.74456/4.78428. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.74997/4.78743. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.75033/4.80136. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.74492/4.81020. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.74519/4.82998. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.74591/4.81792. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.74359/4.84205. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.74203/4.82287. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.74342/4.84021. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.73799/4.86009. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.73980/4.84191. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.74126/4.84543. Took 0.20 sec\n",
      "ACC: 0.390625, MCC: -0.22877657129023765\n",
      "Epoch 0, Loss(train/val) 4.80340/4.83936. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.75976/4.81289. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.76973/4.76931. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.76942/4.75443. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.75048/4.75861. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.74455/4.76741. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.74860/4.76809. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.74785/4.76533. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.74791/4.76383. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.74610/4.76700. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.74426/4.77014. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.74748/4.76958. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.74425/4.77359. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.74401/4.77554. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.74277/4.78061. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.74081/4.77475. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.73946/4.77887. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.74149/4.78452. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.73934/4.79141. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.73775/4.79441. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.73542/4.79674. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.74062/4.76684. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.74051/4.78226. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.73545/4.79363. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.73534/4.79223. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.73727/4.78957. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.73316/4.79402. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.73681/4.78778. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.73605/4.81040. Took 0.22 sec\n",
      "Epoch 29, Loss(train/val) 4.73219/4.79895. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.73098/4.80905. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.73196/4.80297. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.73566/4.77376. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.74211/4.75603. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.73588/4.77797. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.74324/4.77371. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.73554/4.77888. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.73429/4.77532. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.73670/4.77836. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.73280/4.79253. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.73707/4.77396. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.73258/4.78908. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.73152/4.78327. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.73901/4.78098. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.73002/4.78701. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.73209/4.79055. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.73565/4.80149. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.72956/4.78637. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.73404/4.79440. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.73007/4.78720. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.72880/4.80259. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.73282/4.79426. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.73164/4.81134. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.73466/4.79560. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.72589/4.80250. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.74290/4.75893. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.73087/4.83493. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.72847/4.83322. Took 0.22 sec\n",
      "Epoch 58, Loss(train/val) 4.73093/4.82174. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.72595/4.84535. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.72391/4.83414. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.72374/4.81688. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.72939/4.81430. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.72884/4.84671. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.71833/4.87528. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.72104/4.88470. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.72335/4.85714. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.71955/4.88529. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.71922/4.84494. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.71876/4.86527. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.72804/4.90078. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.71854/4.87808. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.71808/4.86754. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.71715/4.84489. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.72220/4.87220. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.71734/4.93033. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.71593/4.92704. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.72390/4.88821. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.70948/4.92877. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.72183/4.89119. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.71452/4.91294. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.71763/4.88153. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.71224/4.95333. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.71274/4.92294. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.70872/4.91119. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.71684/4.89514. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.71490/4.95982. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.70881/4.95392. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.70974/4.88838. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.71365/4.87843. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.71256/4.92857. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.71320/4.95574. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.70927/4.94497. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.72310/4.92725. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.72249/4.83800. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.71108/4.90295. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.72154/4.84678. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.71459/4.87946. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.70299/4.94305. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.71104/4.94052. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.08818077954471167\n",
      "Epoch 0, Loss(train/val) 5.05707/5.04658. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.01496/5.03537. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.00980/5.04655. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.00829/5.02965. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.00072/5.01324. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.99847/5.01385. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.99863/5.01507. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.99657/5.01904. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.99664/5.02140. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.99901/5.02603. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.99602/5.02019. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.99638/5.01840. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.99650/5.02183. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.99335/5.01963. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.99393/5.02014. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.99475/5.01826. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.99347/5.01889. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.99138/5.02209. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.98735/5.03151. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.99235/5.03570. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.98708/5.03462. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.99694/5.03194. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.99322/5.02402. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.99506/5.02425. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.99327/5.02384. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.99279/5.02702. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.99199/5.03549. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.99243/5.03568. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.99048/5.03632. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.98954/5.04012. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.98932/5.03840. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.98947/5.03431. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.98843/5.03960. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.98628/5.04262. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.98749/5.03315. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.99051/5.03294. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.98531/5.04256. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.99145/5.04735. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.98446/5.04864. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.98350/5.04884. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.98453/5.05479. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.98189/5.06101. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.98465/5.06559. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.98272/5.05794. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.98250/5.06176. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.98094/5.06724. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.98125/5.05686. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.97842/5.06628. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.97980/5.05294. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.97253/5.07720. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.97634/5.07098. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.97972/5.07211. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.97964/5.07166. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.97467/5.07864. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.97491/5.08987. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.97210/5.07786. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.97353/5.08697. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.97279/5.09446. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.97590/5.07765. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.97407/5.09960. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.96876/5.10238. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.97525/5.06022. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.96878/5.10925. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.98254/5.06957. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.96703/5.09053. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.96582/5.08112. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.97310/5.09315. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.97459/5.03334. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.98492/5.03726. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.97553/5.06150. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.97637/5.04726. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.97250/5.07810. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.97747/5.06793. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.96673/5.08047. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.97455/5.04356. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.97188/5.07941. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.97237/5.06983. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.97157/5.08476. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.97898/5.02730. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.98927/5.03319. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.98180/5.04123. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.98628/5.06274. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.98251/5.05072. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.98683/5.03317. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.97906/5.05458. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.97442/5.06295. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.98107/5.06211. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.97835/5.08450. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.97428/5.09708. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.97571/5.08747. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.96798/5.11881. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.97853/5.07928. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.96970/5.10684. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.96986/5.10886. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.96892/5.11645. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.97238/5.09556. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.97259/5.10217. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.96695/5.10634. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.96448/5.10902. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.96725/5.12002. Took 0.19 sec\n",
      "ACC: 0.40625, MCC: -0.18670576735092864\n",
      "Epoch 0, Loss(train/val) 5.02656/4.97666. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.95346/4.95664. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.95676/4.95981. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.95745/4.96325. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.95778/4.96490. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.95849/4.97398. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.95915/4.97272. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.96002/4.96147. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.95763/4.95645. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.95430/4.95397. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.94751/4.95263. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.95181/4.94968. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.94561/4.95123. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.95097/4.95094. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.95061/4.94855. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.94384/4.95366. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.95385/4.96375. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.95040/4.95376. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.94412/4.95470. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.94461/4.95361. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.94696/4.94627. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.94869/4.96267. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.94582/4.95797. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.94445/4.95799. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.94244/4.96396. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.94000/4.96280. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.95095/4.94973. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.94609/4.95679. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.94502/4.95242. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.94255/4.96236. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.94414/4.96104. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.94499/4.96081. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.94263/4.97785. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.93974/4.95985. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.94465/4.96581. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.94422/4.97488. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.93810/4.95569. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.93520/4.98696. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.93914/4.98056. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.93924/4.97845. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.93732/4.98515. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 4.93853/4.98371. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.93716/4.99025. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.93315/4.98850. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.93663/5.00919. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.93560/4.98469. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.93428/5.00917. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.93211/5.00767. Took 0.21 sec\n",
      "Epoch 48, Loss(train/val) 4.93649/4.98445. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.92913/5.00462. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.93610/4.99396. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.93177/4.99587. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.92959/4.99634. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.92952/5.02208. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.93177/5.00811. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.92833/5.01531. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.92578/5.04588. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.93150/5.01710. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.93658/4.99890. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.93254/4.98337. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.92786/5.02482. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 4.92983/5.01038. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.93335/5.01281. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.92943/5.00625. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.92755/5.03505. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.92800/5.00419. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.92706/5.03021. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.92851/5.01451. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.92775/5.00579. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.92148/5.05411. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.92247/5.02878. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.92779/5.05668. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.92635/5.01302. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.92519/5.02639. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.92659/5.04437. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.92793/5.03528. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.92564/5.00552. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.93041/4.99945. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.93310/5.00745. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.93235/5.00420. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.93519/4.99933. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.92853/5.01254. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.92360/5.01116. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.92684/5.03246. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.93079/5.01068. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.92648/5.06084. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.93129/4.99356. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.92729/5.03388. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.92433/4.99669. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.92677/5.03632. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.92642/5.00637. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.92305/5.04609. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.92502/5.05824. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.92480/5.05904. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.92297/5.00129. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.92173/5.03110. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.92147/5.02275. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.91961/5.03196. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.92936/5.03052. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.91913/5.08837. Took 0.20 sec\n",
      "ACC: 0.546875, MCC: 0.09379580992210836\n",
      "Epoch 0, Loss(train/val) 4.84193/4.83319. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.76487/4.78847. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.76245/4.79011. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.76919/4.79437. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.76607/4.80173. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.76247/4.80273. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.76306/4.80900. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.76241/4.80744. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.75855/4.80527. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.75489/4.80943. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.75719/4.81575. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.75833/4.82284. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.75292/4.82292. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.75548/4.82901. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.75552/4.82381. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.75124/4.82580. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.74601/4.84550. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.74916/4.84071. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.74875/4.83248. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.74327/4.83488. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.74690/4.83376. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.74545/4.83359. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.75320/4.82485. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.75249/4.82271. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.74794/4.83185. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.74682/4.84420. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.74523/4.83771. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.74236/4.83844. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.74243/4.84511. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.74432/4.83508. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.74049/4.84161. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.73988/4.83722. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.74166/4.85844. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.74161/4.84189. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.74242/4.83599. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.73932/4.84686. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.74098/4.82886. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.73717/4.82498. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.73776/4.83806. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.73556/4.84438. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.73563/4.83883. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.73606/4.82739. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.73624/4.84926. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.73846/4.83493. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.73720/4.84304. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.73590/4.83765. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.73030/4.85129. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.73646/4.83885. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.73873/4.82556. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.73789/4.81660. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.73181/4.83915. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.73366/4.82340. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.73369/4.84418. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.73229/4.82481. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.73647/4.82813. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.73743/4.83065. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.74043/4.80858. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.74377/4.80854. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.74087/4.82169. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.74306/4.83059. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.73499/4.83772. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.73568/4.83251. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.73081/4.83863. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.73424/4.84318. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.73512/4.83182. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.74604/4.83456. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.74473/4.80716. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.74678/4.84479. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.73991/4.84680. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.74441/4.83859. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.73890/4.84435. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.73563/4.84302. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.73629/4.83724. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.73180/4.84480. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.73266/4.84301. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.72939/4.85878. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.73470/4.83904. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.73244/4.83898. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.73471/4.84201. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.73089/4.83969. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.73221/4.85754. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.72837/4.84256. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.73101/4.82834. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.73224/4.84422. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.73496/4.82251. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.72824/4.83465. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.72654/4.84112. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.73161/4.84288. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.73091/4.82690. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.73250/4.82946. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.73300/4.82891. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.73349/4.82374. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.72365/4.84602. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.73320/4.80902. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.73309/4.84277. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.72694/4.82555. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.72692/4.85682. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.73185/4.83329. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.72540/4.84878. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.73272/4.83760. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.15231225557722924\n",
      "Epoch 0, Loss(train/val) 4.81088/4.77888. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.78768/4.78384. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.78114/4.78075. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.78038/4.77355. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.77858/4.77126. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.77891/4.77455. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.77615/4.77482. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.77432/4.77404. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.77144/4.77228. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.76716/4.77504. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.76940/4.77022. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.77279/4.77556. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.76886/4.77653. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.76840/4.77676. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.76897/4.77564. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.76749/4.77447. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.77039/4.77352. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.76785/4.77796. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.76558/4.77514. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.76826/4.77731. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.76663/4.77926. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.76548/4.77629. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.76792/4.77819. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.76834/4.77334. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.76414/4.78248. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.76482/4.77601. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.76309/4.77791. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.76515/4.77542. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.76412/4.77793. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.76154/4.77853. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.76816/4.77567. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.76426/4.78051. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.76041/4.77576. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.76198/4.76853. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.76274/4.77394. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.76105/4.77277. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.76454/4.77045. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.76020/4.76450. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.76263/4.75887. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.76024/4.76294. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.76094/4.76703. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.75559/4.76165. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76297/4.75871. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.76458/4.76474. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.76541/4.76233. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.75678/4.76666. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.75571/4.76622. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.75912/4.76347. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.76249/4.77218. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.75692/4.76041. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.75495/4.77183. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.75408/4.77122. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.75719/4.76090. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.75650/4.77768. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.75690/4.75744. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.75877/4.76452. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.75673/4.77286. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76372/4.76824. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.75861/4.77656. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.75561/4.76572. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.75952/4.76855. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.75421/4.78930. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.75312/4.76564. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.74883/4.77583. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.75347/4.76047. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.75740/4.77363. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75713/4.76531. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.75374/4.78300. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.75044/4.77927. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.75479/4.75202. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.75536/4.79084. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.75213/4.75162. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.75133/4.76404. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.74956/4.76729. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.75113/4.77112. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.75595/4.76417. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.74712/4.76872. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.74795/4.77606. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.74757/4.76760. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.75229/4.77816. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.74864/4.78766. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.74723/4.78435. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.75154/4.76989. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.75344/4.76503. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.75451/4.76424. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.74760/4.75997. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.74750/4.77846. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.75094/4.76218. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.75184/4.78308. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.74718/4.76359. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.74988/4.76921. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.74187/4.77981. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.75002/4.76621. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.75456/4.76017. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.74498/4.77462. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75027/4.76036. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.74417/4.76751. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.74828/4.75756. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.75126/4.76268. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.74910/4.77369. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.015873015873015872\n",
      "Epoch 0, Loss(train/val) 4.65271/4.61584. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.62464/4.63636. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.63096/4.65794. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.62014/4.61785. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.60929/4.61417. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 4.60700/4.62199. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.60844/4.62250. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.60829/4.62109. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.60738/4.61998. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.60579/4.62117. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.60681/4.61811. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.60543/4.61367. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.60369/4.61501. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.60397/4.61248. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.60320/4.61162. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.60343/4.61191. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.60366/4.60900. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.59970/4.61850. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.60189/4.60901. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.60033/4.61081. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.59901/4.61133. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.59967/4.62610. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.59945/4.60619. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.60059/4.61742. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.59847/4.61471. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.59848/4.61145. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.59933/4.60584. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.59553/4.60950. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.59645/4.59982. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.60372/4.61840. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.59661/4.59962. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.59702/4.60809. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.59880/4.60862. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.59251/4.58928. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.59330/4.59049. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.59415/4.58914. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.59067/4.59649. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.58991/4.60358. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.59239/4.60269. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.59072/4.60568. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.59083/4.59949. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.58699/4.60630. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.58366/4.60017. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.58531/4.60983. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.58705/4.59708. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.58166/4.60641. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.57861/4.60605. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.59111/4.61472. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.58002/4.59940. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.58105/4.60710. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 4.57980/4.60001. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.58139/4.58784. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.58299/4.59829. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.57870/4.61313. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.58152/4.59722. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.58334/4.60308. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.57941/4.59515. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.57509/4.61175. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.57998/4.59779. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.57879/4.61482. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.56993/4.61157. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.57554/4.60872. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.57171/4.60885. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.57152/4.60228. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.57453/4.60147. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.57119/4.61227. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.57171/4.61727. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.57306/4.60997. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.56824/4.61923. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.57085/4.61757. Took 0.21 sec\n",
      "Epoch 70, Loss(train/val) 4.56994/4.62217. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.56939/4.61453. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.57166/4.60564. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.57055/4.61161. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.57832/4.62167. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.58029/4.61787. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.56355/4.63061. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.56530/4.61545. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.57113/4.60875. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.56447/4.63018. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.57316/4.60413. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.56660/4.59693. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.55892/4.62117. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.57081/4.60544. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.56893/4.61281. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.56763/4.61757. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.57097/4.60661. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.57123/4.62392. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.57014/4.60731. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.56602/4.62404. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.55763/4.63128. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.56911/4.62973. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.56580/4.62047. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.56788/4.61986. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.56615/4.63061. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.55989/4.63426. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.56167/4.62168. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.56474/4.62160. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.56418/4.61467. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.56733/4.61204. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.1014574359634967\n",
      "Epoch 0, Loss(train/val) 4.78896/4.71060. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.72533/4.70087. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.72422/4.70150. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.72031/4.70109. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.72140/4.70052. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.72241/4.69904. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.71836/4.70087. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.72067/4.70036. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.71941/4.69874. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.71912/4.69804. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.71585/4.69397. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.71695/4.68945. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.71878/4.69349. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.71495/4.69518. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.71449/4.69577. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.71213/4.69222. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.71327/4.69322. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.71173/4.69432. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.70983/4.69997. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.71288/4.69113. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.70620/4.69547. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.70889/4.69291. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.70718/4.69602. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.70663/4.69284. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.70700/4.69735. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.70982/4.69398. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.70766/4.72255. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.70711/4.69399. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.70803/4.69203. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.70678/4.70359. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.71247/4.70093. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.70638/4.68349. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.70878/4.70262. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 4.70623/4.68814. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.70670/4.70138. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.70878/4.70059. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.70362/4.70434. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.70349/4.70541. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.70328/4.72456. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.72003/4.71399. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.71482/4.71522. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 4.70700/4.71067. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.70573/4.70236. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.71112/4.70951. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.71731/4.70990. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.71374/4.70591. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.71364/4.70531. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.70874/4.69653. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.71077/4.69126. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.70965/4.69967. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.71014/4.69288. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.70886/4.70043. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.70627/4.69221. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.70686/4.68799. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.70145/4.68372. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.70440/4.67736. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.70513/4.68451. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.69744/4.69372. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.70317/4.67952. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.70999/4.71234. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.70906/4.69972. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.69746/4.70749. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.69844/4.69081. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.69386/4.69585. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.70208/4.67607. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.70794/4.70212. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.70108/4.70415. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.69332/4.69715. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.69228/4.67470. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.69808/4.68530. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.70081/4.69610. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.69273/4.69703. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.69217/4.68204. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.68937/4.68576. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.70223/4.69726. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.70171/4.70767. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.69475/4.70766. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.69206/4.68873. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.69255/4.71204. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.69349/4.70727. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.69230/4.71689. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.68708/4.70638. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.68130/4.73054. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.68775/4.73827. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.69150/4.69450. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.68928/4.72603. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.69605/4.69884. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.68782/4.71648. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 4.68262/4.71376. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.69207/4.70497. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.68354/4.71077. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.68599/4.73237. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.72240/4.71351. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.71003/4.70941. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.70684/4.70492. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.70292/4.71289. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.70236/4.70313. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.70558/4.71741. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.71313/4.70452. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.71081/4.71709. Took 0.20 sec\n",
      "ACC: 0.4375, MCC: -0.14977900439610384\n",
      "Epoch 0, Loss(train/val) 5.24343/5.23462. Took 0.35 sec\n",
      "Epoch 1, Loss(train/val) 5.23355/5.21478. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.22358/5.21732. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.22375/5.21468. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.21960/5.21755. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.22368/5.21898. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.22635/5.22061. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.22326/5.22155. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.21621/5.23306. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.21534/5.22882. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 5.21774/5.23153. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 5.21485/5.23721. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.21358/5.23652. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.21780/5.22692. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 5.21355/5.23406. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.21407/5.22697. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.21093/5.23579. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.21113/5.23340. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.21342/5.22317. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.20969/5.23062. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 5.20762/5.22746. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 5.20956/5.23207. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.21420/5.21039. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.21347/5.21614. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.21430/5.22670. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 5.21076/5.22575. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.20770/5.23965. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 5.21017/5.21424. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.20606/5.22552. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.20402/5.23357. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.20507/5.22090. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.20208/5.22556. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.20459/5.20694. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 5.19883/5.21557. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.19646/5.22039. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.20716/5.21264. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.20322/5.23233. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.20211/5.21674. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.20239/5.21279. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 5.19803/5.22277. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.19308/5.23390. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 5.19659/5.22534. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.19970/5.22798. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.19824/5.22666. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 5.20361/5.23287. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.19884/5.22726. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 5.19518/5.23470. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.19596/5.23591. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.19298/5.24863. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.21914/5.23048. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.21531/5.23286. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 5.21581/5.23028. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.21561/5.23098. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.21174/5.23522. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.20698/5.24403. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.20301/5.25176. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.20028/5.25460. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.20031/5.24423. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.20074/5.23619. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 5.19896/5.22308. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.19466/5.23188. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.19611/5.23667. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.19862/5.23369. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 5.19976/5.23048. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 5.20209/5.22661. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.19201/5.24065. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.18957/5.23315. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.19594/5.23422. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.19321/5.24900. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.19006/5.25795. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.19359/5.23407. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.19328/5.24060. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.18895/5.25043. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 5.19264/5.23652. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 5.19448/5.23000. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 5.19031/5.23732. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.18963/5.23926. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.18715/5.27606. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.18960/5.26204. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.18579/5.23863. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.18758/5.24010. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.18374/5.23729. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 5.19011/5.23629. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.19219/5.22941. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.18459/5.24234. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.18561/5.23912. Took 0.22 sec\n",
      "Epoch 86, Loss(train/val) 5.18583/5.24981. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.18024/5.25346. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 5.20395/5.20321. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.19540/5.21744. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.20119/5.23907. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.19742/5.26276. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.19355/5.24830. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 5.18525/5.25004. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.18804/5.25401. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.19042/5.24518. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 5.18878/5.24221. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.18809/5.24460. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.18402/5.23301. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 5.18272/5.23902. Took 0.19 sec\n",
      "ACC: 0.390625, MCC: -0.2512366785266634\n",
      "Epoch 0, Loss(train/val) 5.00725/4.92612. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.95811/4.93258. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.95235/4.93285. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.95098/4.93705. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.94921/4.93754. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.94745/4.93493. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.94627/4.93568. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.94749/4.93539. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.94659/4.93551. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.94622/4.93063. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.94688/4.92998. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.94557/4.93352. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.94422/4.92964. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.94332/4.93161. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.94454/4.93278. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.94080/4.93371. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.94377/4.93073. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.94241/4.93226. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.94435/4.93099. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.94399/4.93302. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.94391/4.93754. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.94104/4.93174. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.94139/4.92784. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.93853/4.93002. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.93690/4.92632. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.94153/4.93323. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.94014/4.92782. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.93556/4.92505. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.93882/4.92073. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.93329/4.92257. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.93594/4.93145. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.94166/4.92559. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.93512/4.92974. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.93356/4.93098. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.93582/4.93303. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.93271/4.93099. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.93395/4.92889. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.93321/4.92860. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.93034/4.93581. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.93007/4.92846. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.92928/4.92740. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.92864/4.93956. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.93325/4.92200. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 4.92980/4.93678. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.92647/4.93647. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.92310/4.94118. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.92643/4.93539. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.92463/4.93960. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.92382/4.93346. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.92168/4.93599. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.92517/4.93360. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.93010/4.91353. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.92366/4.94274. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.92269/4.94875. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.92355/4.96191. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.92290/4.92956. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.92089/4.95583. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.92745/4.94635. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.92509/4.94867. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.92383/4.95318. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.92481/4.94406. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.91537/4.94974. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.92164/4.94378. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.91948/4.95570. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.92145/4.96132. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.92111/4.95992. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.92414/4.94398. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.92250/4.94746. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.91976/4.96699. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.92109/4.95447. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.91249/4.95418. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.92001/4.94886. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.92042/4.95663. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.91789/4.96810. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.91785/4.95430. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.91504/4.96903. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.91994/4.95917. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.91480/4.97053. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.91367/4.96022. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.91099/4.98150. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.92327/4.94643. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.92037/4.96662. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.91592/4.96799. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.91438/4.96335. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.91376/4.96694. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.91527/4.97302. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.90917/4.98363. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.91314/4.97270. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.91554/4.96928. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.91263/4.97906. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.91805/4.98352. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.91485/4.95580. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.91213/4.97957. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.91169/4.97444. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.91191/4.96571. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.91178/4.97924. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.90791/4.97268. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.90909/4.99409. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.91194/4.98431. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.92222/4.95589. Took 0.20 sec\n",
      "ACC: 0.546875, MCC: 0.08304547985373996\n",
      "Epoch 0, Loss(train/val) 4.91395/4.82529. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.84799/4.82209. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.84257/4.81827. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.83777/4.82390. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.83264/4.82254. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.82872/4.82629. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.82990/4.82574. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 4.82791/4.82525. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.82595/4.83072. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.81982/4.83689. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.81840/4.84183. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.82013/4.83853. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.81987/4.83676. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.81706/4.83593. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.81352/4.84289. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.81285/4.83682. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.80883/4.85367. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.81428/4.84997. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81261/4.84853. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.81363/4.84328. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.81707/4.83796. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.81238/4.83179. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.80902/4.84709. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.80423/4.85045. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.80947/4.84180. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.80452/4.88240. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.80643/4.84591. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.80971/4.86506. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.80280/4.85794. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.80592/4.85131. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.79977/4.86496. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.80182/4.86574. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.80502/4.86169. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.80098/4.87379. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.79945/4.85635. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.80339/4.88258. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.80101/4.85517. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.79793/4.88002. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.79374/4.86957. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.79769/4.87127. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.79571/4.90311. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.79609/4.85904. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.79344/4.87055. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.79057/4.88226. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.79732/4.86222. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.79520/4.89466. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78936/4.89845. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.79134/4.90206. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.79666/4.86566. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.79426/4.86816. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.78695/4.90863. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.79470/4.90538. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.79109/4.88199. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.78954/4.89028. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.78249/4.85853. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.78681/4.88847. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.79079/4.89196. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.79368/4.87397. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.78884/4.90055. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.78316/4.88767. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.78385/4.88362. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.78506/4.86026. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.78443/4.88275. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.78403/4.91524. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.78069/4.87862. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.78841/4.88750. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.78308/4.88260. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.78475/4.90067. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.78086/4.87173. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77778/4.87997. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.77535/4.94124. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.77748/4.90402. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.77753/4.90440. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.77616/4.89513. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.78156/4.89701. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.77651/4.90120. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.77377/4.88494. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77819/4.89550. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.77708/4.89096. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.77837/4.88614. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.77973/4.89886. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.77338/4.93666. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.77649/4.89188. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.77531/4.92997. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.77427/4.89329. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.77779/4.91165. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.76596/4.92050. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.77262/4.91605. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.77318/4.91962. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.77919/4.90578. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.77519/4.92135. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.77458/4.91231. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.77726/4.90339. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77132/4.92853. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77521/4.88841. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.76766/4.89748. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.76676/4.91676. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.77559/4.92398. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.76904/4.92748. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.77720/4.89493. Took 0.19 sec\n",
      "ACC: 0.40625, MCC: -0.17930563858025494\n",
      "Epoch 0, Loss(train/val) 4.87005/4.88766. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.83698/4.87117. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.83249/4.87879. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.83365/4.88452. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.83514/4.88050. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.83803/4.85801. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.84364/4.83615. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.83916/4.83353. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.83108/4.83447. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.82857/4.84238. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.82929/4.84430. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 4.82974/4.84276. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.82788/4.84167. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.83261/4.83306. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.82923/4.83972. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.82638/4.84501. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.82729/4.84234. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.82479/4.84168. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.82863/4.84941. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.82597/4.84698. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.82399/4.85525. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.82337/4.84743. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.82414/4.84968. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.82124/4.85863. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82499/4.84562. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.82035/4.86053. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.81885/4.85340. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.82042/4.85892. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.82193/4.84538. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.81987/4.85221. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.82200/4.85692. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.81803/4.85740. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.81726/4.86537. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.81865/4.85603. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.81935/4.85796. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.81418/4.85773. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.81544/4.86287. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.81523/4.87382. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.81556/4.86022. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.81157/4.86531. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.81210/4.87269. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.81331/4.87649. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.81332/4.87678. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.80898/4.88946. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.80908/4.87998. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.81423/4.89187. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.81073/4.89449. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.80969/4.87739. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.81125/4.87467. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.80531/4.89246. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 4.80281/4.89149. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.80962/4.89035. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.80227/4.90099. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.80770/4.92084. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.80658/4.89290. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.80584/4.88000. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.80181/4.89449. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.80414/4.89696. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.80202/4.89243. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.79947/4.90467. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.79751/4.95031. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.79775/4.92055. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.80447/4.87285. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.80712/4.86138. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.80288/4.90233. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.80052/4.86538. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.80331/4.85862. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.79223/4.87225. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.80886/4.85832. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.80477/4.85697. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.80496/4.86393. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.80344/4.86795. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.80227/4.88393. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.79998/4.88838. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.80188/4.89277. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.80006/4.86075. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.79726/4.88836. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80181/4.89500. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.79742/4.87194. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.79809/4.88302. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.80165/4.86902. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.79236/4.89226. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.79743/4.89252. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.79812/4.86199. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.79426/4.88498. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.79464/4.91343. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.79090/4.88660. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.79916/4.89768. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.79933/4.87885. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.79235/4.90242. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 4.79717/4.86699. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.79246/4.89180. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.79047/4.90466. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.79590/4.89897. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.79263/4.89325. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79674/4.86087. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.78741/4.90086. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.79096/4.88093. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.79247/4.87951. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.79439/4.88558. Took 0.20 sec\n",
      "ACC: 0.59375, MCC: 0.1972421118046462\n",
      "Epoch 0, Loss(train/val) 5.18616/5.07057. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 5.08972/5.08856. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.09009/5.11274. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.08767/5.11114. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.08461/5.09905. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 5.08130/5.09774. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.07885/5.10156. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 5.07574/5.10839. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.07692/5.10485. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.07440/5.10798. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.07426/5.10722. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.07079/5.10880. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.07293/5.11928. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.07202/5.11391. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 5.07131/5.12912. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.07056/5.11393. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 5.06773/5.12273. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.06539/5.12795. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.06827/5.11845. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.06364/5.13853. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.06326/5.12479. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.06526/5.13578. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.05852/5.13366. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.05852/5.12999. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.06054/5.13152. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 5.06078/5.13662. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.06024/5.12556. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.05449/5.12280. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.06144/5.13368. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.05210/5.13726. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 5.05135/5.12983. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.06421/5.12262. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.05654/5.14197. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 5.05604/5.14045. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 5.05615/5.12987. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 5.05308/5.13752. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.04890/5.13480. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.05296/5.13145. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.05487/5.13692. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.05205/5.12167. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.04775/5.13911. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.05303/5.13178. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.04640/5.14127. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.04910/5.15485. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 5.04840/5.15346. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.04562/5.14707. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.04890/5.14817. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.05127/5.13448. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.04676/5.15861. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 5.04495/5.15515. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 5.04933/5.14949. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.04589/5.14078. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.04249/5.15602. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.04304/5.12112. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 5.06253/5.12462. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.04629/5.16774. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 5.04971/5.15030. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.04733/5.14331. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.04465/5.13950. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.04261/5.14980. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.04183/5.13010. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.06350/5.12019. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.05553/5.13355. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.04495/5.14285. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 5.04961/5.15731. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.04855/5.15053. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 5.05451/5.13773. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 5.05050/5.13949. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 5.04328/5.17187. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.04288/5.16987. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.04828/5.15508. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.05082/5.13984. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.04603/5.13860. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.04355/5.13840. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 5.04091/5.16817. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.04706/5.13474. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 5.05013/5.12803. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 5.04882/5.13873. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.05034/5.14806. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.05018/5.15268. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.04752/5.15462. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.04573/5.16833. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.04153/5.16110. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.03577/5.16130. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.04727/5.16719. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.04498/5.15932. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.04371/5.15207. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 5.03651/5.15273. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 5.04248/5.14030. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 5.05372/5.14046. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.04157/5.13351. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.04478/5.15800. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.04368/5.13383. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 5.04519/5.15178. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.04111/5.13724. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.03899/5.14458. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.03888/5.14212. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 5.04206/5.13776. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 5.03329/5.15679. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.04117/5.15259. Took 0.20 sec\n",
      "ACC: 0.5625, MCC: 0.1241446725317693\n",
      "Epoch 0, Loss(train/val) 4.89976/4.85693. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86589/4.85759. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.86700/4.86164. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.86400/4.87177. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.86382/4.86705. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.85975/4.86408. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.85671/4.86454. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.85790/4.86396. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85801/4.86799. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.85667/4.87035. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.85738/4.87054. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.85803/4.86591. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85365/4.86694. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85605/4.87238. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.85464/4.87524. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85500/4.85538. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.85353/4.85263. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85324/4.85623. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85345/4.86025. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.84846/4.86457. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.85168/4.86807. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.85162/4.86770. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85118/4.87307. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85352/4.86979. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.84653/4.87153. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.84430/4.87074. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84804/4.87848. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84602/4.88402. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.84631/4.87769. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84526/4.87629. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84467/4.87601. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84243/4.88665. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.84588/4.87484. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84362/4.87542. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84650/4.87785. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.83984/4.89112. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84115/4.89812. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84354/4.89012. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84423/4.88220. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.84469/4.88371. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84183/4.88945. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84135/4.88354. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83905/4.89057. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.84680/4.88558. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.83522/4.90292. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84469/4.88595. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84195/4.89320. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84745/4.88063. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.84203/4.88414. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.83884/4.89030. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.85012/4.89040. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.85187/4.87541. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.84663/4.87859. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.85018/4.87361. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.84862/4.88101. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.84693/4.87102. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84692/4.87809. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.84465/4.88065. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.84475/4.87757. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.84423/4.88005. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.84316/4.87908. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84335/4.88406. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.84200/4.88436. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.84706/4.88224. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.84021/4.88814. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.84160/4.88452. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.84190/4.89661. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83631/4.90115. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.84085/4.89316. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84068/4.88769. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.83450/4.90802. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.84190/4.88895. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83530/4.91744. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84259/4.87933. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.83441/4.89879. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.83754/4.90524. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83301/4.91747. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.83652/4.91078. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.83538/4.91335. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.83485/4.91014. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.83803/4.89861. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.82919/4.91536. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.83394/4.91423. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.83221/4.93844. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.83358/4.89525. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83698/4.90190. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.83304/4.92656. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.82485/4.92617. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.82758/4.89467. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82804/4.91087. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.83109/4.90091. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82615/4.91886. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82766/4.91022. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82532/4.90363. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82532/4.92787. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82699/4.90686. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83102/4.91948. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82696/4.91466. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82977/4.91265. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.82805/4.92656. Took 0.20 sec\n",
      "ACC: 0.53125, MCC: 0.0625\n",
      "Epoch 0, Loss(train/val) 4.89132/4.89384. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.87501/4.84777. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.85882/4.85396. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.86513/4.85376. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.87355/4.85658. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.87382/4.88588. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.86357/4.88578. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.85680/4.87751. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85527/4.88084. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.85683/4.88633. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.85620/4.88864. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.85719/4.88645. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85412/4.88739. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85471/4.89126. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85309/4.88944. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85220/4.89456. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85557/4.89052. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85196/4.89356. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85288/4.89622. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.85122/4.89802. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.85185/4.89696. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85335/4.89241. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 4.85027/4.88778. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85020/4.88915. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85026/4.89500. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.85421/4.88882. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84860/4.89333. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.85140/4.88857. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.84948/4.88892. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84977/4.88657. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84954/4.89232. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84924/4.89107. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.84856/4.88974. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84650/4.88853. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84528/4.88946. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84839/4.89084. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84530/4.88983. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84967/4.86537. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84617/4.90119. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84648/4.88972. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84299/4.88803. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84441/4.88513. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.84487/4.90244. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.84682/4.87745. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84316/4.91130. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84520/4.89189. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84151/4.88453. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84245/4.90393. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.84071/4.89450. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.84272/4.89181. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.84413/4.88910. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.84280/4.89005. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.84556/4.88136. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.84592/4.88967. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.84125/4.89814. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.84239/4.90322. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84128/4.90083. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.84447/4.88039. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.84126/4.89077. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.84260/4.89857. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.84211/4.90607. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84145/4.89166. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.83892/4.88924. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83830/4.88336. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.83826/4.89222. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.84184/4.89424. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.84207/4.90686. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83521/4.89071. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.83625/4.91704. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.83600/4.88068. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.84311/4.89886. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83800/4.89261. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83677/4.90078. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83393/4.89971. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.83586/4.89210. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.83715/4.91889. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83373/4.92606. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.83477/4.90972. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.84643/4.89406. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.83309/4.89796. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.84121/4.91772. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83627/4.90456. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.83506/4.92907. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.83245/4.91998. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83502/4.92035. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83540/4.90262. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.83188/4.92912. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83163/4.92766. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.83193/4.92754. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83238/4.91135. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82850/4.91461. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82652/4.91336. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82893/4.92412. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83124/4.90684. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82718/4.92597. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.83099/4.89165. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82484/4.93365. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82818/4.91684. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82429/4.90479. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.82742/4.88975. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.013980301652228746\n",
      "Epoch 0, Loss(train/val) 4.85800/4.83010. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.83842/4.85651. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.83646/4.81795. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.84326/4.80478. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.85490/4.80567. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.84571/4.83742. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.83112/4.82065. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.83441/4.82150. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.83261/4.82034. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.83230/4.81618. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.83200/4.81292. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.83232/4.80898. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.83006/4.80595. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.82996/4.79833. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.82972/4.79760. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.82990/4.79621. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.82572/4.78525. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.82787/4.78736. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83081/4.79976. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.81955/4.77935. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.82458/4.79353. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.82733/4.80147. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.82420/4.80679. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.82398/4.79316. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82082/4.78960. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82549/4.79330. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.81748/4.77764. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.81910/4.78501. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.81648/4.78355. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.81840/4.79331. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.81937/4.78709. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.81541/4.78849. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.81475/4.78059. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.81856/4.79692. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.81096/4.78026. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.81266/4.78050. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.82128/4.79739. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.81447/4.77214. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.81317/4.77697. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.81931/4.81062. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.80698/4.77570. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.81299/4.78622. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.81772/4.79580. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.81380/4.78983. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.81068/4.77791. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.81291/4.79152. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.81351/4.78971. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.81216/4.80108. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.80572/4.77546. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.81448/4.79919. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.80984/4.79924. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.81029/4.77763. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81099/4.77811. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.80778/4.78097. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81309/4.77484. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.80985/4.77884. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.81020/4.77408. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.80843/4.78949. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.80850/4.78670. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.80492/4.77837. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.80899/4.79441. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.80897/4.77828. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.80566/4.78058. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.80845/4.79241. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81076/4.79240. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.80538/4.78386. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.80458/4.78734. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.80822/4.78404. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.80491/4.78112. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.80807/4.78400. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.80532/4.78170. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.80375/4.78221. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.80625/4.78595. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.80611/4.77954. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.80235/4.77923. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 4.80600/4.78937. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.80122/4.78447. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.80554/4.78199. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.80954/4.79657. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.80910/4.79903. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.79996/4.78194. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.80073/4.79003. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.79946/4.79013. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.79935/4.76407. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.80141/4.78927. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.80929/4.78653. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.80245/4.78218. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.79887/4.78375. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.80037/4.76025. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 4.80813/4.80952. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.79341/4.78987. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.80712/4.81179. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.79768/4.79282. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.79866/4.80440. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.80066/4.78672. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.79920/4.78164. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.80183/4.78693. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.79469/4.77676. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.79980/4.79952. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.79866/4.78556. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.019772733298509516\n",
      "Epoch 0, Loss(train/val) 4.60017/4.52946. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.55479/4.54625. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.56172/4.54574. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.56519/4.54022. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.56441/4.54812. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.55719/4.54413. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.55399/4.54082. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.55404/4.53919. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.55361/4.54259. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.55476/4.54328. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.55554/4.54323. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.55203/4.54378. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.55026/4.54059. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.55269/4.54201. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.55081/4.54084. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.54833/4.53757. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.55143/4.53826. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.55153/4.54835. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.54865/4.54426. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.54734/4.53743. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.54743/4.53525. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.54706/4.53304. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.54504/4.53383. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.54399/4.53342. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.54414/4.53724. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.54464/4.53578. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.54337/4.53438. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.54349/4.53583. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.54001/4.53157. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.54155/4.53530. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.54318/4.53492. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.54015/4.53989. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.53811/4.53546. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.53759/4.53269. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.54322/4.53985. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.53713/4.54348. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.53771/4.53609. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.53756/4.53193. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.53925/4.53710. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.53308/4.53200. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.53918/4.53408. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.54059/4.53607. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.53611/4.54411. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.53929/4.53371. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.53723/4.53525. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.53749/4.53101. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.53626/4.53210. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.53481/4.54546. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.53527/4.53311. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.54034/4.54102. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.53418/4.53636. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.53246/4.53068. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.53874/4.53496. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 4.53780/4.53992. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.53372/4.54175. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.53486/4.54200. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.53132/4.53529. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.53667/4.54194. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.52905/4.54120. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.53445/4.54073. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.53239/4.53746. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.52985/4.53665. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.53730/4.51551. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.54024/4.53976. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.53568/4.53061. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.53454/4.51818. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.53278/4.54015. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.53099/4.54084. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.53275/4.55340. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.53486/4.54741. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.53533/4.54492. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.53334/4.54206. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.53261/4.55376. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.53245/4.55079. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.53229/4.55841. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.53000/4.54233. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.52966/4.55533. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.52987/4.56786. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.53303/4.56233. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.53156/4.55630. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.53467/4.55140. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.53199/4.56948. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.52998/4.56108. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.52550/4.57101. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.52914/4.56354. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.52744/4.56789. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.52587/4.56697. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.52753/4.56345. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.52442/4.57161. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.54022/4.53607. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.53393/4.55001. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.52913/4.56263. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.52108/4.57811. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.52529/4.58619. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.52329/4.57301. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.52511/4.57658. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.52534/4.57699. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.52555/4.57531. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.52839/4.58307. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.52614/4.58196. Took 0.21 sec\n",
      "ACC: 0.40625, MCC: -0.15789473684210525\n",
      "Epoch 0, Loss(train/val) 4.78838/4.77468. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.78214/4.77088. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.77315/4.77266. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.77671/4.78198. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.78123/4.77928. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.78250/4.76274. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.77174/4.76006. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.76505/4.76018. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.76564/4.76067. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.76680/4.76079. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.76481/4.76038. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.76422/4.75983. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.76257/4.76030. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.76103/4.76328. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.76077/4.76382. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 4.75998/4.76939. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.76225/4.76147. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.76542/4.75730. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.75914/4.75430. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.75735/4.75259. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.75277/4.76347. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.75709/4.75611. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.75639/4.76258. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.75756/4.76379. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.75486/4.76910. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.75461/4.77902. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.75454/4.75850. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.74874/4.75980. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.75224/4.75924. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.75480/4.75174. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.75170/4.75326. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.75450/4.74943. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.74475/4.76647. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.75061/4.76352. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.75006/4.78198. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.74775/4.76742. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.74643/4.77453. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.74856/4.76789. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.74394/4.78152. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.75204/4.76724. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.74707/4.77731. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.74408/4.77263. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.74591/4.78470. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.75102/4.78800. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.75301/4.79411. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.75242/4.78320. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.74843/4.78141. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.74405/4.78519. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.74227/4.79280. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.74867/4.76795. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.74442/4.78875. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.74335/4.80390. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.75195/4.77383. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.74084/4.79703. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.74496/4.79355. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.74204/4.80054. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.73587/4.80640. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.74136/4.78846. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.74008/4.81748. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.73963/4.81162. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.73629/4.79661. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.73545/4.81527. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.74305/4.79629. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.73272/4.82409. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.72837/4.83862. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.74097/4.79090. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.73581/4.81192. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.73817/4.80818. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.73678/4.80567. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.73950/4.82612. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.73946/4.80601. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.73766/4.81580. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.73446/4.81785. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.73394/4.82634. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.73106/4.82367. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.73317/4.80994. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.73840/4.79889. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.72788/4.81229. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.73655/4.80897. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.73477/4.81741. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.73354/4.81443. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.73232/4.82248. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.73406/4.81603. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.73564/4.81514. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.73299/4.82413. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.72360/4.83471. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.73228/4.82835. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.73717/4.82124. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.72616/4.83776. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.73452/4.82738. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.73556/4.81461. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.72935/4.83302. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.73528/4.82754. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.73182/4.82739. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.72798/4.84344. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.72919/4.83259. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.73016/4.82592. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.72351/4.83325. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.73174/4.82920. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.73019/4.80447. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.17448048290289417\n",
      "Epoch 0, Loss(train/val) 4.72490/4.70304. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.70299/4.70110. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.70083/4.70559. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.70002/4.69563. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.70168/4.68962. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.69717/4.68303. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.70023/4.68642. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.69685/4.68575. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.69370/4.68585. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.69404/4.68612. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.69066/4.68458. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.69064/4.68294. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.69564/4.68389. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.69113/4.68018. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.69155/4.68336. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.69419/4.67943. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.69064/4.68532. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.69029/4.68224. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.69307/4.68522. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.68878/4.68785. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.69103/4.69239. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.69044/4.69016. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.68777/4.69126. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.68514/4.68804. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.68676/4.69150. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.68713/4.69012. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.68176/4.69180. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.68380/4.69029. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.68237/4.69201. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.68776/4.69611. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.68672/4.68938. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.68770/4.68309. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.68279/4.69746. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.68236/4.70180. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.68484/4.69887. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.68076/4.70075. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.68281/4.69615. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.68369/4.69181. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.68115/4.70291. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.68102/4.70029. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.68148/4.69358. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.67886/4.71308. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.67866/4.69787. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.68148/4.69686. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.67994/4.71169. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.67954/4.71584. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.68586/4.72612. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.69550/4.69896. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.68918/4.69846. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.68836/4.70105. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.68477/4.71257. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.68809/4.71091. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.68387/4.71274. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.68526/4.69673. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.68122/4.70930. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.68431/4.70441. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.68340/4.70209. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.68326/4.69516. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.68259/4.70286. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.68360/4.70040. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.67735/4.71609. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.67865/4.71224. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.68153/4.70612. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.67874/4.69599. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.67710/4.69952. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.68122/4.69828. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.69343/4.70653. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.68852/4.71074. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.68782/4.69832. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.68953/4.68755. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.68049/4.70517. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.67886/4.70749. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.68453/4.70278. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.67606/4.70088. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.67882/4.69378. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.67632/4.68906. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.67990/4.70399. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.67778/4.72155. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.67323/4.70682. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.67420/4.71685. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.67459/4.70319. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.67432/4.71787. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.67830/4.69213. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.66991/4.72535. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.67311/4.71977. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.67334/4.70567. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.69337/4.69028. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.69184/4.69101. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.69015/4.69769. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.68542/4.69710. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.68117/4.69563. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.68560/4.69516. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.67983/4.69775. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.67856/4.70905. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.68139/4.69457. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.68148/4.69212. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.68051/4.68930. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.67737/4.69519. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.67712/4.70785. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.67779/4.69339. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.025552902682603722\n",
      "Epoch 0, Loss(train/val) 4.82263/4.80090. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.79878/4.82460. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.80202/4.85110. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.80445/4.85898. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.80133/4.80391. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79334/4.80503. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.79272/4.81498. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79387/4.81346. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.79168/4.81225. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.79258/4.81829. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79122/4.81979. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.79287/4.81472. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79054/4.81543. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79001/4.82226. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.78931/4.82502. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78969/4.82774. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78879/4.81965. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78637/4.82048. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78633/4.82144. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78714/4.82156. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.78696/4.83527. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78816/4.82169. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78529/4.82175. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78406/4.83259. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78657/4.82562. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.78211/4.83280. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78492/4.82949. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78418/4.82365. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78275/4.82060. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78152/4.83093. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.78267/4.83657. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.78432/4.83595. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77922/4.83833. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 4.78491/4.83942. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.78117/4.82865. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77827/4.83658. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78275/4.81842. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.78227/4.82315. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.79367/4.80077. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.78626/4.79680. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.78546/4.80042. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.78286/4.81693. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.78317/4.80803. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.78582/4.82525. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.78444/4.82106. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.78166/4.82053. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77986/4.83565. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.78334/4.82158. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.78617/4.79539. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.78289/4.80116. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.77833/4.79966. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.77735/4.81081. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.78044/4.81597. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.77736/4.83295. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.79214/4.81003. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.78772/4.80231. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.78299/4.78437. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.78074/4.78658. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.77964/4.80828. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.77955/4.80358. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.77838/4.81152. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.77144/4.81599. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.78489/4.82020. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.78294/4.81396. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.78097/4.80929. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.77659/4.82977. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.77981/4.83331. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 4.77947/4.83624. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77560/4.82040. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.77950/4.83319. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.77858/4.83365. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.78025/4.81992. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.77248/4.82367. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.77274/4.83767. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77427/4.82453. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.77697/4.82790. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.77356/4.82229. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.76366/4.83498. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.77201/4.83480. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.77716/4.81981. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.76736/4.82651. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.77312/4.82966. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.76986/4.82712. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.76888/4.81223. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.76283/4.83108. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.77292/4.81089. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.76170/4.83140. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.77352/4.82182. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.76824/4.80507. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.76534/4.83561. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.76939/4.82290. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.76510/4.82378. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.76461/4.82368. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.76727/4.83472. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.76140/4.83017. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.77482/4.81435. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.76326/4.83312. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.76041/4.84000. Took 0.21 sec\n",
      "Epoch 98, Loss(train/val) 4.76436/4.81960. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.76265/4.82401. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.18547738582708967\n",
      "Epoch 0, Loss(train/val) 4.74807/4.70419. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.71588/4.70841. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.71627/4.72285. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.71433/4.72582. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.71316/4.72354. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.71308/4.71946. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.71447/4.71798. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.70664/4.72108. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.71158/4.71823. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.70832/4.71952. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.70725/4.71664. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.70787/4.72459. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.70972/4.71673. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.70870/4.71463. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.71027/4.71115. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.70833/4.71424. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.70553/4.71364. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.71273/4.71448. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.70522/4.71555. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.70440/4.71796. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.70313/4.71648. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.70082/4.72054. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.70052/4.71493. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.70196/4.71331. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.70583/4.71110. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.70440/4.71862. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.70277/4.72585. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.70080/4.72079. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.70004/4.73051. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.69935/4.73599. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.69771/4.72836. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.69785/4.73606. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.69713/4.73794. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.70051/4.73800. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.69647/4.74135. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.69183/4.73027. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.69686/4.73928. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.69824/4.72915. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.69166/4.76315. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.69417/4.73850. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.69358/4.74312. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.69269/4.74054. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.69906/4.73177. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.69221/4.72917. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.69907/4.72389. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.69305/4.73578. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.69172/4.73751. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.68965/4.75631. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.68706/4.76650. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.68763/4.74387. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.70404/4.71659. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.70210/4.73802. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.69952/4.73005. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.69394/4.72895. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.69192/4.73860. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.68976/4.74809. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.69188/4.74920. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.69776/4.74038. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.69020/4.73878. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.68788/4.75764. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.69014/4.74836. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.69168/4.74649. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.69095/4.75080. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.69026/4.74030. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.69366/4.74475. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.69020/4.74747. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.68869/4.74814. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.68860/4.74856. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.68457/4.75365. Took 0.21 sec\n",
      "Epoch 69, Loss(train/val) 4.67854/4.76167. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.68667/4.79282. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.68551/4.75776. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.68373/4.74748. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.68435/4.76280. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.68096/4.76467. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.68325/4.76758. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.68194/4.75340. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.68181/4.75796. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.68339/4.76340. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.67960/4.77196. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.68778/4.76583. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.67820/4.76849. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.67653/4.79160. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 4.67337/4.80095. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.68322/4.79826. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.68248/4.75906. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.67959/4.78035. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.67877/4.76343. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.67630/4.75649. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.67764/4.77957. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.67657/4.75669. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.67558/4.78553. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.67332/4.79161. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.67270/4.78124. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.68268/4.75483. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.67535/4.79734. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.68142/4.76980. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.68196/4.75563. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.66879/4.78269. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.67129/4.76246. Took 0.20 sec\n",
      "ACC: 0.515625, MCC: 0.013980301652228746\n",
      "Epoch 0, Loss(train/val) 4.77783/4.78294. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.74654/4.74478. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.73872/4.73441. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.73950/4.73788. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.74184/4.74147. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.74071/4.73809. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.74085/4.73787. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.73666/4.74079. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.73900/4.74617. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.73615/4.74540. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.73442/4.75070. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.73329/4.75633. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.73411/4.75865. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.73147/4.75668. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.73512/4.76214. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.73071/4.75859. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.73014/4.76325. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.73087/4.75192. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.72831/4.77290. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.73072/4.75793. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.73243/4.76668. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 4.72469/4.77102. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.72765/4.77852. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.72758/4.78211. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.72723/4.78398. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.72611/4.73222. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.74031/4.72948. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.73414/4.73738. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.73103/4.75046. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.73113/4.76328. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.73140/4.74086. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.72769/4.74593. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.73093/4.74946. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.72579/4.76811. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.72253/4.77405. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.72741/4.75826. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.72896/4.76430. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.72628/4.76226. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.72829/4.77560. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.72571/4.78787. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.72324/4.79146. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 4.72301/4.79169. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.72390/4.79836. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.72552/4.78738. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.71825/4.83464. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.72446/4.77824. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.71735/4.83413. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.72460/4.78410. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.71902/4.79859. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.71917/4.81326. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.71955/4.80537. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.71727/4.82463. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.71978/4.79840. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.71555/4.82031. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.71920/4.80253. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.71916/4.81409. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.71438/4.81820. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.71430/4.81581. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.71474/4.81051. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.72174/4.80078. Took 0.22 sec\n",
      "Epoch 60, Loss(train/val) 4.71702/4.82077. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.71418/4.80141. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.71371/4.82077. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.70890/4.83529. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.71636/4.79610. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.71249/4.83429. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.71715/4.78386. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.71087/4.84392. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.71818/4.82059. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.70993/4.82235. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.71366/4.81349. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.70931/4.83881. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.71420/4.79977. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.70953/4.84066. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.71543/4.80055. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.70252/4.87515. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.72099/4.78059. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.70610/4.83666. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.71231/4.80873. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.70714/4.85621. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.71433/4.80395. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.70841/4.83096. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.71234/4.82659. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.70567/4.83182. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.70810/4.82909. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.71058/4.82503. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.70357/4.82984. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.70472/4.84391. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.70813/4.81798. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.70050/4.85037. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.71393/4.80540. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.70011/4.87997. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.70919/4.81644. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.71924/4.73683. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.72014/4.76326. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.71713/4.78906. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.71033/4.79409. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.71264/4.80810. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.70275/4.83873. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.71132/4.79225. Took 0.20 sec\n",
      "ACC: 0.484375, MCC: -0.06052275326688024\n",
      "Epoch 0, Loss(train/val) 5.07783/5.03360. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.03927/5.06172. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.03250/5.07416. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.02880/5.08445. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.03027/5.06808. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.02929/5.06823. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.03076/5.08542. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.03288/5.08462. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.02866/5.08225. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.03035/5.08467. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 5.02814/5.07873. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.03102/5.08514. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.02817/5.07896. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.02800/5.07908. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.02848/5.08003. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 5.02677/5.08037. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.02750/5.08124. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 5.02721/5.08355. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 5.02593/5.09122. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.02272/5.07500. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.02077/5.09151. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.02222/5.08452. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.02459/5.08640. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 5.01802/5.09400. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.02382/5.08116. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 5.01849/5.08707. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.02068/5.09857. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.01772/5.10219. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.01715/5.10527. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.01414/5.08850. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.02097/5.06241. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 5.02083/5.06082. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.01954/5.08710. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.01674/5.08670. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 5.01521/5.09547. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 5.01144/5.10624. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.01424/5.08502. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.02206/5.07053. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.01603/5.09180. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.01357/5.09467. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.01941/5.09443. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.01598/5.08330. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.01777/5.08221. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.01365/5.10175. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.02017/5.08446. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.01921/5.09535. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.01689/5.10157. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 5.01569/5.10106. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 5.01740/5.11285. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.01494/5.10632. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.01391/5.10471. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.01514/5.09071. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.01664/5.08206. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.01674/5.07869. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.01138/5.09638. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 5.01355/5.10723. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.01359/5.10793. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 5.01483/5.09219. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.00989/5.11955. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.01309/5.10540. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 5.00578/5.11685. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.01168/5.11085. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.01050/5.09700. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.00740/5.12555. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.00846/5.09764. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.00549/5.09330. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.00360/5.10906. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 5.00443/5.10670. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.01044/5.10866. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 5.00266/5.13565. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.00362/5.12271. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.99951/5.12251. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.00205/5.11858. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.99966/5.11312. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.00424/5.10611. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 5.00015/5.13454. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.99883/5.11574. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.99864/5.12518. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.99711/5.11406. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.99758/5.13591. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.99717/5.10991. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.99879/5.12176. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.99830/5.10973. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.00132/5.08572. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.99736/5.11830. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.00005/5.09171. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.99131/5.12270. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.99762/5.12004. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 5.00294/5.11267. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.99199/5.12740. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.99567/5.14102. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.99714/5.13605. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.99287/5.13058. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.98989/5.14316. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.98937/5.14706. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.99181/5.10949. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.99844/5.11751. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.99135/5.14229. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.99952/5.09958. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.98557/5.16453. Took 0.21 sec\n",
      "ACC: 0.5, MCC: 0.0009775171065493646\n",
      "Epoch 0, Loss(train/val) 4.83177/4.79934. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.80576/4.77462. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.79837/4.77372. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79841/4.77708. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79740/4.78053. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.79744/4.78381. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.79656/4.79078. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79496/4.80122. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.79198/4.80194. Took 0.22 sec\n",
      "Epoch 9, Loss(train/val) 4.78992/4.80436. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79202/4.80338. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78997/4.80914. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.78688/4.80950. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.78854/4.81364. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.78937/4.81877. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.78756/4.82675. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78842/4.82418. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78446/4.82461. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78217/4.83452. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78613/4.83316. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.78399/4.82553. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78355/4.82558. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78450/4.83077. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.78504/4.84061. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.78115/4.84884. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78019/4.84192. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78100/4.83841. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.78283/4.83717. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.77983/4.84040. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.77866/4.84695. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.78063/4.84504. Took 0.21 sec\n",
      "Epoch 31, Loss(train/val) 4.78072/4.84610. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.78007/4.85267. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.77795/4.85909. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.77681/4.85042. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.78004/4.84692. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.77821/4.85421. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.77815/4.84271. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77704/4.85517. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.78052/4.84556. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77611/4.85955. Took 0.22 sec\n",
      "Epoch 41, Loss(train/val) 4.77596/4.87069. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.77263/4.86556. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.77345/4.87967. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.77297/4.85310. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 4.77909/4.86763. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.77062/4.87828. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.77558/4.86464. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.77108/4.87844. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.77158/4.86605. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.77170/4.87232. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.77219/4.87498. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.77142/4.87000. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.77375/4.88965. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76938/4.88031. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.76997/4.89728. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.77715/4.86893. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.76925/4.88775. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.76630/4.90947. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.77030/4.89340. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.76822/4.88799. Took 0.18 sec\n",
      "Epoch 61, Loss(train/val) 4.76632/4.90883. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.76738/4.90415. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 4.77202/4.90053. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76506/4.90643. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.76557/4.90102. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.77054/4.89605. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.76867/4.89200. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.76600/4.91870. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.76789/4.90035. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.76610/4.89819. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.76242/4.91926. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.76523/4.89179. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.76242/4.91124. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.76745/4.90198. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.76067/4.91768. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.76795/4.89104. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.75954/4.94167. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.75876/4.92614. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.75843/4.92610. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.75452/4.92821. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.76184/4.92582. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.75983/4.93670. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.76170/4.91832. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.75764/4.93378. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.76141/4.90285. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.76318/4.91234. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.75501/4.93019. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.75861/4.91956. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.76340/4.89495. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.76184/4.91989. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.75761/4.95603. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.75394/4.94940. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.75331/4.95768. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.75516/4.97024. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.76108/4.90957. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.75993/4.94048. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.75936/4.93952. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.75940/4.95328. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.75454/4.94521. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.033277916281986085\n",
      "Epoch 0, Loss(train/val) 4.68039/4.63496. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.63277/4.62507. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.62235/4.62475. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.62488/4.62326. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.62368/4.62137. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.62056/4.62078. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.62233/4.62168. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.62156/4.62221. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.62084/4.62451. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.62219/4.62455. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.62061/4.62432. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.62268/4.62535. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.61904/4.62736. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.61952/4.62785. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.61942/4.62799. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.61913/4.62854. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.61993/4.62997. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.62051/4.63075. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.61665/4.63284. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.61741/4.63277. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.61618/4.63517. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.61533/4.63640. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.61451/4.63574. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.62073/4.62805. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.62063/4.62806. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.61574/4.63120. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.61595/4.63384. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.61451/4.63811. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.61386/4.64369. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.61540/4.64702. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.61258/4.64785. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.61248/4.64804. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.61337/4.65457. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.61192/4.65363. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.61289/4.65062. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.61462/4.65195. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.61242/4.64995. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.60948/4.65515. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.61421/4.64821. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.60687/4.65014. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.61597/4.64887. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.61134/4.65082. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.61213/4.65243. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.61025/4.65925. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.60942/4.65374. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.60890/4.65848. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.61006/4.66053. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.60986/4.65596. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.60803/4.63152. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.61834/4.63768. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.61517/4.62247. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.61344/4.63454. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.61122/4.64561. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.60979/4.64727. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.60719/4.64442. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.61118/4.65172. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.60153/4.66115. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.60745/4.65910. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.60805/4.66043. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.60362/4.65632. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.60577/4.64101. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.60548/4.65128. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.60336/4.64918. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.60585/4.64150. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.59568/4.62141. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.60842/4.64726. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.59793/4.69180. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.59912/4.64566. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.60349/4.67573. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.60282/4.65176. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.59549/4.66901. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.60306/4.65400. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.59780/4.64944. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.59534/4.66694. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.59565/4.68654. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.62060/4.62422. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.61613/4.62845. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.61254/4.62811. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.61626/4.62758. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.61016/4.62613. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.60865/4.62853. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.60642/4.63422. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.61009/4.62565. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.60336/4.63315. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.60123/4.63755. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.60177/4.62928. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.60747/4.61841. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.60211/4.63909. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.60207/4.64460. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.60467/4.64616. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.59900/4.64825. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.59884/4.65644. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.59972/4.63967. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.59984/4.65678. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.59881/4.64214. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.59670/4.65725. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.59557/4.65496. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.59585/4.65850. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.60164/4.64895. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.60335/4.65568. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.0778902520624173\n",
      "Epoch 0, Loss(train/val) 4.77404/4.74997. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.76355/4.77236. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.77265/4.74399. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.76053/4.74774. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.74996/4.74295. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.75390/4.74389. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.75305/4.74413. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.75257/4.74538. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.75314/4.74799. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.75124/4.75218. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.74505/4.76864. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.74686/4.74657. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.74271/4.75343. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.74028/4.74948. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.74204/4.75095. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.73996/4.75037. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.74028/4.74740. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.73670/4.74878. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.73766/4.75373. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.73665/4.74279. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.73725/4.74776. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.73708/4.74647. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.73513/4.74280. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.74112/4.74831. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.73731/4.75855. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.74735/4.74125. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.74903/4.74772. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.74640/4.75161. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.74664/4.75324. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.74514/4.74464. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.74547/4.74656. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.74579/4.75861. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.74146/4.75430. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.74157/4.74321. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.75578/4.75044. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.74751/4.74904. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.74813/4.74859. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.74655/4.74758. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.74342/4.74733. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.73996/4.74937. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.74039/4.74959. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.74196/4.75243. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.73750/4.76861. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.73927/4.76138. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.74336/4.74715. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.74143/4.76647. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 4.74122/4.75266. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.73737/4.76751. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.73673/4.75866. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.73428/4.77840. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.73687/4.74859. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.73501/4.77345. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.73475/4.75804. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.72817/4.76652. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.73322/4.76879. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.73384/4.77759. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.72939/4.75397. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.73091/4.78188. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.72644/4.77900. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.72931/4.75776. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.73027/4.77447. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.72560/4.77878. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.72327/4.79136. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.73328/4.73988. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.74794/4.75839. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.74309/4.74451. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.73802/4.75853. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.73699/4.76871. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.73441/4.75080. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.73703/4.76153. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.72971/4.76186. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.73364/4.77087. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.73315/4.75707. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.73151/4.76954. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.72768/4.77660. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.72626/4.78078. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.72916/4.78777. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.73232/4.77750. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.73056/4.78496. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.72466/4.78604. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.72611/4.78590. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.72338/4.77296. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.73658/4.75941. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.73788/4.76304. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.73591/4.77185. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.73327/4.78182. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.73341/4.78165. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.72304/4.77287. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.74331/4.75361. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.73030/4.78788. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.72470/4.80133. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.72418/4.79613. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.72190/4.79798. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.72474/4.77887. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.72795/4.79022. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.72178/4.80286. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.72515/4.81403. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.71624/4.81582. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.72234/4.78115. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.71918/4.81594. Took 0.20 sec\n",
      "ACC: 0.515625, MCC: 0.06052275326688024\n",
      "Epoch 0, Loss(train/val) 4.58090/4.52928. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.49612/4.51066. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.49421/4.49665. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.49589/4.49352. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.49166/4.49405. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.48943/4.48914. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.49306/4.48334. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.49631/4.48165. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.49105/4.48539. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.49376/4.48693. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.48975/4.48002. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.48952/4.48240. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.49086/4.48435. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.48794/4.47976. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.48586/4.47626. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.48720/4.47683. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.48683/4.48358. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.48655/4.49036. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.48856/4.49145. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.48563/4.48847. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.48514/4.47253. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.48668/4.48191. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.48603/4.48144. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.48724/4.47559. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.47765/4.47176. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.47769/4.47516. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.47958/4.47754. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.47708/4.48473. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.48155/4.47619. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.47939/4.48795. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.48419/4.47236. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.48379/4.47123. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 4.48081/4.48344. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.47869/4.48660. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.47760/4.48645. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.47186/4.47139. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.48143/4.48542. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.47601/4.47165. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.48186/4.47259. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.47544/4.47571. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.47963/4.46299. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.47728/4.46755. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.47321/4.47171. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.48041/4.46618. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.47094/4.48025. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.47234/4.47753. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.47198/4.48103. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.47085/4.47304. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.47177/4.47891. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.47512/4.48038. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.47077/4.47550. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.47598/4.49349. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.47210/4.50642. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.47703/4.49677. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.46852/4.49589. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.47072/4.48144. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.47453/4.49077. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.46951/4.51580. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.46901/4.47555. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.46785/4.47161. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.47223/4.50579. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.46751/4.48192. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.46521/4.50546. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.46472/4.49538. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.47423/4.47728. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.46693/4.50364. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.46402/4.47244. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.46412/4.49860. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.46274/4.48271. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.46799/4.45750. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.46999/4.45967. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.47130/4.46830. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.47446/4.50817. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.46811/4.46417. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.46885/4.45747. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.46536/4.47450. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.46837/4.47395. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.46450/4.50408. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.47055/4.47137. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.47093/4.47704. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.46444/4.48625. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.46803/4.46728. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.45929/4.47174. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.47770/4.45918. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.46743/4.51338. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.47374/4.50155. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.46925/4.48466. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.46968/4.49061. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.45999/4.49670. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.46355/4.48147. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.45921/4.47798. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.46646/4.48291. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.45860/4.48059. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.45793/4.50153. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.46548/4.52072. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.47353/4.50122. Took 0.22 sec\n",
      "Epoch 96, Loss(train/val) 4.46024/4.47511. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.46326/4.47754. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.45981/4.47428. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.46278/4.49575. Took 0.19 sec\n",
      "ACC: 0.4375, MCC: -0.0931174089068826\n",
      "Epoch 0, Loss(train/val) 4.90166/4.90429. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.87813/4.90860. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.87427/4.91484. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87241/4.91512. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 4.87983/4.93062. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.87860/4.92976. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.87681/4.91558. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.87368/4.90678. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.86893/4.90641. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.86835/4.91744. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.86801/4.91641. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.87010/4.90967. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.86839/4.90580. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.86107/4.91072. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.86510/4.90969. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.86572/4.93447. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.87368/4.90950. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.86842/4.90876. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.86516/4.91568. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.86648/4.92062. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.86266/4.91233. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.85879/4.91333. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.86183/4.90590. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.86261/4.89887. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85966/4.90397. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.85697/4.93386. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.86575/4.89924. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.86001/4.91624. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.86205/4.88943. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.85512/4.89775. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.85752/4.91099. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.85864/4.90162. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.85965/4.90668. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.85845/4.90608. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85824/4.91329. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.85850/4.90368. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.85302/4.90886. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.85518/4.90853. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.85369/4.89788. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.85325/4.89484. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.85174/4.90604. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.85396/4.90814. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.84946/4.89858. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.85197/4.89804. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84636/4.89954. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84952/4.93156. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.86113/4.90893. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.85125/4.91333. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.85229/4.91151. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.86163/4.89891. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.85954/4.90000. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.86199/4.89564. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.85453/4.89902. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.85540/4.90020. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.85443/4.89409. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.85036/4.90273. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84947/4.89726. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.84576/4.90715. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.84521/4.89342. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.85207/4.89240. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.84700/4.90017. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.84986/4.90122. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85389/4.92426. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.85677/4.91716. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.84937/4.88857. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.84880/4.87912. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.84524/4.88365. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.84657/4.89473. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.84529/4.88275. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84491/4.88978. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.84774/4.89696. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.83870/4.89073. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.84717/4.89633. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84239/4.89425. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.84103/4.90324. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.84414/4.88930. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.84827/4.87436. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.83895/4.88535. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.84389/4.87618. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.84432/4.88714. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.84252/4.89116. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83932/4.87551. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.84132/4.88439. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.83725/4.87640. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83809/4.87711. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83546/4.89251. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.84053/4.88894. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.84339/4.88558. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83930/4.89388. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83053/4.88209. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.83896/4.89425. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.84025/4.88754. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.84066/4.88837. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83356/4.89191. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83192/4.88171. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.83133/4.89231. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.84622/4.89429. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 4.84028/4.88291. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.83890/4.88010. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.82980/4.88407. Took 0.21 sec\n",
      "ACC: 0.484375, MCC: -0.03202563076101743\n",
      "Epoch 0, Loss(train/val) 5.12175/5.08907. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.09417/5.10132. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.08766/5.08705. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.09005/5.08586. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.09170/5.08872. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.09035/5.09240. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.09093/5.10106. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.08714/5.11018. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.08790/5.11401. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.08709/5.11614. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.08320/5.11694. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.08196/5.11511. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.08507/5.11422. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 5.08502/5.11471. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.08503/5.11809. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.08385/5.11334. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 5.08420/5.10686. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.08022/5.11582. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.08334/5.11949. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.08268/5.11818. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.08097/5.11968. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.08259/5.11973. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.07987/5.12032. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.07888/5.11232. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.07581/5.11374. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.07472/5.13315. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 5.07647/5.14632. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 5.08269/5.12438. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.07251/5.11717. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 5.07139/5.10939. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.07153/5.12662. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.07013/5.12673. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.07194/5.11877. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 5.07044/5.11954. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.07204/5.11162. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.06986/5.11441. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.06689/5.12726. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.06986/5.12758. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.07590/5.12086. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.07921/5.12126. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.07872/5.12078. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.07822/5.12432. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.07825/5.12791. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.07483/5.11932. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.07573/5.13345. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.07729/5.12645. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.06970/5.11644. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.07606/5.13341. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.07325/5.14920. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.07244/5.14236. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 5.06599/5.15018. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.07255/5.12618. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 5.07113/5.14241. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.07176/5.13570. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.07243/5.13839. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 5.06997/5.13221. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 5.07050/5.13523. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.06745/5.12886. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.06583/5.12237. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 5.06561/5.13255. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.06427/5.14720. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.06787/5.14694. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.07177/5.13790. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.06773/5.14852. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.06433/5.13733. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.06822/5.12713. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.06037/5.13561. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.06360/5.14484. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 5.06503/5.13324. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.06680/5.12229. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.05858/5.14850. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.06619/5.14506. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 5.06464/5.13387. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.06040/5.14149. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.06043/5.15006. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.06213/5.15454. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.06285/5.14008. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.06510/5.13513. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.06139/5.12458. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 5.05924/5.13476. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.06349/5.12630. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.05597/5.13931. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.06774/5.13939. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.06657/5.14157. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 5.06292/5.12122. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.05862/5.13116. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.06226/5.13685. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.05811/5.13188. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.05534/5.15061. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 5.05498/5.14921. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.05226/5.14539. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.06517/5.13028. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 5.06452/5.12583. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.05889/5.12337. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 5.05916/5.13984. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.06267/5.13495. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.06877/5.11448. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 5.06446/5.12849. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.05890/5.13537. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 5.06597/5.13097. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.059863071616150634\n",
      "Epoch 0, Loss(train/val) 4.90822/4.90558. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.89972/4.89498. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.89591/4.89582. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.89296/4.92029. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.89569/4.93166. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.89692/4.90916. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.89146/4.90074. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.88589/4.90246. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.88594/4.91118. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.88661/4.91117. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.88304/4.91603. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.88396/4.91928. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.88167/4.93171. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.88690/4.92710. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.88322/4.91885. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.88007/4.92896. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.87915/4.92980. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.87846/4.93281. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.88269/4.92125. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.87700/4.92842. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.87622/4.93670. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.87412/4.93823. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.87365/4.94240. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.87682/4.94772. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.87776/4.93679. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.87641/4.93459. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.87302/4.92964. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.87438/4.92522. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.87162/4.93405. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.87056/4.93213. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.87333/4.93362. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86917/4.94504. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.87650/4.93224. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.87617/4.93130. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.87168/4.93926. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.87134/4.95513. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.86975/4.96222. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.87372/4.93911. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.86795/4.94249. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.87046/4.92701. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.87027/4.94320. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.86421/4.95710. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.86699/4.94839. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.86899/4.94611. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.87699/4.92741. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.87913/4.92749. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.86965/4.93982. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.87341/4.94024. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.86896/4.94570. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.87031/4.92645. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.86362/4.97038. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.88443/4.92356. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.87946/4.93939. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.87751/4.93630. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.87315/4.93605. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.87080/4.93581. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.86308/4.96064. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.86239/4.94722. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.86567/4.95777. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.86540/4.94557. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.86702/4.94959. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.86950/4.94006. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.86241/4.97516. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.86936/4.99345. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.88010/4.91705. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.87054/4.95513. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.87049/4.94672. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.86704/4.95266. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.86652/4.94700. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.86449/4.95238. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.86062/4.97619. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.86138/4.95550. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.86580/4.94449. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.86013/4.97018. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.85675/4.93670. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.85786/4.94896. Took 0.21 sec\n",
      "Epoch 76, Loss(train/val) 4.86262/4.98313. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.85721/4.92312. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.86275/4.94824. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.85695/4.94467. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.85584/4.96040. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.86023/4.93355. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.86333/4.94232. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.85586/4.94686. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.85240/4.92983. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.85883/4.91347. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.84603/4.97452. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.85723/4.96032. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.84720/4.99020. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.85099/4.97244. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.85436/4.93483. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.85539/4.97966. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.85223/4.93863. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.84873/4.99192. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.85745/4.95361. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.84965/4.98550. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.84580/4.97512. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.84475/4.95793. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.83739/5.00198. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.85160/4.92988. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.1513518081969605\n",
      "Epoch 0, Loss(train/val) 4.92802/4.83385. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.84372/4.84566. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.83346/4.85057. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.83201/4.84441. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.82695/4.84743. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.82686/4.84975. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.82552/4.85957. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.82099/4.86286. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.82210/4.86414. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.82199/4.87679. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.82247/4.87671. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.82120/4.87269. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.82427/4.87091. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 4.82177/4.87717. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.82162/4.88080. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.82286/4.87966. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.81635/4.89351. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.81816/4.89035. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81968/4.89053. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.81612/4.88973. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.81307/4.89987. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.81213/4.89498. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.81471/4.91136. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.81379/4.90034. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.80952/4.90002. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.81640/4.89547. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.81060/4.89310. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.81347/4.88359. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.81336/4.89278. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.81271/4.90088. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.81141/4.89920. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.80555/4.92346. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.81157/4.91252. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.80500/4.90657. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.80309/4.92104. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.80354/4.90827. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.80359/4.92262. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.79763/4.93413. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.80415/4.91464. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.80529/4.91239. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.80401/4.91886. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.80419/4.93089. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.80088/4.92348. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.80469/4.91096. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.80262/4.93090. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.79712/4.92887. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.80319/4.91425. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.79586/4.93205. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.79964/4.92438. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.80130/4.93389. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.79388/4.93083. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.80550/4.92575. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.80491/4.91811. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.80006/4.94553. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.80589/4.91361. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.79734/4.95967. Took 0.21 sec\n",
      "Epoch 56, Loss(train/val) 4.79825/4.92286. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.79901/4.94337. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.79638/4.94211. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.79450/4.94169. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.79745/4.92804. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.79681/4.92177. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.79015/4.95860. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.79093/4.92942. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.79747/4.94099. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.79665/4.96128. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.78646/4.93226. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.79590/4.96968. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.79338/4.96503. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.78903/4.97695. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.79072/4.96005. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.78725/4.97436. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.78890/4.93845. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.79347/4.96319. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.78569/4.96481. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.79057/4.95733. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.78980/4.92999. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.79088/4.96966. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.79430/4.94564. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.78700/4.96072. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.77960/4.99764. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.78225/4.99359. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.78365/4.98304. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.78764/4.96082. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.78234/4.97330. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.77965/4.99293. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.78204/4.96708. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.78843/5.01318. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.77839/4.97680. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.78045/5.00905. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.78591/5.00266. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.78359/4.98366. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.77850/4.99154. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77272/5.01026. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.78425/4.95505. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.77709/4.96638. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.77609/4.96828. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.78291/4.97597. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.77885/5.01703. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.77681/4.98987. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.07570682517832987\n",
      "Epoch 0, Loss(train/val) 4.86116/4.85213. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86662/4.82956. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.84394/4.82961. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.84796/4.83035. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.84710/4.83088. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.84395/4.83164. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.84180/4.82818. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.83783/4.82743. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.83822/4.82901. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.83980/4.83139. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.83775/4.83212. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.83847/4.83275. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.83276/4.83567. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.83277/4.83962. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.83054/4.84136. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.83203/4.84444. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.83494/4.84567. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.83122/4.84365. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83021/4.84372. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.83007/4.84760. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.82680/4.85003. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.82849/4.85408. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.82807/4.85318. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.82515/4.85364. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82487/4.86157. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82718/4.86226. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82621/4.86706. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.82328/4.86389. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.82568/4.85909. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.82804/4.85692. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.82665/4.86181. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.81965/4.86403. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.82742/4.85750. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.82285/4.86208. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.82113/4.86828. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82232/4.87220. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.81934/4.86623. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.82550/4.86841. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.81965/4.86231. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.81486/4.87657. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.82120/4.87471. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.82091/4.87158. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.82080/4.87161. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.81561/4.87597. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.81933/4.86143. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.82119/4.87059. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.81728/4.85732. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.82791/4.85498. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.81647/4.86654. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.82289/4.86788. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.82066/4.86932. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.81584/4.87478. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81797/4.87738. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.82257/4.87656. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81750/4.86061. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.81536/4.87755. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.81734/4.86758. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.81582/4.87183. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.81707/4.89543. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.81711/4.88434. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.81762/4.87659. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81792/4.87199. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.81566/4.87650. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.81352/4.88863. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81546/4.88084. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81166/4.89291. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.81581/4.87612. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.81389/4.89131. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.81365/4.88531. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.81099/4.87781. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.81208/4.89203. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81239/4.90633. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.80596/4.92127. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.81968/4.89332. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81386/4.88910. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.81166/4.91024. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.80906/4.88976. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.81359/4.89196. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.81269/4.89587. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.80980/4.88390. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.80873/4.88875. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.81277/4.87881. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.81286/4.89797. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80887/4.90109. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.80813/4.90454. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.80337/4.90772. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.80795/4.91155. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.81129/4.88194. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.80788/4.92084. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.80775/4.88089. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.80642/4.91464. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.80980/4.89566. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.80415/4.90063. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.80828/4.90717. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.81087/4.89568. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.80720/4.92263. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.80978/4.89498. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.80892/4.88398. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.80252/4.93505. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.80918/4.91265. Took 0.20 sec\n",
      "ACC: 0.515625, MCC: 0.04350001325672093\n",
      "Epoch 0, Loss(train/val) 4.77362/4.79208. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.77210/4.75310. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.75971/4.78609. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.76753/4.82374. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.77007/4.76659. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.75660/4.75396. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.75460/4.76188. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.75318/4.76388. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.75309/4.76507. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.75391/4.77482. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.75342/4.76889. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.75213/4.77087. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.74874/4.78263. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.74685/4.79132. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.75021/4.78340. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.74952/4.77896. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.73940/4.79692. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.74251/4.78349. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.74436/4.79036. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.73978/4.80200. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.74323/4.79781. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.74263/4.78267. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.73913/4.78614. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.73992/4.79877. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.74462/4.79377. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.73843/4.79424. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.73734/4.80914. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.73929/4.80033. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.73298/4.80940. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.73935/4.81106. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.73677/4.79162. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.74262/4.77928. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.73865/4.80133. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.73624/4.80479. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.73889/4.79968. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.73272/4.81556. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.74129/4.78914. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.73596/4.80085. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.73764/4.79233. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.73365/4.80513. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.73174/4.81484. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.72954/4.82817. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.73492/4.79898. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.73513/4.80583. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.73088/4.81225. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 4.73645/4.80882. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.73334/4.80957. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.73131/4.80472. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.73006/4.81314. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.72955/4.81375. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.72911/4.80644. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.72734/4.82063. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.73458/4.81377. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.73094/4.81349. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.73135/4.81858. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.73246/4.81829. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.72826/4.81971. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.72993/4.82174. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.72486/4.83861. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.73106/4.82070. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.73238/4.82546. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.73191/4.80017. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.72582/4.83855. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.72657/4.81981. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.72495/4.84168. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.72524/4.84248. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.72333/4.83345. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.72517/4.82579. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.72113/4.83958. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.72477/4.81792. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.72343/4.88019. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.72989/4.81779. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.72592/4.86602. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.72439/4.83289. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.72146/4.87939. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.72903/4.82510. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.72422/4.84592. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.72012/4.86360. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.72270/4.85730. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.72401/4.86436. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.72453/4.87152. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.71865/4.83898. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.72409/4.85021. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.72618/4.85074. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.72316/4.84273. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.71853/4.86272. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.71192/4.85912. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.71759/4.87349. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.72377/4.84161. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.71803/4.85583. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.72340/4.85749. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.72637/4.84791. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.71738/4.88868. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.72008/4.84938. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.72397/4.86112. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.72063/4.85995. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.71477/4.91537. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.71264/4.90525. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.70712/4.86846. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.72059/4.86855. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.0657512425947884\n",
      "Epoch 0, Loss(train/val) 4.94247/4.92689. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.91615/4.93954. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.91279/4.94399. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.90961/4.94384. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.90655/4.95198. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.90429/4.94998. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.90654/4.95560. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.90488/4.95976. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.90859/4.95172. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.90645/4.94218. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.90618/4.96041. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.90382/4.96293. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.90329/4.95405. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 4.90634/4.95990. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.90411/4.95963. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.90073/4.96748. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90418/4.95932. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.90241/4.95683. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.90538/4.95073. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.90476/4.95080. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.90174/4.96078. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.89956/4.95987. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.90004/4.95956. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.90288/4.95692. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.89717/4.96270. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.90214/4.95691. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.89983/4.96245. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.89687/4.96749. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.89953/4.97203. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.90174/4.95859. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.89777/4.96245. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.90014/4.96266. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.89806/4.96689. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.90131/4.95792. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.89987/4.95848. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.89728/4.96452. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.89917/4.95924. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.89568/4.97590. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89866/4.96944. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.89449/4.96388. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.89428/4.97470. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.89513/4.96892. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.89771/4.96750. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.89771/4.97378. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.89397/4.97159. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.89595/4.97732. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.89365/4.97924. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89523/4.98265. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89489/4.97497. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.89413/4.96892. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89692/4.98105. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.89544/4.97385. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89285/4.98408. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.89146/4.98398. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.88944/4.98681. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.89231/4.98156. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.88905/4.98279. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.89228/4.97974. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.88921/4.98522. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89313/4.96918. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.89080/4.99398. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.89767/4.97201. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.89259/5.00115. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.89239/4.98156. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.88504/4.98719. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.88915/4.98083. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.89054/5.00132. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.88625/4.98346. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.88618/4.99257. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.89043/4.99339. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.88838/4.98236. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.88984/4.98930. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.89179/4.99461. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88934/5.00854. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.88991/4.97063. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.88526/4.99406. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.88424/4.97681. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.89102/4.97145. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.88638/4.98068. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.88567/5.01934. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88957/4.98144. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.88456/5.00131. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.88182/5.00217. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.88541/5.00475. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88528/5.00442. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.88169/5.02902. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.88800/4.97711. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.88317/4.99412. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.88648/4.98168. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.88209/4.98742. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.87942/5.03829. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.89212/5.00864. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.88505/5.00854. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.88895/4.97762. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88838/4.97686. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.87867/4.99517. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.88015/4.99062. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.87967/4.98825. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.88315/4.97422. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.88088/4.99776. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.03163859985841663\n",
      "Epoch 0, Loss(train/val) 4.75477/4.73136. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.69969/4.70583. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.69740/4.70191. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.69961/4.70203. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.69410/4.70825. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.69632/4.71055. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.69581/4.70951. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.69471/4.72230. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.69644/4.70862. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.69461/4.72023. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.69670/4.70939. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.69551/4.70647. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.69239/4.71435. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.69232/4.71762. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.69360/4.71575. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.68938/4.71030. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.69479/4.71313. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.68686/4.72083. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.69382/4.71617. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.69169/4.71456. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.68672/4.71278. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.69000/4.71144. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.69243/4.70711. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.68800/4.70957. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.69040/4.68644. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.68925/4.69836. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.69274/4.69167. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.68919/4.69150. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.68872/4.69736. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.68787/4.69372. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.68753/4.69997. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.68824/4.70226. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.68503/4.70536. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.68734/4.70102. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.68619/4.68687. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.69051/4.69182. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.68857/4.69869. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.68423/4.70913. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.68005/4.69689. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.68415/4.68895. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.68437/4.69398. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.67996/4.71764. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.68336/4.70607. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.68303/4.69828. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.68250/4.70072. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.68874/4.71982. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.68331/4.71165. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.67958/4.71424. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.67746/4.71133. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.67511/4.71067. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 4.67667/4.71951. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.67894/4.71278. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.67613/4.71575. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.68088/4.71671. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.68056/4.70928. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.67857/4.72066. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.67599/4.72039. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.67340/4.72219. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.67341/4.71683. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.67056/4.70753. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.67632/4.69980. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.67546/4.72219. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 4.67031/4.71242. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.67565/4.71175. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.67138/4.74570. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.67085/4.72176. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.66628/4.71763. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.67388/4.73270. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.67111/4.70848. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.66724/4.71069. Took 0.21 sec\n",
      "Epoch 70, Loss(train/val) 4.67588/4.71666. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.67045/4.72282. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.66618/4.72031. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.66355/4.71742. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.66418/4.72043. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 4.67332/4.71166. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.66942/4.71631. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.67167/4.74153. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.66846/4.71784. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.66521/4.70833. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.66329/4.73856. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.66232/4.76176. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.66566/4.71653. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.65945/4.72141. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.67061/4.71376. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.66452/4.72811. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.66974/4.72736. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.66192/4.74820. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.66076/4.72583. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.66566/4.71695. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.66328/4.71544. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.66218/4.71976. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.65965/4.72853. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.66234/4.73386. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.66264/4.76847. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.65860/4.74263. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.65995/4.71834. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.66164/4.72356. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.65784/4.71754. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.65208/4.76388. Took 0.20 sec\n",
      "ACC: 0.453125, MCC: -0.11383959916814836\n",
      "Epoch 0, Loss(train/val) 5.06257/5.00898. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.00750/5.00548. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.00528/5.01223. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.00845/5.01323. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 5.00648/5.01089. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 5.00297/5.01086. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 5.00696/5.01064. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.00126/5.01381. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.00458/5.00764. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.00359/5.00549. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 5.00671/5.00775. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 5.00111/5.01290. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.00173/5.00900. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.00285/5.00502. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.00126/5.00103. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.99892/5.00173. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 5.00220/5.00057. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.99617/4.99379. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.00098/4.99784. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.99509/4.99378. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.99440/4.98838. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.99493/4.98512. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.99791/4.98844. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.99339/4.98434. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.99268/4.98735. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.99473/4.98860. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.99362/4.99498. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.00226/5.00416. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.99860/4.99623. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.99183/4.98845. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.98948/4.98786. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.99061/4.99201. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.99121/4.98587. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.99183/4.98917. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.99485/5.00017. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 4.98662/4.99788. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.99377/4.99941. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.98840/4.98979. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.98636/5.00807. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.99084/4.99527. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.99520/5.01023. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.99167/5.00308. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.99329/5.00425. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.99116/4.99787. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.98508/5.00054. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.99179/5.00313. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.99198/5.01562. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.98560/5.01114. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.98631/5.01048. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.98676/5.01516. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.98669/5.00649. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.98522/5.03519. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.98191/5.03264. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.99586/4.98867. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.98698/4.98289. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.98493/4.98683. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.98752/4.99032. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.98394/4.98743. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.98975/4.98636. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.98707/4.99143. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.98227/4.98485. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.98695/4.98454. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.98335/4.98594. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.98093/4.97515. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.97746/5.00094. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.98816/5.00925. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.97834/5.01678. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.97668/5.02922. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.97685/5.04268. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.98453/5.01549. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.97720/5.00107. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.97905/5.01736. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.97736/5.01440. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.97341/5.03582. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.97597/4.99874. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.97930/5.02407. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.98536/5.00852. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.97322/5.01284. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.97707/5.01255. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.97494/5.01886. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.97239/4.99646. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.97229/5.02321. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.97179/5.04214. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.97284/5.04572. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.97196/5.01580. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.97023/5.04465. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.97457/5.01434. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.96976/5.05628. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.97217/5.03331. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.97065/5.02901. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.96941/5.03414. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.96996/4.98674. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.98252/5.04087. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.96843/5.04752. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.96641/5.05800. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.97118/5.00789. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.97921/4.97181. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.97793/4.98609. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.96197/5.04000. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.97044/5.02894. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.02757337634153148\n",
      "Epoch 0, Loss(train/val) 4.92647/4.82362. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.82716/4.81279. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.81543/4.81410. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.81297/4.81396. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.81039/4.81434. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.81034/4.81422. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.80851/4.81370. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.81191/4.81184. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.80906/4.81224. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.81028/4.81623. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.81002/4.80981. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.80460/4.80371. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.80258/4.81133. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.80103/4.81426. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.80333/4.81609. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.80051/4.81159. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.79850/4.82053. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.79928/4.81478. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.80080/4.80836. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.80219/4.80662. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.80217/4.79970. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.79950/4.79821. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.80476/4.79734. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.80072/4.81366. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.80397/4.81283. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.79854/4.79935. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.79899/4.80142. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.79666/4.80159. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.79854/4.80699. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.79729/4.81074. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.79757/4.80304. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.79375/4.81133. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.79340/4.81038. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.79479/4.81311. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.79315/4.81175. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.79483/4.81550. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.79376/4.81107. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.79427/4.81542. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.78863/4.81609. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.79294/4.81468. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.79058/4.81391. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.79065/4.81471. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.79245/4.81703. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.79063/4.81706. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.79060/4.81701. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.78699/4.82117. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78930/4.82347. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.78834/4.82682. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.78619/4.82235. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.78525/4.82675. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.78875/4.83081. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.78944/4.82268. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.78981/4.82707. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.78696/4.82824. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.78560/4.82708. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.78185/4.84301. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.78827/4.82884. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.78573/4.83471. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.78463/4.83370. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.78614/4.83636. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.78799/4.84638. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.78353/4.84024. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.78119/4.84792. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.78308/4.84451. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.78031/4.85032. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.77802/4.85190. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.78217/4.84789. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.78267/4.84091. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77694/4.85236. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.78494/4.84572. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.79115/4.82215. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.78881/4.81669. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.78519/4.82160. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.78278/4.83639. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.78145/4.83952. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.78131/4.83257. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.78358/4.84178. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.78840/4.84926. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.78313/4.84261. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.77949/4.84739. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.78401/4.85501. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.77936/4.84008. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.77682/4.84512. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.78451/4.84384. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.78030/4.83284. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.77344/4.84856. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.77694/4.84532. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.77867/4.83823. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.77935/4.85722. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.78551/4.83439. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.77618/4.85498. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.77198/4.85197. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.77342/4.85140. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77431/4.84152. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77779/4.84874. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.77409/4.82924. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.77337/4.86045. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.77297/4.84499. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.77416/4.84000. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.77295/4.84760. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.1308525335766877\n",
      "Epoch 0, Loss(train/val) 4.85098/4.80473. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.84789/4.79857. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.84717/4.79608. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.84336/4.81115. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.81598/4.79665. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.82154/4.79623. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.82117/4.79830. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.81985/4.79667. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.81980/4.79636. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.82058/4.79691. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.82031/4.79733. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.81879/4.79725. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.81971/4.79873. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.81668/4.79923. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.81879/4.79846. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.81956/4.79892. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.81702/4.80120. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.81689/4.80626. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81683/4.80152. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.81557/4.80274. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.81317/4.80529. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.81387/4.80534. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.81283/4.80464. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.81356/4.80594. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.81202/4.80579. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.81194/4.80749. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.81195/4.80580. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.80908/4.80654. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.80869/4.80501. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.80998/4.80325. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.80981/4.79907. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.80828/4.79619. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.80775/4.80164. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.80173/4.78667. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.81327/4.79501. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.80576/4.79785. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.80445/4.79202. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.80430/4.79091. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.81450/4.80536. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.79943/4.79733. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.80284/4.79673. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.80424/4.80027. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.79970/4.80057. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.79785/4.78320. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.80026/4.79394. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.79949/4.79565. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.80111/4.79229. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.79147/4.80369. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.79406/4.78897. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.79616/4.79544. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.79488/4.80014. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.79058/4.79431. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.79734/4.82945. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.78787/4.81902. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.79183/4.81029. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.78915/4.80563. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.79054/4.81940. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 4.78888/4.81877. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.79076/4.82454. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.78680/4.81026. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.78643/4.82696. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.78622/4.81925. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.79489/4.82158. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.78883/4.81560. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.79302/4.80026. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.77642/4.82234. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.78562/4.82907. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.78191/4.82272. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.78425/4.81132. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.78859/4.81176. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.78214/4.80836. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.78226/4.82504. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.78038/4.82387. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.77594/4.82072. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77687/4.82538. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.78422/4.81451. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.77862/4.82975. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.78000/4.83358. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.77884/4.82201. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.78659/4.80172. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.77959/4.83088. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.76997/4.83592. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.79080/4.83210. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.79499/4.82333. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.79848/4.80186. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.79769/4.81093. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.79256/4.82085. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.78056/4.85391. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.77807/4.81181. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.77964/4.82924. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.77454/4.83866. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.77533/4.83118. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.77779/4.81923. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.77634/4.82451. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77115/4.80950. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.77742/4.80669. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.79646/4.82154. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.79144/4.81178. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.78903/4.82898. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.78409/4.82949. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.018049705127885604\n",
      "Epoch 0, Loss(train/val) 4.73182/4.76708. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.72440/4.77101. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.71596/4.75335. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.71775/4.75052. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.72230/4.73744. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.72170/4.71352. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.71893/4.71039. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.71479/4.72178. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.71324/4.72020. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.71481/4.71629. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.71488/4.71662. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.71196/4.71750. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.71190/4.71552. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.71265/4.71380. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.70876/4.71711. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.71102/4.71005. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.70646/4.71383. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.70930/4.70971. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.70655/4.71202. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.70448/4.70360. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.70741/4.69856. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.70418/4.70602. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.70012/4.70751. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.70173/4.71333. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.69916/4.70745. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.70243/4.70043. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.69943/4.70245. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.69934/4.69879. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.70164/4.70553. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.69922/4.71413. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.69731/4.71515. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.69815/4.71226. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.69633/4.70750. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.69664/4.70460. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.69686/4.70788. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.68815/4.71656. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.69769/4.71766. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.69921/4.71254. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.69170/4.71563. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.69275/4.70468. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.69445/4.70235. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.68996/4.70497. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.68814/4.70871. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 4.69372/4.71119. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.68734/4.70406. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.69117/4.69663. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.69476/4.71433. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.68982/4.70909. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.68049/4.70635. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.69075/4.70780. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.69619/4.71805. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.68868/4.71551. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.69124/4.70682. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.68666/4.70904. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.68805/4.72367. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.68494/4.71818. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.67996/4.70546. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.68037/4.71203. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.68573/4.69732. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.68491/4.68985. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.68392/4.70374. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.68026/4.69262. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.68867/4.71165. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 4.68988/4.71368. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.69388/4.73696. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.69925/4.71632. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.69084/4.70842. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.69096/4.70546. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.68314/4.73249. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.69143/4.72546. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.68194/4.73516. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.69010/4.72468. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.68025/4.72460. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.69018/4.72784. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.68700/4.73153. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.68358/4.72179. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.68670/4.72432. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.68364/4.74045. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.68627/4.72583. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.68510/4.71825. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.67892/4.73124. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.68709/4.73591. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.68323/4.74103. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.68077/4.73177. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.68368/4.71809. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.68226/4.75769. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.68984/4.72412. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.67890/4.70572. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.68259/4.72467. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.68091/4.72760. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.68334/4.73805. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.67934/4.73441. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.68238/4.72961. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.68092/4.74331. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.67430/4.72531. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.67602/4.72858. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.68259/4.73525. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.67396/4.74806. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.68254/4.74208. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.67463/4.73467. Took 0.19 sec\n",
      "ACC: 0.359375, MCC: -0.2771507137113173\n",
      "Epoch 0, Loss(train/val) 4.87803/4.80008. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.79365/4.79605. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.79922/4.78859. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79568/4.78958. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79593/4.79082. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79131/4.79127. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.79288/4.78858. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79260/4.79071. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79346/4.79470. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.79382/4.79377. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79191/4.78969. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78737/4.79039. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.79105/4.78135. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.78951/4.78162. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.79302/4.78096. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.78886/4.78132. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.78538/4.78188. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78530/4.78503. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78685/4.78941. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78779/4.78846. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.78404/4.78878. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78471/4.78574. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78421/4.78462. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78191/4.78723. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78559/4.78766. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78105/4.79025. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.78477/4.78987. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78500/4.80124. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78934/4.79306. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78608/4.79505. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.78616/4.79687. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.78421/4.80071. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.78393/4.79979. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.78609/4.79618. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.78278/4.79828. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.78365/4.79867. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78207/4.79572. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.78444/4.79970. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.78247/4.80026. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.78224/4.80024. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.78043/4.79894. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.78060/4.79827. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.78194/4.79773. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.77977/4.80157. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.78259/4.79307. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.78374/4.79049. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77843/4.78551. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.77646/4.78776. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.78169/4.77425. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.78342/4.77544. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.78336/4.78013. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.78058/4.78189. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.77562/4.78171. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.77917/4.78663. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.77834/4.78092. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.77777/4.79477. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.77909/4.81143. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.78232/4.79698. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.77196/4.80620. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.77926/4.78729. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.77511/4.79258. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.77678/4.79255. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.77393/4.79606. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.77087/4.80517. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.77390/4.79582. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.77280/4.79164. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.77506/4.82434. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.76738/4.80536. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.78081/4.78010. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.77502/4.80443. Took 0.21 sec\n",
      "Epoch 70, Loss(train/val) 4.77023/4.82117. Took 0.21 sec\n",
      "Epoch 71, Loss(train/val) 4.77236/4.79414. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.76572/4.80121. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.76936/4.80659. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.77833/4.79379. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.76772/4.80034. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.76638/4.82264. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77679/4.78466. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.77553/4.76938. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.77362/4.77925. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.77417/4.79530. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.76565/4.82565. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.77369/4.75792. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.76729/4.76817. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.77254/4.75808. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.76687/4.78224. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.76450/4.80564. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.76692/4.79257. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.76814/4.78995. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.76723/4.78373. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.76519/4.78279. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.76565/4.78435. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.76823/4.78748. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.76074/4.79080. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77936/4.82689. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79387/4.79748. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.78692/4.79370. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.78736/4.79537. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.78571/4.79833. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.78340/4.80347. Took 0.19 sec\n",
      "ACC: 0.375, MCC: -0.2393322795582759\n",
      "Epoch 0, Loss(train/val) 4.66301/4.63787. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.61823/4.64139. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.61537/4.64327. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.61303/4.64070. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.61293/4.63720. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.61433/4.63697. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.61021/4.63766. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.60994/4.64057. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.60752/4.64190. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.60829/4.63585. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 4.60850/4.64039. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.60894/4.63723. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.60567/4.64004. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.60273/4.64717. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.60466/4.64274. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.60518/4.64175. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.60151/4.64918. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.60513/4.64595. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.60346/4.63163. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.60541/4.63493. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.60270/4.64533. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.60374/4.64132. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.59783/4.64851. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.59785/4.64831. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.60162/4.64624. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.59287/4.66021. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.59654/4.65144. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.60102/4.64649. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.59701/4.65221. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 4.59617/4.65736. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 4.59695/4.66353. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.59132/4.67869. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.59460/4.66220. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.59758/4.66694. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.59566/4.67196. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.59238/4.67597. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.59638/4.65219. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 4.59879/4.67196. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.59508/4.67471. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.59756/4.66143. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.59361/4.68145. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.59477/4.67410. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.59492/4.67636. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.59175/4.68797. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.59249/4.68034. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.59325/4.68478. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.59212/4.69550. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.59163/4.69134. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.59322/4.67519. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.58735/4.69211. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.58964/4.67618. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.59048/4.67595. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.58839/4.69031. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.58614/4.69887. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.59521/4.66300. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.59071/4.68549. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.59101/4.68057. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.58806/4.68465. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.58958/4.68287. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.58665/4.70273. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.59183/4.67694. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.59372/4.68283. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.58789/4.69589. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.59080/4.68574. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.58809/4.68077. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.58627/4.68820. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.58573/4.68456. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.58953/4.68459. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.58487/4.69783. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.58711/4.69742. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.59167/4.67777. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.58763/4.68637. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.58565/4.68350. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.59794/4.65040. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.59524/4.66798. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.58934/4.67941. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.58974/4.69068. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.58934/4.67973. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.58333/4.70251. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.58488/4.70003. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.58469/4.70626. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.58305/4.68933. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.58266/4.74430. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.61381/4.63003. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.59713/4.64743. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.59560/4.67242. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.59266/4.66469. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.59223/4.65665. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.59366/4.65873. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.59091/4.66653. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.58831/4.68146. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.58937/4.68275. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.59114/4.67537. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.58679/4.69073. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.59018/4.66839. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.58790/4.69537. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.58423/4.69146. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.58827/4.69728. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.58786/4.69054. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.58822/4.68205. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.11383959916814836\n",
      "Epoch 0, Loss(train/val) 4.85612/4.79667. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 4.79028/4.79234. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.78900/4.78878. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.78933/4.78520. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.78788/4.78528. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.78808/4.79608. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.79316/4.78854. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.78770/4.78621. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79296/4.78715. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79342/4.78703. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79197/4.78754. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78790/4.78875. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.78875/4.79117. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.78750/4.79492. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78280/4.80297. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78746/4.79858. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78309/4.79974. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78636/4.79780. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.77945/4.80933. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.78713/4.80414. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.78950/4.80276. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78072/4.81485. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78442/4.81162. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78067/4.82148. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.77985/4.81664. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.77718/4.82657. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.77570/4.81773. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.77603/4.79322. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78009/4.79571. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.77744/4.79778. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.77603/4.80341. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.78197/4.79638. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77585/4.81082. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.77611/4.82024. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.78151/4.80896. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77679/4.80935. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77489/4.81155. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.77272/4.80971. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.77379/4.81198. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 4.77163/4.82890. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77004/4.81996. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.77411/4.82245. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76487/4.84622. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.76604/4.81160. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77815/4.82169. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77686/4.82585. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77348/4.82643. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.76306/4.85809. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.76246/4.83836. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77308/4.83004. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.76490/4.83852. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.76978/4.83027. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.76202/4.83437. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.77731/4.81239. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.77802/4.81442. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.77647/4.81739. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.77018/4.82528. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76975/4.83642. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.76952/4.81604. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.76615/4.82546. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.76855/4.80611. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.76887/4.81879. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.77014/4.83154. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.76527/4.82753. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76606/4.83427. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.76066/4.83199. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.76411/4.82928. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.76217/4.85046. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.75790/4.82918. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.75970/4.84188. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.75902/4.84739. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.76307/4.83809. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.75969/4.83650. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.76286/4.83167. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.75795/4.85192. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.76325/4.83485. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.75631/4.84728. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.75219/4.85257. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.76210/4.82394. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.75940/4.84608. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.75965/4.81823. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.76153/4.83295. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.75562/4.83832. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.75392/4.82549. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.75846/4.82781. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.75669/4.85331. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.76173/4.83209. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.75198/4.84186. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.74832/4.84467. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.75163/4.83776. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.75438/4.84291. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.75332/4.84813. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.75106/4.83830. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.75254/4.84789. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.75479/4.83509. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.74540/4.85622. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.74967/4.84893. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.75409/4.87492. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.75431/4.84191. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75777/4.84143. Took 0.19 sec\n",
      "ACC: 0.390625, MCC: -0.20959097296420087\n",
      "Epoch 0, Loss(train/val) 4.74236/4.70226. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.69670/4.69915. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.70083/4.69869. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.70249/4.69874. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.69893/4.69941. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.70209/4.70031. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.69582/4.69919. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.70139/4.69779. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.69840/4.69547. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.69709/4.69871. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.69633/4.69795. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.69373/4.69605. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.69440/4.69244. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.69871/4.69186. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.69545/4.69349. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.69368/4.69388. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.69216/4.69474. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.69296/4.68981. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.69408/4.69743. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.69359/4.69941. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.69223/4.69845. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.68825/4.69790. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.68997/4.69809. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.69216/4.70025. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.68711/4.70461. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.68907/4.70708. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.68755/4.71066. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.68945/4.71349. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.68424/4.72196. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.68844/4.71097. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.68850/4.71539. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.68671/4.71833. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.68398/4.72159. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.68619/4.71538. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.68655/4.71817. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.68430/4.72393. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.68260/4.72453. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.67812/4.72296. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.68251/4.72270. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.67965/4.72536. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.67887/4.73258. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.68065/4.73872. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.68079/4.72029. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.68536/4.71780. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.68278/4.73173. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.67723/4.73799. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.67826/4.75524. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.67930/4.72979. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.68006/4.75646. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.67798/4.74194. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.67742/4.76189. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.67118/4.76464. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.67532/4.75310. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.67589/4.78007. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.67508/4.77822. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.67125/4.75317. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.67190/4.76571. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.67166/4.76339. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.66769/4.76290. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.67026/4.74885. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.67605/4.74539. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.67584/4.75278. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.66735/4.77746. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.67349/4.75402. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.66902/4.78698. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.66196/4.74856. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.68636/4.72621. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.69190/4.71554. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.68634/4.72838. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.68724/4.72462. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.68124/4.72791. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.67795/4.74650. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.67876/4.70207. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.67447/4.72846. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.67576/4.71017. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.67516/4.74319. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.68437/4.73248. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.67477/4.77774. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.67425/4.76461. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.67387/4.74936. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.66887/4.79082. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.66459/4.81973. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.68027/4.75425. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.68232/4.71127. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.66832/4.77385. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.67557/4.74804. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.67144/4.78896. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.66907/4.75823. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.68141/4.73620. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.67760/4.78031. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.68260/4.72626. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.67557/4.76464. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.66984/4.77770. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.66994/4.76531. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.67025/4.76968. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.67632/4.74483. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.67260/4.77585. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.66771/4.77439. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.67328/4.77876. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.68254/4.74468. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.0009775171065493646\n",
      "Epoch 0, Loss(train/val) 4.98277/4.96684. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.95542/4.94230. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.94734/4.94372. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.94582/4.94618. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.94912/4.94741. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.94655/4.95068. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.94416/4.95437. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.94101/4.95168. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.94451/4.95127. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.94389/4.95362. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.93868/4.95561. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.93959/4.95797. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.93782/4.96379. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.93902/4.96847. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.94158/4.96628. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.93934/4.96632. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.93684/4.97038. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.93632/4.97209. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.93848/4.96731. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.93440/4.97744. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.93970/4.97448. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.93521/4.97470. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.93089/4.97897. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.93212/4.98003. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.93761/4.97073. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.93189/4.97321. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.93475/4.98019. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.93366/4.98316. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.94353/4.95281. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.93824/4.95601. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.93392/4.97070. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.93623/4.98597. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.93320/4.98654. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.93149/4.98869. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.93278/4.98830. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.93089/4.98706. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.93507/4.98348. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.93101/4.99773. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.93376/4.98252. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.93072/4.98551. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.92872/4.99136. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.93184/4.98226. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.93169/4.99111. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.93168/4.99288. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.92907/4.98758. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 4.92833/4.98510. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.92960/4.99281. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.93379/4.97632. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.92943/4.98365. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.93180/4.98313. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.93035/4.99376. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.93205/4.99389. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.92909/4.99560. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.92852/4.98719. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.91968/5.02704. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.93498/4.97159. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.92601/4.99396. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.93043/4.98363. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.92470/4.98922. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.92031/5.00490. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.93280/4.97914. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.92860/4.97432. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.92403/4.99697. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.92669/4.97611. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.92525/4.99578. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.93098/4.97152. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.92406/4.98776. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.92098/4.99532. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.92569/4.98985. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.92335/4.98540. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.92399/5.00317. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.92479/4.99684. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.92105/4.99349. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.92507/4.99250. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.92064/4.99826. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.92327/4.97330. Took 0.22 sec\n",
      "Epoch 76, Loss(train/val) 4.91748/4.99588. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.92645/4.97100. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.92253/4.98428. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.92478/4.97410. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.92468/4.98010. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.91925/4.99474. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.92849/4.98789. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.93637/4.97738. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 4.92498/5.01650. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.92818/4.99201. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.92828/5.00058. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.92745/4.98997. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.92241/4.99434. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.92503/4.98796. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.92518/4.98983. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.92683/5.00675. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.92786/4.98343. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.92560/4.99994. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.92018/4.99601. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.92274/5.00455. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.92341/4.99216. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.91950/4.99077. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.91999/4.99231. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.92221/4.99396. Took 0.20 sec\n",
      "ACC: 0.53125, MCC: 0.0625\n",
      "Epoch 0, Loss(train/val) 4.93315/4.85546. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.87550/4.88163. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.87589/4.89847. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87777/4.90744. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.87854/4.91532. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.87838/4.92363. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.88062/4.90455. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.87729/4.88723. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.87421/4.88545. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87370/4.88314. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.87291/4.88430. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.86986/4.88849. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.87212/4.89176. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.87360/4.89497. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87032/4.89173. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.87265/4.88851. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.87200/4.89218. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.87021/4.88992. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.86884/4.89176. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.86765/4.89822. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.87275/4.89778. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.86741/4.89899. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.87062/4.89510. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.86676/4.89986. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.86825/4.90328. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.86871/4.90561. Took 0.22 sec\n",
      "Epoch 26, Loss(train/val) 4.86751/4.89462. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.86551/4.89448. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.86615/4.89730. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.86530/4.89899. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.86465/4.90280. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.86177/4.91747. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.86549/4.89606. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.85895/4.91790. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 4.86266/4.89579. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.86485/4.90023. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.87050/4.87661. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.86893/4.88168. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.86498/4.88383. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.86587/4.87909. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.86522/4.87592. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.86367/4.89768. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.86501/4.88066. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.86485/4.88591. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.86755/4.87007. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.87235/4.85253. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.87367/4.86581. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.87346/4.84684. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.87274/4.84469. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.86817/4.84646. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.86855/4.84381. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.87058/4.85817. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.86963/4.86812. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.86778/4.86538. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.86535/4.86655. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.86909/4.87180. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.86490/4.87240. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.86657/4.87453. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.86240/4.87819. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.86342/4.87996. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85995/4.87238. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.86151/4.88237. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.86140/4.88895. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.86289/4.88884. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.85887/4.89586. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.86216/4.89227. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.85770/4.89489. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.85896/4.89480. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.85532/4.89968. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.86260/4.89390. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.85784/4.89713. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.85608/4.89622. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.85993/4.89340. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.86220/4.89131. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.85385/4.89151. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.85240/4.90894. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.85496/4.89721. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.85363/4.91800. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.85472/4.89511. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.85814/4.89748. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.85417/4.91258. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.84689/4.92751. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.85386/4.89059. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.86685/4.88069. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.86492/4.88817. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.86169/4.89619. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.85663/4.90489. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.86248/4.90132. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.85813/4.89308. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.85613/4.90799. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.86433/4.90750. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85525/4.90771. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.85614/4.91229. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.85727/4.91605. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.85674/4.91399. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.85570/4.90845. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.85076/4.91932. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.85574/4.91952. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.85077/4.92412. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.85439/4.93353. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.07570682517832987\n",
      "Epoch 0, Loss(train/val) 4.83091/4.86044. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.81249/4.84630. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.81256/4.86283. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.81294/4.86889. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.81258/4.86156. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.82034/4.82272. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.82177/4.80538. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.81394/4.80899. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.80888/4.81291. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.80827/4.81157. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.80858/4.81206. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.80952/4.81233. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.80670/4.81445. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.80557/4.81948. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.80760/4.81764. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.80529/4.81830. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.80056/4.83949. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.79804/4.84015. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.80779/4.80491. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.80748/4.80673. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.80201/4.81373. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.80084/4.81919. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.80159/4.82329. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.79991/4.81663. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.80018/4.81710. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.80036/4.81431. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.79926/4.81132. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.79956/4.81127. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.80336/4.81313. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.79862/4.81346. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.79753/4.81705. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.79275/4.82110. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.79752/4.81234. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.79746/4.80680. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.80069/4.80620. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.79990/4.81445. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.79644/4.80106. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.79609/4.81297. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.79798/4.80649. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.79446/4.80973. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.79499/4.81847. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.79741/4.80302. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.79322/4.82389. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.79708/4.81686. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.79396/4.81315. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.79305/4.81757. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78885/4.81952. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.79972/4.80585. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.79347/4.84042. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.79675/4.83090. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.79579/4.82864. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.79146/4.82666. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.79021/4.84458. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.79669/4.83665. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.79429/4.86060. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.79291/4.83056. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.79000/4.84190. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.79412/4.82119. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.79616/4.83434. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.79124/4.80357. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.79358/4.81508. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.80680/4.79591. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.80631/4.80215. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.80303/4.80310. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.80026/4.82112. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.79717/4.83690. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.79677/4.82411. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.79056/4.82675. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.79338/4.82350. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.79352/4.84432. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.80256/4.81115. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.79419/4.82366. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.78835/4.81110. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.80225/4.79454. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.79921/4.82299. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.79575/4.81758. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.79376/4.83993. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.79706/4.82683. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.79019/4.85905. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.79041/4.81929. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.79075/4.83533. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.79155/4.82242. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.78997/4.85246. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.78943/4.80864. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.78593/4.85292. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.78336/4.85398. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.79398/4.80871. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.79191/4.85427. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.78288/4.80781. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.79122/4.82414. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.79491/4.78075. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.78883/4.81830. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.78084/4.85535. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.79351/4.80117. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.79942/4.79576. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79081/4.82219. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.78908/4.82793. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.78981/4.82572. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.78549/4.79664. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.78544/4.79805. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.17004550636718183\n",
      "Epoch 0, Loss(train/val) 4.75099/4.72807. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.72297/4.73680. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.72099/4.73784. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.71779/4.73813. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.71663/4.73859. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.71873/4.74234. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.71705/4.74026. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.71872/4.73499. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.71518/4.73033. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.71607/4.73077. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.71409/4.72721. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.71571/4.72608. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.71380/4.72886. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.71432/4.72893. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.71503/4.72964. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.71407/4.72677. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.71559/4.72634. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.71374/4.72542. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.71424/4.72466. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.71593/4.72654. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.71382/4.72901. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.71175/4.72362. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.71352/4.72576. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.71025/4.72720. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.70919/4.72805. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.71117/4.73521. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.71023/4.73099. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.71235/4.72019. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.70961/4.72423. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.70714/4.73029. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.70487/4.71720. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.70828/4.72853. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.70854/4.72647. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.70700/4.72679. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 4.70485/4.71484. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 4.70664/4.72471. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.70761/4.71573. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.70931/4.72010. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.70814/4.72514. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.70504/4.72039. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.70625/4.72148. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.70325/4.72313. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.70311/4.73097. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.70586/4.72063. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.70223/4.71869. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.69906/4.72627. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.70568/4.71519. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.70371/4.70988. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.70488/4.72037. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.69946/4.71720. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.69819/4.73799. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.70404/4.71939. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.70145/4.71921. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.70520/4.71304. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.70040/4.72113. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.69882/4.71578. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.70138/4.71726. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.69607/4.72279. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.69993/4.72583. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.69904/4.71993. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.69857/4.73122. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.69741/4.72312. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.69490/4.74887. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.70140/4.72828. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.69434/4.73468. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 4.69312/4.73240. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.69262/4.73791. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.69618/4.71793. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.69655/4.73209. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.69449/4.73100. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.69457/4.72514. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.69181/4.74069. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.69309/4.71068. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.68991/4.72516. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.69023/4.72889. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.69679/4.70367. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.69486/4.69804. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.69137/4.71089. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.68993/4.71763. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.68848/4.70325. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.69145/4.73035. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.69541/4.70855. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.68848/4.71277. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.69190/4.72535. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.69276/4.73719. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.68939/4.70104. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.69201/4.72186. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.68950/4.73458. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.69170/4.72341. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.69062/4.72483. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.69097/4.70292. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.69302/4.70013. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.68577/4.72510. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.68571/4.70611. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.68588/4.74735. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.68908/4.72394. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.69268/4.70720. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.69330/4.71675. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.69326/4.71506. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.68270/4.72389. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.040082172520323485\n",
      "Epoch 0, Loss(train/val) 5.08562/5.00657. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.03982/5.01773. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.03139/5.02175. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.03041/5.02094. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.03170/5.02003. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.03041/5.02089. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 5.03194/5.02236. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.02621/5.02131. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.02830/5.02204. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.02615/5.01849. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 5.02758/5.02306. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 5.02945/5.01919. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.02445/5.01563. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.02610/5.01399. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 5.02411/5.01587. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.02418/5.01209. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 5.02383/5.01175. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.02412/5.00811. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.02075/5.00638. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.02019/5.00699. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.01778/5.00878. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.01469/5.01530. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.01614/5.00639. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.01859/5.00581. Took 0.22 sec\n",
      "Epoch 24, Loss(train/val) 5.01302/5.00515. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.01661/5.01234. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.00895/5.01075. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.01170/5.01997. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.00888/5.01377. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.01281/5.01511. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.01606/5.01422. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.01276/5.01566. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.00658/5.01771. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.01097/5.01663. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.00660/5.01505. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.00704/5.02878. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.99404/5.02530. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.00986/5.02032. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 5.00454/5.02337. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.00585/5.01065. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.99546/5.02192. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.00522/5.02406. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.99206/5.04158. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.99556/5.02047. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.00596/5.01703. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.99877/5.03325. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.99498/5.03337. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.99413/5.03716. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.99822/5.03476. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.99669/5.04953. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 5.00218/5.02907. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.99596/5.01940. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.99063/5.02978. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.99982/5.02128. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.99627/5.03434. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.99273/5.03706. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.99224/5.03451. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.99245/5.02465. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.99662/5.02026. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.99345/5.02174. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.99363/5.03317. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.99362/5.01959. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.99742/5.03034. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.99595/5.03716. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.99304/5.03606. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.98663/5.03088. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.98818/5.02999. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.99064/5.02905. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.99194/5.04136. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.98663/5.03442. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.99060/5.02944. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.98011/5.03920. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.99233/5.03422. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.98977/5.03271. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.97843/5.04341. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.99458/5.03165. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.98613/5.03687. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.99058/5.02783. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.99167/5.02979. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.98763/5.04214. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.98791/5.02210. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.97788/5.05137. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.98158/5.03015. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.99235/5.03604. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.97890/5.04468. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.98393/5.03432. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.98463/5.02449. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.98741/5.04584. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.98897/5.03027. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.98086/5.03506. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.98310/5.04337. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.98170/5.04663. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.98164/5.02851. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.98750/5.05383. Took 0.21 sec\n",
      "Epoch 94, Loss(train/val) 4.98758/5.03922. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.98594/5.04891. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.97865/5.05437. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.00108/5.02782. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.98470/5.05224. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.98292/5.04637. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.018049705127885604\n",
      "Epoch 0, Loss(train/val) 4.85889/4.80788. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.81705/4.80594. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.81380/4.80973. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.81325/4.80932. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.81257/4.80690. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.81231/4.81005. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.81235/4.81033. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.81159/4.80901. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.81024/4.81068. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.80874/4.81480. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.80857/4.81367. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.80835/4.81640. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.80989/4.81629. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.80507/4.81934. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.80822/4.82021. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.80645/4.82073. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.80080/4.82469. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.80376/4.82713. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.80131/4.82446. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.80313/4.82604. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.79986/4.82696. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.80218/4.81541. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.80100/4.81836. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.80079/4.82192. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.79542/4.83168. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.80107/4.81635. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.79551/4.82881. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.79944/4.82063. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.79786/4.82807. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.79489/4.82435. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.79511/4.82822. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.78820/4.84547. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.79431/4.82802. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.79370/4.81201. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.79176/4.85396. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.79866/4.80847. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78773/4.82343. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.79085/4.82549. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.79199/4.82967. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.79199/4.82540. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.79087/4.84558. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.78952/4.85436. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.79131/4.81472. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.79027/4.85512. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.78864/4.83312. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.78502/4.83795. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78622/4.83235. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.78978/4.83283. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.78827/4.82649. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.78538/4.84803. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.78836/4.82473. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.78191/4.83487. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.78792/4.83064. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.78631/4.83509. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.78965/4.84677. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.78652/4.83434. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.78527/4.84029. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.78122/4.83183. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.78368/4.85047. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.77865/4.85149. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.78313/4.83025. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.78421/4.85128. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.78615/4.82951. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.77701/4.85571. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.78721/4.82834. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.78103/4.85476. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.78253/4.82686. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.78692/4.83570. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77976/4.85054. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77663/4.85157. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.77740/4.84968. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.77721/4.83290. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.78138/4.83858. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.77983/4.85239. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.78318/4.82262. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.78027/4.83336. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.78405/4.83755. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.78352/4.84898. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.78083/4.82732. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.78517/4.83575. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.77960/4.85537. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.77697/4.83525. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.78201/4.83677. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.78451/4.83975. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.78466/4.83508. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.78167/4.83162. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.78208/4.84493. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.77919/4.83936. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.78458/4.83862. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.77476/4.85991. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.78038/4.83650. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.78363/4.82423. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.77564/4.82466. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77625/4.83923. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.78052/4.83985. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.77975/4.83649. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.77772/4.84915. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.78368/4.83371. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.78317/4.82779. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.77879/4.82358. Took 0.20 sec\n",
      "ACC: 0.46875, MCC: -0.054187192118226604\n",
      "Epoch 0, Loss(train/val) 5.01803/5.00451. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.98288/4.96553. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.97937/4.97169. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.97955/4.97220. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.98122/4.97520. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.97829/4.97057. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.97809/4.97011. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.97246/4.97119. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.97350/4.97104. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.97960/4.97240. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.97788/4.97249. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.97828/4.97083. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.97832/4.96997. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.97334/4.97289. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.97569/4.97434. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.97265/4.97573. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.96823/4.97563. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.96775/4.97377. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.97137/4.97585. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.97420/4.98175. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.97565/4.97794. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.97212/4.98094. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.97019/4.98473. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.97218/4.97787. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.97090/4.97536. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.96102/4.97517. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.96806/4.97420. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.96621/4.98033. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.96902/4.97944. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.96442/4.98117. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.96137/4.97658. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.96289/4.97816. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.96254/4.98008. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.96083/4.98532. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.96143/4.97966. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.96619/4.97615. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.96531/4.97898. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.96108/4.97709. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.95916/4.98201. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.95761/4.98858. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.95845/4.98533. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.96067/4.98222. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.96469/4.98178. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.95782/4.98254. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.95612/4.99192. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.95794/4.98604. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.95901/4.98296. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.95812/4.97107. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.95867/4.97873. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.95162/4.99729. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.95640/4.99042. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.95543/4.99866. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.95513/4.99041. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.95633/4.99688. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.95536/4.99646. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.95347/4.98650. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.95390/4.99578. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.95621/4.99255. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.95353/4.99606. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.96810/4.98656. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.96378/4.99121. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.94918/5.01788. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.95622/4.99718. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.95750/5.00778. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.95361/5.00056. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.95425/4.99451. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.95580/4.98744. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.94360/5.00531. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.95002/5.00083. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.94881/4.98885. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.95345/4.99118. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.94793/4.99174. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.95731/5.00005. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.95252/4.99027. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.94557/5.00275. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.95213/4.98866. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.94542/5.01506. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.94905/4.99464. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.95252/5.00321. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.94437/5.00594. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.94853/5.00586. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.95120/5.00733. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.94873/5.01138. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.95039/4.99610. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.94475/5.00385. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.94793/4.99437. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.94884/5.00685. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.94632/5.00072. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.95456/4.99061. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.93904/4.99168. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.94502/5.00421. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.94884/5.00750. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.94489/5.01207. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.94146/5.00875. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.94597/5.00456. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.94716/5.01344. Took 0.21 sec\n",
      "Epoch 96, Loss(train/val) 4.93874/5.01919. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.94276/5.03669. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.94315/5.01996. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.94721/5.00218. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.16314545558893145\n",
      "Epoch 0, Loss(train/val) 4.62295/4.56865. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.56336/4.56649. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.56285/4.56420. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.56520/4.56551. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.56519/4.56620. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.56383/4.56525. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 4.56598/4.56597. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.56346/4.56444. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.56172/4.56124. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.56311/4.56191. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.55975/4.56190. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.55724/4.55780. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.56029/4.56632. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.55737/4.55177. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.56575/4.56952. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.56505/4.56864. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.56085/4.56819. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.56017/4.57099. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.55691/4.57610. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.55765/4.57672. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.55722/4.57748. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.56212/4.55862. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.56666/4.56359. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.56087/4.56411. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.55880/4.56464. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.55992/4.56389. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.56081/4.56473. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.56138/4.56766. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.56227/4.57098. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.56244/4.56718. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.55614/4.56683. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.55340/4.57062. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.55030/4.57108. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.55371/4.58444. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.55053/4.55670. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.55535/4.57266. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.55154/4.57030. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.55638/4.55985. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.56504/4.56395. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.55753/4.56110. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.55764/4.56067. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 4.55888/4.56468. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.55874/4.56393. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.55585/4.56580. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.55452/4.56722. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.55387/4.57237. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.55635/4.56988. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.56136/4.56656. Took 0.22 sec\n",
      "Epoch 48, Loss(train/val) 4.56044/4.56154. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.55610/4.56276. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.55123/4.56482. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.55661/4.56859. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.55379/4.56076. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.55086/4.57204. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.54748/4.56594. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.55150/4.57030. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.54547/4.56941. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.54671/4.57726. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.54257/4.57013. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.55039/4.57445. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.54538/4.57050. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.54682/4.57240. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.55204/4.57045. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.54621/4.57407. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.54677/4.57730. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.54780/4.57214. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.54310/4.58548. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.54134/4.58964. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.54076/4.59130. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.54483/4.57418. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.54150/4.57292. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.54276/4.58450. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.54369/4.58309. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.54393/4.59264. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.54078/4.59553. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.54166/4.58120. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.53915/4.58479. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.54392/4.58188. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.54364/4.59099. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.53983/4.58885. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.53543/4.60314. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.53850/4.60314. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.54846/4.59243. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.54737/4.60440. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.53945/4.58838. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.53653/4.58542. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.54000/4.58438. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.53237/4.60783. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.54064/4.60352. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.53856/4.60336. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.52983/4.61673. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.53576/4.60298. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.53962/4.59843. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.53639/4.60804. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.53361/4.62446. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.53528/4.60643. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.53416/4.60294. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.53627/4.60091. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.53129/4.61563. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.53087/4.61644. Took 0.19 sec\n",
      "ACC: 0.609375, MCC: 0.2556657194498359\n",
      "Epoch 0, Loss(train/val) 4.91362/4.88869. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.91619/4.90288. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.90844/4.88869. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.90826/4.89023. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.89403/4.88613. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.89137/4.88341. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.89685/4.88224. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.89293/4.87910. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.89264/4.88037. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.89289/4.88454. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.89353/4.88421. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.89141/4.87907. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.88814/4.87676. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.88931/4.87820. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 4.88998/4.87784. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.89167/4.87793. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.89100/4.88076. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.88646/4.88045. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.88712/4.87983. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.88485/4.88426. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.88658/4.87736. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.88549/4.88104. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.88694/4.88413. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.88577/4.88094. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.88562/4.88206. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.88424/4.89234. Took 0.21 sec\n",
      "Epoch 26, Loss(train/val) 4.88535/4.88198. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.88272/4.88890. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 4.88517/4.89003. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.88198/4.89222. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.88334/4.89400. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.88570/4.89054. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.88294/4.89110. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.88506/4.88680. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.89102/4.88607. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 4.88820/4.88564. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.89016/4.88127. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.88437/4.87916. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.88648/4.87555. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.88580/4.87645. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.88331/4.88344. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.88469/4.88498. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.88396/4.88955. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.88184/4.88051. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.88155/4.88553. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.88226/4.88506. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.88172/4.88850. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.88350/4.88558. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.88173/4.88995. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.87819/4.88114. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.88070/4.88298. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.88110/4.88820. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.87632/4.88383. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.88075/4.88627. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.87648/4.88861. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.87507/4.90854. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.88147/4.88089. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.87902/4.88476. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.88017/4.88541. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.87591/4.89322. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.87594/4.89510. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.87626/4.88763. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.87537/4.89074. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.87555/4.89487. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.87500/4.89929. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.87453/4.90173. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.87147/4.88784. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.87308/4.89297. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.88143/4.89265. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.87582/4.89576. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.87257/4.90105. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.87376/4.88889. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.87481/4.90171. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.87392/4.89434. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.86883/4.92479. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.87040/4.89479. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.86708/4.90156. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.86964/4.90162. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.86638/4.95091. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.87593/4.88757. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.87253/4.89023. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.87200/4.90287. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.86866/4.89152. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.86834/4.89321. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.87054/4.92895. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.87232/4.88660. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.86533/4.89791. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.87125/4.90943. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.86801/4.89879. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.86666/4.90837. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.86817/4.89528. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.86714/4.92324. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.86772/4.89000. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.86555/4.90932. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.86722/4.89800. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.86433/4.89465. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.86964/4.95229. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.86108/4.89179. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.86297/4.92580. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.86612/4.90274. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.0518191557963571\n",
      "Epoch 0, Loss(train/val) 4.93555/4.89680. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.90010/4.90057. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.90032/4.90741. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.90379/4.90503. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.90489/4.90516. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.90695/4.91473. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.90238/4.92059. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.89602/4.91804. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.89615/4.91531. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.89568/4.91573. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.89672/4.92035. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.89447/4.91819. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.89513/4.92222. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.89328/4.92015. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.89288/4.92168. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.88964/4.92770. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.89000/4.91485. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.89337/4.91693. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.89662/4.91670. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.89297/4.91362. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.89269/4.91546. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.89034/4.91047. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.89119/4.91442. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.88877/4.91829. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.88902/4.92001. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.88753/4.91879. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.89072/4.92495. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.88600/4.91518. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.88672/4.92571. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.88615/4.91933. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.88533/4.92216. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.88596/4.94196. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.88728/4.92304. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.88725/4.92531. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.88260/4.92358. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.88281/4.94045. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.88100/4.92034. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.88619/4.92723. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.88219/4.93366. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.88211/4.92851. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.88084/4.93656. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.88613/4.93992. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.87944/4.94204. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.88403/4.94369. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.88106/4.94786. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.88082/4.95426. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.87570/4.96903. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.88302/4.96216. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.87853/4.94832. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.88020/4.95531. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.87693/4.97968. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.87524/4.98756. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.87626/4.95134. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.87564/4.97861. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.88434/4.93408. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.87867/4.95404. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.87429/4.96788. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.88128/4.95616. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.88064/4.95060. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.87501/4.97049. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.87743/4.96768. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.87454/4.96161. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.87608/4.97221. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.87574/4.95099. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.87372/4.96334. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.87114/4.97885. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.87584/4.97835. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.87905/4.95510. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.87400/4.96448. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.87913/4.96534. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.87499/4.97923. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.87457/4.97076. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.88049/4.95791. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.87092/4.97237. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.87029/4.98138. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.87316/4.97558. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.86865/4.98415. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.87448/4.99648. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.87651/4.95382. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.87407/4.96339. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.87476/4.97339. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.87467/4.97438. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.86962/4.97992. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.86842/5.00301. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.87371/4.98381. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.86611/4.97673. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.86908/4.99037. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.86963/4.97195. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.87800/4.97398. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.86817/4.97888. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.86425/5.00048. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.87061/4.98905. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.86641/4.99496. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.87130/4.98358. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.87097/4.98500. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.86619/5.01163. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.86284/5.02072. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.87216/4.98633. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.86945/5.00081. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.87031/5.00555. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.09379580992210836\n",
      "Epoch 0, Loss(train/val) 5.02218/4.89173. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.89762/4.90076. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.89100/4.90952. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.88914/4.91673. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.89365/4.92216. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.88934/4.92435. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.89022/4.93103. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.88732/4.92778. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.89030/4.92353. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.89009/4.92568. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.88721/4.93647. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.88685/4.94202. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.88484/4.94703. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.88339/4.94699. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.88397/4.94304. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.88333/4.94603. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.87874/4.95190. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.88050/4.94752. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.87883/4.95013. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.87729/4.94898. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.87811/4.94738. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.87625/4.95805. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.87635/4.95850. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.87496/4.95960. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 4.87670/4.95846. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.87661/4.95492. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.87184/4.96201. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.87188/4.95393. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.87235/4.96436. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.87167/4.95913. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.86784/4.97139. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.87355/4.94710. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.87253/4.95955. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.87070/4.97217. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.88307/4.92442. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.88282/4.92488. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.87859/4.94534. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.87523/4.95325. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.87662/4.96589. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.87057/4.98561. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.86750/4.98855. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.86761/4.97202. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.87045/4.98414. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.86744/4.97626. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.86823/4.97006. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.86630/4.97975. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.86844/4.96862. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.86648/4.98792. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.86758/4.96346. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.86681/4.99133. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.86491/4.96908. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.86462/4.98611. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.86805/4.96916. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.86616/4.97754. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.86576/4.98085. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.86064/4.97895. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.86649/4.97798. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.86358/4.97229. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.86425/4.97301. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.86743/4.95503. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85874/5.00917. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.86514/4.94842. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.87937/4.90822. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.87869/4.93407. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.86837/4.93980. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.86528/4.97401. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.86864/4.98022. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.86628/4.97296. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.86168/4.99178. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.87202/4.97109. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.86798/4.94560. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.86054/4.97963. Took 0.21 sec\n",
      "Epoch 72, Loss(train/val) 4.85980/4.96565. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.85898/4.98628. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.86244/4.97692. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.86292/4.97778. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.86291/4.97720. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.86310/4.96879. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.85578/4.98111. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.85714/4.97621. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.86029/4.98583. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.85856/4.98411. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.85788/4.97102. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.86627/4.96933. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.85773/4.98366. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.86201/4.95012. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.85651/4.98949. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.85767/4.98502. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.86054/4.97579. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.85770/4.98060. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.85754/4.99796. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85556/4.98115. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.85811/4.97568. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.85818/4.99428. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.85643/4.98833. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.85851/4.95855. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.85261/4.99786. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.85866/4.98252. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.85681/4.97373. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.85449/5.00167. Took 0.20 sec\n",
      "ACC: 0.671875, MCC: 0.34527065131588947\n",
      "Epoch 0, Loss(train/val) 4.72834/4.72684. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.68233/4.71056. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.67679/4.70134. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.67606/4.70146. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.67252/4.69821. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.67194/4.69856. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.67127/4.70186. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.67033/4.70580. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.67123/4.70575. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.66967/4.70970. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.66772/4.71350. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.66570/4.72087. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.66764/4.70957. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.67059/4.70730. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.66814/4.69279. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.66480/4.69500. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.66146/4.69930. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.66298/4.70684. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.66033/4.69713. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.66667/4.69557. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.66165/4.69698. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.65997/4.69975. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.65984/4.70515. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.65878/4.71337. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.65556/4.70180. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.66085/4.69984. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.66189/4.70623. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.65848/4.70302. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.65960/4.70330. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.65978/4.70599. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.65517/4.70006. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.65501/4.70866. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.65322/4.71836. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.65783/4.69620. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.65355/4.71098. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.65411/4.70354. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.65511/4.70950. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.65325/4.71175. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.65176/4.71828. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.65243/4.71464. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.65167/4.72084. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.65416/4.71764. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.65389/4.72626. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.65271/4.71853. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.65443/4.70029. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.65731/4.70056. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.65208/4.71274. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.65274/4.70946. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.65046/4.72443. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.65466/4.71693. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.65185/4.72340. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.65233/4.70705. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.64881/4.71714. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.65078/4.72791. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.65111/4.71584. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.65452/4.70753. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.65114/4.71467. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.64588/4.74120. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.65137/4.72445. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.65247/4.71736. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.65903/4.70432. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.65921/4.69693. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.65252/4.69807. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.65276/4.70085. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.64811/4.70538. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.65332/4.71892. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.65044/4.71105. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.64867/4.70784. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.65347/4.70139. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.64973/4.70814. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.64865/4.71252. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.64873/4.70703. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.64770/4.69684. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.64898/4.71052. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.65377/4.70696. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.65184/4.70937. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.64993/4.69962. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.65156/4.71199. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.64877/4.72403. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.64844/4.71001. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.64605/4.71733. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.65129/4.70810. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.64851/4.71365. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.64281/4.71481. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.64812/4.71463. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.64487/4.71710. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.64646/4.71868. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.64472/4.71733. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.64699/4.71482. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.64507/4.72234. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.64355/4.72150. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.64207/4.71981. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.64867/4.71944. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.64512/4.71118. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.63801/4.73102. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.64395/4.72551. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.64002/4.71967. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.64071/4.72010. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.64258/4.71602. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.64826/4.70766. Took 0.19 sec\n",
      "ACC: 0.390625, MCC: -0.22877657129023765\n",
      "Epoch 0, Loss(train/val) 4.89623/4.79987. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.85037/4.81332. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.83950/4.85441. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.82293/4.84674. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.81921/4.83574. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.82244/4.83519. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.82161/4.83964. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.82119/4.84307. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.82176/4.83981. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.81998/4.83884. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.81755/4.83729. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.81578/4.84370. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.82215/4.84193. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.81776/4.84451. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.81571/4.83987. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.81674/4.83454. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.81872/4.83452. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.81656/4.84571. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81621/4.84165. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.81241/4.84405. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.81259/4.84351. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.81221/4.86355. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.81378/4.83846. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.81077/4.84611. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.81114/4.85507. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.81226/4.85052. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.80929/4.86485. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.81211/4.84772. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.81139/4.85280. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.80957/4.85962. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.80679/4.85447. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.81082/4.85218. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.80994/4.85266. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.80718/4.86071. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.80909/4.85330. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.80650/4.84631. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.80950/4.84366. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.80757/4.85398. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.80884/4.85334. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.80585/4.85621. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.80177/4.85117. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.80589/4.85600. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.80304/4.85966. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.80716/4.84226. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.80692/4.85057. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.80460/4.84672. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.80404/4.85979. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.80628/4.84385. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.80264/4.84125. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.80217/4.85334. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.80555/4.83952. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.79991/4.86064. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.80632/4.84642. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.79637/4.83921. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.80123/4.84347. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.79962/4.85578. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.80348/4.84519. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.79882/4.84188. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.80093/4.85681. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.80015/4.82906. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.80002/4.85862. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.80571/4.83684. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.79743/4.84891. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.80001/4.84289. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.80261/4.84784. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.79755/4.84926. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.79752/4.84244. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.80246/4.84318. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.79638/4.84135. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.80395/4.84198. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.79802/4.84109. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.79434/4.84021. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.80162/4.84093. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.79856/4.84428. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.79464/4.83299. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.79474/4.84663. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.79125/4.83608. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.79372/4.82985. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.79418/4.83089. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.79623/4.83967. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.79122/4.83961. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.80313/4.82485. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.79462/4.84001. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.79038/4.83439. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.79443/4.84526. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.79602/4.85550. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.79156/4.84375. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.79125/4.84102. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.79650/4.82921. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.79190/4.83230. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.79191/4.84197. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.79780/4.82785. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.79165/4.84693. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.79066/4.83042. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.79465/4.83595. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79182/4.83518. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.78814/4.84850. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.78898/4.83915. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.79536/4.83752. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.79347/4.83642. Took 0.19 sec\n",
      "ACC: 0.5, MCC: -0.015873015873015872\n",
      "Epoch 0, Loss(train/val) 4.93290/4.90887. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.90066/4.89746. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.89973/4.91451. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.89998/4.91474. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.89821/4.89618. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.88632/4.89731. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.88620/4.90095. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.88645/4.90122. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.88686/4.90066. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.88700/4.90550. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.88560/4.90383. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.88293/4.90419. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.88409/4.89770. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.88125/4.90235. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.88284/4.89711. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.88370/4.89652. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.88128/4.89522. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.88006/4.89811. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.87979/4.89729. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.88097/4.89626. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.87550/4.89980. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.88505/4.90198. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.88069/4.89072. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.87699/4.89425. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.87888/4.89104. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.87690/4.89779. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.87874/4.89299. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.87700/4.88910. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.87127/4.90900. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.88374/4.89356. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.87860/4.89425. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.87614/4.88372. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.87516/4.88177. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.87509/4.88031. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.87503/4.88349. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.86770/4.88072. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.87585/4.87372. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.87237/4.87660. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.86959/4.87995. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.87506/4.87239. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.87126/4.87141. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.87017/4.88414. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.86860/4.87647. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.86592/4.87931. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.87341/4.87638. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.86616/4.88592. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.87082/4.87081. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.86838/4.87635. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.86830/4.86759. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.86740/4.87899. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.86197/4.88640. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.86567/4.89015. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.86735/4.88968. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.86446/4.89545. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.86230/4.88463. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.87608/4.89853. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.86740/4.90404. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.87055/4.90026. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.86813/4.89906. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.86312/4.89576. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.86864/4.88530. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.86102/4.90235. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.86324/4.90366. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.86629/4.91193. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.86470/4.89009. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.87310/4.87768. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.86002/4.88619. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.86294/4.89616. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.86586/4.88688. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.85882/4.89883. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.86168/4.88935. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.85777/4.90101. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.86674/4.90388. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.86946/4.85747. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.86744/4.87691. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.86593/4.86583. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.85927/4.88732. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.86443/4.88317. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.86476/4.86876. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.86566/4.88718. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.87133/4.87579. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.86223/4.90285. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.86119/4.90017. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.86139/4.89341. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.86294/4.89116. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.86156/4.89143. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.85765/4.90475. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.85949/4.90722. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.85951/4.89856. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.85959/4.90685. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.86161/4.89326. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85827/4.89797. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.85976/4.90436. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.85507/4.90508. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.85984/4.89543. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.85406/4.91086. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.86383/4.88762. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.85376/4.89747. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.85515/4.88231. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.85495/4.90003. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.02834008097165992\n",
      "Epoch 0, Loss(train/val) 4.83308/4.90853. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.80383/4.74872. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.80612/4.73335. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79943/4.76669. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.78980/4.75407. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79325/4.75365. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.79248/4.75603. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.79051/4.75536. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.79188/4.75480. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79375/4.75313. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79044/4.76368. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78824/4.76270. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.79009/4.75861. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79069/4.76127. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78986/4.76496. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.78740/4.76272. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78717/4.75925. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78635/4.76004. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78869/4.75623. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78620/4.76242. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.78457/4.76125. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.77978/4.74640. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78286/4.75886. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78160/4.75551. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.77875/4.75765. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.77844/4.74836. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78111/4.77623. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78452/4.76243. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.77644/4.75407. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.77291/4.75569. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.77773/4.75763. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77349/4.76487. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77340/4.75294. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.77402/4.75270. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.76993/4.75442. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.77328/4.76153. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77639/4.75119. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.76951/4.74993. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77094/4.76115. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.76596/4.75870. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 4.76849/4.75433. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.76991/4.76637. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76571/4.75878. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.77036/4.74760. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.76723/4.76869. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.76824/4.76002. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.76571/4.76794. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.76629/4.76142. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.76251/4.76060. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.75953/4.76842. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.75991/4.76213. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.76692/4.77942. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.76088/4.76974. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76590/4.76907. Took 0.18 sec\n",
      "Epoch 54, Loss(train/val) 4.77784/4.79067. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.77114/4.76534. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.76974/4.75310. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.76488/4.77334. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.76171/4.77590. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.75279/4.76899. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.76260/4.77692. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.75727/4.76264. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.75304/4.78919. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 4.75420/4.75979. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.75741/4.77922. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.75308/4.77485. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75526/4.76589. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.75815/4.77666. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.75625/4.77182. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.75223/4.78757. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.75525/4.76904. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.74966/4.76571. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.75956/4.76398. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.76122/4.75205. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.75550/4.75803. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.75620/4.77393. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.75259/4.77066. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.75432/4.77584. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.74929/4.78556. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.75588/4.77144. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.75339/4.77217. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.75377/4.76559. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.75105/4.77323. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.75482/4.77804. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.74958/4.76997. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.75673/4.77820. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.74868/4.77982. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.75157/4.77830. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.75002/4.78089. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.74586/4.77864. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.74240/4.78473. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.74700/4.78346. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.75209/4.77456. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.74547/4.77000. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.75142/4.79192. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75166/4.77265. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.75109/4.79083. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.74844/4.79372. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.74489/4.77124. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.74551/4.78933. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.17189402517942118\n",
      "Epoch 0, Loss(train/val) 4.94643/4.86299. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.84687/4.84905. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.84804/4.85796. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.85288/4.86183. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.85062/4.86043. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.85020/4.86249. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.84656/4.86340. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 4.84497/4.86506. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.84482/4.86924. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.84557/4.86760. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84444/4.86895. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.84234/4.87005. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.83858/4.87290. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.84143/4.87695. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.83833/4.87430. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.83694/4.87849. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.83733/4.87495. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.83513/4.87344. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83124/4.87793. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.83304/4.87483. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 4.83434/4.87879. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.83256/4.87648. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.83356/4.87543. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.82893/4.88560. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.83396/4.86646. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.83097/4.88875. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82990/4.88986. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.82674/4.89576. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.82377/4.88138. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.82930/4.88444. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.81876/4.89245. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.82748/4.89368. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.82912/4.89492. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.82203/4.89159. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.82287/4.88887. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82846/4.87369. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.81892/4.89033. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.82086/4.88936. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.82181/4.88064. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.82037/4.87648. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.81717/4.88260. Took 0.18 sec\n",
      "Epoch 41, Loss(train/val) 4.82017/4.89059. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.82538/4.89297. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 4.81749/4.88408. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.82164/4.87318. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.81400/4.90178. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.82403/4.88270. Took 0.18 sec\n",
      "Epoch 47, Loss(train/val) 4.81644/4.87177. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.81239/4.87944. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.81616/4.88039. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.81371/4.87116. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.81425/4.87347. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81728/4.88304. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.81807/4.88781. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81903/4.87991. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.81608/4.87826. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.80949/4.87282. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.80766/4.89586. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81104/4.87905. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.81726/4.87247. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.81247/4.89028. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81227/4.87709. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.81112/4.87717. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.81335/4.88269. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81693/4.87695. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.80640/4.87818. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.80861/4.87913. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.81262/4.87084. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.81457/4.87651. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.81103/4.88618. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.81047/4.89341. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81217/4.86964. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.80488/4.89245. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.80584/4.91017. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.80765/4.88086. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.81581/4.87347. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.81086/4.88320. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80925/4.87471. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.79968/4.87900. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.80557/4.87692. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.81322/4.88165. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.80614/4.89356. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.81043/4.88071. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80463/4.88456. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.80678/4.88078. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.80572/4.89755. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.80054/4.90186. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.81120/4.86564. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.80813/4.86885. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.80819/4.87643. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.80433/4.88296. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.80463/4.89158. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.80258/4.89608. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.80010/4.89441. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.80490/4.88038. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79917/4.88421. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.80204/4.87334. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81049/4.87892. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.80396/4.90465. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.81142/4.87057. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.06815142307594586\n",
      "Epoch 0, Loss(train/val) 5.06523/4.97589. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.99615/5.08807. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.00133/5.12049. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.00231/5.12042. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.00208/5.11409. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.99701/5.11502. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.99933/5.12856. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.00020/5.13973. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.00206/5.11671. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.00493/5.06306. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.00140/5.01941. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.99357/5.02202. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.99314/5.03209. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.99238/5.04515. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.99422/5.04809. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.99372/5.04308. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.99263/5.06936. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.99294/5.04410. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.99732/5.02882. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.99254/5.03443. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.99504/5.02265. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.99300/5.02502. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.99373/5.04170. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.99235/5.03267. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.99085/5.04861. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.99491/5.04749. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.99600/5.01425. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.98929/5.04567. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.99225/5.05081. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.99029/5.03398. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.98917/5.03598. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.98788/5.05808. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.98907/5.04403. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.98448/5.07535. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.99174/5.03996. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.98731/5.04982. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.98632/5.04974. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.98795/5.05096. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.98375/5.04562. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.98409/5.05189. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.98575/5.06019. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.98500/5.05768. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.98565/5.05533. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.98239/5.03749. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.98406/5.06145. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.98354/5.06572. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.98497/5.04958. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.98582/5.04700. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.98401/5.05992. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.98415/5.05427. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.98204/5.06018. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.98681/5.04322. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.97427/5.07923. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.97944/5.05581. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.97996/5.04956. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.97314/5.02667. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.98206/5.06546. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.97927/5.06007. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.97791/5.06347. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.97954/5.03711. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.97502/5.05067. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.97708/5.06799. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.98306/5.03903. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 4.98477/5.02773. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.98324/5.03902. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.98184/5.05169. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.98761/5.02974. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.98652/5.02661. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.98340/5.04075. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.97908/5.04414. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.97772/5.08216. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.98072/5.01994. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.98205/5.06118. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.97872/5.05997. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.97557/5.03902. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.98235/5.02573. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.97492/5.01577. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.97812/5.04772. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.97829/5.02860. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.97545/5.05160. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.97383/5.06889. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.97515/5.07875. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.97566/5.04457. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.98424/5.01923. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.97977/5.02929. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.97485/5.03231. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.97524/5.05439. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.97505/5.04737. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.97282/5.04159. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.97752/5.04688. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.97288/5.05010. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.97362/5.07822. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.97792/5.04421. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.97548/5.04869. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.97067/5.06430. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.97922/5.05243. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.98075/5.01363. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.99973/5.00757. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.99529/5.02224. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.98984/5.04147. Took 0.19 sec\n",
      "ACC: 0.4375, MCC: -0.12855839970025792\n",
      "Epoch 0, Loss(train/val) 5.02247/4.93573. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.96230/4.94222. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.95675/4.94297. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.95238/4.94401. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.95403/4.94826. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.95125/4.95836. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.95053/4.96538. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.94802/4.96940. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.94494/4.97772. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.94418/4.98159. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.94353/4.98005. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.94547/4.98052. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.95245/4.97431. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.94415/4.98570. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.94669/4.98431. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.94088/4.99854. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.94360/4.97930. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.94258/4.98744. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.94112/4.98902. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.94406/5.00106. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.93705/5.00183. Took 0.22 sec\n",
      "Epoch 21, Loss(train/val) 4.93793/4.99604. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 4.94334/5.01662. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.93592/5.00567. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 4.93918/5.00340. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.93827/5.01168. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.93894/4.99402. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.93606/5.00992. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.93539/5.00602. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.93749/5.00386. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.93985/4.98245. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.93795/4.98927. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.93751/5.00517. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.93587/5.00902. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.93694/5.00426. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.93854/5.01343. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.93915/4.99844. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.93641/5.01233. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.93774/5.00734. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.93190/5.02709. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.93423/5.00670. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.93514/4.98489. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.93824/4.99485. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.93311/4.99771. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.93563/5.00868. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.93444/5.00599. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.93188/4.99039. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.93390/5.00852. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.93127/5.01292. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.93186/4.99884. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.92845/5.00443. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.93266/5.00830. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.93643/4.99746. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.93314/5.00827. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.92931/5.01610. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.93290/5.02514. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.92879/5.01037. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.93463/5.02075. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.93047/5.01844. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.93492/5.00946. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.93727/4.99827. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.93068/5.00318. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.92686/5.01187. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.92616/5.02285. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.92645/5.01461. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.93260/5.00496. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.92639/5.01408. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.92526/5.02791. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.92812/5.00215. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.92654/5.03314. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.92810/5.01516. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.92246/5.02319. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.92240/5.04743. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.91693/5.03049. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.91929/5.04188. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.92827/5.02201. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.91859/5.02981. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.92150/5.06844. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.91961/5.04711. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.92321/5.04679. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.92634/5.03176. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.92286/5.02410. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.92381/5.00352. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.92398/5.00564. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.92149/5.00487. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.92711/5.02454. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.92203/5.04432. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.92372/4.99374. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.92210/4.98914. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.92377/5.00719. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.92969/5.00971. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.93465/5.01754. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.92781/5.01678. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.92410/5.02217. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.92196/5.01758. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.92534/5.01243. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.92615/4.98222. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.92676/4.96949. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.95393/4.93810. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.95613/4.94599. Took 0.19 sec\n",
      "ACC: 0.625, MCC: 0.24305875451990117\n",
      "Epoch 0, Loss(train/val) 4.94325/4.95756. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.90889/4.86877. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.89408/4.85906. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 4.87937/4.86076. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.87725/4.86084. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.87991/4.85955. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.88097/4.86110. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.87858/4.86093. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.88084/4.86082. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.88010/4.86022. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.87730/4.86052. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.87731/4.86317. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.87660/4.86306. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.87511/4.86116. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87471/4.85902. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.87412/4.86041. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.87315/4.86096. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.87444/4.86203. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.87332/4.85902. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.87349/4.86099. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.87255/4.86457. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.87029/4.86332. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.87135/4.86646. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.86736/4.85145. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.86820/4.86994. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.86717/4.86282. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.86420/4.87622. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.86129/4.87540. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.87188/4.87439. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.87217/4.86957. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.87676/4.87300. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.87209/4.86692. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.87021/4.87495. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.87000/4.87906. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.86945/4.88430. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.86620/4.90658. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.87045/4.87676. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.86660/4.86094. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.86647/4.87150. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.86036/4.87164. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.86189/4.88122. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.86191/4.89204. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.85986/4.89688. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.86034/4.89563. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.86237/4.88717. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.85765/4.89423. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.85611/4.88670. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.85716/4.88523. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.85683/4.88115. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 4.85539/4.90071. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.85731/4.89891. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.85617/4.88879. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.85519/4.89250. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.87173/4.87392. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.86542/4.88693. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.86661/4.88797. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.85973/4.89468. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.86101/4.89604. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.85920/4.91822. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.86071/4.90750. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.85784/4.90923. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.85789/4.89393. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85142/4.91192. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.85296/4.90638. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.85585/4.90093. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.85062/4.92393. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.85534/4.89013. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.85261/4.90128. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.85315/4.90036. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.85038/4.90017. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.85141/4.90998. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.85484/4.90358. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.84835/4.90308. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84932/4.92285. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.85425/4.90137. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.84914/4.91354. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.85030/4.91662. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.85095/4.92167. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.84978/4.92066. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.85150/4.91235. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.84868/4.91781. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.84350/4.93976. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.84796/4.92443. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84649/4.93429. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.84543/4.91793. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.84510/4.92529. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.84602/4.93779. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.84736/4.93370. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.84081/4.93147. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.84601/4.93866. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 4.84846/4.92480. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.84612/4.93629. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.84681/4.91786. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.84319/4.91623. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.84759/4.93694. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.84493/4.92245. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.84346/4.92133. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.84654/4.92061. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.84074/4.94652. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.84046/4.94002. Took 0.20 sec\n",
      "ACC: 0.4375, MCC: -0.1111111111111111\n",
      "Epoch 0, Loss(train/val) 4.81947/4.82526. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.80119/4.82403. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.80176/4.82538. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79791/4.82455. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.80058/4.82660. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79854/4.83726. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.80001/4.83423. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79529/4.83397. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79894/4.83780. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79896/4.83119. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79425/4.83729. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.79691/4.83132. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79743/4.82796. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.80201/4.83323. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.80349/4.83376. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.79719/4.83781. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.79676/4.83853. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.79267/4.84401. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.79507/4.85061. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.79196/4.84905. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.79097/4.86047. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.79066/4.85847. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.79287/4.86025. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.79018/4.86552. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78968/4.87034. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78934/4.87104. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78842/4.88207. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78641/4.86962. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.79070/4.86665. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78853/4.88160. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.78651/4.88329. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.79049/4.87349. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.78605/4.89340. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.78637/4.88489. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.78738/4.88846. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.78798/4.89334. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.78245/4.89087. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.78726/4.88911. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.78696/4.89436. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.78515/4.89285. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.78254/4.90642. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 4.78522/4.89681. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.78521/4.89687. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.78039/4.90220. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.78398/4.89874. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.78202/4.91673. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78629/4.87759. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.77928/4.90318. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.78112/4.90953. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77954/4.89614. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.77985/4.90844. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.78114/4.88555. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.78202/4.89270. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.78200/4.89488. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.77858/4.90663. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.77979/4.89345. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.78340/4.89103. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.77685/4.89612. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.78318/4.88083. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.77888/4.88973. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.77793/4.89729. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.77695/4.90408. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.77554/4.90697. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.77261/4.89013. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.77512/4.89961. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.78408/4.86173. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.78071/4.88927. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.77570/4.90636. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77632/4.88633. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.77658/4.88856. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.77364/4.89778. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.79476/4.83259. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.78689/4.85852. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.78505/4.85998. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.78929/4.85954. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.78208/4.87596. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.78299/4.86162. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77766/4.87773. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.78368/4.86161. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.78434/4.86299. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.77859/4.87933. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.77773/4.89474. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.77413/4.87344. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.77929/4.88952. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.77832/4.90180. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.77688/4.88513. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.77849/4.90049. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.77784/4.88320. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.77849/4.88469. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.77389/4.88275. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.77164/4.89929. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.77671/4.88129. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.77650/4.87328. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77888/4.87649. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.78046/4.84784. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.78154/4.87490. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.77676/4.87043. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.77572/4.88035. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.77415/4.85927. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.77659/4.87899. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.16150120428611295\n",
      "Epoch 0, Loss(train/val) 4.80434/4.82581. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.74719/4.80024. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.73868/4.77697. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.73302/4.76998. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.73491/4.77361. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.73470/4.77610. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.73276/4.76567. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.73343/4.77794. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.73356/4.77849. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.73035/4.77707. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.72822/4.78407. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.72851/4.78237. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.72672/4.78966. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.72692/4.78820. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.72809/4.79110. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.72536/4.79086. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.72327/4.78449. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.72139/4.76807. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.72898/4.77402. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.72586/4.78837. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.72753/4.78777. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.72337/4.78432. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.72757/4.78139. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 4.72280/4.78195. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.72130/4.79459. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.71915/4.80281. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.72287/4.79351. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.72232/4.78315. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.71873/4.79436. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.72180/4.79816. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.72135/4.78434. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.72084/4.80162. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.71925/4.79145. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.71622/4.81748. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.71572/4.80512. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.71513/4.81330. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.71755/4.80211. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.71120/4.81445. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.71489/4.80313. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.71438/4.80827. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.71408/4.77857. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.72957/4.75055. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.72502/4.76561. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.72279/4.77504. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.72055/4.78099. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.72035/4.78211. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.71883/4.78433. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.71836/4.79428. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.71441/4.82007. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.71782/4.79228. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.71510/4.80861. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.71459/4.79730. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.71278/4.81357. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.71288/4.80103. Took 0.21 sec\n",
      "Epoch 54, Loss(train/val) 4.71246/4.79657. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.70748/4.81305. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.71932/4.79756. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.71691/4.80080. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.71512/4.80130. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.71327/4.79784. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.70990/4.82269. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.71421/4.81157. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.70692/4.82439. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.71295/4.81700. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.71022/4.82577. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.71327/4.79945. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.70919/4.80799. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.70791/4.81384. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.71694/4.77769. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.71002/4.80523. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.70441/4.82516. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.71100/4.81718. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.71106/4.81025. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.70947/4.81492. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.70538/4.81834. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.70841/4.83332. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.70548/4.83573. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.71079/4.81644. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.70570/4.81730. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.70860/4.81060. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.71216/4.79886. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.70532/4.82790. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.70323/4.84162. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.70874/4.81229. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.70265/4.84071. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.70330/4.84005. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.70451/4.82150. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.70884/4.82668. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.70505/4.81693. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.70118/4.82279. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.70256/4.81598. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.70251/4.83137. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.70396/4.81625. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.69922/4.81486. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.70187/4.83361. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.70890/4.80264. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.70282/4.80947. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.70427/4.80492. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.70186/4.81155. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.71028/4.79560. Took 0.19 sec\n",
      "ACC: 0.390625, MCC: -0.21885688981825285\n",
      "Epoch 0, Loss(train/val) 4.96212/4.99240. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.93395/4.96398. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.93967/4.92015. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.93226/4.91499. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.91963/4.91446. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.91849/4.91490. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.91992/4.91395. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.92057/4.91434. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.91906/4.91705. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 4.91527/4.92573. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.91752/4.92381. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.91714/4.92267. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.91408/4.92844. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.91481/4.93226. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.91398/4.93114. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.91213/4.93366. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.91103/4.93518. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.91073/4.93925. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.91115/4.93516. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.90995/4.93659. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.91251/4.93134. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.90852/4.93940. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.90765/4.94591. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.90813/4.94402. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.90598/4.93969. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.90979/4.93909. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.90714/4.94244. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.90582/4.94493. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90659/4.94808. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.90428/4.94551. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90717/4.94517. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.90649/4.93742. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.90119/4.94647. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.90927/4.93418. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.90706/4.94376. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90317/4.94100. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.90060/4.94719. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.90690/4.93217. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.90341/4.94545. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.90109/4.94283. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90033/4.94092. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.90091/4.94078. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.89938/4.93831. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.90137/4.93516. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.90036/4.94604. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.90023/4.94036. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.89530/4.94011. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.89866/4.93432. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89922/4.93414. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.89183/4.93598. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89915/4.92991. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.89308/4.93655. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.90083/4.93981. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.89279/4.93273. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.89534/4.94036. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.89121/4.94909. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.89126/4.94426. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.89018/4.94653. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.88826/4.93583. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89541/4.95115. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.89675/4.93115. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.89436/4.93596. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.90023/4.93463. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.89019/4.95683. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.88835/4.93974. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.89246/4.93583. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.89780/4.95061. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.88767/4.94783. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.88838/4.94754. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.88505/4.96262. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.88618/4.95804. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.88831/4.94617. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.88829/4.95057. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.89030/4.96131. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.88818/4.94844. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.89064/4.95025. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.88515/4.93642. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.88559/4.98900. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.88182/4.96935. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.89084/4.95894. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88664/4.94447. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.88557/4.98028. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.88745/4.95466. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.88981/4.96016. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88114/4.99368. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.88290/4.96367. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.88474/4.94992. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.88225/4.96295. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.88155/4.96987. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.88203/4.94418. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.88080/5.01096. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.88546/4.96005. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.87665/4.95803. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.88360/4.97116. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88124/4.96496. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.88844/4.96936. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.88189/4.97043. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.88806/4.97261. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.88404/4.96751. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88127/4.96894. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.13945994111797608\n",
      "Epoch 0, Loss(train/val) 4.90350/4.84156. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.83329/4.82087. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.82612/4.81882. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.82516/4.81996. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.82897/4.82126. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.82634/4.82514. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.82293/4.82751. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.82397/4.83208. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.82528/4.83814. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.82240/4.83718. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.81989/4.84205. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.82069/4.83943. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.81983/4.84633. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.82060/4.83999. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.82276/4.83881. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.82013/4.84254. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.81954/4.84546. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.81584/4.84725. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81333/4.84409. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.81745/4.84096. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.81596/4.85024. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.81899/4.84251. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.81195/4.85578. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.81223/4.84963. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.81277/4.85885. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.81577/4.84921. Took 0.21 sec\n",
      "Epoch 26, Loss(train/val) 4.81231/4.85670. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.81485/4.85058. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.81321/4.85885. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.81333/4.86036. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.81241/4.86385. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.81244/4.85852. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.81112/4.86940. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.81392/4.85797. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.80562/4.87161. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.80564/4.87173. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.81230/4.84005. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.81081/4.84633. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.81042/4.85328. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.81104/4.85800. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.80968/4.85321. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.80632/4.85824. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.80974/4.84753. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.80590/4.85920. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.80575/4.85463. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.80467/4.85716. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.80643/4.84872. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.80374/4.85950. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.81117/4.85444. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.80160/4.85157. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.81251/4.82507. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.81404/4.82767. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81258/4.83229. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.80738/4.83930. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.80799/4.84159. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.80677/4.86029. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.81185/4.84059. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.81198/4.82701. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81126/4.83331. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.80951/4.84124. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.80449/4.84576. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.80745/4.85064. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.80464/4.85476. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.80776/4.84584. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.80737/4.84339. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.80459/4.85261. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.81329/4.84443. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.80437/4.85020. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.80246/4.85519. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.80314/4.84293. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.80313/4.85807. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.80740/4.83066. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.81379/4.83304. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.80606/4.84665. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.80513/4.84168. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.80688/4.84507. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.80359/4.84492. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80435/4.84316. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.80287/4.84421. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.79762/4.85612. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.79940/4.84180. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.80199/4.84453. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.79714/4.85892. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80740/4.84183. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.80046/4.84697. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.79929/4.84486. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.80288/4.84532. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.79870/4.84347. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.79734/4.84996. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.79755/4.85539. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.79947/4.85546. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.80054/4.84968. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.79549/4.85784. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.79505/4.86020. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.79756/4.87011. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.80358/4.84407. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.79483/4.86560. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.79800/4.84303. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.80284/4.84747. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.79158/4.86210. Took 0.20 sec\n",
      "ACC: 0.46875, MCC: -0.07100716024967263\n",
      "Epoch 0, Loss(train/val) 4.93277/4.84839. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.84898/4.85152. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.85685/4.84752. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.86526/4.85772. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.86518/4.87988. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.85837/4.88361. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.85349/4.87667. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.85094/4.87535. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.84980/4.87020. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.85152/4.87429. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84988/4.87538. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.84593/4.87476. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85012/4.87243. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.84726/4.87459. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.84680/4.86999. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.84518/4.87118. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.84602/4.86447. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.84274/4.86488. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.84299/4.86853. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.84203/4.87148. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.83981/4.86534. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.84171/4.86031. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.84284/4.86556. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84260/4.86951. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.83981/4.86958. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.83493/4.87552. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84107/4.87450. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84003/4.86765. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.83340/4.87488. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.83992/4.86828. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.83427/4.86575. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.83449/4.87062. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.83483/4.86954. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.83326/4.86785. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.83067/4.86736. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84558/4.88558. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84719/4.86164. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84417/4.87294. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84103/4.86855. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.83756/4.87820. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.83824/4.88169. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.83607/4.86045. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83044/4.86450. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.83040/4.86406. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.82847/4.87111. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.83370/4.87245. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.82448/4.86790. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83046/4.87638. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.83304/4.86924. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.82702/4.88691. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.83041/4.86788. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.82644/4.88032. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.83021/4.88367. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.82833/4.87672. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.82954/4.87423. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.82405/4.88170. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.82863/4.87839. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.82315/4.88315. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.82904/4.86854. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.83246/4.86390. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.82601/4.87289. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.82849/4.86763. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.82186/4.87291. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.82537/4.86824. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.82421/4.86908. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.82451/4.86418. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.82305/4.86626. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.81449/4.86988. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.83002/4.87244. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.82106/4.88564. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.83140/4.87246. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.82283/4.86788. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.82160/4.86728. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.82366/4.87024. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.82053/4.86767. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.81900/4.85407. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.82927/4.86613. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.81722/4.86900. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.82405/4.86427. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.82139/4.85666. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.81937/4.85875. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.82266/4.84313. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.82005/4.86280. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.81717/4.86861. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.81836/4.86239. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.81951/4.85021. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.81417/4.85746. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.81935/4.86835. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.81971/4.86359. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82136/4.85276. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.81222/4.85479. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.81691/4.86475. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.81166/4.86168. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.81631/4.85616. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.81795/4.85342. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.81852/4.85592. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.81056/4.84960. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81444/4.85892. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.81986/4.84188. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.81601/4.86264. Took 0.20 sec\n",
      "ACC: 0.609375, MCC: 0.24165949998852546\n",
      "Epoch 0, Loss(train/val) 4.93784/4.90549. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.89049/4.90271. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.88474/4.90723. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.88418/4.90749. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.88295/4.90887. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.88228/4.90837. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.87875/4.91018. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.87864/4.90790. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.87720/4.90957. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87583/4.91184. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.87707/4.91128. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.87821/4.90856. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.87257/4.91067. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.87420/4.91418. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87457/4.91000. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 4.87104/4.91068. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.87154/4.90721. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.87105/4.90800. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.87204/4.90847. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.87072/4.90993. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.86799/4.90726. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.86859/4.91245. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.86462/4.90922. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.86711/4.91110. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.86787/4.91021. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.85903/4.92159. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.86779/4.90617. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.86280/4.91982. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.86559/4.91020. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.86397/4.90919. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.86135/4.91481. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.85731/4.91322. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.86393/4.89988. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.86147/4.90459. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85945/4.90653. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.86134/4.89637. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.86247/4.88958. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.87613/4.89427. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.87438/4.90131. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.87082/4.90389. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.86847/4.90788. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.86794/4.90941. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.86518/4.91121. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.86342/4.90994. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.86266/4.90575. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.86400/4.90537. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.86451/4.90448. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.86424/4.90837. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.86535/4.90823. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.86335/4.90625. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.86219/4.90869. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.86088/4.89677. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.86140/4.89472. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.86201/4.89685. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.86677/4.89584. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.86239/4.90523. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.86100/4.90043. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.85839/4.90754. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.85911/4.90992. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.85636/4.90625. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85713/4.90916. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.85679/4.90656. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85861/4.90527. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.86000/4.90417. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.85237/4.91730. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.85799/4.89602. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.85945/4.89584. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.86282/4.90072. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.85279/4.90017. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.85724/4.89970. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.85960/4.89636. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.85294/4.91593. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.85722/4.90177. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.85386/4.91844. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.85029/4.90416. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 4.85852/4.90382. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.85263/4.91976. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.85597/4.91302. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.85088/4.92132. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.84617/4.92655. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.85528/4.91187. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.85376/4.91915. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.85582/4.92054. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.85247/4.91401. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.85894/4.90330. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.85427/4.91944. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.85382/4.92171. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.85980/4.91880. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.85660/4.92576. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.84813/4.93299. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.85129/4.93955. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85054/4.91753. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.85171/4.92647. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.85386/4.92258. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.85514/4.91955. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.86339/4.91390. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.86383/4.91489. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.86380/4.90915. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.85999/4.91145. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.86218/4.90245. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.16012815380508713\n",
      "Epoch 0, Loss(train/val) 4.96822/4.93350. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.91814/4.90971. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.91928/4.90545. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.92093/4.90421. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.91571/4.90389. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.91362/4.90212. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.91744/4.90290. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.91623/4.90535. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.91222/4.90672. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.91368/4.90649. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.91232/4.90644. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.91122/4.90625. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.90675/4.90480. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.91112/4.89934. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.90817/4.90849. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.91188/4.90269. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90308/4.90765. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.91070/4.92530. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.90876/4.90667. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.90496/4.90959. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.90449/4.90729. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.90546/4.93247. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 4.90701/4.90905. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.90271/4.92437. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.90190/4.92074. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.90398/4.93655. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.90225/4.92537. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.90228/4.92358. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.89415/4.94725. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.91385/4.91212. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90111/4.91955. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.90667/4.91166. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.90333/4.91498. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.90325/4.93172. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.90173/4.93371. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90011/4.94139. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.90359/4.91669. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.89772/4.92816. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89318/4.95254. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.89952/4.94604. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90554/4.96039. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.89898/4.96577. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.90122/4.96037. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.90321/4.92823. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.89993/4.92721. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.90120/4.93045. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.89229/4.95086. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89657/4.94915. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89044/4.95582. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.89169/4.93969. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.88995/4.96376. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.89082/4.97479. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89904/4.95106. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.90571/4.93873. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.89589/4.95647. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.89972/4.93032. Took 0.18 sec\n",
      "Epoch 56, Loss(train/val) 4.89849/4.93784. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.89596/4.91977. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.89584/4.93644. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.89270/4.92202. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.89223/4.93556. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.88704/4.96206. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.89169/4.96230. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.89217/4.93644. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.89019/4.93868. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.88756/4.95079. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.88896/4.92827. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.88677/4.94299. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.88891/4.94142. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.88134/4.96426. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.89042/4.95409. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.87971/4.98277. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.89111/4.94463. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88570/4.96202. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.87589/4.98057. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.89300/4.94013. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.88296/4.95676. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.88311/4.97013. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.88348/4.95315. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.88872/4.95182. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88301/4.96821. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.88647/4.95944. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.88218/4.96650. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.87906/4.98533. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.87658/4.98203. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.88018/4.96923. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.87802/4.97429. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.87579/4.99813. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.87598/4.97938. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.87583/4.97841. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.87526/5.00631. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.89018/4.95044. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.89266/4.98886. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.88557/4.98244. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.87906/4.99401. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.89065/4.96849. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.88158/4.98070. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.88534/4.98251. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.88346/4.98921. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88540/4.99629. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.002944085489318884\n",
      "Epoch 0, Loss(train/val) 4.99945/4.94415. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.93859/4.93112. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.93056/4.93853. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.92732/4.93321. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.92743/4.93706. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.92031/4.93551. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.92092/4.93672. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.92066/4.93846. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.92495/4.93896. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.92379/4.93925. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.91733/4.94353. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.91685/4.94544. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.91702/4.94552. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.91718/4.95052. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.91528/4.94369. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.91707/4.95030. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.91408/4.94325. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.91970/4.92696. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.92173/4.93048. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.91802/4.93613. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.91825/4.92905. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.91781/4.92900. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.91255/4.93052. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.91241/4.94805. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.91212/4.94005. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.91021/4.95124. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.91045/4.94263. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.90809/4.95103. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90700/4.95276. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.90277/4.95706. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90719/4.95506. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.90318/4.94882. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.90303/4.96209. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.89911/4.96083. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 4.89779/4.96384. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.89673/4.97988. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.89927/4.96674. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.90036/4.97208. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89788/4.97289. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90471/4.95111. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.89498/4.97889. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.90445/4.95208. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89457/4.97918. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.90102/4.96406. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.90001/4.99895. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.89967/4.95695. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.89611/4.97523. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.89484/4.97732. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89840/4.96449. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.90238/4.96306. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89376/4.98406. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.89544/4.97746. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89774/4.97615. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.90206/4.96176. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.89526/4.98643. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.89818/4.96530. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.89689/4.96822. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.89272/4.98218. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.89994/4.95720. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89166/4.98246. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.89838/4.97425. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.90105/4.96778. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.89744/5.00990. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.89849/4.96042. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.90062/4.95504. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 4.89627/4.99165. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.89211/4.96998. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.89827/4.97780. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.88365/4.98604. Took 0.21 sec\n",
      "Epoch 69, Loss(train/val) 4.89868/4.95793. Took 0.21 sec\n",
      "Epoch 70, Loss(train/val) 4.89592/4.98267. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.91011/4.94752. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.91188/4.94477. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.90905/4.95573. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.90493/4.95279. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.90704/4.94730. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.90194/4.95391. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 4.90347/4.95623. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.90123/4.95656. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.90135/4.98116. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.89711/4.96884. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.89987/4.96804. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.89591/4.95382. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.88945/5.00582. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.90054/4.95593. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.89104/4.98059. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.89920/4.96278. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.89707/4.96114. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.89861/4.98810. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.88983/4.99098. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.89006/4.96327. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.89984/4.95771. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.89187/4.99641. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.89311/4.98509. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.89656/4.98550. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.88743/4.99858. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.89759/4.98149. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.89080/4.96953. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.89374/4.97097. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.89096/5.00078. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.0657512425947884\n",
      "Epoch 0, Loss(train/val) 4.95426/4.98183. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.91681/4.93097. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.91161/4.93941. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.91025/4.93679. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.91103/4.93249. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.90806/4.92858. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.90858/4.93187. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.90670/4.92935. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.90332/4.92608. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.90815/4.92663. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.90723/4.93092. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.90676/4.92337. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.90830/4.91169. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.91245/4.91418. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.90895/4.91301. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.90683/4.91021. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90248/4.91724. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.90757/4.92174. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.90842/4.92407. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.90928/4.92540. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.90754/4.92586. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.90529/4.93366. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.90555/4.93139. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.90403/4.92941. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.90698/4.91309. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.91099/4.91854. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.90900/4.91747. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.90614/4.92105. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.90723/4.92419. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.90941/4.91761. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90788/4.91798. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.90551/4.92488. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 4.90386/4.93467. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.90068/4.94330. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.90363/4.94111. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.89969/4.95236. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.89883/4.94656. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.89965/4.94973. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89618/4.95600. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90149/4.91470. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90048/4.94042. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.90688/4.93004. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.90466/4.93496. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.90169/4.94116. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.89946/4.94663. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.89992/4.94793. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.90010/4.94559. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.89789/4.94909. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.89646/4.96029. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.90051/4.95157. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89524/4.95108. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.89691/4.94976. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89577/4.94759. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.89482/4.93859. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.90883/4.92494. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.90573/4.92597. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.90360/4.93328. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.90111/4.94302. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.89933/4.95870. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.89967/4.93513. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.89851/4.94344. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.89899/4.94800. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.89924/4.94600. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.89854/4.96063. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.89662/4.95805. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.89636/4.96299. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.89995/4.94705. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.89559/4.95627. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.89346/4.95474. Took 0.21 sec\n",
      "Epoch 69, Loss(train/val) 4.89513/4.95601. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.89695/4.96736. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.89440/4.93847. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.89455/4.96135. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.89646/4.94538. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.89826/4.95336. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.89427/4.95329. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.89306/4.97119. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.89113/4.95231. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.89217/4.97240. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.89176/4.96301. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.89519/4.96824. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.89160/4.97082. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.89264/4.96244. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.88977/4.95935. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.88538/4.96815. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.89007/4.97213. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.89052/4.97750. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.88525/4.97291. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.89081/4.95925. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.88649/4.98962. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.88415/4.97861. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.89522/4.96014. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.89065/4.97597. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.88582/4.97530. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.88752/4.96081. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.90000/4.93670. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.89300/4.95944. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.88881/4.96696. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.88612/4.98312. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88592/4.97529. Took 0.20 sec\n",
      "ACC: 0.4375, MCC: -0.11555252264060875\n",
      "Epoch 0, Loss(train/val) 5.01890/5.01694. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.01035/4.99154. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.00651/4.99813. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.00577/4.99321. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.00166/4.99909. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.00214/4.99476. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.99911/5.00040. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.99788/5.00115. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.99601/5.00232. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.99949/4.99860. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.99168/4.99860. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.99659/4.99682. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.98873/4.99982. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.99184/4.99319. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.99238/4.99776. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.98953/5.01135. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.98995/4.99408. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.99288/4.99250. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.98880/5.00187. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.99221/4.98048. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.98926/4.98372. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.98936/4.98409. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.99146/4.97653. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.99364/4.97694. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.99490/4.99919. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.98870/5.00718. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.99046/5.00806. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.98384/5.01575. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.99056/5.02509. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.99593/5.02595. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.99355/5.02985. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.98701/5.02587. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.98753/5.01888. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.98761/5.01300. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.99020/5.01529. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.98244/5.02518. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.98564/5.01827. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.98423/5.02748. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.98396/5.02025. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.98682/5.01507. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.98325/5.01907. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.98255/5.01805. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.98349/5.01994. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.98182/5.01060. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.98572/5.00506. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.98350/5.00641. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.98303/5.00560. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.98339/5.00444. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.98090/5.01992. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.98656/5.01204. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.98370/5.02510. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.98069/5.00775. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.98252/5.01162. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.97858/5.01322. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.98398/5.01278. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.97724/5.02522. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.98067/5.02750. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.98173/5.00496. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.98384/5.02482. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.97862/5.03280. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.97888/5.02179. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.98168/5.02342. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.98439/5.02571. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.98042/5.03985. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.98082/5.02718. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.98182/5.03810. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.97471/5.04647. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.98014/5.02053. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.98068/5.02921. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.98361/5.03726. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.97929/5.04258. Took 0.18 sec\n",
      "Epoch 71, Loss(train/val) 4.97918/5.03004. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.97519/5.02291. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.97914/5.03389. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.97773/5.03962. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.97724/5.03579. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.97567/5.03986. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.97345/5.04397. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.97149/5.03908. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.97077/5.04288. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.97921/5.03808. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.97470/5.05732. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.97426/5.04610. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.96983/5.03263. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.97541/5.04682. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.97160/5.05869. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.97381/5.06643. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.97566/5.03611. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.97283/5.04830. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.97806/5.04809. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.97125/5.06364. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.97058/5.04254. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.97002/5.06672. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.96979/5.04318. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.96491/5.07212. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.96671/5.05614. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.96978/5.07135. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.96755/5.06336. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.97220/5.04647. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.96966/5.05979. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.007889684472185849\n",
      "Epoch 0, Loss(train/val) 5.10205/5.00450. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.02546/5.00246. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.01991/5.00568. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.02327/5.00122. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.01758/5.00212. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.02127/4.99950. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.01620/5.00058. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.01603/5.00259. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.01286/5.00183. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.01590/5.00360. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.01382/5.00525. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.01602/5.00378. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.01251/5.00677. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.01196/5.00603. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 5.01015/5.00553. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.00907/5.00761. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.01301/5.00540. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 5.01220/5.00334. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.01101/5.00743. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.01163/5.00590. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.00831/5.00918. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 5.00974/5.01104. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.00890/5.01257. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.00912/5.01542. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.00974/5.01493. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.00842/5.02202. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.00659/5.02680. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.00546/5.02268. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.00603/5.03227. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.00797/5.02585. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.00637/5.02846. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.00175/5.03867. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.00585/5.04124. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.00202/5.07240. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.00535/5.03512. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.00507/5.04119. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.99862/5.04758. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.99861/5.05007. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.00377/5.03884. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 5.01547/5.03040. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 5.00637/5.03517. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 5.00647/5.03562. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.00620/5.04656. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.00248/5.04604. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.00362/5.03885. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.00447/5.04583. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.00461/5.04125. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.00359/5.05042. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.00281/5.07036. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.00328/5.05932. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.00099/5.07054. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.99850/5.07867. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.99643/5.07611. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 5.00043/5.06993. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.99764/5.08441. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.00035/5.09113. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.99235/5.09294. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.99368/5.08781. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.99458/5.08089. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.98972/5.11039. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.99331/5.08807. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.99146/5.09994. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.99087/5.11212. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.99436/5.11846. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.98903/5.11391. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.98535/5.12568. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.99261/5.11825. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.98708/5.13557. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.99361/5.11821. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.98859/5.13974. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.98936/5.12075. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.98676/5.11620. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.98575/5.10806. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.97879/5.16180. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.98993/5.11526. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.99191/5.09937. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.98065/5.12469. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.98004/5.13730. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.98858/5.12586. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.98824/5.10875. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.98326/5.12628. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.98166/5.13004. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.98053/5.13503. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.99766/5.10349. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.99091/5.08963. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.99004/5.10053. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.98600/5.10092. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.99637/5.07656. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.98901/5.08281. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.99699/5.02867. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.00441/5.03938. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.00134/5.04423. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.99465/5.05819. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.98889/5.07991. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.99425/5.06326. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.00265/5.01844. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.99595/5.03153. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 4.99901/5.03363. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.99238/5.05972. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.99304/5.06540. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.06158357771260997\n",
      "Epoch 0, Loss(train/val) 4.91085/4.89428. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.85076/4.88129. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.83862/4.88584. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 4.84572/4.88797. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.84277/4.89419. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.84087/4.89923. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.84147/4.89981. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.83783/4.89748. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.83872/4.89459. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.83928/4.90252. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.83506/4.90363. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.83731/4.89878. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.83736/4.90610. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.83670/4.90490. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.83123/4.90792. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.83457/4.90762. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.83311/4.91231. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.83126/4.91485. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83371/4.91684. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.83534/4.91902. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.83177/4.92573. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.83149/4.92565. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.82597/4.92668. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.83069/4.93207. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.83035/4.93051. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.83071/4.93034. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82871/4.92796. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.82752/4.93745. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.82960/4.93815. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.82763/4.95106. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.82889/4.92996. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.82644/4.94887. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.82090/4.96230. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.82823/4.94608. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.82558/4.94612. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82625/4.94421. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.82517/4.95859. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.82324/4.95360. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.82289/4.96286. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.82382/4.96042. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.82347/4.94339. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.82013/4.95293. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.82219/4.94396. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.82599/4.96273. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.82242/4.94454. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.81906/4.96370. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.81985/4.96585. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.82371/4.94931. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.82220/4.95719. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.82167/4.94912. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.81760/4.96123. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.81881/4.98052. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.81965/4.93897. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.82166/4.95285. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81834/4.96509. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.81749/4.97099. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.81643/4.95413. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.81932/4.97064. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81952/4.95755. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.81880/4.97336. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.81927/4.96487. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81538/4.95954. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.81886/4.97600. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.81697/4.97718. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81956/4.96105. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81349/4.96751. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.81541/4.98182. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.81789/4.95238. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.81242/4.97866. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.81548/4.97497. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.81719/4.98221. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81604/4.96472. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.81240/4.98960. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.80981/4.96993. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81496/4.97877. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.81600/4.96097. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.81243/4.97768. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.81313/4.99676. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.81007/4.98870. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.81153/4.98125. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.81684/4.98827. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.81293/4.93902. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.81230/4.98891. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.81733/4.94888. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.81314/4.94832. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.81469/4.95735. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.81205/4.99303. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.82084/4.93315. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.81971/4.96264. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.81680/4.98307. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.81910/4.96962. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.81568/4.95479. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.81542/4.97713. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.81313/4.95469. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.81962/4.96035. Took 0.21 sec\n",
      "Epoch 95, Loss(train/val) 4.81088/4.96514. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.81665/4.95503. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81792/4.97194. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.81556/4.94589. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.81332/4.97393. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.09379580992210836\n",
      "Epoch 0, Loss(train/val) 4.92407/4.83508. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.87021/4.83918. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.87225/4.83654. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87126/4.83870. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.86668/4.84273. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.86426/4.84306. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.86476/4.84180. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.86365/4.84156. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.86304/4.84616. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.86286/4.84497. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.86308/4.84468. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.86045/4.84373. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85946/4.84556. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85756/4.84507. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.85397/4.84630. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.86027/4.85447. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85480/4.84830. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.85343/4.85462. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85462/4.85674. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.85389/4.86077. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.85273/4.86157. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85004/4.86590. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85142/4.86381. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85228/4.86333. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.84985/4.87208. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.84880/4.86880. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84922/4.86902. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.85004/4.87553. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.84524/4.87804. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.85008/4.87278. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.84775/4.88179. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84573/4.88221. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.84577/4.88617. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84887/4.88627. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.84604/4.88111. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84883/4.88260. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84645/4.89111. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84274/4.89614. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84322/4.89834. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84454/4.90050. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.84230/4.89158. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84356/4.89347. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.84830/4.89297. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.84440/4.88594. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84391/4.89147. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.83732/4.89890. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84162/4.90580. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.84029/4.87788. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.83990/4.90550. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.84122/4.88590. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.84080/4.90425. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.83692/4.89547. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.83547/4.91277. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.83923/4.90078. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.83570/4.89455. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.83604/4.92635. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84058/4.88789. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.83523/4.90046. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.83485/4.90128. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.83741/4.90631. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.83634/4.90022. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.83354/4.90916. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.83422/4.92109. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83934/4.91364. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.83374/4.89997. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.83673/4.91818. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.83249/4.92602. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83113/4.93562. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.83339/4.90337. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.83275/4.90376. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.83567/4.92572. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83126/4.89378. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.83781/4.92853. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83429/4.92412. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.83179/4.91596. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.82948/4.95810. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83090/4.92618. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.83246/4.93450. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.83439/4.90822. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.82951/4.93868. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.82875/4.93875. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.82796/4.93172. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.83217/4.91958. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.82889/4.91047. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.82471/4.91507. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83256/4.92101. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.83164/4.94550. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83331/4.89849. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 4.83738/4.89565. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82966/4.90810. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.83003/4.90661. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82783/4.90932. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82317/4.92098. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82005/4.91974. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82604/4.93214. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82622/4.91705. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82104/4.88803. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82660/4.93513. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.81962/4.93352. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.82025/4.90550. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.12022678317975014\n",
      "Epoch 0, Loss(train/val) 4.86450/4.85575. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.85421/4.83860. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.84724/4.84248. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.84677/4.84751. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.84441/4.84431. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.84386/4.84760. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.84565/4.84616. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.84602/4.84723. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.84249/4.85170. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.84304/4.84916. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.84097/4.84821. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.84178/4.84920. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.84221/4.85525. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.84128/4.85403. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.84102/4.85560. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.84219/4.85329. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.84270/4.85680. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.84086/4.86123. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.83947/4.86266. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.83947/4.86304. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84128/4.86296. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.83719/4.86589. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.84000/4.86623. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.83587/4.87523. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.83822/4.87696. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.83327/4.86986. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.83317/4.87241. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.83417/4.87183. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.83537/4.86477. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.83390/4.88337. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.83265/4.86761. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.82739/4.86565. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.82659/4.87225. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.82618/4.86853. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.83326/4.87519. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82614/4.85267. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.82398/4.85632. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.82820/4.87513. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.82630/4.86195. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.82714/4.84869. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.82227/4.85457. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.82267/4.86146. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.82351/4.85209. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.82118/4.85571. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.82239/4.87538. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.82091/4.87601. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 4.82091/4.87237. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.82172/4.86554. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.82137/4.90677. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.82470/4.86245. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.82778/4.84356. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.81918/4.87828. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.82409/4.88383. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.82255/4.85197. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.81631/4.89536. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.81754/4.87879. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.82288/4.87198. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.82121/4.85983. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81876/4.85427. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.82000/4.86906. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.81883/4.87511. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 4.81975/4.88979. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.81466/4.89174. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.81646/4.87107. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81821/4.87023. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81560/4.86708. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.82090/4.89101. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.82495/4.84646. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.81564/4.85624. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.81093/4.87932. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.81725/4.89512. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81744/4.85119. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.81540/4.87318. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.81741/4.88866. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81423/4.86038. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.81346/4.86880. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.81116/4.86581. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80895/4.88062. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.81764/4.87911. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.81336/4.84648. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.81489/4.89508. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.80677/4.91491. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.80848/4.89483. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.81399/4.90943. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.81324/4.86761. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.81069/4.88675. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.80777/4.89391. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.80885/4.90232. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.80400/4.93911. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.81432/4.88073. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.80708/4.87930. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.81022/4.85184. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.80761/4.86917. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.80531/4.92317. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.80492/4.89518. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.81537/4.87036. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.81287/4.85560. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81493/4.88706. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.80015/4.87598. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.80387/4.90815. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.1463182264089704\n",
      "Epoch 0, Loss(train/val) 4.81118/4.92862. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.74035/4.79368. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.73779/4.74941. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.73214/4.73817. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.73234/4.73853. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.73363/4.74028. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.73195/4.74163. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.73284/4.72781. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.73047/4.72014. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.73028/4.72462. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.72883/4.72384. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.72716/4.72191. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.72553/4.72313. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.73004/4.71666. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.72496/4.73436. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.73468/4.71171. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.72122/4.74488. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.72930/4.71791. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.72117/4.73326. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.72354/4.72782. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.72401/4.72401. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.72397/4.72745. Took 0.22 sec\n",
      "Epoch 22, Loss(train/val) 4.72044/4.73465. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.72354/4.72258. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.71951/4.73384. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.72432/4.71954. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.71761/4.74020. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.72563/4.72636. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.72181/4.74056. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.71528/4.74598. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.71734/4.73243. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.72276/4.73329. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.71964/4.73584. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.71906/4.73694. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.71883/4.72858. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.71345/4.75015. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.72056/4.73060. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.71477/4.74538. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.71978/4.74108. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.72007/4.73616. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.71558/4.74464. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.73161/4.73317. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.72088/4.74129. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.72566/4.72864. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.71963/4.73330. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.72651/4.72376. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.72223/4.74540. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.71780/4.73144. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.71960/4.71644. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.72634/4.72991. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.71740/4.74029. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.71950/4.71913. Took 0.22 sec\n",
      "Epoch 52, Loss(train/val) 4.71910/4.73369. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.71871/4.73975. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.72130/4.74099. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.71813/4.73762. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.71719/4.73024. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.71832/4.74470. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.71457/4.74668. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.71454/4.73326. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.71395/4.73376. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.71628/4.74371. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.71089/4.74606. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.71615/4.73288. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.71367/4.74429. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.71083/4.74229. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.71450/4.74091. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.71320/4.72999. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.71836/4.74080. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.71669/4.76100. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.71332/4.75383. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.70938/4.74785. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.70795/4.77233. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.70811/4.77076. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.71588/4.74916. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.71284/4.76501. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.71381/4.76118. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.72213/4.75226. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.71054/4.74474. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.70875/4.76381. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.71385/4.75636. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.70957/4.77863. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.71152/4.76882. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.71029/4.76822. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.71346/4.75084. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.71561/4.75176. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.70853/4.75614. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.70491/4.76219. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.70807/4.75415. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 4.70535/4.75927. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.71381/4.73304. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.70782/4.73804. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.70957/4.75453. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.70867/4.75159. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.70116/4.77800. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.70355/4.75202. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.70797/4.76228. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.69858/4.77675. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.70352/4.76424. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.70343/4.75550. Took 0.20 sec\n",
      "ACC: 0.484375, MCC: -0.03779786830012622\n",
      "Epoch 0, Loss(train/val) 4.97662/4.94465. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.93206/4.91573. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.92992/4.92578. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.92749/4.92532. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.92510/4.91943. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.92479/4.91597. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.92211/4.91515. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.91990/4.90733. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.92274/4.91242. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.92048/4.91210. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.92293/4.90865. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.92209/4.90617. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.91875/4.90030. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.91878/4.89919. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.91996/4.90128. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.92434/4.91448. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.92513/4.90980. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.92458/4.90294. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.91996/4.90113. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.91482/4.90183. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.91655/4.90041. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.91621/4.90415. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.91545/4.90524. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.91583/4.90545. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.91843/4.91615. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.91886/4.91059. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.91591/4.91427. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.91573/4.91560. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.91483/4.91450. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.91560/4.91633. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.91390/4.91776. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.91486/4.91948. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.90491/4.91448. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.90951/4.93596. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.91117/4.91390. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90508/4.92522. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.91144/4.92484. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.91295/4.92035. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.90980/4.90886. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.91174/4.90788. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.91320/4.91097. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.90894/4.90515. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.91168/4.91008. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.91156/4.90008. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.91030/4.90252. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.90693/4.90092. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.91425/4.90678. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.90399/4.91029. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.90489/4.91972. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.90718/4.92154. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.90495/4.92641. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.90292/4.91997. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89774/4.92453. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.90891/4.91891. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.90409/4.91256. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.90077/4.91760. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.90088/4.91917. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.90024/4.91983. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.90180/4.93092. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.89787/4.92381. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.90157/4.92008. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.90165/4.92507. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.89842/4.90713. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.89795/4.92540. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.89740/4.92509. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.89782/4.91483. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.89444/4.93061. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.89976/4.91991. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.90248/4.93206. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.89817/4.94073. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.90155/4.93783. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.90037/4.92779. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.89822/4.92825. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 4.89669/4.92509. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.90056/4.92411. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.90148/4.89698. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.90073/4.91020. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.89759/4.90431. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.89515/4.93385. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.89818/4.91872. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.89483/4.92999. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.90162/4.90683. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.89790/4.90315. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.89462/4.92060. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.89359/4.93032. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.89378/4.93274. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.89850/4.93575. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.89373/4.91469. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.89727/4.92000. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.88898/4.92995. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.89389/4.92762. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.89266/4.93783. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.88993/4.93834. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.89726/4.91506. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.89136/4.92028. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.89867/4.91839. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.89591/4.92562. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.89140/4.92981. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.88740/4.93910. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.88991/4.90629. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.15711066381299738\n",
      "Epoch 0, Loss(train/val) 4.89372/4.85566. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.85134/4.84019. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.84413/4.84146. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.84296/4.84027. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.84180/4.83940. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.83818/4.84357. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.83833/4.84450. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.83833/4.84432. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.83868/4.84348. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 4.83657/4.84717. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84292/4.84723. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.83185/4.84766. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.83685/4.84825. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.83473/4.85159. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.83681/4.85578. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.83758/4.85388. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.83706/4.85561. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.83532/4.85889. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83893/4.86341. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.83258/4.86130. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.83008/4.86774. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.83442/4.86484. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.83003/4.86489. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.83279/4.86552. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.82701/4.87577. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82953/4.87263. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.83056/4.87325. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.82644/4.88555. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.82677/4.87440. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.83134/4.86187. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.82616/4.87826. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.82582/4.88277. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.82681/4.87483. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.82552/4.87897. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.82684/4.87110. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82416/4.87446. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.82910/4.86979. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.82984/4.86202. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.83203/4.85020. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.82760/4.86140. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.82771/4.86167. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.82575/4.86757. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.82055/4.87280. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.82113/4.87123. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.82002/4.86859. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.82088/4.86294. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.82593/4.86745. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.81435/4.88718. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.82440/4.87107. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.81898/4.88023. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.82035/4.88997. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.82113/4.87824. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.81835/4.89133. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.81478/4.90700. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81453/4.88588. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.81116/4.90850. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.81145/4.91049. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.81640/4.88574. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.82104/4.88482. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.81502/4.88955. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.80538/4.90815. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81244/4.88590. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.81011/4.90166. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.81469/4.88739. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81232/4.89941. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81479/4.86668. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.80667/4.90842. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.81355/4.88370. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 4.80894/4.88700. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.80826/4.93326. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.81544/4.88704. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81276/4.89280. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.80957/4.91819. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.81352/4.90645. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81069/4.90624. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.81373/4.88302. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.80728/4.89553. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80313/4.89692. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.80267/4.92111. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.80953/4.90138. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.81186/4.90253. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.80610/4.91352. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.81284/4.89584. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80268/4.91295. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.80892/4.95708. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.82097/4.89697. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.81948/4.91826. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.82312/4.91725. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.81773/4.91861. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.81839/4.91040. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.80851/4.90886. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.81230/4.90037. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.81364/4.90072. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.80998/4.90898. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.81014/4.93033. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.81231/4.90321. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.80891/4.92609. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.80798/4.90378. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.80426/4.90497. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.80369/4.91894. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.019772733298509516\n",
      "Epoch 0, Loss(train/val) 5.00900/4.98131. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.02637/4.99725. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.99125/4.97688. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.98914/4.98498. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.99199/4.98370. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.98899/4.98043. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.98636/4.97730. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.98475/4.97702. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.98392/4.97427. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.98346/4.97266. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.98533/4.97067. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.98059/4.96789. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.98278/4.96425. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.98194/4.96149. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.98043/4.95543. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.97809/4.95339. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.97770/4.94686. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.97683/4.95183. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.97867/4.95158. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.97741/4.95175. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.98007/4.95103. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.97534/4.94558. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.97495/4.94455. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.97759/4.94571. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.97649/4.94414. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.97600/4.94489. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.97511/4.94742. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.97490/4.94764. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.97200/4.94712. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.97470/4.95365. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.97228/4.94658. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.97273/4.93997. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.97624/4.94619. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.97365/4.94550. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.97018/4.94705. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.97084/4.95980. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.96811/4.95221. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.96758/4.95994. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.96710/4.97238. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.96768/4.96580. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.97054/4.97880. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.96726/4.98021. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.96457/4.98314. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.96538/4.98195. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.96178/5.00417. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.96193/4.98734. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.96432/5.00164. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.96215/4.98042. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.96023/4.98601. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.96217/4.98074. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.95995/4.99219. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.95327/5.01089. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.95827/4.99203. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.95926/4.98136. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.96030/4.97651. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.95785/4.98597. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.95790/5.00976. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.95826/4.98925. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.96162/4.98317. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.95724/4.98772. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.95666/4.99142. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.95443/5.00105. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 4.95657/4.98862. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.95874/4.97921. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.95001/4.99720. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.95862/4.98150. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.95217/4.99136. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.95879/4.99216. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.95099/5.00106. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.95123/4.99071. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.95797/4.97252. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.95448/4.99754. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.95355/4.99252. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.95006/4.99368. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.95702/4.97415. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.95014/4.99114. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.95391/5.00910. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.94699/4.99097. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.94523/4.98567. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.95538/4.99566. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.94995/4.98346. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.95009/4.97509. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.94960/4.98365. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.95283/4.98974. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.94674/4.99238. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.94277/5.00921. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.94860/4.99199. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.94643/5.00478. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.95161/5.00383. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.96319/4.99479. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.94955/5.00508. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.94839/5.01864. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.94661/5.00466. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.94419/5.01508. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.94686/4.99407. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.94886/5.00473. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.95064/5.00245. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.94711/5.00327. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.94717/5.00781. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.94342/5.00158. Took 0.20 sec\n",
      "ACC: 0.515625, MCC: 0.03734655007457467\n",
      "Epoch 0, Loss(train/val) 4.88669/4.91785. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.83747/4.88635. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.83530/4.87996. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.82875/4.87467. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.82727/4.87897. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.82813/4.88853. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.82558/4.89886. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.82175/4.89751. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.82036/4.89814. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.82131/4.88978. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.82095/4.88871. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.81836/4.88279. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.81637/4.88172. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 4.81681/4.87924. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.81743/4.87327. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.81515/4.87565. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.81420/4.88679. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.81578/4.88326. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81585/4.89026. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.81361/4.88981. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 4.81211/4.88128. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.81235/4.87689. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.80963/4.88144. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 4.81025/4.88264. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.80878/4.88626. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.81212/4.90935. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.81039/4.87723. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.80670/4.89201. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.80589/4.91403. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.80892/4.89999. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.80350/4.90003. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.80408/4.89782. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.80418/4.90275. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.80581/4.91276. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.80205/4.93810. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 4.80148/4.91081. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.80011/4.91635. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.79997/4.91744. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.80070/4.91747. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.79969/4.91828. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.79925/4.92761. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.80397/4.90695. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.79754/4.93722. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.79655/4.90552. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.79141/4.92279. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.79286/4.95321. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.79767/4.93420. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.79779/4.92528. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.78788/4.95139. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.79221/4.94246. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.79682/4.91095. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.79776/4.91632. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.78981/4.92213. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.79634/4.91005. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.79255/4.92708. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.79517/4.91967. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.79320/4.95370. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.79249/4.92428. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 4.79167/4.93554. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.79187/4.94073. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.78917/4.94822. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.79409/4.93594. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.79603/4.93334. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.78697/4.93233. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.79288/4.91321. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.79778/4.93752. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.79022/4.91883. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.79051/4.94521. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.78757/4.96049. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.79013/4.92477. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.78944/4.94421. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.78563/4.93240. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.78895/4.92663. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.78403/4.93898. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.79512/4.90021. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.79215/4.91084. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.79447/4.91639. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.79653/4.92284. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.79208/4.92623. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.79103/4.93825. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.79287/4.94559. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.79173/4.94902. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.79036/4.94379. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.78974/4.96412. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.79054/4.92035. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.78576/4.95490. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.78532/4.94856. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.78801/4.94184. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.78619/4.92468. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.78648/4.94784. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.78486/4.92245. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.78945/4.93515. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.79029/4.93126. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.78375/4.95372. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.78840/4.93736. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.78300/4.95486. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.78348/4.95560. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.78147/4.95847. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.78410/4.93778. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.78726/4.96282. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.049929648757959806\n",
      "Epoch 0, Loss(train/val) 4.92952/4.90364. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.91624/4.91242. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.90490/4.90404. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.90738/4.90502. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.90143/4.89997. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.90379/4.89958. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.90122/4.90077. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.90270/4.90070. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.90149/4.89882. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.90042/4.90040. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.89988/4.89986. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.90384/4.89935. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.90019/4.90003. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.90250/4.90093. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.90421/4.90786. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.90155/4.91031. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90374/4.91785. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.90345/4.92303. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.89741/4.92095. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.89866/4.91794. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.89787/4.91885. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.89453/4.91524. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.89763/4.91067. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.89494/4.91385. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.89626/4.91727. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.90005/4.91931. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.89525/4.92262. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.89671/4.92351. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.89384/4.92073. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.89445/4.92566. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.89031/4.93118. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.89283/4.92943. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.89198/4.93299. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.89162/4.93643. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.89115/4.93287. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.89219/4.92400. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.88909/4.91871. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.88738/4.92424. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.88979/4.92336. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.88802/4.93293. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.88577/4.93544. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.88688/4.92310. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89302/4.93276. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.88950/4.93153. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.88555/4.95533. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.88856/4.92639. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.88414/4.93757. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.88650/4.94013. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.88966/4.93965. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.88422/4.95320. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.88657/4.93735. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.89843/4.90663. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.89828/4.90636. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.89066/4.91676. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.89351/4.92125. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.88778/4.94132. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.89706/4.93805. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.90352/4.92923. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.89700/4.93977. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89457/4.94527. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.89036/4.93331. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.89220/4.92225. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.88697/4.92486. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.89026/4.92172. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.89586/4.93004. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.88729/4.94214. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.89094/4.92669. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.88358/4.96562. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.89075/4.92286. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.88382/4.96438. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.88639/4.92198. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.88272/4.95006. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.88028/4.94337. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88424/4.94154. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.88420/4.98076. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.89096/4.93784. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.88341/4.95739. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.88835/4.91795. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.89652/4.90603. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.89486/4.92032. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.90367/4.92961. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.89521/4.93204. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.90132/4.91561. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.90285/4.91665. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.90010/4.91311. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.89747/4.91194. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.90217/4.92228. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.89666/4.92452. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.89754/4.92508. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.89809/4.93156. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.89703/4.92873. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.89047/4.95421. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.90706/4.91472. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.90001/4.91221. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.89737/4.92637. Took 0.21 sec\n",
      "Epoch 95, Loss(train/val) 4.89689/4.93182. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.89348/4.93396. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.89432/4.92645. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.89448/4.92896. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.89330/4.94114. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.055227791305300936\n",
      "Epoch 0, Loss(train/val) 4.77212/4.76484. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.72999/4.75553. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.72880/4.75497. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.72180/4.74900. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.72268/4.74839. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.72151/4.75035. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.72572/4.74576. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.72194/4.74122. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.72399/4.74092. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.71696/4.74165. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 4.71586/4.74435. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.71591/4.74702. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.71806/4.74999. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.71532/4.74655. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.71850/4.74197. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.71300/4.74330. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.71464/4.73875. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.71394/4.73927. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.71201/4.74169. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.71102/4.74745. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.70943/4.75864. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.71191/4.74729. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.70825/4.74975. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.70590/4.76138. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.71367/4.75097. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.70744/4.75911. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.70597/4.76817. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.70780/4.75887. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.70881/4.76037. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.70293/4.77176. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.69902/4.77392. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.70449/4.76343. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.70593/4.76701. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.70066/4.78259. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.69669/4.79236. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.70329/4.79366. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.70033/4.77334. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.70068/4.80048. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 4.69826/4.78993. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.70025/4.78755. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.69647/4.80596. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.69374/4.80300. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.69875/4.78236. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.69829/4.80324. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.69751/4.79084. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.69544/4.80401. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.69926/4.79578. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.69727/4.80195. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.70048/4.80844. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.69575/4.79844. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.68928/4.81940. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.69422/4.80974. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.69625/4.80467. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.69852/4.80202. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.69258/4.82135. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.68705/4.82239. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.69227/4.82039. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.68806/4.82147. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.69459/4.81074. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.69044/4.82597. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.69872/4.80191. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.69169/4.82449. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.68525/4.82077. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.69140/4.83624. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.69124/4.82394. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.69531/4.80955. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.69279/4.82338. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.68536/4.81719. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.69460/4.78327. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.69128/4.82613. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.68826/4.82940. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.69539/4.80322. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.68651/4.83592. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.69281/4.82777. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.69142/4.81285. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.68977/4.83063. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.69128/4.82016. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.68507/4.83007. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.68678/4.81112. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.68722/4.84542. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.68909/4.83996. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.68394/4.86067. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.68657/4.85813. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.68556/4.83686. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.68801/4.79520. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.69466/4.81728. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.69110/4.80980. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.68707/4.86567. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.68987/4.83822. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.69129/4.81276. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.68927/4.83502. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.68492/4.81534. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.68479/4.83704. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.68956/4.84598. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.69011/4.82485. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.68488/4.86502. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.69010/4.83905. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.68555/4.84353. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.68343/4.87229. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.68771/4.82435. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.06825406626599889\n",
      "Epoch 0, Loss(train/val) 4.95086/4.91580. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.89461/4.86009. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.88978/4.87089. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.86856/4.86600. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.86039/4.86312. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.86297/4.86590. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.86728/4.87055. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.86345/4.87136. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.86319/4.86965. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.86142/4.86895. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.86126/4.86965. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.85927/4.87170. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.86125/4.87182. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.85892/4.87082. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.85783/4.87256. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85574/4.87319. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85911/4.87092. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85624/4.87469. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85682/4.87046. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.85450/4.87198. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.85487/4.87087. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.85417/4.87687. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85355/4.87558. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85207/4.87811. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85195/4.87757. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.85212/4.88348. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.85160/4.87437. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.85159/4.87745. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.85763/4.88420. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84831/4.89973. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84693/4.90890. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.84947/4.89219. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.84616/4.89247. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.85039/4.90300. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84945/4.90137. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84589/4.88740. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84034/4.90490. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84881/4.89520. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84330/4.91118. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84280/4.91112. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84534/4.90257. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84252/4.89962. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.84169/4.91064. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.84035/4.91369. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.83715/4.91533. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.83980/4.91011. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84095/4.90452. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.83610/4.92833. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.84117/4.90519. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.84471/4.91358. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.84046/4.92168. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.83505/4.92654. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.84228/4.90992. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.83239/4.93206. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.83816/4.92671. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.83659/4.92680. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.83918/4.93209. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.84046/4.91453. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.83395/4.93052. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.83117/4.93979. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.83025/4.93470. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.83082/4.93289. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.83863/4.92932. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83172/4.94573. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.83524/4.91439. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.83724/4.92630. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.83396/4.92125. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.83497/4.94211. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.83121/4.94352. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.83163/4.94234. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.83132/4.93694. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.83190/4.93763. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.82517/4.96983. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83436/4.91785. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.82906/4.94467. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.82519/4.96774. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.82869/4.94966. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.82968/4.94515. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.82933/4.95148. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.82981/4.94739. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.83385/4.92111. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83561/4.95356. Took 0.21 sec\n",
      "Epoch 82, Loss(train/val) 4.82982/4.93585. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.82585/4.96943. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.82702/4.93603. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.82835/4.96337. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.82762/4.96321. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.83335/4.94819. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.82525/4.95631. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.83090/4.94344. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82708/4.93822. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.83601/4.95843. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82809/4.95913. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.81770/4.97229. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83222/4.92408. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.83142/4.99298. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83315/4.97554. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83365/4.94818. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82334/4.98706. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.82559/5.00324. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.1563263498701806\n",
      "Epoch 0, Loss(train/val) 5.05782/5.09094. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.02858/5.07595. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.02744/5.06034. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.02234/5.05908. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.02232/5.06088. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.02382/5.05058. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 5.02437/5.03429. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 5.02208/5.02720. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 5.02047/5.02575. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.01712/5.03094. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 5.01809/5.03617. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.01887/5.03718. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.01793/5.03679. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.01581/5.03662. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.01856/5.03177. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.01513/5.02821. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.01643/5.03754. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.01523/5.03689. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 5.01432/5.03640. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.01120/5.04132. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.01142/5.04230. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.01197/5.03967. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.00886/5.05488. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.01203/5.03944. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.01132/5.03310. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.01012/5.04806. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 5.00737/5.05838. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.01126/5.04625. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.01229/5.04010. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 5.00646/5.04859. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.00688/5.04713. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.00349/5.05755. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.00615/5.05450. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.00983/5.06117. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 5.00988/5.05177. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.00289/5.04191. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.00802/5.05939. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.00543/5.06377. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.00459/5.06412. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.00381/5.07202. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.00461/5.05778. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.00416/5.07491. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.00036/5.07248. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.00029/5.06615. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.99909/5.06806. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.00149/5.07414. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.00003/5.06186. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.99821/5.07796. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.99957/5.06570. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.00247/5.06571. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.00854/5.04446. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.01137/5.05081. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.00877/5.04258. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 5.00377/5.05492. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.99855/5.07187. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.00581/5.05734. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 5.00048/5.06067. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 5.00088/5.06721. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.99881/5.07695. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.99607/5.07967. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.99579/5.09617. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.99716/5.08136. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.99673/5.08178. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.99619/5.08955. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.99485/5.09006. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.99392/5.08694. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.99329/5.08415. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.99757/5.09220. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.99115/5.09183. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.99777/5.08433. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.99339/5.08535. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.99104/5.09935. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.99312/5.09106. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.98977/5.10099. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.99173/5.10872. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.99267/5.08829. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.99449/5.08545. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.98721/5.09394. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.98168/5.11756. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.99063/5.10571. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.99288/5.08735. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.99700/5.10661. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.98894/5.11129. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.99117/5.11124. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.99239/5.07510. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.99529/5.10557. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.99739/5.09157. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.99147/5.09945. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.98931/5.10557. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.99881/5.08672. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.99196/5.08705. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.99401/5.09779. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.98586/5.10605. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.99157/5.09182. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.98996/5.09862. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.98243/5.09333. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.99414/5.09513. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.02629/5.03668. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.01540/5.03410. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.01771/5.02807. Took 0.19 sec\n",
      "ACC: 0.375, MCC: -0.2324766006178762\n",
      "Epoch 0, Loss(train/val) 4.95622/4.89376. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.89418/4.88556. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.89935/4.89392. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.89551/4.90312. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.89791/4.89787. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.89351/4.90374. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.89394/4.90243. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.88865/4.90404. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.89042/4.90804. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.89026/4.90922. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.88993/4.89340. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.89067/4.89344. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.88689/4.88826. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.88914/4.88621. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.88872/4.88245. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.89130/4.89506. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.88869/4.87808. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.88488/4.89762. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.88363/4.89601. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.88423/4.89593. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.88251/4.89846. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.88030/4.88255. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.88103/4.92199. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.88410/4.88777. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.88221/4.90335. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.88341/4.88940. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.87535/4.91673. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.88051/4.91935. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.87873/4.92111. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.87666/4.92280. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.88450/4.89000. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.87635/4.92426. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.87645/4.92142. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.87555/4.92336. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.87725/4.92047. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.87149/4.92193. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.87805/4.92330. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.87459/4.91706. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.88135/4.87199. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.87958/4.90309. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.87492/4.93066. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.87989/4.92463. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.88052/4.92614. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.87287/4.93673. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.87606/4.92414. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.87380/4.93519. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.87620/4.92570. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.86924/4.92467. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.87276/4.92678. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.87440/4.93093. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.87609/4.88529. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.87496/4.95944. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.87300/4.93615. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.87251/4.93756. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.87055/4.94092. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.87514/4.92690. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.86719/4.92972. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.87163/4.93469. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.86303/4.94136. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.86223/4.93615. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.87008/4.93349. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.86819/4.93680. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.86494/4.94637. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.86802/4.91427. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.86494/4.92064. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.86876/4.93238. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.86557/4.92952. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.86946/4.93606. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.86779/4.93975. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.86204/4.95716. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.86002/4.94107. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.86858/4.96389. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.86939/4.92930. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.87507/4.91435. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.86460/4.92236. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.87238/4.93337. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.86820/4.92880. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.86928/4.92539. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.86855/4.93335. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.86999/4.91857. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.86581/4.92828. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.86545/4.93475. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.86881/4.90735. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.87128/4.93805. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.86894/4.93607. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.87441/4.90739. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.87913/4.88524. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 4.87075/4.94646. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.86816/4.90595. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.86611/4.91260. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.86179/4.90533. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.86287/4.91974. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.86034/4.94763. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.85928/4.93727. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.86337/4.94595. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.85773/4.93943. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.86211/4.97421. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.85766/4.94069. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.86153/4.96991. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.85610/4.93506. Took 0.19 sec\n",
      "ACC: 0.390625, MCC: -0.2333182529224123\n",
      "Epoch 0, Loss(train/val) 4.91657/4.88095. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.87540/4.88950. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.87022/4.88785. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87020/4.89028. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.86899/4.89472. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.86824/4.89727. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.87022/4.89395. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.86705/4.89645. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.86621/4.90033. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.86336/4.89945. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.85924/4.90153. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.86236/4.89874. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85946/4.90055. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85850/4.90222. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.85905/4.90198. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85967/4.89810. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.86148/4.89692. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85907/4.90100. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85924/4.90129. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.86159/4.89593. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.85864/4.90371. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85951/4.90400. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85805/4.90489. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85883/4.90384. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85662/4.91474. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.85861/4.89997. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.85736/4.90325. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.85719/4.90943. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.85598/4.90067. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.85570/4.90488. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.85581/4.90389. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.85407/4.90283. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.85215/4.90175. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.85485/4.89886. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85148/4.90444. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84954/4.90262. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.85069/4.89673. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.85053/4.90416. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.85246/4.89765. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.84739/4.90603. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.85322/4.89192. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.85167/4.89821. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.84572/4.90411. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.84989/4.89491. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84858/4.89612. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84950/4.89410. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84995/4.90499. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84929/4.89226. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.85060/4.90108. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.85177/4.88208. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.84651/4.89965. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 4.84864/4.87928. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.85071/4.88660. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.84742/4.88192. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.84248/4.88982. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.84344/4.87794. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84739/4.88406. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.84638/4.88659. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.84466/4.87594. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.84651/4.87627. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.84531/4.87695. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.83819/4.87659. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.84795/4.87605. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.84233/4.88351. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.84294/4.88411. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.84247/4.88489. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.84644/4.87827. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.84190/4.88020. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.83947/4.88657. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84064/4.88265. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.84276/4.88190. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.84039/4.88702. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.83808/4.87053. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83965/4.88374. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.83949/4.88520. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.83961/4.88626. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83798/4.87540. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.84489/4.87663. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.84234/4.88927. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.83795/4.88747. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.83720/4.88710. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.84255/4.88214. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.83352/4.88188. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.83516/4.89133. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.84290/4.87674. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83548/4.88254. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.84168/4.87712. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.83693/4.87491. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83711/4.87805. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.84040/4.87527. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.83646/4.87886. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.83711/4.88231. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.83300/4.88265. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83160/4.88091. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83698/4.88780. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.83745/4.87589. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83362/4.89403. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.83575/4.88456. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.83466/4.87653. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.83065/4.88450. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.07539645724831788\n",
      "Epoch 0, Loss(train/val) 5.00581/5.10267. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.98812/4.98617. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.99783/4.96754. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.00183/4.96872. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.98656/4.97235. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.97691/4.96821. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.97992/4.96840. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.97681/4.96905. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.97930/4.97085. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.97826/4.97148. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.97588/4.96923. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.97592/4.96883. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 4.97530/4.96858. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.97533/4.97115. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.97247/4.97724. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.97184/4.97900. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.97076/4.97291. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.96954/4.97934. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.96668/4.97747. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.96820/4.98230. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.96424/4.97806. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.96366/4.98872. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.95964/4.99061. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.96420/4.99117. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.96019/5.00039. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.95593/4.99961. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.95835/4.99360. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.95988/4.99388. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.95455/4.99487. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.95659/5.01137. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.95683/5.00524. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.95602/4.98216. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.95226/5.01323. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 4.95585/5.00894. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.95745/4.99498. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 4.95064/5.01890. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.95441/5.00820. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.95112/5.02280. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.94811/5.01799. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.95048/5.02460. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.97142/4.97368. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.97501/4.95613. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.96780/4.95685. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.96617/4.97616. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.96430/4.97642. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.95852/4.98336. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.95336/5.00173. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.95673/4.99999. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.95423/4.99305. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.95520/5.00576. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.96150/4.98579. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.95380/4.98753. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.95206/5.01450. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.95344/4.99814. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.95499/4.99394. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.95757/4.98750. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.95567/4.98354. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.95136/4.98949. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.95220/4.99179. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.95008/4.98720. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.95090/5.00026. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.94873/4.98443. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.95450/4.99175. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.94711/5.00188. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.94613/5.00197. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.94739/4.99966. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.94306/5.00915. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.94653/5.00651. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.94818/5.01086. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.94898/5.00415. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.95048/4.99651. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.94547/5.00251. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.94871/5.01404. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.94314/5.01034. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.94259/5.00729. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.94875/5.00506. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.94192/5.02980. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.94415/5.01600. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.94550/5.01573. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.94993/5.00939. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.94536/5.01930. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.94502/5.02799. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.94612/5.02433. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.94632/5.02295. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.94702/5.00599. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.95000/5.01242. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.93837/5.02325. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.94428/5.02549. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.94588/5.00431. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.94365/5.01019. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.94092/5.02316. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.94323/5.00649. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.94167/5.02310. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.94108/5.01380. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.94238/5.02130. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.93941/5.01214. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.94024/5.04144. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.94152/5.02128. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.94024/5.02090. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.93596/5.02218. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: -0.010188710438961876\n",
      "Epoch 0, Loss(train/val) 5.12712/5.01443. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.03983/5.03222. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.03506/5.04237. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.03484/5.04073. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 5.03258/5.03372. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 5.03364/5.02968. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.03601/5.03549. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.03326/5.03039. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.03432/5.02900. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.03067/5.03028. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.03354/5.03771. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.03228/5.04409. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.03120/5.05028. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.02731/5.05031. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.02896/5.05295. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.02794/5.05835. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.02469/5.05324. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.02814/5.04866. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 5.03186/5.04606. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.02731/5.04980. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.03058/5.05888. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.02661/5.04945. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.02774/5.04910. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.02748/5.05161. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.02412/5.05246. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.02718/5.04423. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.02575/5.04768. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.02586/5.05363. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.02426/5.05188. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 5.02418/5.05036. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.02269/5.04883. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.02046/5.05203. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.02158/5.06431. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.01845/5.05619. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.01896/5.04686. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.02256/5.05266. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.01661/5.06328. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.02626/5.06391. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.01706/5.07475. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.02263/5.06997. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.02107/5.06022. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.01539/5.06447. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.01866/5.05925. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.01439/5.06476. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.01720/5.06958. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.02037/5.06732. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.01599/5.06492. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.02048/5.05690. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.00942/5.04787. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.01572/5.06740. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.01423/5.06200. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.01618/5.06385. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.01787/5.07131. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.01317/5.06010. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 5.01210/5.06531. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 5.00909/5.07681. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 5.01421/5.06440. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 5.01613/5.05128. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.01042/5.07220. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.01367/5.07530. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.01370/5.06820. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.01416/5.06008. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.01094/5.06288. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.01505/5.07198. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.01340/5.07352. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 5.01296/5.06735. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 5.00847/5.08373. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.01266/5.05941. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.01120/5.06898. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.00344/5.06940. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.00745/5.07420. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.00952/5.06597. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.00588/5.07796. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.01063/5.07276. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.00818/5.06708. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 5.01116/5.06957. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 5.00637/5.05755. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.00386/5.06991. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 5.00559/5.08522. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.00803/5.07615. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.00514/5.07409. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.00598/5.08696. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.00016/5.06811. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 5.00938/5.07281. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.00481/5.06787. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.00341/5.09079. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.00622/5.05983. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.01000/5.06658. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.00159/5.07206. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.00461/5.06723. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.00260/5.06945. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.99754/5.08048. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.99459/5.07905. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.01196/5.06254. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.99653/5.08228. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.00387/5.08063. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.00698/5.06050. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.00144/5.09033. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.00384/5.07206. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.00232/5.05707. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.059863071616150634\n",
      "Epoch 0, Loss(train/val) 4.78546/4.72212. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.73346/4.73480. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.73290/4.75627. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.73732/4.75419. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.73771/4.73867. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.73713/4.73160. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.73397/4.73870. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.72981/4.74012. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.72742/4.74142. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.72682/4.74179. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.72665/4.75361. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.72949/4.74453. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.72410/4.75520. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.72606/4.75978. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.72544/4.75477. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.72317/4.76611. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.72367/4.77296. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.72557/4.75961. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.72382/4.77655. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.72591/4.75422. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.72308/4.76866. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.72381/4.76405. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.72200/4.77495. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.72437/4.76819. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.71989/4.77610. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.71946/4.77978. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.72035/4.78125. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.71773/4.77458. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.71425/4.78319. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.71223/4.79771. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.71821/4.76634. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.71520/4.77276. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.71485/4.78437. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.71537/4.77464. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.72699/4.74154. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.71750/4.77541. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.71633/4.76252. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.71698/4.75889. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.70657/4.78969. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.71386/4.76471. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.71092/4.76576. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.71078/4.76821. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.71037/4.76497. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.71100/4.75732. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.70867/4.77477. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.70921/4.76055. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.70934/4.75942. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.70898/4.75639. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.70828/4.75933. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.70565/4.76902. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.70566/4.77322. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.71277/4.75064. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.71002/4.74217. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.70616/4.79594. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.70801/4.74784. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.72276/4.76334. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.71839/4.75670. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.71606/4.76236. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.71359/4.76850. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.71045/4.76831. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.71792/4.75029. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.71287/4.77651. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.70759/4.78361. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.70357/4.78428. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.70636/4.76691. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.70766/4.76788. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.70431/4.76803. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.70058/4.78624. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.70801/4.75567. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.70626/4.76809. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.69996/4.77116. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.70607/4.76410. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.70208/4.77314. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.70623/4.76562. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.69649/4.76751. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.70627/4.74076. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.70423/4.77977. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.70008/4.75719. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.70061/4.77824. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.70842/4.74956. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.70653/4.75911. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.70198/4.75552. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.69835/4.76837. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.70134/4.76165. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.69544/4.78025. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.70254/4.74923. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.69811/4.77250. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.69964/4.77759. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.69735/4.75931. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.69609/4.77225. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.69623/4.74908. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.69505/4.75522. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.71370/4.75584. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.71050/4.74720. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.70654/4.74792. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.70997/4.74347. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.70464/4.77263. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.69888/4.75698. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.70687/4.74464. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.70053/4.77242. Took 0.20 sec\n",
      "ACC: 0.46875, MCC: -0.08544439848099883\n",
      "Epoch 0, Loss(train/val) 4.77764/4.72733. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.72376/4.71975. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.71617/4.72585. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.71800/4.72629. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.71747/4.72538. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.71718/4.72793. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.71799/4.73180. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.71755/4.73399. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.71480/4.73454. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.71357/4.73733. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.71251/4.73830. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.70871/4.74425. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.70883/4.74216. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.70723/4.74719. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.71190/4.72710. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.71696/4.72595. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.71276/4.73595. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.70915/4.72821. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.71207/4.72822. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.71046/4.72108. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.70691/4.72974. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.70764/4.73147. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.70658/4.73175. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.70668/4.72248. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.70625/4.72493. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.70215/4.72672. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.70563/4.73409. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.70440/4.73478. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.70948/4.72545. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.70154/4.75753. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.70731/4.75920. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.71039/4.75131. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.70772/4.75264. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.70602/4.75091. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.70334/4.75377. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.70428/4.74452. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.70279/4.75076. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.70579/4.74833. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.70190/4.74416. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.70261/4.75655. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.70319/4.74869. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.70064/4.76220. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.69910/4.75893. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.69568/4.76793. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.70311/4.74955. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.69745/4.76139. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.69337/4.77506. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.69626/4.75889. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.69831/4.77759. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.69318/4.78065. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.69212/4.78819. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.69466/4.77947. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.69342/4.77740. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.69247/4.77885. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.68795/4.77562. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.68865/4.77953. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.69560/4.77007. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.68372/4.79504. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.68959/4.80026. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.69038/4.76642. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.68287/4.77569. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.68710/4.78010. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.68950/4.78243. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.69015/4.80501. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 4.68491/4.81747. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.68267/4.79017. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.68401/4.81333. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.68306/4.79779. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.68215/4.83336. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.68343/4.79281. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.68936/4.79384. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.68508/4.81460. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.68350/4.78389. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.67893/4.79627. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.68102/4.79449. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.67734/4.81636. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.68458/4.79852. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.68537/4.80916. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.67973/4.80223. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.68425/4.80364. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.67352/4.79767. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.67714/4.81033. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.68313/4.80136. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.68422/4.79921. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.67685/4.79979. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.67585/4.84995. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.67728/4.82351. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.67641/4.81480. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.68076/4.81502. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.67755/4.83746. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.67730/4.79542. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.68196/4.79708. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.67803/4.84241. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.67623/4.88352. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.67740/4.81460. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.67655/4.82296. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.67548/4.84628. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.67640/4.86892. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.66996/4.85046. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.67727/4.81423. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.14180020247260586\n",
      "Epoch 0, Loss(train/val) 4.96581/4.93347. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 4.93065/4.92664. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.92908/4.93311. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.93068/4.93626. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.93596/4.93228. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.94195/4.92954. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.92813/4.93329. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.92424/4.93265. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.92521/4.93740. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.92451/4.94244. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.92315/4.94599. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.92269/4.94731. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.92494/4.94507. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.92059/4.94979. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.92063/4.95064. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.92053/4.95905. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.91975/4.95036. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.91655/4.96517. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.92177/4.94672. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.91938/4.95600. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.91660/4.94716. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.91484/4.95451. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.91850/4.93951. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.91301/4.94073. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.91617/4.93691. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.91262/4.94860. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.90881/4.95581. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.91338/4.96442. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.91163/4.95118. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.91268/4.95965. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.91450/4.96925. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.90983/4.96555. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.91316/4.96581. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.90738/4.97608. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.90472/4.97616. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90595/4.97375. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.90782/4.97509. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.90333/4.97462. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.90362/4.97796. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90596/4.97134. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90731/4.98073. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.90512/4.97781. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.90577/4.98406. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.90014/4.98688. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.90582/4.99061. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.90059/4.96714. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.90258/4.97240. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.90760/4.96909. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.90021/4.97383. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.90548/4.97964. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89832/4.97610. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 4.90808/4.94341. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.90828/4.95281. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.89701/4.97222. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.90093/4.97939. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.90282/4.95886. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.90754/4.97563. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.90883/4.95748. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.90282/4.96829. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 4.90557/4.97150. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.90434/4.98734. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.90157/4.97980. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.90350/4.98407. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.89484/4.98107. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.90026/4.97033. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.89495/4.99371. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.90194/4.99163. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.89990/4.98412. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.89592/4.98912. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.89819/4.97858. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.89561/5.03611. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.90117/4.96275. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.90718/4.95136. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.90923/4.96694. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.90383/4.96225. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.90413/4.96188. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.90366/4.95340. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.90445/4.95308. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.89894/4.97366. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.89813/4.96650. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.89908/4.94309. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.90008/4.95223. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.89642/4.97396. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.90017/4.98086. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.89737/4.97674. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.89363/4.97391. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.89443/4.98904. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.90068/4.96924. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.89778/4.97925. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.89949/4.98199. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.89407/4.97269. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.89684/4.96548. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.89546/4.97046. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.90086/4.99548. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.89396/5.00026. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.89220/4.97412. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.89308/4.99006. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.89410/5.00002. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.89434/5.00854. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.89664/4.99197. Took 0.20 sec\n",
      "ACC: 0.5625, MCC: 0.09759000729485331\n",
      "Epoch 0, Loss(train/val) 4.94781/4.88067. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.89042/4.87207. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.88287/4.87092. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.88230/4.86635. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.87858/4.86146. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.87778/4.86313. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.87932/4.86031. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.88091/4.85953. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.88070/4.86169. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87653/4.86060. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.87716/4.86267. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.87699/4.86273. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.87393/4.86560. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.87347/4.86696. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87335/4.86753. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.87572/4.87272. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.87348/4.87734. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.87295/4.88158. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.87068/4.88444. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.87083/4.88747. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.86841/4.89364. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.86849/4.89546. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.86440/4.90731. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.86610/4.90754. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 4.86877/4.90652. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.86486/4.90682. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.86480/4.91404. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.86559/4.92404. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.86721/4.91500. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.86177/4.93248. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.86172/4.92989. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86912/4.91563. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.86340/4.91744. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.86510/4.92137. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.86378/4.92211. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.86405/4.92918. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.86296/4.93188. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.85999/4.93218. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.85946/4.92943. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.85969/4.94576. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.86237/4.93026. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.86102/4.94078. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.85764/4.94527. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.86139/4.93203. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.86162/4.92896. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.85800/4.94652. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.85633/4.94834. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.85594/4.94654. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.85435/4.94879. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.86158/4.93806. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.85301/4.96596. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.85970/4.95181. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.85469/4.95395. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.86009/4.94587. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.85306/4.96379. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.85205/4.97949. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.85939/4.94038. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.85650/4.95450. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.85513/4.94264. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.84980/4.96169. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85756/4.95204. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.85714/4.93374. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.84541/4.97158. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.85028/4.96458. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.84847/4.98306. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.85751/4.95344. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.87048/4.90132. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.86438/4.90640. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.86107/4.91882. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.85802/4.95070. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.85492/4.93855. Took 0.22 sec\n",
      "Epoch 71, Loss(train/val) 4.84782/4.96202. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.84913/4.96261. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.85212/4.96963. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.85510/4.94885. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.84525/4.97301. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.86866/4.90661. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.86414/4.89589. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.85696/4.92399. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.85215/4.93099. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.85433/4.94415. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.85905/4.95694. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.85014/4.96131. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.85208/4.97075. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.85726/4.95085. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.84959/4.97122. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.84902/4.96539. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.85536/4.94643. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.85232/4.98077. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.84518/4.97908. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.84890/4.96132. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85156/4.96005. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.85006/4.97071. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.84557/4.99122. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.85115/4.97662. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.84933/4.94696. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.84859/4.95840. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.84968/4.95574. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.84479/4.97435. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.84419/4.96920. Took 0.20 sec\n",
      "ACC: 0.59375, MCC: 0.19487030513625772\n",
      "Epoch 0, Loss(train/val) 4.76650/4.76805. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.75080/4.74616. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.73836/4.75695. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.73804/4.75989. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.73606/4.76274. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.73997/4.76970. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.74133/4.77817. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.74402/4.76929. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.74306/4.76138. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.73599/4.76872. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.73555/4.76184. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.73645/4.76321. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.73776/4.76393. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.73687/4.76530. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.73516/4.76451. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.73384/4.77097. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.73528/4.76478. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.73437/4.76455. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.73072/4.77308. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.72966/4.77094. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.73267/4.76578. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.73126/4.77139. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.73190/4.77525. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.73274/4.76870. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.72930/4.77760. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.73000/4.76573. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.73588/4.76923. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 4.73163/4.77801. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.73165/4.76808. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.73056/4.77153. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.73185/4.76388. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.72781/4.77413. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.72994/4.76790. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.72849/4.76866. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.72750/4.77094. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.72954/4.76281. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.72798/4.77977. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.72592/4.77781. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.72875/4.78245. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.72400/4.79258. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.72657/4.77888. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.72399/4.80437. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 4.71921/4.80050. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.72548/4.78487. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.72453/4.79421. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.72001/4.79456. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.72367/4.80606. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.72008/4.80019. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.72316/4.80367. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.72327/4.79926. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.73104/4.76476. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.73067/4.77307. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.72951/4.78208. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.72620/4.79237. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.72660/4.78849. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.72331/4.80492. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.72260/4.80466. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.72643/4.79733. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.71707/4.81665. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.71702/4.80149. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.72582/4.79697. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.72167/4.81264. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.72261/4.80131. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.71760/4.82155. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.72207/4.81843. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.71937/4.81283. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.71897/4.81414. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.71688/4.83578. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.71370/4.83893. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.71826/4.81461. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.71820/4.82628. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.71785/4.82379. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.71146/4.82954. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.71796/4.82352. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.71152/4.82693. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.71489/4.83971. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.71574/4.82323. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.70809/4.87810. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.71345/4.81259. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.70651/4.86127. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.71517/4.84286. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.70560/4.85494. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.72068/4.79789. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.71638/4.84050. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.71073/4.81844. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.70957/4.83669. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.70886/4.85662. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.71365/4.85639. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.70879/4.86153. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.70665/4.87901. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.71263/4.85391. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.70244/4.90049. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.71414/4.86372. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.70387/4.86784. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.70299/4.84030. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.70548/4.85435. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.70023/4.84504. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.69794/4.89744. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.70391/4.87952. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.70736/4.87202. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.09416472308615167\n",
      "Epoch 0, Loss(train/val) 4.71279/4.68779. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.67520/4.67489. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.66878/4.68760. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.66836/4.69438. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.66907/4.68744. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.66988/4.68895. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.67052/4.68585. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.66808/4.68685. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.66874/4.68439. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.66982/4.68659. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.67225/4.69123. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.66717/4.69387. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.67032/4.69592. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.66896/4.69853. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.66843/4.69620. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.66777/4.69527. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.66798/4.69839. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.66879/4.69464. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.66662/4.69642. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.66966/4.69391. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.66538/4.69147. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.66702/4.69247. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.66609/4.69252. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.66745/4.69275. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.66635/4.69386. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.66385/4.69634. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.66627/4.69549. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.66665/4.69306. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 4.66205/4.70329. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.66284/4.70308. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.66247/4.70105. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.66536/4.70602. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.66123/4.70041. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.65884/4.70216. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 4.66415/4.69643. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.66390/4.69902. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.66375/4.69706. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.66132/4.70354. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.65990/4.70576. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.66072/4.69340. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.66023/4.70572. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.65873/4.70015. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.66005/4.70039. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.65888/4.69997. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.65906/4.69885. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.66180/4.70421. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.65674/4.70238. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.65379/4.70017. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.66140/4.68461. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.65382/4.70405. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.65032/4.72431. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.65822/4.69239. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.65399/4.70120. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.65278/4.70860. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.64985/4.70646. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 4.65495/4.70176. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.65180/4.69823. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.65515/4.70017. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.65809/4.70481. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.65613/4.70745. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.65216/4.70638. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.65626/4.70149. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.65314/4.71716. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.65135/4.71335. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.64798/4.71029. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.64872/4.70919. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.65104/4.70575. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.65027/4.70670. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.64687/4.71533. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.65012/4.70027. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.64820/4.70337. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.64813/4.70151. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.65015/4.69936. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.64698/4.70077. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.64516/4.70024. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.65226/4.70519. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.64746/4.70415. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.65406/4.69053. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.64656/4.71881. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.64088/4.70901. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.64264/4.70362. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.64257/4.71486. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.63787/4.71385. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.64696/4.71507. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.64595/4.71124. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.64919/4.70845. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.65062/4.71036. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.64694/4.70851. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.64536/4.71324. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.63909/4.72813. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.64390/4.71783. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.63790/4.72374. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.64019/4.71382. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.64398/4.72607. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.64315/4.71506. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.63928/4.73618. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.64488/4.71940. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.64340/4.72276. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.63410/4.74043. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.64393/4.73382. Took 0.19 sec\n",
      "ACC: 0.34375, MCC: -0.3068118704595474\n",
      "Epoch 0, Loss(train/val) 4.81455/4.75002. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.74381/4.74707. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.74513/4.75148. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.75040/4.75674. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.74520/4.75537. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.74765/4.75143. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.74855/4.75096. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.74761/4.75037. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.74573/4.75016. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.74230/4.75239. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.74019/4.75489. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.73989/4.75429. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.74035/4.75453. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.73982/4.75859. Took 0.18 sec\n",
      "Epoch 14, Loss(train/val) 4.73802/4.75640. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.73715/4.75164. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.73774/4.75436. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.73796/4.75362. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.73822/4.74457. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.73506/4.74175. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.73080/4.75072. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.72999/4.74405. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.73505/4.74833. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.73393/4.74649. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.73468/4.74445. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 4.73004/4.75189. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.72972/4.75360. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.72440/4.74729. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.73319/4.74125. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.73304/4.74521. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.72616/4.73873. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.72685/4.73941. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.72520/4.75025. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.72340/4.74732. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.72414/4.74371. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.71990/4.75481. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.72815/4.73536. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.72025/4.75993. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.72279/4.74902. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.72395/4.73978. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.72496/4.74767. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.72232/4.75651. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.72278/4.75012. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.71834/4.76687. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.72665/4.74753. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.72302/4.73988. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.72374/4.75523. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.71295/4.74724. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.71841/4.76651. Took 0.18 sec\n",
      "Epoch 49, Loss(train/val) 4.71962/4.74996. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.71984/4.75167. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.71426/4.75462. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.71808/4.76175. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.71752/4.74673. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.71552/4.75687. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.71324/4.74620. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.71531/4.76196. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.72169/4.74528. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.71636/4.74794. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.71349/4.75925. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.71272/4.74821. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.71946/4.73890. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.71863/4.76030. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.71217/4.76251. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.71775/4.74860. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.71028/4.76609. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.71698/4.75626. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.72102/4.74291. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.72255/4.75747. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.71300/4.75660. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.71031/4.75240. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.71510/4.75301. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.71513/4.75048. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.71790/4.75828. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.71389/4.74682. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.71603/4.73619. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.71519/4.76289. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.72676/4.75151. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.72063/4.77949. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.71464/4.76609. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.71364/4.78144. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.71277/4.77881. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.71308/4.78769. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.71009/4.77183. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.71556/4.79588. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.71322/4.77451. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.71550/4.75142. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.71121/4.78363. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.71041/4.74952. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.70895/4.77030. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.70995/4.76362. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.70450/4.76827. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.71237/4.77644. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.70869/4.77410. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.70651/4.77017. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.70897/4.76442. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.70607/4.75771. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.71364/4.78335. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.70223/4.79079. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.70860/4.75693. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.18963796023622476\n",
      "Epoch 0, Loss(train/val) 4.95540/4.89016. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.90613/4.89745. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.90972/4.88915. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.90883/4.89303. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 4.89656/4.88852. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.89376/4.88793. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.89559/4.89053. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.89227/4.88977. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.89059/4.89115. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.88934/4.88568. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.88977/4.88106. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.88336/4.88603. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.88942/4.88070. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.88659/4.88406. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.88151/4.88130. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.88415/4.88467. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.88561/4.87768. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.88153/4.88326. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.87998/4.88598. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.88263/4.88122. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.88037/4.88137. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.88213/4.88008. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.88006/4.88465. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.88484/4.88254. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.87906/4.87912. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.88061/4.88811. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.87391/4.88184. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.87631/4.88158. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.87885/4.87794. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.87856/4.88508. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.87546/4.88161. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.87569/4.88116. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.87142/4.87363. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.87379/4.87913. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.87631/4.88052. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.87343/4.88288. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.87526/4.87169. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.87614/4.87456. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.87578/4.87457. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.87535/4.87160. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.87731/4.87466. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.87772/4.88276. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.87166/4.88604. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.87184/4.88037. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.86859/4.88214. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.87097/4.87906. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.86816/4.88199. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.87654/4.87557. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.86875/4.87411. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.86997/4.88586. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.86745/4.88432. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.87186/4.87863. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.87087/4.87729. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.87309/4.88274. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.87069/4.88230. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.86891/4.88701. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.86953/4.88525. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 4.86786/4.88210. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.86889/4.87588. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.86435/4.88410. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.86837/4.89352. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.87927/4.87599. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.87155/4.87666. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.86680/4.88662. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.86467/4.88898. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.87082/4.87499. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.86543/4.87796. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.87207/4.86838. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.86894/4.87908. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.86886/4.87782. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.86693/4.88605. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.86085/4.88043. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.87097/4.86771. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.86720/4.87511. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.86307/4.87011. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.86655/4.87526. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.86502/4.88236. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.86316/4.87189. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.86508/4.88018. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.86445/4.86889. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.86636/4.87933. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.86159/4.87829. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.86574/4.85965. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.87632/4.86507. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.87337/4.87137. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.87623/4.87187. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.87359/4.87294. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.86716/4.86779. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.87553/4.87604. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.85998/4.89767. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.86847/4.88740. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.86647/4.88817. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.86756/4.87248. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.86563/4.87978. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.86296/4.87051. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.86676/4.88847. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.86917/4.86826. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.86398/4.88921. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.86459/4.89434. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.86295/4.88555. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.040082172520323485\n",
      "Epoch 0, Loss(train/val) 5.14283/5.02858. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.02807/5.03645. Took 0.22 sec\n",
      "Epoch 2, Loss(train/val) 5.02395/5.04213. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.02334/5.04186. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.02526/5.04283. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.02764/5.04790. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.02764/5.04611. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.02634/5.04432. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.02547/5.04965. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.02610/5.04818. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.02444/5.04666. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.02090/5.04930. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.02112/5.04653. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.02000/5.04564. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.02112/5.04786. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.01807/5.05046. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 5.01999/5.04729. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 5.01844/5.04584. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 5.01575/5.05118. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.01998/5.05418. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 5.01916/5.04739. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.01660/5.04881. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.01289/5.04734. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.01213/5.04696. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.02040/5.03483. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 5.01909/5.04299. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 5.01525/5.05227. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.00976/5.06175. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.01230/5.04411. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.00973/5.05277. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 5.01276/5.05358. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 5.01010/5.04681. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.00924/5.04713. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.00953/5.04692. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.00416/5.05008. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.00645/5.04381. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.00602/5.04448. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.00323/5.04235. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 5.00379/5.04227. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.00263/5.04627. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.00230/5.02967. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.99647/5.03301. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.99858/5.06122. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.99884/5.03740. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.99111/5.03680. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.99720/5.01589. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.99491/5.04622. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.99452/5.03518. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.99452/5.05283. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.99244/5.05034. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.99245/5.03811. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.99686/5.03865. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.99290/5.03563. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.99530/5.02645. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.98908/5.05489. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.99412/5.02990. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.98567/5.02780. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.98723/5.06672. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.99481/5.02377. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.98653/5.04049. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.98786/5.04593. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.00349/5.01143. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.98993/5.05971. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.99701/5.02025. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.98439/5.04543. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.99120/5.02322. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.99071/5.03913. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.98794/5.02925. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.98437/5.04045. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.99017/5.03164. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.98832/5.02714. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.98567/5.04408. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.98400/5.01940. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.98995/5.03530. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.98574/5.04497. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.98545/5.02913. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.98382/5.03111. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.98374/5.04696. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.98655/5.02043. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.99244/5.03122. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.98150/5.03727. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.98866/5.01867. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.98447/5.05004. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.98809/5.03453. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.98657/5.03192. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.98194/5.03692. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.98598/5.03514. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.97799/5.02496. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.98765/5.03289. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.97845/5.05803. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.98024/5.04350. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.98957/5.03425. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.98265/5.05192. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.98502/5.02528. Took 0.21 sec\n",
      "Epoch 94, Loss(train/val) 4.98026/5.03090. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.97599/5.06824. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.98546/5.03087. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.97563/5.05381. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.98363/5.02734. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.98284/5.04510. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.17189402517942118\n",
      "Epoch 0, Loss(train/val) 4.85794/4.81083. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.82082/4.80973. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.81767/4.80484. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.81572/4.81326. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.81280/4.81116. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.80811/4.80723. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.80391/4.80725. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.80431/4.80878. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.80583/4.81426. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.80373/4.81094. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.80280/4.80946. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.80042/4.81148. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.80324/4.81454. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79966/4.82024. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.79934/4.81543. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.80200/4.81023. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.79923/4.81214. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.79746/4.81714. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.79883/4.82203. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.79980/4.81386. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.79532/4.81734. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.79858/4.81673. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.79903/4.81850. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.79747/4.81806. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.79654/4.81516. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.79617/4.81652. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.79181/4.82625. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.79663/4.82559. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.79677/4.81948. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.79243/4.81806. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.79810/4.82644. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.79733/4.82371. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.79838/4.80287. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.79612/4.81436. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.79887/4.81739. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.79385/4.83216. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.79528/4.82339. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.79405/4.82803. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.79244/4.81696. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.78724/4.83303. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.79439/4.83139. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.79658/4.82570. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.79215/4.82606. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.79031/4.82807. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.78731/4.83735. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.78887/4.84377. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.80122/4.82285. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.80623/4.82753. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.80504/4.83038. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.79885/4.83247. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.80031/4.83401. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.79478/4.83802. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.79558/4.84279. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.79341/4.84054. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.78975/4.83481. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.79229/4.82982. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.78763/4.83711. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.78932/4.83690. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.79232/4.83276. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.78734/4.83435. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.78891/4.84068. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.78735/4.83995. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.78885/4.82811. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.78641/4.83451. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.80255/4.81700. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.80054/4.81903. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.79153/4.83583. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.79245/4.82483. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.79333/4.81482. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.78813/4.83113. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.78503/4.83143. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.78500/4.83696. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.78729/4.82859. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.78802/4.83392. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77933/4.83446. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.77739/4.82806. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.78874/4.84865. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.79614/4.81368. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.78636/4.84845. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.78946/4.82412. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.78686/4.82955. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.78393/4.83515. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.78215/4.84100. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.78765/4.84188. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.77776/4.84141. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.79085/4.82943. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.79438/4.82215. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.79766/4.81946. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.79816/4.82144. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.79856/4.83065. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.79883/4.82863. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.79468/4.82775. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.79838/4.81967. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.79224/4.83922. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.79799/4.83579. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79611/4.83820. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.78899/4.83721. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.79557/4.82930. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.79067/4.83505. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.79184/4.83320. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.03734655007457467\n",
      "Epoch 0, Loss(train/val) 4.80838/4.76316. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.76495/4.75225. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.76036/4.75598. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.75687/4.75549. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.75718/4.75276. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.75842/4.75501. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.75555/4.75439. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.75471/4.75651. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.75600/4.75516. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.75458/4.76521. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.75207/4.75603. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.75452/4.76902. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.75237/4.75689. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.75207/4.77291. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.74911/4.75676. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.75190/4.77339. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.75440/4.75614. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.75373/4.76103. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.75062/4.76662. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.74903/4.77567. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.74790/4.76185. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.75064/4.78434. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.74661/4.76278. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.74799/4.77148. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.74545/4.78631. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.74307/4.76369. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.74324/4.79827. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.74518/4.76928. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.73882/4.78501. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.74203/4.77869. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.74048/4.80386. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.74200/4.77549. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.73865/4.79497. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.73644/4.78110. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.73752/4.80358. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.73698/4.79757. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.73728/4.79055. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.73504/4.78914. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.73293/4.80578. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.73854/4.78835. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.73187/4.80637. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.73131/4.81600. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.73505/4.79768. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.73399/4.81332. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.73229/4.79885. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.72845/4.81149. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.73029/4.81383. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.73246/4.81436. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.73380/4.81018. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.73077/4.80414. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.73314/4.79897. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.73241/4.81186. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.72431/4.82474. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.72877/4.81679. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.72719/4.83040. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.72873/4.81151. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.72047/4.82370. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.72370/4.83652. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.73082/4.81709. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.72722/4.83465. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.72317/4.81878. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.72924/4.79863. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.72594/4.78924. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.74654/4.78200. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.73774/4.77807. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.73025/4.79806. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.73675/4.79155. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.73634/4.78399. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.73270/4.80544. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.72722/4.82065. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.73093/4.81030. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.72345/4.83934. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.72657/4.81901. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.72473/4.85703. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.72107/4.82190. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.72449/4.86446. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.72160/4.85354. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.71840/4.85080. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.72664/4.80718. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.72997/4.77462. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.72503/4.81712. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.72373/4.81001. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.71932/4.86526. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.71889/4.82557. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.72165/4.81310. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.72394/4.83578. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.72487/4.83034. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.71318/4.82522. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.72386/4.80635. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.72321/4.84622. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.72309/4.84216. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.72141/4.81989. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.72252/4.83086. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.71495/4.85054. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.71868/4.81113. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.71468/4.85446. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.71564/4.81230. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.71657/4.82211. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.71725/4.84259. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.71962/4.81116. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.008866995073891626\n",
      "Epoch 0, Loss(train/val) 4.84831/4.80786. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.80138/4.88520. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.81503/4.86297. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.81802/4.80781. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.80344/4.79929. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.79344/4.81943. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.79184/4.82898. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79663/4.81761. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79674/4.81662. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79196/4.82145. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.79321/4.81521. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.79344/4.81318. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79229/4.82780. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79304/4.81798. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.79337/4.82448. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.78784/4.82555. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78945/4.83399. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.79082/4.82748. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.78918/4.83550. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78883/4.82940. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.79079/4.83693. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.78746/4.83173. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78773/4.84414. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78786/4.83496. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.78803/4.84636. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78889/4.83672. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78915/4.82921. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78947/4.83502. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78649/4.85406. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78523/4.83946. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.78752/4.84075. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.78555/4.85377. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.78409/4.85481. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.78112/4.86725. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.78250/4.85781. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.78469/4.85866. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78145/4.86387. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.78133/4.86045. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77578/4.88027. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.78001/4.85875. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.78058/4.87594. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.77519/4.86918. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.78164/4.87596. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.77416/4.86672. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.77584/4.87694. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77717/4.86674. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77211/4.88430. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.77542/4.86218. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.77220/4.88443. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77340/4.89068. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.78203/4.86818. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.77523/4.89094. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.77430/4.88315. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76717/4.90373. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76795/4.89885. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.77144/4.89293. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.78239/4.85002. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76919/4.87995. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.77224/4.87522. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.77292/4.87935. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.76738/4.90950. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.77326/4.89533. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.76919/4.89205. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.76396/4.90063. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76318/4.93246. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.76614/4.86976. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.77442/4.88952. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.76216/4.89773. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.76293/4.92688. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.76010/4.90637. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.76055/4.91235. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.76572/4.90420. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.76731/4.91088. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.75901/4.90203. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.76047/4.94011. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.75765/4.93817. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.76267/4.93467. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.76054/4.90167. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.76339/4.91660. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.75774/4.93827. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.76102/4.91461. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.75684/4.92299. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.76078/4.90602. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.75842/4.94974. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.75847/4.93285. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.75567/4.91785. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.75792/4.91297. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.76016/4.94380. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.75538/4.91993. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.75364/4.93916. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.76089/4.92130. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.75521/4.94072. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.75517/4.92383. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.75882/4.94801. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.75610/4.93252. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.74823/4.95761. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.75938/4.94106. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.75742/4.94013. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.74695/4.92599. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75330/4.95648. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.07570682517832987\n",
      "Epoch 0, Loss(train/val) 4.91823/4.95817. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.88072/4.92605. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.87363/4.90199. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87062/4.89140. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.86665/4.90713. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 4.86261/4.91222. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.86777/4.91088. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.87045/4.90193. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.86804/4.89476. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.86439/4.89745. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.86071/4.90766. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.85705/4.92241. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.85591/4.91800. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.86028/4.90852. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85925/4.90562. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85616/4.90582. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85679/4.90230. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85367/4.91612. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85275/4.92154. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.85261/4.91751. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.85169/4.91920. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85074/4.91226. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.84852/4.91193. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84806/4.93529. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85435/4.89951. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.84653/4.94139. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.85377/4.89302. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.85070/4.91383. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.84846/4.91625. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84832/4.90848. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84495/4.91799. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84555/4.90457. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.84460/4.91793. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84517/4.91558. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84383/4.92523. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84138/4.91599. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84247/4.91492. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84265/4.91822. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84284/4.91481. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84211/4.90764. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84166/4.92239. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84430/4.89664. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83572/4.90611. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.83635/4.90904. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.83489/4.93680. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84290/4.91016. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.83494/4.90948. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83533/4.93176. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.83731/4.93736. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.82519/4.95706. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.84389/4.90925. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.83858/4.93580. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.83331/4.96242. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.83075/4.94189. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.83649/4.92205. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.83031/4.94773. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.83958/4.90981. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.83076/4.93193. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.82709/4.93295. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.83953/4.90314. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.83219/4.92415. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.83525/4.93847. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.82327/4.95066. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83405/4.92758. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.82915/4.93233. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.84617/4.91523. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.83430/4.93372. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.84145/4.89651. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.84007/4.92560. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.83400/4.92968. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.82568/4.94852. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.82679/4.95044. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83198/4.93587. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.82400/4.95065. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.82993/4.94303. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.82052/4.95836. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.82390/4.91573. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.82196/4.98225. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.83161/4.90647. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.82496/4.93546. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.83400/4.92626. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.82133/4.94525. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.82455/4.97393. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.82571/4.90426. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.82668/4.93149. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.82044/4.92840. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.81562/4.94881. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.82542/4.92034. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.81329/4.95059. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82243/4.90417. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.81845/4.92976. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.81603/4.91136. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.81529/4.91205. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.81786/4.91699. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.81664/4.96475. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82646/4.94402. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83633/4.95362. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.84020/4.94399. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.83656/4.92948. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.83469/4.95018. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.14285714285714285\n",
      "Epoch 0, Loss(train/val) 4.82146/4.81991. Took 0.48 sec\n",
      "Epoch 1, Loss(train/val) 4.80448/4.81753. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.80523/4.81182. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.80341/4.78827. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79102/4.78734. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.78780/4.78950. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.78869/4.78702. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.78974/4.78874. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.78946/4.78829. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.78758/4.78757. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.78778/4.78779. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.78660/4.78806. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.78706/4.78801. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.78654/4.78801. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.78684/4.78783. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78496/4.78869. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78629/4.79304. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78355/4.79138. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78600/4.79303. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78444/4.79484. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.78213/4.79195. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78176/4.79464. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78425/4.78815. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78494/4.78559. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78255/4.78868. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78100/4.79118. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.78394/4.79121. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.77981/4.78694. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.77904/4.79728. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78184/4.80027. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.78052/4.79300. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77557/4.81151. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.77478/4.80096. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.77770/4.80208. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.77630/4.80366. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.77558/4.80577. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.77290/4.81983. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.77351/4.81582. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.77638/4.82490. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.77426/4.81359. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 4.77101/4.82393. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.76327/4.81714. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.77006/4.83895. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.76376/4.82390. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.76566/4.84590. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.76234/4.86221. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.76675/4.83193. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.76761/4.81527. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.76536/4.84376. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.76140/4.86492. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.75885/4.84944. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.76045/4.83593. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.75718/4.86254. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.75230/4.86421. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76309/4.82882. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.75739/4.86203. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.76251/4.82545. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 4.75418/4.86063. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.75126/4.85694. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 4.76283/4.85673. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.75377/4.88208. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.75433/4.85181. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 4.74837/4.85574. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.75516/4.89075. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.75444/4.85002. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.74586/4.89787. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.74894/4.87433. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 4.76189/4.85519. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.75918/4.84366. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.75078/4.88843. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.74970/4.88603. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.74400/4.90700. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.75342/4.85965. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.74109/4.89760. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.75568/4.89572. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.74116/4.85903. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.74995/4.88917. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.74832/4.87308. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.74511/4.88126. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 4.74394/4.86424. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.74487/4.89626. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.74196/4.85940. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.74035/4.90801. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.74779/4.91628. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.73903/4.86996. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.73848/4.86748. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.73149/4.89621. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.75085/4.88171. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.75044/4.83600. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.73752/4.93856. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.73993/4.86395. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.74484/4.89143. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.75355/4.94506. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.73982/4.88285. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.73789/4.91536. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.74915/4.88642. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.73274/4.94079. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.75464/4.85134. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.74125/4.87568. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.74213/4.88120. Took 0.20 sec\n",
      "ACC: 0.5, MCC: -0.002944085489318884\n",
      "Epoch 0, Loss(train/val) 5.00500/4.99963. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.96918/4.96362. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.96154/4.97727. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.96145/4.99123. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.96110/5.00509. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.96030/4.99909. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 4.95893/4.98752. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.95353/4.97079. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.95284/4.98290. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.95007/4.98657. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.95396/4.98250. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.95338/4.98425. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.94990/4.98940. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.94837/4.98413. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.95001/4.98990. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.94906/4.98079. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.94833/4.97665. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.94802/4.98898. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.94339/4.99292. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.94826/4.99121. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.94022/4.99224. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.94307/4.99469. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.94458/5.00569. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.94123/4.99275. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.94284/5.00586. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.94491/4.98836. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.94270/4.99978. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.94299/4.99944. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.94323/4.99669. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.94168/5.00587. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.94526/5.00243. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.94503/4.99769. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.94528/5.00712. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.94368/4.99342. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.94738/5.00478. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.93896/5.00918. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.94133/5.00493. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.94473/5.00603. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.94026/4.99934. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.93925/5.00901. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.94426/5.00790. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.93900/5.01064. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.94101/5.00175. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.94208/5.00779. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.93987/5.00028. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.93775/5.01213. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.94125/4.99080. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.94704/4.98097. Took 0.21 sec\n",
      "Epoch 48, Loss(train/val) 4.94357/5.00636. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.94490/5.00532. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.94461/5.00610. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.94339/4.99940. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.93920/4.99816. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.94029/4.99907. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.94442/5.01719. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.94245/4.99529. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.93786/4.99825. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.94306/4.99936. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.94010/5.00883. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.93675/5.01546. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.93947/5.00767. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.93542/5.01085. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.93969/5.01207. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.94104/5.02074. Took 0.22 sec\n",
      "Epoch 64, Loss(train/val) 4.93559/5.01743. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 4.93239/5.02313. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 4.93413/5.02081. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 4.93424/5.01436. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 4.93703/5.00696. Took 0.21 sec\n",
      "Epoch 69, Loss(train/val) 4.93155/5.00688. Took 0.23 sec\n",
      "Epoch 70, Loss(train/val) 4.93306/5.01223. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.93483/5.00921. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.93876/5.00545. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.92634/5.01825. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.93584/5.01098. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 4.93470/5.02267. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.93285/5.01456. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 4.92966/5.02860. Took 0.21 sec\n",
      "Epoch 78, Loss(train/val) 4.92724/5.01485. Took 0.22 sec\n",
      "Epoch 79, Loss(train/val) 4.93373/5.01550. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.92723/5.02001. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.93206/5.01161. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.92751/5.03093. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.93062/5.02111. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.92894/5.00652. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.93497/5.02770. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.92932/5.02487. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.93543/5.01805. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.92747/5.02829. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.92485/5.02095. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.92958/5.00216. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.92633/5.02799. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.93659/5.01367. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 4.93277/5.02453. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.92650/5.01941. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.92680/5.01797. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.93184/5.02139. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.93057/5.00680. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.92662/5.02218. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.92720/5.01525. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.1259881576697424\n",
      "Epoch 0, Loss(train/val) 4.93868/4.88936. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 4.89853/4.88960. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.89818/4.88842. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.89544/4.88775. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.89057/4.88902. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.88979/4.88682. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.89190/4.88383. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.88635/4.88625. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.88707/4.88807. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.88896/4.88977. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.88320/4.89553. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.88297/4.91007. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.88346/4.90824. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.88205/4.90777. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.88377/4.89527. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.87927/4.89992. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.88395/4.89726. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.88147/4.89630. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.88085/4.89911. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.88183/4.89638. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.87715/4.89736. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.88199/4.90536. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.88316/4.87708. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.88152/4.88670. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.87776/4.88879. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.88110/4.89465. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.88028/4.89127. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.87957/4.88506. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.87401/4.88144. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.87740/4.88516. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.87646/4.88436. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.87738/4.89801. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.87478/4.88783. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 4.87895/4.89124. Took 0.22 sec\n",
      "Epoch 34, Loss(train/val) 4.87341/4.89310. Took 0.22 sec\n",
      "Epoch 35, Loss(train/val) 4.87395/4.89438. Took 0.22 sec\n",
      "Epoch 36, Loss(train/val) 4.87294/4.88673. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 4.87429/4.89297. Took 0.22 sec\n",
      "Epoch 38, Loss(train/val) 4.87242/4.89759. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.87336/4.89828. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.87297/4.88853. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.87144/4.89266. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.87609/4.86553. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.87738/4.88185. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.87641/4.88915. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.87726/4.88777. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.87241/4.89517. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.87165/4.91005. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.87638/4.88460. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.87073/4.89059. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.87151/4.89604. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.87049/4.89921. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.87083/4.89532. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.86227/4.91455. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.86621/4.90603. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.86796/4.90780. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.86733/4.90212. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.86975/4.90159. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.86511/4.90388. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.86827/4.89583. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.86334/4.90428. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.86343/4.90428. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.86264/4.90286. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.86287/4.90564. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.86272/4.90692. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.86674/4.91797. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.86961/4.90059. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.86236/4.90552. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.86396/4.90740. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.86716/4.90037. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.86226/4.92175. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.86084/4.92220. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.86660/4.91480. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.86192/4.90685. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.85971/4.92537. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.86225/4.91638. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.86110/4.90240. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.85918/4.91899. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.86329/4.92110. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.86565/4.91108. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.86456/4.91339. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.85715/4.91109. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.86431/4.90438. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.85846/4.91489. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.85815/4.91609. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.86358/4.90122. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.87047/4.91088. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.86470/4.92164. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.85916/4.93517. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.86639/4.90126. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.87484/4.89705. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.87344/4.88771. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.86737/4.89438. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.86916/4.89493. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.87110/4.89624. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.87231/4.89906. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.87120/4.89379. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.87032/4.90278. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.86481/4.89255. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.86579/4.90050. Took 0.19 sec\n",
      "ACC: 0.5, MCC: -0.03643724696356275\n",
      "Epoch 0, Loss(train/val) 4.90431/4.86255. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.87465/4.87604. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.87342/4.87061. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87327/4.87273. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.87141/4.87195. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.87142/4.87225. Took 0.18 sec\n",
      "Epoch 6, Loss(train/val) 4.87377/4.87381. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.86774/4.87072. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.87282/4.87647. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87175/4.86930. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.87324/4.87651. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.87349/4.88454. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.87123/4.87067. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.86851/4.88674. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87298/4.86144. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.86578/4.86875. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.86527/4.87037. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.86928/4.86697. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.86816/4.86086. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.86648/4.86397. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.86606/4.86431. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.86253/4.86688. Took 0.18 sec\n",
      "Epoch 22, Loss(train/val) 4.86138/4.85961. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.86460/4.86209. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.86097/4.86160. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.86259/4.85273. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.85987/4.85816. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.86061/4.84936. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.86096/4.86769. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.85813/4.84945. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.86602/4.86351. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86181/4.85146. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.85577/4.86130. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.85891/4.85009. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85628/4.84410. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.85199/4.86002. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.85705/4.84757. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.86238/4.85330. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.86276/4.84814. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.85818/4.83800. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.86000/4.83649. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.85754/4.83333. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.86095/4.83606. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.85744/4.84446. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.85552/4.84336. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.85709/4.84902. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.85656/4.85577. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.85775/4.85784. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.85852/4.85127. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.85503/4.84160. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.85331/4.86216. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.85008/4.84968. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.85264/4.85250. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.85484/4.85051. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.84998/4.85502. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.85369/4.85150. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.85436/4.85810. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.85497/4.85722. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.85194/4.85285. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.85630/4.85975. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.86169/4.84325. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.85962/4.84514. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85941/4.84643. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.85781/4.85485. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.85488/4.84362. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.86074/4.84243. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.86004/4.83054. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.85857/4.84144. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.86039/4.83613. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.85895/4.83332. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.85791/4.83381. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.85600/4.83579. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 4.84645/4.84382. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84577/4.84350. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.85515/4.83088. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.86028/4.83934. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.85836/4.84329. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.85157/4.84651. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.85414/4.82827. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.85683/4.83180. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.85175/4.83920. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.84790/4.83773. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.84968/4.83905. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84968/4.84592. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.85306/4.84121. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.85094/4.83423. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.85074/4.84538. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.85178/4.84660. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.84725/4.83664. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.85681/4.84115. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.85172/4.84297. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.84905/4.84983. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.85099/4.83893. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.85124/4.83792. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.84893/4.84175. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.84838/4.83862. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.85081/4.83452. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.85183/4.83818. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.84767/4.84342. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.85383/4.84036. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.08818077954471167\n",
      "Epoch 0, Loss(train/val) 4.93246/4.87871. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.85995/4.85784. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.85497/4.85103. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.85393/4.85058. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.85185/4.85131. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.85135/4.85334. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.85333/4.85285. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.85017/4.85150. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85117/4.84968. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.85013/4.84732. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84891/4.84649. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.84702/4.84754. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.84821/4.84721. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.84742/4.84654. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.84696/4.84744. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.84694/4.84623. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 4.84376/4.84543. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.84820/4.84477. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.84527/4.84227. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.84378/4.84163. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84593/4.84074. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.84482/4.83981. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.84396/4.83896. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84352/4.83851. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.84267/4.83492. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.84441/4.83378. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.84068/4.83077. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84039/4.83132. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.84198/4.82975. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.83894/4.82471. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.83804/4.82867. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.83624/4.82572. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.84296/4.81962. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.83752/4.81898. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.83527/4.82179. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.83994/4.81871. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.83745/4.81444. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.83639/4.82060. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.83477/4.82266. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.83644/4.81120. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.83412/4.81263. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.83516/4.81064. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83146/4.81534. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.83340/4.79684. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.83098/4.80317. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.83309/4.80085. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.83211/4.81164. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83393/4.80170. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.82841/4.80198. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.82672/4.80790. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.83252/4.80865. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.83235/4.80684. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.82454/4.80476. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.82482/4.80413. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.82551/4.78727. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.82620/4.79667. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.82746/4.80451. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.82686/4.81292. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.82626/4.81180. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.82718/4.80366. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.82250/4.79923. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.82511/4.79370. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.82573/4.82919. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83429/4.83724. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.83159/4.83891. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.83037/4.83225. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.82803/4.82327. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83049/4.83280. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.82998/4.83118. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.82460/4.82502. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.82557/4.81870. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.82660/4.81901. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.82485/4.82114. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83249/4.81921. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.82710/4.82435. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.82323/4.82753. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.82526/4.81697. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.82672/4.81338. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.82406/4.82402. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.82486/4.82352. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.82544/4.81746. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.82884/4.80433. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.82481/4.80587. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.81682/4.84458. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.82022/4.80248. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83039/4.81720. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.82136/4.81710. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.82222/4.80907. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.81878/4.81798. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82143/4.80981. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.82176/4.81384. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82121/4.81065. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82198/4.81687. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.81906/4.83122. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.81789/4.82168. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.83129/4.81975. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82232/4.82610. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81890/4.81272. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.81638/4.82532. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.81752/4.82078. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.14180020247260586\n",
      "Epoch 0, Loss(train/val) 5.10889/5.09358. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.07591/5.08527. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.07866/5.08203. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.07508/5.08508. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.07327/5.08858. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.07185/5.08689. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.07346/5.07965. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.06961/5.07856. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.07097/5.07143. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.06503/5.07373. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.07058/5.07093. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.06863/5.07242. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.06514/5.06915. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.06504/5.07268. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.06753/5.06597. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.06275/5.07431. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.06446/5.06808. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 5.06482/5.07629. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 5.06104/5.07125. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.06295/5.07359. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.06053/5.07673. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.06699/5.07420. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.05837/5.08810. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.05415/5.07889. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.06206/5.07951. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.05903/5.08159. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.05692/5.07521. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.05420/5.08624. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.05449/5.07811. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.05802/5.07719. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.05051/5.08270. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.05576/5.07861. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.05771/5.07251. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.04889/5.07782. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.05447/5.07084. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.04917/5.07508. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.04879/5.07051. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.05244/5.06961. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.05378/5.06335. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.04822/5.07510. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.05163/5.06826. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.05272/5.07643. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.04875/5.07840. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.04295/5.08375. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.04635/5.07768. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 5.04639/5.07658. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.04739/5.07066. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.04920/5.07934. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.04593/5.07632. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.04343/5.06450. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.04208/5.07934. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.04255/5.07513. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.04499/5.07683. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.04338/5.06871. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.04366/5.06415. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.04427/5.09645. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.04010/5.08769. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.04026/5.08896. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.04081/5.07460. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.04190/5.07705. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 5.03901/5.09256. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.04220/5.06623. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.02978/5.07923. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.04021/5.07952. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.03908/5.06994. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.04120/5.07434. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.03772/5.07078. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 5.03594/5.06129. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.04358/5.06017. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.03632/5.08079. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.03573/5.08071. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 5.03678/5.06415. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.03063/5.08216. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.03475/5.10908. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.03552/5.08788. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.03308/5.08447. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.03347/5.09851. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 5.03735/5.06695. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.03367/5.09536. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.03701/5.09550. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.03348/5.09742. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.03438/5.07954. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.03056/5.11653. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.03080/5.07077. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.03772/5.07979. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 5.03062/5.08496. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 5.03056/5.08326. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.02076/5.09105. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.02718/5.11439. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.03081/5.09202. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.03159/5.07367. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.02499/5.10598. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.03275/5.08660. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.03136/5.07055. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.03896/5.07881. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.02626/5.08911. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.03223/5.09022. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.02434/5.08883. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.02617/5.09115. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 5.03039/5.09183. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.015384615384615385\n",
      "Epoch 0, Loss(train/val) 4.96740/4.90456. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.92082/4.90683. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.92534/4.89864. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.92399/4.89592. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.91857/4.89488. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.91290/4.89445. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.91029/4.89212. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.90897/4.89210. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.90985/4.89235. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.91106/4.89498. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.90663/4.90279. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.90827/4.90204. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.90565/4.90283. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.90482/4.89903. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.90255/4.89903. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.89939/4.90420. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90007/4.91334. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.89851/4.91304. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.89881/4.91311. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.89661/4.91780. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.89155/4.92977. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.89776/4.92453. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.89367/4.93849. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.89284/4.93115. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.89042/4.93753. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.89196/4.92154. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.89250/4.91908. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.89355/4.92933. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.89369/4.92021. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.89606/4.92946. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.89277/4.91877. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.89732/4.91808. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.89767/4.91441. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.89836/4.89834. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.89558/4.90571. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.88986/4.90360. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.88835/4.90880. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.89368/4.90114. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.88807/4.91856. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90534/4.90280. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.91038/4.89090. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.91094/4.89969. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.90806/4.89024. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.90449/4.88340. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.90420/4.88380. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.90049/4.86845. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.90032/4.87145. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89574/4.86195. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.90195/4.86594. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.90276/4.88236. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.89242/4.88764. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.89368/4.87952. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89255/4.89069. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.89272/4.87787. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.89266/4.89203. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.88799/4.89170. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.88773/4.90507. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.89382/4.88646. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.89026/4.90681. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.90032/4.88017. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.90081/4.89449. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.90245/4.88803. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.89778/4.88641. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.89709/4.88471. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.88902/4.88779. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.89256/4.89456. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.89581/4.88936. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.89153/4.89391. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.88711/4.88667. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.89374/4.87431. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.88862/4.87921. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.88768/4.89287. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.88900/4.89735. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88868/4.89153. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.88494/4.89561. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.88222/4.90082. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.88399/4.90052. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.88719/4.90215. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.87552/4.93995. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.88841/4.89685. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88504/4.91803. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.87816/4.91921. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.88428/4.90725. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.88482/4.92028. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88524/4.90931. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.88551/4.90990. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.87989/4.92426. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.88305/4.92234. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.88260/4.91746. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.88288/4.91347. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.88110/4.91554. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.87868/4.91083. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 4.88397/4.91875. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.89373/4.90694. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.89343/4.92328. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.88447/4.94179. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.88771/4.92973. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.88405/4.94615. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.88688/4.95388. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 4.88288/4.93542. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.08544439848099883\n",
      "Epoch 0, Loss(train/val) 5.05505/5.00039. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.99795/5.00722. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.00196/5.01392. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.00281/5.01768. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.00232/5.01923. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.00057/5.02180. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.99808/5.02565. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.99765/5.02449. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.99720/5.02718. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.00050/5.01843. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.99677/5.01732. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.99777/5.01588. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.99352/5.01159. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.99579/5.02261. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.99398/5.01140. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.99085/5.02910. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.99596/5.01458. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.99069/5.03054. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.98894/5.01476. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.98833/5.03059. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.99237/5.03420. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.99722/5.02618. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 4.99510/5.02262. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.99491/5.02019. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.99304/5.01848. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.99267/5.02029. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.99009/5.02687. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.99306/5.02089. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.99054/5.02457. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.98741/5.02937. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.99046/5.02716. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.98958/5.01977. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.98524/5.02657. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.98965/5.01510. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.99063/5.01328. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.98988/5.00786. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.98756/5.02919. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.98901/5.02882. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.98992/5.02450. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.98834/5.03322. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.98537/5.02655. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.98343/5.02290. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.98363/5.03513. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.98888/5.02876. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 4.98785/5.03463. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.98563/5.03819. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.98563/5.02330. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.98288/5.01897. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.97907/5.02737. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.98558/5.02564. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.98520/5.02796. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.97918/5.04005. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.98293/5.02810. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.98088/5.02215. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.98312/5.01681. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.97829/5.03366. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.97934/5.03637. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.97589/5.04033. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.97990/5.04468. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.98057/5.03455. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.98069/5.02791. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.97451/5.02971. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.97565/5.03656. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.97480/5.03545. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.97725/5.03358. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.97695/5.02985. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.97187/5.04455. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.98035/5.03192. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.97470/5.01574. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.97464/5.01992. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.97571/5.01443. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.97020/5.01744. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.97486/5.04306. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.97426/5.03222. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.97001/5.03628. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.97407/5.02923. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.97338/5.02103. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.97325/5.01685. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.97180/5.02877. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.97209/5.03122. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.97794/5.01723. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.96933/5.03538. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.96943/5.02984. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.96896/5.04029. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.96473/5.03554. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.96587/5.03798. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.96746/5.01956. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.97108/5.01399. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.96242/5.02239. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.96767/5.01725. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.97180/5.01458. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.96392/5.02043. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.96789/5.01584. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.96505/5.02963. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.96283/5.02286. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.96165/5.03852. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.96567/5.02244. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.96738/5.01741. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.96755/5.02354. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.96755/5.01930. Took 0.20 sec\n",
      "ACC: 0.453125, MCC: -0.08845235543978211\n",
      "Epoch 0, Loss(train/val) 4.53329/4.48385. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.49852/4.48050. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.51042/4.49135. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.51198/4.54434. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.49456/4.52651. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.49119/4.51804. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.49488/4.52484. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.49487/4.52468. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.49339/4.52709. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.49507/4.52673. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.49321/4.52588. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.49096/4.52921. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 4.49016/4.52956. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.49055/4.53522. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.48709/4.52996. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.48641/4.52902. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.48704/4.53510. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.48426/4.53330. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.48636/4.52577. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.48301/4.54885. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.48482/4.53821. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 4.48423/4.53665. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.47859/4.52332. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.48269/4.53845. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.49279/4.54716. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.48004/4.53669. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.48654/4.52949. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.48599/4.57208. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.48550/4.53299. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.48106/4.53006. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.47918/4.51711. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.47714/4.53146. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.48014/4.53017. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.48148/4.53142. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.48451/4.52075. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.48340/4.53138. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.47709/4.53123. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.47640/4.54248. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.47573/4.54207. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.47639/4.54292. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.47091/4.52039. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.47366/4.56186. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.47820/4.53930. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.47309/4.54358. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.47602/4.53522. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.47354/4.52200. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.47557/4.53680. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.47123/4.53257. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.47802/4.55831. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.46567/4.54157. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.47258/4.54317. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.46609/4.53602. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.46644/4.56165. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.47392/4.54053. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.47726/4.53530. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.47194/4.54112. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.47098/4.56106. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.46479/4.54900. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.47359/4.54894. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.46824/4.55983. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.47155/4.54690. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.46287/4.55621. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.47053/4.54295. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.47285/4.54410. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.46623/4.55247. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.46224/4.56273. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.46949/4.56497. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.46029/4.55953. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.46841/4.57119. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.46241/4.56410. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.45748/4.55489. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.46612/4.55142. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.45792/4.55585. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 4.46289/4.56446. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.46654/4.55538. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.46129/4.56714. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.46496/4.55907. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.45492/4.56779. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.45843/4.57488. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.46690/4.56213. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.45585/4.56710. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.45792/4.54971. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.46071/4.55912. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.45796/4.56837. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.45486/4.55884. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.45128/4.57425. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.46388/4.52996. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.46300/4.54528. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.46423/4.56327. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.45847/4.56577. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.45474/4.56982. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.46181/4.54900. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.46365/4.57034. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.45616/4.56599. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.46176/4.56047. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.45741/4.58129. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.45197/4.57062. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.45646/4.57028. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.45983/4.53845. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.46840/4.55092. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.19088542889273333\n",
      "Epoch 0, Loss(train/val) 4.94888/4.86554. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.87366/4.87572. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.87343/4.86769. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.86921/4.87150. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.86818/4.87325. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.86423/4.87810. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.86523/4.87584. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.86768/4.87148. Took 0.22 sec\n",
      "Epoch 8, Loss(train/val) 4.86852/4.87021. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 4.86493/4.87217. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.86078/4.87122. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.86046/4.86943. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85666/4.87062. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.85467/4.86718. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.85622/4.86567. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85656/4.87912. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85954/4.87107. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85492/4.87253. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.85078/4.87276. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.85332/4.87183. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84975/4.88670. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.84714/4.88318. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.84541/4.89252. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85229/4.87916. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.84710/4.88607. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.84757/4.88368. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84719/4.88481. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84647/4.88734. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.84667/4.88635. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84219/4.88659. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84599/4.89157. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84465/4.89133. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.84690/4.88301. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84197/4.90380. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84387/4.90036. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84154/4.89552. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.84413/4.89188. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84489/4.89561. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.83468/4.90499. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.84120/4.89595. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.84217/4.89501. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84234/4.90439. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83781/4.89553. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.84242/4.89735. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 4.83435/4.91320. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84297/4.91105. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 4.83532/4.90453. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83982/4.91239. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.83648/4.91144. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.83871/4.91288. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.83896/4.89899. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.84301/4.88304. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.85495/4.89186. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.84775/4.88918. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.84634/4.89969. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.84698/4.90316. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84253/4.92138. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.84813/4.89354. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.84652/4.90395. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.84327/4.91469. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.84301/4.90933. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.84331/4.89745. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.84266/4.91109. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83747/4.91250. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.83805/4.92090. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.83776/4.93046. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.84155/4.92349. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83722/4.92767. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.83862/4.94094. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.83888/4.93074. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.83891/4.94095. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83727/4.94095. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.84254/4.94886. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84132/4.93450. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.83451/4.95089. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.83669/4.92933. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83477/4.95824. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.83759/4.90399. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.83432/4.92921. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.83607/4.93092. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.83288/4.94946. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83735/4.93562. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.83380/4.95003. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.83400/4.96424. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83539/4.92468. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83832/4.93783. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.82849/4.97855. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83789/4.95088. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.83619/4.95631. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82794/4.94752. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.83359/4.93036. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82903/4.98280. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82999/4.94861. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83284/4.95074. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82505/4.94884. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.83533/4.92773. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83142/4.95742. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83711/4.93400. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.82770/4.95541. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.82984/4.94686. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.04350001325672093\n",
      "Epoch 0, Loss(train/val) 4.82041/4.79576. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.80000/4.79139. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.79577/4.79145. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.79588/4.80733. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79742/4.80825. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79249/4.79803. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.78981/4.79734. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.78714/4.80299. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.78813/4.80804. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.78863/4.80770. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.78910/4.80877. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78909/4.81101. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.78803/4.80098. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.78523/4.80287. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78433/4.80804. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78729/4.81692. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78650/4.81132. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78381/4.81241. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78307/4.81814. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 4.78353/4.82226. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.77939/4.82667. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78262/4.81444. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78182/4.81674. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78176/4.82461. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78230/4.81850. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.77922/4.82927. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78067/4.83173. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78130/4.82555. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.77776/4.84122. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.77665/4.83215. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.77740/4.84257. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77705/4.84886. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77704/4.82939. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.77619/4.86139. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.77661/4.85574. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77296/4.85397. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.76815/4.88165. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.77747/4.83568. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77872/4.85749. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77038/4.86957. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77582/4.85592. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.76956/4.87586. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.77310/4.84646. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.76760/4.87851. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77224/4.84968. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.76810/4.86601. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77167/4.86371. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.76417/4.88509. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.76987/4.87962. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77096/4.86226. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76635/4.87404. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.76511/4.86204. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.77092/4.85604. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76718/4.88397. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76740/4.87590. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.76443/4.90255. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.76323/4.88028. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.77336/4.81706. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.76984/4.85694. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.76673/4.87615. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.76776/4.86665. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.76283/4.89405. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.77017/4.85663. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.76473/4.87042. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.77347/4.88420. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.76139/4.89641. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.76127/4.89250. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.75693/4.91307. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.76058/4.89506. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.76029/4.93426. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.75919/4.89164. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.76502/4.87745. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.75622/4.96068. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.75947/4.91240. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.76104/4.91922. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.75594/4.91372. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.75610/4.92961. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.75796/4.90861. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.75739/4.90220. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.75947/4.88978. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.74932/4.91783. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.76082/4.90479. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.75432/4.91574. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.75548/4.91290. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.75301/4.95046. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.75872/4.88417. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.74624/4.97627. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.75964/4.88532. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 4.75569/4.91925. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.75051/4.93109. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.75764/4.90597. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.75399/4.92918. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 4.75071/4.92709. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.74872/4.92713. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.75348/4.92584. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75010/4.95543. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.75184/4.92363. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.74784/4.94198. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.75281/4.92291. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.75062/4.93930. Took 0.19 sec\n",
      "ACC: 0.609375, MCC: 0.21732769392835002\n",
      "Epoch 0, Loss(train/val) 4.86617/4.84374. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.86036/4.88461. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.85200/4.86968. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.85414/4.85052. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.85363/4.83814. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.85105/4.83638. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.84913/4.84200. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.84927/4.84248. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.84674/4.84535. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.84806/4.84974. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84347/4.85989. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.84564/4.86500. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.84417/4.86509. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.84069/4.87374. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.84436/4.88696. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.84173/4.87525. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.83673/4.88182. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.83981/4.88313. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.84127/4.87440. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.84273/4.87388. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.83676/4.89423. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.83568/4.90337. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.83576/4.89500. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.83325/4.91291. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.83751/4.90024. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.83281/4.90186. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.83377/4.91271. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.83188/4.91181. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.83028/4.91396. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.83328/4.90813. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.83304/4.90470. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.83426/4.90038. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.83414/4.90881. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.83125/4.91044. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.83208/4.91171. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82808/4.92888. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.82391/4.92751. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.82817/4.92050. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.83166/4.92449. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.82302/4.93333. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.82706/4.91924. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.82433/4.91870. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.82798/4.91852. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.82955/4.90839. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.82929/4.92258. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.82790/4.94177. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.82292/4.93330. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.81918/4.94242. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.82325/4.93864. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.82723/4.94328. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.82467/4.91431. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.82470/4.95023. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.82752/4.93661. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.83065/4.90776. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81910/4.95543. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.82823/4.92696. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.82230/4.94633. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.82708/4.93380. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81972/4.94242. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.82215/4.93030. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.82067/4.92987. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.82128/4.93737. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 4.82182/4.96189. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83683/4.88651. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.83342/4.85963. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.83509/4.92874. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.82793/4.90820. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.81854/4.93513. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.82413/4.91190. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.83038/4.89257. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.82813/4.92221. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.82271/4.93953. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.82313/4.93152. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.81974/4.94339. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.82460/4.95208. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.81980/4.92822. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.82539/4.94512. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.82193/4.95383. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.81721/4.94344. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.81977/4.96320. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.82546/4.95900. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.81931/4.93044. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.82013/4.92494. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.82408/4.94263. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.81728/4.95515. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.81877/4.95115. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.81840/4.96646. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.82549/4.90136. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.83064/4.89677. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82242/4.92143. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82227/4.92221. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.82213/4.91198. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82611/4.91296. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.82317/4.91495. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.82085/4.93141. Took 0.21 sec\n",
      "Epoch 95, Loss(train/val) 4.82460/4.92901. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82525/4.92943. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82594/4.91857. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82707/4.91831. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.81812/4.92703. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.1889822365046136\n",
      "Epoch 0, Loss(train/val) 4.82351/4.82620. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.79585/4.80568. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.79305/4.79929. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.80077/4.78211. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.80682/4.78026. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.80000/4.77674. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.79125/4.77191. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.78876/4.76859. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.79166/4.76985. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.78878/4.76965. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.78636/4.76751. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.78824/4.77046. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.78830/4.76970. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.78682/4.77166. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78973/4.77177. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78745/4.77038. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.78659/4.77146. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.78836/4.77103. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78681/4.77064. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78514/4.77007. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.78565/4.76883. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78485/4.76962. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.78425/4.77032. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78453/4.77018. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78540/4.76903. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.78361/4.77045. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.78557/4.77198. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78322/4.77523. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78207/4.77273. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.78216/4.77731. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.77931/4.77882. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.78072/4.79563. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.79290/4.78204. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.78672/4.77509. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 4.78110/4.77559. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77799/4.77799. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77772/4.78170. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.78356/4.78194. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77974/4.78457. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77626/4.78892. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.78284/4.79021. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.78109/4.78785. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.77717/4.79231. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 4.77543/4.80290. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77827/4.79840. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77522/4.79306. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.77686/4.78853. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.77487/4.79940. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.77332/4.80165. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77388/4.79979. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.77033/4.80988. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.77653/4.80161. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.77826/4.79724. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.77482/4.80099. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.77401/4.80601. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.77276/4.80236. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.77084/4.79048. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76899/4.81029. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.77509/4.80809. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.77500/4.79318. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.76913/4.79843. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.76877/4.79994. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.76445/4.78659. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.76793/4.80288. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76529/4.80075. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.77233/4.79121. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.76888/4.79435. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.76971/4.80492. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.76375/4.78990. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.76500/4.80260. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.76718/4.79834. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.76420/4.80603. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.76451/4.79501. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.76228/4.80814. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.76358/4.80194. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.76269/4.81225. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.76599/4.80385. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.76479/4.80550. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.76246/4.79773. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.76892/4.78556. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.75934/4.80488. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.75333/4.82231. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.76840/4.79780. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.76566/4.80437. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.75889/4.78036. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.75980/4.78067. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.76128/4.78229. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.76995/4.80046. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.76418/4.80426. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.77195/4.80358. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.76624/4.81146. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.76579/4.81372. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.76250/4.80657. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 4.76282/4.80381. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.75632/4.82675. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75910/4.79061. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.76442/4.80715. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.76260/4.79337. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.76066/4.79692. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75812/4.79260. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.07570682517832987\n",
      "Epoch 0, Loss(train/val) 5.00554/4.91868. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.91893/4.90934. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.92054/4.92165. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.92371/4.92964. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.92376/4.92724. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.91926/4.92180. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.91470/4.92088. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.91276/4.91903. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.91061/4.92176. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.90915/4.93095. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.91283/4.92163. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.90827/4.93159. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.90954/4.92090. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.90620/4.92538. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.90678/4.92601. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.90773/4.92122. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90486/4.92649. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.90572/4.90384. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.90134/4.92331. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.90144/4.91376. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.91053/4.90218. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.90196/4.90291. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.90402/4.90427. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.90274/4.90666. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.89949/4.90681. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.90248/4.91589. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.89717/4.92850. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.89947/4.91526. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.89511/4.93269. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.89741/4.91565. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.89933/4.91866. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.89507/4.92767. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.89860/4.91572. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.89367/4.91253. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.89342/4.93358. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90045/4.92509. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.89845/4.92441. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 4.89627/4.90898. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89051/4.91580. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.88967/4.91760. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.89179/4.91532. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.89269/4.92470. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.89271/4.90972. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.89381/4.92420. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.89347/4.91240. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.89387/4.91995. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.88965/4.92026. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89793/4.91473. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.88986/4.92180. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.89137/4.91316. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89036/4.91072. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.89009/4.92040. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.89111/4.90978. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.88872/4.90731. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.89135/4.90569. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.89303/4.92312. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.89020/4.91991. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.88772/4.91214. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.88876/4.92299. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.88586/4.93697. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.88549/4.91396. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.89002/4.91902. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.88439/4.92481. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.88479/4.91969. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.88555/4.92049. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.88909/4.92799. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.88563/4.92835. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.88881/4.92675. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.88414/4.90961. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.88260/4.92772. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.88574/4.92649. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.88775/4.91419. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.88192/4.93373. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88357/4.92761. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.88311/4.91970. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.88028/4.91623. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.87877/4.92631. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.88121/4.91994. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.88790/4.92170. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.87748/4.92592. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.87432/4.92901. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.88340/4.91568. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.88012/4.91491. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 4.87823/4.91676. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.87614/4.92988. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.88241/4.91216. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.87913/4.92103. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.87995/4.91525. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.90747/4.88319. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.90337/4.91894. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.89616/4.91961. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.88844/4.93583. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.89047/4.92435. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.89213/4.92207. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88650/4.93569. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.88804/4.92893. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.88874/4.93047. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.87996/4.94099. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.88435/4.93213. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88638/4.93185. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.0629940788348712\n",
      "Epoch 0, Loss(train/val) 4.98623/4.99916. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.98041/4.97641. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.98501/4.98803. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.00615/4.98541. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.98557/4.99687. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.97298/4.98335. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.97649/4.98839. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.97684/4.98676. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.97508/4.98836. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.97602/4.99133. Took 0.18 sec\n",
      "Epoch 10, Loss(train/val) 4.97637/4.99211. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.97212/4.99301. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.97592/4.99310. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.97426/4.99510. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.97382/4.99412. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.97345/4.99428. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.97525/4.99495. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.97301/4.99426. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.97400/4.99506. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.97165/4.99333. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.97295/4.99767. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.97130/4.99339. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.97219/4.99653. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.96915/4.99532. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.97078/4.99560. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.97196/4.99656. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.97324/4.99981. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.97160/4.99826. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.96907/4.99929. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.96711/4.99976. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.96591/4.99375. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.97037/5.00446. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.96783/5.00675. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.96769/5.00322. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.96408/5.02075. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.96126/5.01377. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.96372/5.01527. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.96262/5.00586. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.96380/5.03308. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.96735/5.02119. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.96254/5.01336. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.96413/5.01314. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.96363/4.99190. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.96619/4.99971. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.96776/4.98444. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.96685/4.99568. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.95770/5.00349. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.96486/4.99931. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.95900/5.01727. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.96034/5.01385. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.95717/5.02559. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.95790/5.01281. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.96067/5.01659. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.94974/5.02008. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.95972/5.02133. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 4.95773/5.01823. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.95586/5.00725. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.95363/5.01126. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.95010/5.03185. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.95451/5.02331. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.95326/5.02643. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.95281/5.02432. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.94948/5.05322. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.95348/5.02923. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.95731/5.02544. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.94799/5.02337. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.95163/5.00039. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.95130/5.02046. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.95711/5.01583. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.95371/5.01784. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.95287/5.03031. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.95122/5.01284. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.94438/5.02373. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.94565/5.02866. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.94855/5.01678. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.94581/5.01648. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.93840/5.03062. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.95422/5.02000. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.94415/5.02385. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.94797/5.02419. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.94684/5.01828. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.94375/5.01213. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.94454/5.01732. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.94797/5.00613. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.94421/5.02413. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.94645/5.00127. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.94202/5.00726. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.95031/4.99026. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.95025/5.00196. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.94474/4.99251. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.94382/5.00896. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.94239/5.01292. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.95093/5.01352. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.94624/5.01626. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.94202/5.01058. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.94513/5.02397. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.93826/5.03476. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.94253/5.00874. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.93782/4.99988. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.93911/5.01835. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.013980301652228746\n",
      "Epoch 0, Loss(train/val) 5.07815/5.03550. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.02574/5.03632. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.02238/5.02586. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.01981/5.01586. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 5.01264/5.01152. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.01004/5.01124. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.00808/5.01438. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.00962/5.01495. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.01284/5.01317. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.01134/5.01216. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.01009/5.00786. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.00755/5.00845. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.00776/5.01088. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.00508/5.01733. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.00683/5.01227. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.00702/5.01045. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.00178/5.01178. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.00531/5.00983. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.00609/5.00914. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.00078/5.01077. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 5.00325/5.01742. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.00289/5.01670. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.99895/5.02090. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.99984/5.02118. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.00190/5.01605. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.00226/5.01269. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.99957/5.01851. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.99453/5.01971. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.99553/5.01755. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.99701/5.02002. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.99716/5.01733. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.99640/5.01323. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.99609/5.01130. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 5.00215/5.00703. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.99702/5.00823. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.99453/5.01829. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.00240/4.99832. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 5.00003/5.00316. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.99710/5.01592. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.99595/4.99990. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.99664/5.00554. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.99503/5.01057. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.98957/5.01506. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.99043/5.02534. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.99291/5.00767. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.99314/5.01450. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.98729/5.02390. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.98759/5.02530. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.98962/5.01393. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.98407/5.01910. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 4.99163/5.01231. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.99167/5.01201. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.98595/5.00967. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.98134/5.01726. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.98532/5.01129. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.98289/4.99868. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.98288/5.00244. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.98110/5.00552. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.99056/4.99487. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.98715/4.99685. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.98148/4.99917. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.97926/5.00242. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.98311/5.00792. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.98504/5.01487. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.00052/5.00597. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.99364/4.99806. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.98607/4.98436. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.98559/4.97735. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.98572/4.98174. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.98727/5.00221. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.98041/5.00057. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.97553/4.99846. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.97755/4.98401. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.98258/4.98957. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.97891/4.98228. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.97601/4.99625. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.96944/4.99512. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.97786/4.98649. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.97675/4.98682. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.97388/5.00100. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.96771/4.98738. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.97433/5.00781. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.97278/5.02456. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.97658/5.03953. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.98787/5.00539. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.97920/5.00742. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.97646/5.01296. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.98139/5.01450. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.97548/5.02818. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.96834/5.01239. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.97522/5.02919. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.98525/5.00322. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.97696/4.99963. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.97720/5.00511. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.98008/4.99315. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.97526/5.01567. Took 0.21 sec\n",
      "Epoch 96, Loss(train/val) 4.97436/5.02860. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.97349/5.00767. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.97136/5.01902. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.96640/5.01076. Took 0.20 sec\n",
      "ACC: 0.484375, MCC: -0.07539645724831788\n",
      "Epoch 0, Loss(train/val) 4.73098/4.69941. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.70217/4.69770. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 4.69733/4.69580. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.69727/4.69935. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.69454/4.69592. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.69348/4.69728. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 4.69335/4.70013. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.69505/4.69959. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.69732/4.69661. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.70188/4.69439. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.70813/4.70374. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.70171/4.71333. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.69444/4.69988. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.69275/4.70486. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.69640/4.70910. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.69796/4.71482. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 4.69396/4.71145. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.69256/4.70854. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.69509/4.71219. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.69327/4.71069. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.69096/4.71528. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.69240/4.71462. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.68997/4.72145. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.69316/4.71993. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.69563/4.72659. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.68836/4.72481. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.69285/4.72052. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.69153/4.74237. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.69467/4.72482. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.68646/4.71995. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.68762/4.73084. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.68888/4.74350. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.69013/4.73242. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.68634/4.75535. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.68017/4.74112. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.68267/4.75183. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.68573/4.74209. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.68843/4.74308. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.68675/4.73591. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.68111/4.74796. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.68779/4.73763. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.68299/4.75380. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.68729/4.73531. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.68433/4.74884. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.68611/4.72920. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.68189/4.75506. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.68330/4.73700. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.67560/4.74986. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.68671/4.72204. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.67878/4.75983. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.69141/4.73099. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.68074/4.75761. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.68313/4.73946. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.67762/4.74316. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.67892/4.75780. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.68201/4.75211. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.68025/4.75099. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.68318/4.74508. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.67665/4.76624. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.68597/4.73696. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.67931/4.75518. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.67639/4.74672. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.67741/4.74544. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.67360/4.75132. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.67228/4.74668. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.67782/4.77126. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.68269/4.73181. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.67941/4.74767. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.67829/4.74167. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.67376/4.77320. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.67615/4.75267. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.67856/4.75254. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.67430/4.76134. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.67778/4.72978. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.67702/4.74961. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.67648/4.75062. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.67541/4.75441. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.67230/4.74597. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.66573/4.75533. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.67648/4.74930. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.67869/4.73701. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.66898/4.75263. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.66974/4.75882. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.67264/4.74772. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.67091/4.74952. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.67016/4.75100. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.67212/4.73343. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.67509/4.74015. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.67124/4.74946. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.67476/4.73768. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.67335/4.74310. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.66550/4.75053. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.67286/4.73022. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.66255/4.75686. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.66908/4.74195. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.67323/4.74252. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.66425/4.76191. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.66778/4.72284. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.66860/4.74332. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.66447/4.75365. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.03734655007457467\n",
      "Epoch 0, Loss(train/val) 4.89682/4.84667. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.85818/4.84890. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.85777/4.84676. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.85299/4.84954. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.85837/4.85714. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.85575/4.86712. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.85393/4.86377. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.85682/4.85623. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.84813/4.84987. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.84454/4.85633. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84774/4.87054. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.84752/4.86408. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.84598/4.86692. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.84415/4.87101. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.84132/4.88529. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.83906/4.89511. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.83539/4.89975. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.84237/4.89405. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83715/4.90408. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.83601/4.89164. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.83484/4.89378. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.83026/4.90792. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.82992/4.91079. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.83691/4.89416. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82654/4.89740. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82860/4.89971. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.83266/4.89297. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.82870/4.90013. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.83073/4.89849. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.82617/4.89993. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.82717/4.90150. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.82348/4.91088. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.82325/4.90624. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.82624/4.89292. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.82270/4.91598. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82888/4.88587. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.82419/4.89929. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.82080/4.91312. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.82293/4.90464. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.82449/4.89912. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.82360/4.90638. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.82390/4.90075. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.82448/4.88991. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.81716/4.91198. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.82008/4.90589. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.82162/4.91186. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.81496/4.91092. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.82369/4.89534. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.81372/4.92053. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.81776/4.90172. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.81620/4.91128. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.81571/4.91592. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.81495/4.92191. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.81404/4.93241. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.82054/4.89556. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.81750/4.91824. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.81732/4.90420. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.81190/4.89605. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.81776/4.93061. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.81598/4.91565. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.81680/4.91358. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81400/4.92379. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.81423/4.89923. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.81165/4.91873. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81042/4.92178. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81364/4.91262. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.81339/4.89727. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.82049/4.87419. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 4.80864/4.89522. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.81438/4.91647. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.81267/4.89973. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81417/4.93012. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.81792/4.92650. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.80896/4.91529. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81539/4.90602. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.81624/4.90633. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.81102/4.92082. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.81191/4.90785. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.81133/4.90545. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.80793/4.93352. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.81221/4.89130. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.81171/4.90019. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.81390/4.91699. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80447/4.92968. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.81428/4.90519. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.80959/4.90188. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.81181/4.91103. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.80496/4.92150. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.81050/4.91555. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.81031/4.89871. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.80701/4.92753. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.81318/4.89320. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.80773/4.90497. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.80270/4.93312. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.81174/4.87913. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.81344/4.89353. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.80946/4.90581. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.80630/4.92115. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.80693/4.90498. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.80970/4.89524. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.049929648757959806\n",
      "Epoch 0, Loss(train/val) 4.85990/4.82989. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.81949/4.81414. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.81778/4.81661. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.81222/4.81793. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.81632/4.81990. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.81126/4.82113. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.81154/4.82637. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.81141/4.82891. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.80960/4.82747. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.81028/4.82755. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.80822/4.82890. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.80809/4.82765. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.80489/4.82779. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.80252/4.83475. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.80484/4.83435. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.79912/4.83473. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.79810/4.83992. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.79932/4.83951. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.80322/4.82949. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.79747/4.84292. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.80650/4.82839. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.80289/4.82568. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.80014/4.82467. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.79718/4.83087. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.80193/4.82716. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.79832/4.83568. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.79532/4.84230. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.79972/4.83815. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.79697/4.83519. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.79637/4.84274. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.79687/4.83837. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.79393/4.84411. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.79314/4.85489. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.79543/4.85294. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.79317/4.84727. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.79396/4.84176. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78975/4.86144. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.79450/4.86081. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.79036/4.87063. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.79388/4.84687. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.79340/4.84961. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.79167/4.86891. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.79248/4.86760. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.78575/4.87146. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.78420/4.89901. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.78583/4.88525. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78737/4.87850. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.78980/4.87805. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.78626/4.87327. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.78782/4.87329. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.78358/4.91135. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.79076/4.88105. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.78471/4.91166. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.78051/4.90496. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.79578/4.84427. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.79910/4.85757. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.78794/4.89398. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.79005/4.87350. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.78390/4.89390. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.78486/4.86945. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.78606/4.88719. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.78427/4.88637. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.78294/4.89656. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.78406/4.89068. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.78109/4.89346. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.77863/4.89972. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.78081/4.91312. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.78057/4.89470. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77804/4.89710. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77871/4.91791. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.78397/4.89473. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.78296/4.88545. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.77740/4.89671. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.77342/4.94510. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77592/4.89751. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.77720/4.92129. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.77172/4.92228. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77104/4.93432. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.77639/4.90111. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.77871/4.91973. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.78124/4.91254. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.77026/4.93602. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.77747/4.90185. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.77226/4.90944. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.77700/4.89425. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.77500/4.90880. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.77757/4.92915. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.77638/4.89656. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.77659/4.90680. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.76945/4.93495. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.76503/4.94221. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.77513/4.90604. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.77169/4.90579. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77141/4.93811. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77996/4.86411. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79650/4.85640. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.78521/4.86239. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.77436/4.88676. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.76935/4.90821. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.76753/4.90070. Took 0.20 sec\n",
      "ACC: 0.578125, MCC: 0.15318083468998522\n",
      "Epoch 0, Loss(train/val) 4.76285/4.74140. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.70257/4.68990. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.69730/4.67852. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.69704/4.67488. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.69681/4.67016. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.70104/4.66761. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.69805/4.66697. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.69750/4.66589. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.69681/4.66353. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.69478/4.66238. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.69457/4.66304. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.69350/4.66587. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.69272/4.66453. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.69110/4.66716. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.69134/4.66372. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.69504/4.66588. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.69347/4.66336. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.68968/4.66594. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.68679/4.66526. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.69085/4.67268. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.68568/4.67396. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.69069/4.68064. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.68559/4.68354. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.68697/4.68312. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.68399/4.68408. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.68706/4.68569. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.68468/4.68527. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.68180/4.68358. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.68387/4.68858. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.68220/4.68358. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.67916/4.69462. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.68040/4.69939. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.68161/4.69138. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.67911/4.70391. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.68257/4.70331. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.67936/4.70435. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.68132/4.69490. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.67668/4.71798. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.68308/4.69151. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.67884/4.70557. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.67925/4.69092. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.67736/4.70478. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.67528/4.70660. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.67643/4.70603. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.67215/4.72730. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.67716/4.70712. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.67923/4.70175. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.67482/4.71262. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.67071/4.70422. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.67205/4.73202. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.67201/4.70372. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.67439/4.71437. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.67751/4.71583. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.67326/4.70727. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.67129/4.72364. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.67105/4.72929. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.67341/4.71760. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.66822/4.74584. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.67510/4.72236. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.67158/4.72948. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.67256/4.73814. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.66817/4.73655. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.66963/4.73918. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.67107/4.72843. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.67010/4.72461. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.66969/4.75013. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.66896/4.74868. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.67015/4.72558. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.67149/4.75617. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.66836/4.75563. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.67033/4.72833. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.66163/4.72273. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.67935/4.69726. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.67393/4.73173. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.67452/4.73130. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.67818/4.70411. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.67005/4.73150. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.66816/4.73143. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.66681/4.71295. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.67180/4.71819. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.66618/4.73195. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.66531/4.72788. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.66803/4.72038. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.68176/4.68036. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.67671/4.70999. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.67038/4.73074. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.66761/4.71753. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.66986/4.72368. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.67097/4.75568. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.66096/4.74688. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.66680/4.73940. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.66616/4.76290. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.66687/4.71607. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.66475/4.76816. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.65899/4.75370. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.66655/4.75118. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.65918/4.75286. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.66239/4.76412. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.66418/4.74675. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.65782/4.76875. Took 0.19 sec\n",
      "ACC: 0.359375, MCC: -0.2875509247045426\n",
      "Epoch 0, Loss(train/val) 5.22872/5.24317. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 5.16595/5.19889. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.15688/5.18493. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.15310/5.18366. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 5.15335/5.18593. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.15180/5.19055. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.15186/5.18693. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.15147/5.18643. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.15202/5.17706. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.14825/5.18668. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.14937/5.18850. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.15008/5.18124. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.15058/5.18817. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.14585/5.17847. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 5.14869/5.17881. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 5.14388/5.18445. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 5.14631/5.19312. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 5.14564/5.17751. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 5.14944/5.18564. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 5.14295/5.17151. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.14429/5.16798. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.14459/5.17344. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.14151/5.16986. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.14147/5.18006. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.14377/5.16468. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 5.14628/5.18795. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.14289/5.16260. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.14296/5.17789. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 5.14520/5.16325. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.14096/5.17005. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.14255/5.16753. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 5.14086/5.15579. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.14092/5.18251. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.14069/5.15813. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.13693/5.16413. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.13871/5.15937. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.14171/5.17514. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 5.13853/5.15929. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.14045/5.17626. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.13740/5.20122. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.13390/5.19280. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 5.13706/5.18965. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.13631/5.19228. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.13517/5.19111. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.13571/5.17468. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.13753/5.19154. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 5.13774/5.18362. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 5.13206/5.18709. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.13472/5.19743. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 5.13781/5.18964. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 5.13191/5.19266. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.13140/5.18879. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 5.12913/5.22773. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 5.13149/5.19309. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.13825/5.19507. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.12597/5.20292. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.12858/5.21864. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.13116/5.20881. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.13035/5.18897. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.13027/5.21215. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.12632/5.20314. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.13196/5.20739. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.12582/5.18926. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.12996/5.23139. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.12789/5.23470. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 5.12194/5.22115. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.12242/5.22989. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.12611/5.21438. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.12324/5.19357. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.12210/5.22937. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.12407/5.23696. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.12821/5.22804. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.12251/5.21905. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.11977/5.22830. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.11886/5.25809. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 5.12384/5.21792. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.11980/5.22565. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.12288/5.24512. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.12278/5.24926. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.11560/5.26615. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.12150/5.22574. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.12172/5.23140. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.11958/5.23906. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.11918/5.22065. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.11892/5.26151. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.11114/5.21690. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 5.11519/5.22831. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.11920/5.24752. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.12395/5.22208. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.11845/5.24524. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.11800/5.22493. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 5.11502/5.25998. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.11432/5.26743. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.11573/5.25600. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.10941/5.25205. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 5.11952/5.24151. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 5.11540/5.23384. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.11678/5.23433. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.11228/5.26865. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 5.10491/5.27520. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.058362172604259924\n",
      "Epoch 0, Loss(train/val) 4.90568/4.85164. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.89526/4.85058. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.87100/4.85296. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.85932/4.86218. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.85585/4.87099. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.85214/4.87545. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.85463/4.87762. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.85938/4.84663. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85890/4.85690. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.85369/4.86261. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.85017/4.87409. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.85334/4.87473. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85272/4.87374. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85216/4.87754. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.84912/4.87655. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.84751/4.87525. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.84591/4.87530. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.84712/4.87380. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.84570/4.87598. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.84731/4.87474. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84581/4.87388. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.84614/4.87122. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.84190/4.87649. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.84170/4.88681. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.84280/4.87744. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.84263/4.87990. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84580/4.87883. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.84246/4.88032. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.84010/4.88793. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.83812/4.88669. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84310/4.87339. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84279/4.87869. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.83937/4.89648. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84334/4.89095. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84108/4.89633. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84109/4.89214. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84002/4.89401. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.83637/4.89595. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.83493/4.90875. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.83795/4.88823. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.83875/4.87646. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.83710/4.89610. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83215/4.91321. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.83512/4.89488. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.83869/4.89021. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.83382/4.89887. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.83563/4.90333. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83044/4.91398. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.83413/4.89873. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.83132/4.91222. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.83103/4.90499. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.82808/4.91465. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.83829/4.89832. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.83389/4.91377. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.83039/4.92136. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.82919/4.93315. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.83211/4.91401. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.83062/4.90965. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.82847/4.92302. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.82992/4.91890. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.82932/4.92915. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.82725/4.94094. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.83120/4.92633. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.82549/4.92825. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.82702/4.92998. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.82529/4.93679. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.83167/4.91239. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.82673/4.91773. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.82629/4.93218. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.82439/4.93194. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.82827/4.91181. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83805/4.88285. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.84456/4.85823. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84212/4.86845. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.83393/4.88424. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.82965/4.90559. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.82990/4.91382. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.82951/4.92170. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.82671/4.91493. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.82406/4.93489. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.83093/4.92904. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.82880/4.92557. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.82174/4.95207. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.82705/4.92146. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.82518/4.92528. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.82409/4.93640. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.82411/4.92557. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.82695/4.90901. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.82346/4.92972. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.81972/4.93748. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82671/4.91676. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82349/4.95703. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82608/4.94552. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82572/4.93049. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82833/4.93800. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.81828/4.96338. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82592/4.94769. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82206/4.96081. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82446/4.94352. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.82278/4.95558. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.15819299929208316\n",
      "Epoch 0, Loss(train/val) 4.86593/4.82072. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.79710/4.81751. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.79854/4.80888. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79776/4.80821. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79866/4.81224. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79868/4.82459. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.80074/4.84471. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79737/4.84944. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79717/4.84607. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79594/4.84190. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79199/4.83388. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.79349/4.83810. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79442/4.84318. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79336/4.83656. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.79278/4.84133. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.79344/4.83556. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78914/4.83461. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.79140/4.83859. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.79134/4.83784. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78973/4.84155. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.78508/4.84314. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78888/4.83614. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78844/4.84207. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.79155/4.83917. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78895/4.84028. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78430/4.83942. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78617/4.84014. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78637/4.83439. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78355/4.83634. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78872/4.83667. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.78955/4.84011. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.78547/4.83851. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.78848/4.84034. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.78374/4.83793. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.78528/4.83182. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.78488/4.83159. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78603/4.83366. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.78197/4.84182. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.78274/4.83637. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.78330/4.83612. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.78327/4.82924. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.78325/4.83635. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.77962/4.83593. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.78126/4.84058. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77988/4.83908. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.78136/4.84430. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77985/4.83911. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.78176/4.83166. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.77595/4.84103. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77742/4.85003. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.77851/4.83391. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.77446/4.84326. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.78275/4.83449. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.77621/4.84667. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.77912/4.82780. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.77446/4.84909. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.77933/4.85045. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.77902/4.84676. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.77620/4.84599. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.77630/4.83859. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.77541/4.83376. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.77612/4.85092. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.77674/4.84313. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.77452/4.84750. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.77548/4.84484. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.77378/4.84659. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.77472/4.85621. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.77664/4.83248. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.78339/4.82642. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77312/4.85182. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.77191/4.85672. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.77337/4.85064. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.77122/4.86071. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.77165/4.86996. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77786/4.84550. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.77460/4.84732. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.77283/4.84339. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.76882/4.85175. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.77130/4.85856. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.76968/4.86817. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.77381/4.84712. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.76788/4.85095. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.76944/4.84865. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.77326/4.84386. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.77077/4.84334. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.76748/4.86302. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.77201/4.85547. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.76701/4.86339. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.77003/4.85318. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.77093/4.83869. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.76877/4.87758. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.76226/4.86151. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.77310/4.84899. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.76409/4.85063. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.76571/4.85690. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.76407/4.87534. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.76711/4.88444. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.76925/4.85748. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.77032/4.86424. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.76321/4.86980. Took 0.21 sec\n",
      "ACC: 0.453125, MCC: -0.1014574359634967\n",
      "Epoch 0, Loss(train/val) 4.95094/4.91141. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.92732/4.90503. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.91914/4.90609. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.91945/4.90088. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.91533/4.90258. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.90865/4.90331. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.90994/4.90276. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.91156/4.90311. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.91016/4.90456. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.91113/4.90521. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.90919/4.90466. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.90951/4.90583. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.90731/4.90294. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.90937/4.90445. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.90750/4.90673. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.90345/4.91137. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90375/4.90605. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.90491/4.90831. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.90241/4.90841. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.90447/4.90918. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.89862/4.90462. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.90313/4.89900. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.90000/4.90822. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.89836/4.90536. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.89758/4.91375. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.90385/4.90347. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.89797/4.90042. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.89449/4.90704. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.89434/4.91317. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.89415/4.90223. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.89613/4.89914. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.89465/4.90035. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.89614/4.90383. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.89868/4.90152. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.89676/4.91462. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.89388/4.90539. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.89566/4.89731. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.89468/4.89457. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.89320/4.90126. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.89051/4.89522. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.89607/4.90635. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.89056/4.91145. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.89531/4.89781. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.89623/4.91071. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.89430/4.91521. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.89469/4.91249. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.89092/4.90817. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.88862/4.89546. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.89313/4.90819. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.89024/4.92798. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89270/4.90917. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.88594/4.89769. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.88931/4.90495. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.88985/4.89621. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.88684/4.90075. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.88628/4.90210. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.88825/4.90838. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.89094/4.90903. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.88573/4.92081. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.88697/4.90777. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.88720/4.91974. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.88515/4.90165. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.88297/4.90313. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.88168/4.90685. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.88725/4.91709. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.88292/4.90150. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.88154/4.89862. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.88139/4.89668. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.88437/4.90883. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.88075/4.91055. Took 0.21 sec\n",
      "Epoch 70, Loss(train/val) 4.88566/4.89976. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.88115/4.90906. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.87602/4.89491. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88183/4.91190. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.88217/4.91237. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.88498/4.90046. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.87762/4.90576. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.88094/4.90898. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.88142/4.90681. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.88287/4.90275. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.88184/4.89810. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.87815/4.91551. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.87610/4.89790. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.87448/4.87884. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88033/4.92514. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.87211/4.90455. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.87083/4.90777. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.86701/4.90666. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.87106/4.91211. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.87210/4.90788. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.87121/4.90113. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.87429/4.91672. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.87501/4.91270. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.87608/4.89829. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.86157/4.90549. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.87425/4.91647. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.86760/4.91282. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.87280/4.89710. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.87635/4.94519. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.87554/4.91526. Took 0.20 sec\n",
      "ACC: 0.46875, MCC: -0.07100716024967263\n",
      "Epoch 0, Loss(train/val) 5.13520/5.05736. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.05805/5.01491. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 5.03596/5.01660. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.03127/5.01292. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.03122/5.01153. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.03067/5.01517. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 5.03182/5.01598. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.03041/5.01130. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.03232/5.01759. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.03169/5.01221. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 5.02730/5.01377. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.02482/5.01403. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.03059/5.00562. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.03558/5.01272. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 5.03425/5.01710. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 5.03089/5.01873. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.02950/5.01574. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.02819/5.01465. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 5.02596/5.01934. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 5.02784/5.01425. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 5.02826/5.01776. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.03001/5.00754. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 5.02868/5.01117. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.02893/5.01123. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.02319/5.00853. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 5.02497/5.01416. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 5.02401/5.01401. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 5.02505/5.01935. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 5.02198/5.01870. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 5.02277/5.01558. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 5.01815/5.02013. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.02226/5.01942. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.01760/5.01719. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.01733/5.02471. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.01777/5.02152. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 5.01540/5.02115. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.02067/5.02630. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 5.01342/5.02460. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.01612/5.02634. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 5.01764/5.01304. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.01555/5.01956. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 5.02113/5.03477. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.02778/5.02553. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.02211/5.02475. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 5.02358/5.02634. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 5.02476/5.03353. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 5.01916/5.03451. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 5.01725/5.03640. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.02355/5.03911. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.02032/5.04246. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.02056/5.04816. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.01861/5.04714. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 5.01980/5.05085. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 5.01834/5.04898. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.01829/5.04769. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.01546/5.05415. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 5.01984/5.03765. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 5.01708/4.99012. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.02000/5.00573. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 5.01591/5.02509. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.01404/5.04348. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.02020/5.04160. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 5.02208/5.03647. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 5.01567/5.03504. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 5.01430/5.04344. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 5.01565/5.04136. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.01280/5.04689. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.01428/5.04167. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.01602/5.04013. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 5.01763/5.04300. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 5.01581/5.03031. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 5.01070/5.03159. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 5.01379/5.04025. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 5.01435/5.03275. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.01222/5.04310. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.01212/5.04671. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 5.00970/5.05045. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.00954/5.04260. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 5.01406/5.03830. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.01221/5.03410. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.01561/5.04663. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 5.00913/5.04117. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.00763/5.04580. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 5.00616/5.03733. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.00831/5.04809. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 5.00805/5.04194. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.00481/5.03014. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.00984/5.03770. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.00894/5.03841. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 5.00519/5.03872. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.00885/5.01846. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 5.00585/5.03457. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.00898/5.02836. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.01288/5.02898. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.01012/5.07051. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 5.02341/5.04787. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 5.01458/5.05074. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 5.01191/5.06032. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.01224/5.05432. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 5.00917/5.05352. Took 0.20 sec\n",
      "ACC: 0.59375, MCC: 0.2464320290450721\n",
      "Epoch 0, Loss(train/val) 4.74830/4.71414. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.71643/4.73144. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.70980/4.73302. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.70480/4.73450. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.70517/4.73680. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.70544/4.74670. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.70596/4.75201. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.70398/4.74917. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.70528/4.73455. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.70654/4.71938. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.70269/4.71824. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.70027/4.72076. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.69895/4.72430. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.69891/4.72119. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.69855/4.71905. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.70033/4.71792. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.69932/4.71608. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.69348/4.71952. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.69862/4.72506. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.69654/4.72107. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.69513/4.71967. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.69664/4.71822. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.69218/4.72722. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.69505/4.72318. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.69301/4.72612. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.69606/4.72622. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.69313/4.72160. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.69220/4.72159. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.68999/4.72325. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.68874/4.72561. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.68625/4.74113. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.69023/4.73886. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.69012/4.73203. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.68920/4.73573. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.68809/4.73025. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.68583/4.74617. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.68546/4.73377. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.68622/4.73483. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.68026/4.72779. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.68778/4.72765. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.68503/4.73224. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.68668/4.72711. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.68538/4.72894. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.68897/4.72097. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.68570/4.71056. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.68687/4.73816. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.68179/4.73766. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.68115/4.72550. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.68573/4.71868. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.68510/4.71990. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.68641/4.71187. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.68713/4.72032. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.67913/4.73649. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.68198/4.72386. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.68349/4.73199. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.68094/4.72591. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.68159/4.72601. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.68146/4.71645. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.68116/4.72152. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.68410/4.73186. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.68603/4.72479. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.67805/4.73380. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.68363/4.72220. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.68031/4.72932. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.68093/4.73103. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.68269/4.72320. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.67975/4.73812. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.67775/4.72582. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.67627/4.72717. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.68077/4.72565. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.67465/4.74624. Took 0.21 sec\n",
      "Epoch 71, Loss(train/val) 4.68319/4.73165. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.68574/4.71800. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.67588/4.72244. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.68024/4.71681. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.68558/4.71867. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.68961/4.72679. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.68991/4.71983. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.68844/4.71427. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.68898/4.71536. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.68355/4.72190. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.68185/4.68776. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.68931/4.69626. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.68419/4.70814. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.68438/4.71703. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.67853/4.71547. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.68164/4.70477. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.68419/4.70908. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.68262/4.73147. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.67883/4.71869. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.68082/4.72096. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.67799/4.75042. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.67982/4.73244. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.67886/4.73507. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.67847/4.73933. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.69535/4.71175. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.68531/4.74322. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.68408/4.72556. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.68383/4.72615. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.67271/4.72976. Took 0.19 sec\n",
      "ACC: 0.296875, MCC: -0.38105777041717415\n",
      "Epoch 0, Loss(train/val) 4.84132/4.76456. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.78651/4.75717. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.78280/4.75617. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.77851/4.75572. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.77823/4.75468. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.77744/4.75410. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.77739/4.75642. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.77750/4.75681. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.77875/4.75828. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.77788/4.75744. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.77679/4.75794. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.77403/4.75693. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.77654/4.75811. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.77266/4.75611. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.77568/4.75523. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.77378/4.75758. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.77382/4.75854. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.77251/4.75607. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.77126/4.75646. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.77372/4.75834. Took 0.18 sec\n",
      "Epoch 20, Loss(train/val) 4.77367/4.75626. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.77346/4.75508. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.77174/4.75517. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.77169/4.75314. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.77188/4.75715. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.76916/4.75952. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.77012/4.75377. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.77002/4.75447. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.76856/4.75940. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 4.77001/4.75654. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.76625/4.75916. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77250/4.76642. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77173/4.75292. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.77081/4.76136. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.76773/4.75630. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.76878/4.75483. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.76650/4.75781. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.76514/4.73929. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.76647/4.74295. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.76738/4.75185. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.76466/4.74924. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.76606/4.75014. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76976/4.77385. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.76482/4.76368. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.76571/4.76451. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.76248/4.77256. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.76048/4.75837. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.76177/4.77003. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.76036/4.75648. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.76379/4.75783. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76208/4.75288. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.75776/4.76695. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.76102/4.75846. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76278/4.75650. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76172/4.76217. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.76374/4.76240. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.76001/4.76682. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76220/4.77898. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.75961/4.78214. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.76182/4.76330. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.75965/4.77220. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.75592/4.77231. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.75442/4.76915. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.75966/4.77482. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76125/4.75977. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.76017/4.76113. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75300/4.77028. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.75713/4.78176. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.76038/4.77336. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.76449/4.77744. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.75940/4.78078. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.75479/4.77277. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.76844/4.76869. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.77738/4.76713. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77550/4.75904. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.77305/4.75389. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.77177/4.75427. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.76942/4.75787. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.76819/4.76252. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.76504/4.76421. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.76070/4.75522. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 4.75987/4.76306. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.76179/4.74003. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.76197/4.75093. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.75879/4.75262. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.76511/4.76483. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.75852/4.78180. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.75922/4.78752. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.75968/4.78706. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.75926/4.77659. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.75663/4.77479. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.75442/4.76706. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.75641/4.75935. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.75327/4.76967. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.75492/4.75785. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75633/4.76352. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.75551/4.77636. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.75910/4.78406. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.75498/4.78377. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75372/4.77401. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.11724137931034483\n",
      "Epoch 0, Loss(train/val) 4.95283/4.88176. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.89493/4.87952. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.88554/4.87726. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.88194/4.88038. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.88148/4.88156. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.88079/4.88421. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.87994/4.89330. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.87929/4.89345. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.87799/4.89230. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87605/4.89664. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.87290/4.90599. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.87233/4.91095. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.87695/4.89709. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.87441/4.90512. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87095/4.90790. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.87566/4.89300. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.87032/4.90043. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.86976/4.91050. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.87053/4.90819. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.86989/4.89411. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.86856/4.90344. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.86879/4.89851. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.86630/4.91492. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.86750/4.90646. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.87033/4.90449. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.86902/4.90795. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.86701/4.90647. Took 0.22 sec\n",
      "Epoch 27, Loss(train/val) 4.86663/4.91089. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.86827/4.89918. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.86500/4.90808. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.86499/4.90964. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86018/4.90283. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.86865/4.90964. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.86653/4.92579. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.86741/4.90383. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.86131/4.91511. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.86271/4.90397. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.86143/4.91417. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.86329/4.92499. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.86111/4.90930. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.87155/4.91273. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.86283/4.91944. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.86011/4.93561. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.85942/4.92270. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.86289/4.91536. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.86410/4.91234. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.85879/4.92243. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.86612/4.89749. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.85721/4.92797. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.85667/4.92381. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.86553/4.90847. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.85304/4.92625. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.86229/4.91244. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 4.85639/4.94256. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.86090/4.89691. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 4.85835/4.92811. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.86027/4.90322. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.86875/4.90782. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.86085/4.93097. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.85608/4.92948. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85815/4.91527. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.85629/4.93689. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85398/4.95060. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.85323/4.94128. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.85387/4.93574. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.85979/4.91535. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.85639/4.93200. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.85487/4.92692. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.85276/4.95359. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.85821/4.92874. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.84913/4.94398. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.85468/4.92740. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.84862/4.93802. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.85183/4.93203. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.84791/4.96418. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.85407/4.93320. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.85301/4.92576. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.85156/4.93989. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.84950/4.92166. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.84775/4.94403. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.84767/4.94211. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.84603/4.93938. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.85848/4.92353. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.85032/4.94298. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.84950/4.90722. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.84978/4.94188. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.85014/4.93107. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.84592/4.96539. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.84351/4.93738. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.84657/4.92328. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.84913/4.94523. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85218/4.92805. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.85137/4.93322. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.84668/4.92892. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.85381/4.90873. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.84955/4.93328. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.84539/4.93293. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.84972/4.93494. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.84682/4.95397. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.84632/4.94372. Took 0.19 sec\n",
      "ACC: 0.671875, MCC: 0.34527065131588947\n",
      "Epoch 0, Loss(train/val) 4.76815/4.73061. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.74034/4.75101. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.73848/4.74087. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.73829/4.72983. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.74156/4.72994. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.73655/4.74113. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.73296/4.74619. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.72682/4.75019. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.72572/4.75078. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.72537/4.75517. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.72153/4.75412. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 4.72452/4.76461. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.72294/4.76572. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.72178/4.76257. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.72081/4.76817. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.72441/4.75499. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.71880/4.76549. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.71974/4.76186. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.71991/4.77444. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.71809/4.76648. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.71456/4.77037. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.71449/4.77497. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.71806/4.76138. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.71878/4.75626. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.71923/4.75980. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.71966/4.76092. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.71594/4.76779. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.71417/4.76716. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.71374/4.77465. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.71331/4.77825. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.71308/4.76488. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.71503/4.77500. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.71391/4.78029. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.71324/4.76843. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.71213/4.78995. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.71258/4.76440. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.70852/4.77255. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.70901/4.77540. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.70738/4.77640. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.71169/4.77646. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.71192/4.78499. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.70311/4.79644. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.70869/4.77153. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.71025/4.77776. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.70486/4.78824. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.71082/4.76635. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.70741/4.77361. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.70376/4.79438. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.70926/4.77540. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.71016/4.78017. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.70921/4.77160. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.70606/4.78821. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.70513/4.77851. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.70347/4.79629. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.70579/4.77193. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.70726/4.77025. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.70404/4.79130. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.70304/4.77535. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.70228/4.78334. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.70579/4.78036. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.70480/4.78645. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.69923/4.81192. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.71007/4.79510. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.70432/4.79791. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.70147/4.80167. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.70277/4.77901. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.70704/4.78240. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.70453/4.78699. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.70437/4.78579. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.70177/4.78741. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.70262/4.79308. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.70327/4.81692. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.69890/4.80268. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.70446/4.78664. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.70426/4.79442. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.70434/4.78125. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.69770/4.81125. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.69967/4.80131. Took 0.21 sec\n",
      "Epoch 78, Loss(train/val) 4.70419/4.78942. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.70060/4.79909. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.70116/4.78986. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.69571/4.82397. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.70032/4.80324. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.70403/4.77981. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.69232/4.82151. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.69704/4.79057. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.69861/4.80280. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.70010/4.82035. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.70195/4.80518. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.69867/4.81273. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.69570/4.80477. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.69624/4.80102. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.70130/4.81236. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.69907/4.81934. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.69619/4.80149. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.69894/4.80099. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.69760/4.80578. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.70036/4.79857. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.69516/4.80906. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.69873/4.80179. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.16834512458535864\n",
      "Epoch 0, Loss(train/val) 4.92672/4.87184. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86981/4.86296. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.86287/4.86455. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.86243/4.86199. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.86248/4.86290. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.86116/4.86344. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.86156/4.86292. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.85912/4.86422. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.85647/4.86410. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.85720/4.86614. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.85428/4.86419. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.85950/4.86060. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85456/4.85916. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85790/4.86378. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85167/4.86950. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.85866/4.87326. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85631/4.88329. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85897/4.87928. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85274/4.89034. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.85458/4.88721. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.85465/4.88833. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85615/4.88373. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.85417/4.88736. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.85071/4.89646. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85291/4.88587. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.85448/4.88570. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84922/4.89899. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.85170/4.88878. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.85276/4.88845. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84733/4.89734. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.85399/4.88655. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84715/4.89079. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.84741/4.89072. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84791/4.89188. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.85086/4.89734. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84978/4.90076. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84615/4.89844. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84893/4.89732. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84904/4.89685. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84615/4.89896. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84948/4.90266. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84822/4.90131. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.84348/4.91104. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.84790/4.89682. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.84587/4.90213. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84348/4.90265. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.84439/4.90749. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84821/4.89386. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.85044/4.88711. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.85040/4.88230. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.84265/4.89973. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.84170/4.88585. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.84882/4.88686. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.84432/4.89274. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.84059/4.89531. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.84181/4.90209. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84308/4.90297. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.83808/4.91902. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.84553/4.88460. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.84224/4.90504. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.84269/4.91898. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.83951/4.90319. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.84267/4.90481. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.84097/4.90687. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.84072/4.89403. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.83980/4.89061. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.83657/4.90636. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.84106/4.91016. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.84096/4.90066. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84092/4.90773. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.83675/4.92851. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.84013/4.91663. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83971/4.92881. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83247/4.91966. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.83254/4.93180. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.83926/4.90440. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83751/4.90486. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.83817/4.91377. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.84037/4.91200. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.83965/4.88906. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.84345/4.89017. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.84563/4.89248. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.83460/4.90392. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84078/4.89897. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.83573/4.88963. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.84053/4.89584. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.83711/4.91222. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83948/4.89216. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83071/4.90220. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83072/4.90642. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.84060/4.90269. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.83460/4.88549. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.83517/4.89266. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83550/4.91397. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83966/4.90079. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82980/4.90287. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83474/4.90014. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82858/4.91256. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.83056/4.92276. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.83075/4.92636. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.061541083462958945\n",
      "Epoch 0, Loss(train/val) 4.71621/4.64304. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.63894/4.63718. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.63740/4.63470. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.63704/4.63336. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.64033/4.63649. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.63821/4.63566. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.63723/4.63834. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.63793/4.63975. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.63680/4.63776. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.63471/4.64112. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.63362/4.64868. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.63401/4.65230. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.63294/4.65257. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.63433/4.65778. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.63436/4.65566. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.63073/4.66284. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.63381/4.66188. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.63319/4.66314. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.63189/4.66793. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.62972/4.67549. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.63108/4.67138. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.62744/4.67966. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.63020/4.67031. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.62490/4.68820. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.62412/4.68981. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.62353/4.68853. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.62615/4.67943. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.62175/4.69298. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.62381/4.69211. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.62458/4.67572. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.62472/4.68554. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.62138/4.68501. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.62612/4.67777. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.62020/4.68182. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.62013/4.68847. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.62002/4.68181. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.62320/4.67828. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.61860/4.68525. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.61708/4.70808. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.61686/4.68730. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.61726/4.69359. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.61746/4.69476. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.61515/4.69284. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.62230/4.68669. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.61557/4.69662. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.61766/4.69211. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.61513/4.71313. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.61864/4.68274. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.61273/4.70069. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.60804/4.71588. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.61809/4.67640. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.61051/4.70997. Took 0.18 sec\n",
      "Epoch 52, Loss(train/val) 4.60837/4.71615. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.60841/4.72784. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.61013/4.72477. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.60632/4.71688. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.61699/4.70992. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.60833/4.75990. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.61224/4.70356. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.60978/4.71691. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.60662/4.71890. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.60924/4.71502. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.60269/4.75590. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.60683/4.71668. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.61175/4.71406. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.60642/4.72277. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.60912/4.71921. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.60074/4.75422. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.60672/4.71609. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.61399/4.72073. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.60689/4.70544. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.60336/4.72969. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.60917/4.72540. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.61145/4.70676. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.60638/4.71794. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.59854/4.75060. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.60932/4.70530. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.60745/4.71514. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.61965/4.66883. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.61716/4.67455. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.61180/4.69703. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.61204/4.69638. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.61208/4.70714. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.61500/4.70192. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.61084/4.64198. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.63402/4.64644. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.61634/4.66803. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.61434/4.68922. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.60945/4.71047. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.60604/4.71094. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.60250/4.72349. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.61112/4.70120. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.60536/4.72883. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.60752/4.70009. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.60444/4.71846. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.60665/4.70148. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.61375/4.68791. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.62232/4.63652. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.61799/4.65418. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.62134/4.65428. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.13156218570112832\n",
      "Epoch 0, Loss(train/val) 4.91374/4.88188. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.87630/4.86477. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.87447/4.87140. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.87210/4.87546. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.87580/4.88688. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.87521/4.89730. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.87603/4.88912. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.87088/4.87820. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.86558/4.87797. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.86542/4.88645. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.86559/4.89174. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.86575/4.89087. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.86367/4.89658. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.87160/4.88665. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.86725/4.87722. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.86365/4.87740. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.86294/4.88100. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.86371/4.88253. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85950/4.88968. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.86071/4.90044. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.86080/4.90543. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85698/4.91223. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.85430/4.93158. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85663/4.91234. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85470/4.92368. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.85290/4.92463. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.85191/4.91924. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.85315/4.93517. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.85396/4.91791. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84843/4.91962. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.85610/4.89693. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.84977/4.93394. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.85650/4.90676. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.85254/4.93310. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85451/4.92434. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84594/4.93310. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.84609/4.93635. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84326/4.93420. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84399/4.94075. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.84539/4.95483. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84391/4.92954. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.83977/4.95424. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.83877/4.95694. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.84213/4.95873. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84384/4.96009. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.83671/4.95795. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.83771/4.96170. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83558/4.97638. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.84606/4.94641. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.84061/4.96099. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.84321/4.95086. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.84033/4.96293. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.83637/4.94951. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.83279/4.98340. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.83840/4.97081. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.83203/4.99566. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.83319/4.98677. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.83889/4.96727. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.83513/4.98297. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.83312/4.99880. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.83462/4.97351. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.82880/5.01243. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.83162/4.98907. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83783/4.96612. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.83267/4.99252. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.83359/5.00106. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 4.83599/4.98851. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.82827/5.00626. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.84135/4.94210. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.83646/4.98361. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.83162/5.01707. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83376/4.96955. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83278/4.96533. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83524/4.97074. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.82948/4.96731. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.83369/4.94651. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83081/4.99295. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.83387/4.93773. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.82729/5.00323. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.83417/4.94756. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.82304/5.00581. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83104/4.96932. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.82806/4.99276. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.82268/5.02261. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83250/4.95871. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.82413/4.99796. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.82788/5.03248. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83005/5.00925. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.82460/5.01288. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.83083/5.00924. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82566/5.02600. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.83321/4.99128. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82516/5.03557. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82471/5.02196. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82373/5.06276. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.82657/5.02016. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82439/5.05877. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83052/5.00366. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.83307/4.98149. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.83279/5.01555. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.03126526997403612\n",
      "Epoch 0, Loss(train/val) 4.77704/4.74792. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.74051/4.73890. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 4.73874/4.73388. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.73861/4.73033. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.73776/4.73243. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.73549/4.73211. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.73473/4.73203. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.73420/4.73150. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.73660/4.73689. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.73443/4.73945. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.73472/4.74069. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.73519/4.74150. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.73282/4.74254. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.72905/4.74743. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.73136/4.75310. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.73221/4.76115. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.72957/4.75822. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.73076/4.76307. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.72686/4.76136. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.72710/4.76783. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.73011/4.77676. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.72523/4.77733. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.72437/4.77044. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.72422/4.77373. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.72478/4.77848. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.71999/4.78553. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.72218/4.76993. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.72415/4.77015. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.72071/4.77508. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.72031/4.77646. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.72515/4.77674. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.72497/4.77751. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.72456/4.77600. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.72388/4.77394. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.72346/4.77261. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.72044/4.77178. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.72580/4.77673. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.72255/4.78291. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.72427/4.77726. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.72254/4.77341. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.72162/4.78126. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.71732/4.77413. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.72418/4.76658. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.71981/4.77219. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.71930/4.76446. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.72177/4.76672. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.71746/4.76801. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.71909/4.76272. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.72121/4.76790. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.72009/4.77720. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.72019/4.76780. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.71801/4.77078. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.71668/4.77329. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.72014/4.76922. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.72098/4.76624. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.71906/4.76658. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.71902/4.76563. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.71632/4.77227. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.71902/4.76988. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.71735/4.77076. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.71832/4.77873. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.71334/4.78063. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.71713/4.77133. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.71481/4.77569. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.71544/4.77605. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.71891/4.77390. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.71981/4.78505. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.71756/4.77704. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.71884/4.77482. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.71239/4.77377. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.71163/4.77839. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.71831/4.77867. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.71563/4.77527. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.71782/4.76449. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.71544/4.77005. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.72167/4.76203. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.71843/4.77987. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.71658/4.79147. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.71895/4.77637. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.71451/4.78232. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.71310/4.78686. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.71736/4.78231. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.71845/4.78145. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.71794/4.77554. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.71291/4.78290. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.71172/4.77609. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.71054/4.77652. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.71204/4.80332. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.71629/4.77920. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.71660/4.77297. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.71599/4.77335. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.71166/4.78332. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.71511/4.77854. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.71644/4.77943. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.71422/4.77669. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.70937/4.78390. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.71554/4.78417. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.71192/4.79279. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.71483/4.77845. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.71248/4.78625. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.0778902520624173\n",
      "Epoch 0, Loss(train/val) 4.93770/4.89855. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.89671/4.90206. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.89785/4.90053. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.89322/4.90178. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.89750/4.90173. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.89644/4.89798. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.89717/4.89502. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.89499/4.89316. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.89354/4.89087. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.89248/4.89089. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.89493/4.89201. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.89113/4.89308. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.89140/4.89057. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.88806/4.90054. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.88615/4.89655. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.89280/4.89242. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.88880/4.89727. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.88521/4.90090. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.88455/4.90427. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.88572/4.90532. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.88574/4.90310. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.88423/4.89991. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.88052/4.91331. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.88397/4.91384. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.88245/4.90464. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.88541/4.89976. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.88871/4.89886. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.88614/4.90607. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.88827/4.90848. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.88223/4.91252. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.88191/4.90249. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.88092/4.90983. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.87739/4.90359. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.88059/4.90327. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.88557/4.88922. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.88213/4.92708. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.88102/4.89137. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.88148/4.91106. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.88037/4.89406. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.88116/4.89531. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.87763/4.90238. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.87947/4.93086. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89028/4.89104. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.88613/4.90988. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.88136/4.88178. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.88157/4.92421. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.87909/4.90478. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.88314/4.90309. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.87897/4.91734. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.88221/4.89546. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.87680/4.90393. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.87661/4.90330. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.87827/4.89912. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.87304/4.89951. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.87932/4.89536. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.87526/4.90319. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.87902/4.89755. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.87091/4.90272. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.87295/4.90750. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.87368/4.91014. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.87956/4.89680. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.87452/4.92451. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.87644/4.91305. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.87776/4.90689. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.87800/4.90406. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.87579/4.91597. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.87688/4.91158. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.88089/4.90358. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.87218/4.90567. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.87509/4.91354. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.87441/4.90130. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.87116/4.92170. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.87498/4.91182. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.87276/4.91293. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.87450/4.90483. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.87170/4.91170. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.86864/4.91358. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.87028/4.92612. Took 0.18 sec\n",
      "Epoch 78, Loss(train/val) 4.87198/4.90470. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.87039/4.89169. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.87666/4.89867. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.87342/4.90288. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.87109/4.92461. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.87878/4.90178. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.87085/4.90769. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.87328/4.90153. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.86840/4.91230. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.87194/4.90920. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.86983/4.90469. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.87163/4.91234. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.87648/4.90254. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.88268/4.92028. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.88792/4.89207. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.88561/4.89708. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88094/4.89501. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.87932/4.90090. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.87683/4.90833. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.88062/4.90003. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.87930/4.90183. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.87884/4.90208. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.15231225557722924\n",
      "Epoch 0, Loss(train/val) 4.93620/4.92629. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.88434/4.91098. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.87546/4.88853. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.86953/4.87357. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.86636/4.87409. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.86712/4.87934. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.86633/4.88394. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.86719/4.88056. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.86746/4.87818. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.86556/4.87387. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.86326/4.86749. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.86295/4.87091. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 4.86206/4.87221. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.86176/4.86705. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.86215/4.87147. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.86088/4.87060. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.86162/4.87204. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.86216/4.86963. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.86144/4.86624. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.85927/4.86730. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.86007/4.86857. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85965/4.86636. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85895/4.86076. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.86067/4.86578. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85964/4.86657. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.85868/4.86461. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.86164/4.85803. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.85722/4.85620. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.85975/4.85804. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.85645/4.86167. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.85741/4.86016. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.85712/4.86105. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.85729/4.86303. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.85502/4.86265. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85691/4.86392. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.85229/4.86870. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.85323/4.86495. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.85312/4.85403. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.85378/4.85750. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.85324/4.86485. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.85658/4.86756. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84978/4.85513. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.85211/4.85932. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.85128/4.86499. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.85241/4.85118. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84944/4.84989. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.85009/4.85204. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84785/4.85912. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.84729/4.84721. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.84492/4.84980. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.84567/4.86472. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.84450/4.85814. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.84490/4.86022. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.84648/4.85852. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.84470/4.85068. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.84571/4.85339. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84248/4.85500. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.83867/4.85801. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.84962/4.84900. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.84263/4.85329. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.83621/4.85945. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84248/4.85952. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.83856/4.84507. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.83843/4.85333. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.84431/4.84377. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.83949/4.84205. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.84196/4.85598. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83801/4.85531. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.84728/4.85293. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.83978/4.85065. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.84121/4.85273. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.84183/4.85866. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83973/4.85363. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84102/4.85067. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.83844/4.84842. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.83620/4.85189. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.84068/4.85327. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.84240/4.85414. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.83502/4.85241. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.83302/4.85736. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.84235/4.85992. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83762/4.84945. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.83400/4.85213. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84060/4.85477. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.84087/4.85620. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83402/4.85718. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.83989/4.86083. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.83708/4.85107. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83765/4.87105. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.84142/4.85304. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.83526/4.84056. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.83961/4.84037. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.83985/4.84222. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.84071/4.84070. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83824/4.84868. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.83498/4.84374. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.83580/4.85182. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83868/4.85866. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.83518/4.86242. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.83530/4.86093. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.03779786830012622\n",
      "Epoch 0, Loss(train/val) 4.96088/4.93523. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.94222/4.93101. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.93513/4.92953. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.93998/4.92655. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.94318/4.92545. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.94117/4.93572. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.93591/4.94491. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.93154/4.94176. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.92671/4.93562. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.92857/4.93149. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.93125/4.92806. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.93090/4.92725. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.92822/4.93223. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.92482/4.92781. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.92638/4.92602. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.92691/4.92235. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.92405/4.91626. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.92445/4.92036. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.92493/4.91967. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.92149/4.91708. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.92229/4.91188. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.92221/4.91761. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.92028/4.91068. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.92062/4.91280. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.92084/4.91891. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.91634/4.91688. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.92012/4.91196. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.91443/4.91068. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.91602/4.91075. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.91369/4.91586. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.91693/4.91438. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.91126/4.90976. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.91903/4.91924. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.92041/4.93143. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.91914/4.93043. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.92055/4.92964. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.91783/4.93363. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.91186/4.93264. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.91030/4.93563. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.91313/4.93233. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90806/4.93358. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.90836/4.93255. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.91190/4.92572. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.91110/4.92437. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.90730/4.92598. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.90964/4.92731. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.90969/4.92887. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.90680/4.92599. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.90703/4.92808. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.90913/4.92961. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.90705/4.92826. Took 0.18 sec\n",
      "Epoch 51, Loss(train/val) 4.91102/4.92119. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.91055/4.91853. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.90096/4.92681. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.90738/4.92508. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.90565/4.92930. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.90836/4.91583. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.90333/4.92651. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.90930/4.92577. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 4.90216/4.92330. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.90166/4.92129. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.89915/4.92971. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.90787/4.92410. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.90290/4.91534. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.90769/4.91890. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.89914/4.91970. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.90128/4.92646. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.90197/4.92128. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.89603/4.92902. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.90270/4.92614. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.90383/4.92220. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.90485/4.93265. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.90236/4.92883. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.89599/4.91935. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.90151/4.91930. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.90392/4.93699. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.90076/4.92627. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.90074/4.92050. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.89902/4.92709. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.89881/4.93388. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.90326/4.93878. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.90587/4.92606. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.90533/4.92399. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.90289/4.92268. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.90001/4.92527. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.89642/4.92338. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.89156/4.93016. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.89881/4.91845. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.89925/4.91461. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.89997/4.93055. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.89963/4.92867. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.89658/4.92993. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.90063/4.93375. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.90384/4.92280. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.89495/4.92425. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.89710/4.92276. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.89633/4.91846. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.89909/4.93406. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.89909/4.93040. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.89628/4.93186. Took 0.20 sec\n",
      "ACC: 0.46875, MCC: -0.043224813349211175\n",
      "Epoch 0, Loss(train/val) 5.02116/4.93462. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.96141/4.98182. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.95204/4.96896. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.93625/4.94099. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.93111/4.95398. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.93603/4.96456. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.93683/4.95669. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.93431/4.95135. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.93321/4.95548. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.93434/4.95620. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.93294/4.95462. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.93102/4.94803. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.93216/4.95490. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.92923/4.94870. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.92670/4.94211. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.92419/4.95671. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.92486/4.94007. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.92070/4.94965. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.91946/4.95431. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.91848/4.93691. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.91584/4.95463. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.91717/4.95740. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.91607/4.95187. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.91313/4.95740. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.90826/4.98221. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.91306/4.95894. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.91120/4.97374. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.90937/4.97439. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90871/4.95558. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.90546/4.97794. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.90719/4.97197. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.91102/4.95304. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.91820/4.95546. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.90629/4.95270. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.90582/4.96679. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.91212/4.95266. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.91304/4.93527. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.90574/4.94897. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.90969/4.95528. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.90809/4.96384. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90743/4.94766. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.90417/4.95869. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.90416/4.95231. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.90052/4.97184. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.90752/4.94729. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.89990/4.95330. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.90326/4.97305. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.90237/4.94304. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89712/4.96080. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.90081/4.97541. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.90184/4.95796. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.90978/4.94091. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.90312/4.96105. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.89911/4.95554. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.90072/4.96156. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.90501/4.95980. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.89744/4.94580. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.89956/4.97183. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.89682/4.97344. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.90050/4.96251. Took 0.21 sec\n",
      "Epoch 60, Loss(train/val) 4.90171/4.96176. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.89491/4.99771. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 4.89925/4.98913. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.89758/4.96495. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.89395/4.99016. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.89564/4.98742. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 4.89478/4.98167. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.90249/4.96357. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.89517/4.97899. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.89302/4.97299. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.89183/4.96535. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.89070/5.00867. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.89136/4.99260. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.89308/4.97091. Took 0.21 sec\n",
      "Epoch 74, Loss(train/val) 4.88238/4.98853. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.89773/5.00999. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.89339/4.95371. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.88775/4.99567. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.89314/4.99331. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.89244/4.96194. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88280/5.03157. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.88936/5.00249. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.90563/4.96121. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.91292/4.96556. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.90527/4.96504. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.90704/4.96848. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.90343/4.93727. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.89342/4.98208. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.90017/4.95997. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.89722/4.94484. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.89088/4.94883. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.89231/5.00998. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.90468/4.95926. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.89436/4.97320. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.89466/5.00505. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.88467/4.96070. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.89341/4.98892. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.88605/4.95694. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.89022/4.94616. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88510/4.99621. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.03138824102871723\n",
      "Epoch 0, Loss(train/val) 4.82081/4.81855. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.78667/4.77965. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.78417/4.78265. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.77857/4.78684. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.78091/4.78651. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.77909/4.79159. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.77669/4.79323. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.77779/4.79837. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.77480/4.79658. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.77682/4.79611. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.77550/4.80426. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.77413/4.80979. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.77158/4.80989. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.77468/4.80638. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.77030/4.80880. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 4.77378/4.80471. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.77305/4.81103. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.77039/4.81754. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.76744/4.81422. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.77090/4.81306. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.77090/4.81064. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.76828/4.80292. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.76807/4.79635. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.76883/4.79519. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.77017/4.79775. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.76622/4.81733. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.76543/4.81676. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 4.76567/4.80333. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.76772/4.80801. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.76670/4.80917. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.76666/4.80290. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.76472/4.80278. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.76677/4.80526. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.76775/4.80890. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.76424/4.81536. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.76242/4.80084. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.75726/4.81347. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.76568/4.79005. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.76267/4.80851. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.76294/4.79877. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.76511/4.80193. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.76525/4.79775. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.75881/4.79249. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.76038/4.78788. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.75859/4.79132. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.76527/4.78503. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.76048/4.80019. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.75603/4.80267. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.75993/4.79745. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.75814/4.79566. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76095/4.79977. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 4.76178/4.80927. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.75526/4.81235. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76289/4.79612. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.75317/4.79883. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.75733/4.78960. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.75823/4.80328. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.75796/4.79850. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.75667/4.78611. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.75683/4.78574. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.74995/4.79347. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.75377/4.79685. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.75744/4.79607. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.75502/4.79772. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.75764/4.80458. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.75202/4.80096. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.74974/4.79121. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.75256/4.78914. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.75003/4.80247. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.74942/4.79872. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.74506/4.80501. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.74809/4.78256. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.74761/4.78442. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.75077/4.80467. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.75927/4.87270. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.76574/4.80687. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.76153/4.81516. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.74942/4.81418. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.75253/4.81145. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.75841/4.81250. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.75642/4.79463. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.74977/4.79960. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.74719/4.79127. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.74825/4.80104. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.75252/4.80337. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.74412/4.79911. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.74937/4.78757. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.74844/4.79300. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.74777/4.79786. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.74667/4.78843. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.74975/4.79977. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.74561/4.80659. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.75122/4.79617. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.74649/4.79852. Took 0.21 sec\n",
      "Epoch 94, Loss(train/val) 4.74551/4.79767. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.73866/4.79884. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.74271/4.80759. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.74185/4.80618. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.74453/4.81054. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.73621/4.82228. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.021109792565893827\n",
      "Epoch 0, Loss(train/val) 4.73416/4.74753. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.71922/4.79170. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.72695/4.77740. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.75203/4.71625. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.72474/4.71508. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.71026/4.71739. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.71184/4.71590. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.71248/4.71411. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.70939/4.71293. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.70975/4.70932. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 4.71155/4.70921. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.70820/4.70538. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.70521/4.70601. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.70799/4.71394. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.70855/4.70664. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.70815/4.71018. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.70231/4.71345. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.70115/4.70755. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.69989/4.72703. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.70042/4.72230. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.70136/4.73066. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.69842/4.72335. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.70330/4.71260. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.70146/4.75216. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.69465/4.72751. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.69888/4.72147. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.69806/4.73102. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.69753/4.72767. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.69978/4.74896. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.69732/4.74375. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.69246/4.75552. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.69264/4.74888. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.69732/4.72825. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.69617/4.75238. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.69194/4.75917. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.69076/4.75054. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.69486/4.75523. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.69556/4.75476. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.68895/4.76299. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.69549/4.73650. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.69038/4.79237. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.69396/4.71851. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.69486/4.78235. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.69232/4.73312. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.69233/4.77594. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.68905/4.74241. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.69122/4.76740. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.69118/4.77581. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.69373/4.72320. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.69062/4.78247. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.69186/4.73746. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.69101/4.75200. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.69989/4.73867. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.70587/4.72182. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.70718/4.72392. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.70297/4.72221. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.70031/4.71301. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.70023/4.73013. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.69382/4.73202. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.69104/4.76959. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.69230/4.74338. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.69460/4.73343. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.69440/4.74710. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.69153/4.75054. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.68939/4.73574. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.69233/4.77379. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.69099/4.74852. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.69085/4.76508. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.69019/4.79116. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.68853/4.75771. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.68627/4.79585. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.68787/4.75219. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.68951/4.78423. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.68628/4.77637. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.68754/4.76430. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.68533/4.79163. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.68730/4.80963. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.68613/4.76111. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.68725/4.79113. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.68648/4.76585. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.68479/4.79816. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.69204/4.71495. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.69408/4.78274. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.69600/4.72591. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.68626/4.77174. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.69706/4.75353. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.69153/4.76719. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.69181/4.75210. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.68713/4.74275. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.68909/4.76158. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.68921/4.77107. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.68034/4.77473. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.68832/4.78355. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.68666/4.77413. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.68430/4.78906. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.68571/4.76818. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.68285/4.79514. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.69064/4.75332. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.68489/4.79680. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.68876/4.73887. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.08818077954471167\n",
      "Epoch 0, Loss(train/val) 5.00621/4.97188. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.96722/5.00342. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.97473/5.00456. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.97326/5.00482. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.97540/4.98967. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.97883/4.97276. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.96753/4.97276. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.96303/4.97397. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.96460/4.97538. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.96384/4.97492. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.96445/4.98104. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.96535/4.97904. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.96250/4.98095. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.96106/4.98626. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.96339/4.98654. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.95646/4.99291. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.96165/4.99104. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.95741/4.98821. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.95400/4.99649. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.95747/5.00145. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.95732/4.98898. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.95398/4.99761. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.95823/4.99178. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.95747/4.98875. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.95958/4.99037. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.94933/4.99651. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.95418/4.99833. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.95537/4.99493. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.95055/4.99618. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.94954/5.00610. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.95020/4.99112. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.95685/4.99324. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.95399/4.99255. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 4.95034/5.00838. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.95040/5.00430. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.95355/4.99503. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.95434/5.01111. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.95084/5.00713. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.94640/5.00824. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.94830/5.02609. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 4.94437/5.02228. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.95409/5.00405. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.94639/5.00882. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.94881/5.01004. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.94992/5.00005. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.94555/5.01446. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.94522/5.00678. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.94664/5.00923. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.94088/5.01766. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.94587/4.99973. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.94429/5.00868. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.94274/5.01166. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.94489/5.02222. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.94301/4.99793. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.94423/5.01641. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.94323/5.00044. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.93919/5.02028. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.93931/5.02497. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.93690/4.99370. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.94096/4.99813. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.94144/5.02421. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.94274/4.97726. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.93545/5.02415. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.93941/5.03034. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.93768/5.01281. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.94273/5.00830. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.93434/4.99896. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.94043/4.99075. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.95207/4.99060. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.95274/4.99676. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.94656/4.99382. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.94251/4.99931. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.94434/5.01176. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.94292/5.01808. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.94268/4.99630. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.94363/5.01526. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.94313/5.01967. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.93866/5.02354. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.93670/5.02060. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.93748/5.04648. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.94268/5.00590. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.94106/5.01538. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.93798/5.02081. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.93819/5.01020. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.94467/5.00368. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.93489/5.02572. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.93814/5.02933. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.93346/5.02958. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.93744/5.06035. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.94883/4.99001. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.93616/5.00604. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.93803/5.01964. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.93987/5.00474. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.93714/5.00886. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.93564/5.01016. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.93337/5.01672. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.94259/5.02483. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.93686/5.03071. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.93741/5.00132. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.93038/5.02141. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.1341246360271594\n",
      "Epoch 0, Loss(train/val) 4.82097/4.74453. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.74395/4.74447. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.74041/4.74333. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.74098/4.74443. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.73833/4.74180. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.73615/4.74304. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.73805/4.74849. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.73429/4.74684. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.73567/4.74188. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.73566/4.74083. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.73429/4.74467. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.73873/4.73326. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.73674/4.73264. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.73875/4.73417. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.73911/4.73404. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.73790/4.73379. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.73563/4.73728. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.73619/4.73928. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.73375/4.74578. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.73467/4.74846. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.72700/4.75770. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.72921/4.76306. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.73260/4.76756. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.73150/4.75831. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.72278/4.79292. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.73063/4.77174. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.72995/4.77304. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.72715/4.77205. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.72220/4.78305. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.72781/4.78508. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.72403/4.78307. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.72453/4.78263. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.72491/4.78845. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.72262/4.79900. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.71976/4.79560. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.72438/4.78635. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.71523/4.81732. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.71692/4.79839. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.72314/4.78485. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.71481/4.81014. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.72354/4.78034. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.71653/4.79003. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.72139/4.78952. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.71839/4.79610. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.71763/4.78616. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.71193/4.79504. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.71301/4.82281. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.71899/4.78620. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.71579/4.81061. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.71074/4.80379. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.71656/4.80353. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.71650/4.80041. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.71245/4.81982. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.71780/4.79334. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.71006/4.80143. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.71237/4.80287. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.71604/4.78672. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.70882/4.81558. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.71158/4.79838. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.70134/4.82256. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.71528/4.78394. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.71203/4.80337. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.71023/4.79438. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.71215/4.78265. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.70889/4.80624. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.71189/4.81159. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.71095/4.78924. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.70977/4.79629. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.70967/4.80419. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.71347/4.79516. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.70988/4.80942. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.70424/4.78446. Took 0.21 sec\n",
      "Epoch 72, Loss(train/val) 4.69956/4.80511. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.71353/4.77711. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.70385/4.81523. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.70833/4.78935. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.70945/4.79546. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.70731/4.78501. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.69845/4.83300. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.70998/4.79527. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.70685/4.80230. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.70345/4.79782. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.70046/4.79446. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.70278/4.78459. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.70169/4.81692. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.69995/4.80473. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.70811/4.79105. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.70543/4.81904. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.70003/4.81103. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.70517/4.79873. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.69656/4.81653. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.70185/4.77622. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.70488/4.78943. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.69612/4.80910. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.70252/4.81096. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.69562/4.80233. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.69596/4.82972. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.70224/4.80268. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.70321/4.79367. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.70347/4.81014. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.0905982365507463\n",
      "Epoch 0, Loss(train/val) 5.00237/4.94187. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.91987/4.93408. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.92763/4.92630. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.93611/4.92021. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.93784/4.91967. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.92930/4.92150. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.92096/4.92705. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.92143/4.93497. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.92226/4.93616. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.91954/4.93996. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.91951/4.92628. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.91774/4.95946. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.91641/4.94169. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.91575/4.95147. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.91540/4.95013. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.91333/4.95387. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.91275/4.96085. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.91117/4.96647. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.91072/4.96144. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.91032/4.97987. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.90885/4.97412. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.91261/4.95430. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.90678/4.97962. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.90674/4.96985. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.90936/4.98773. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.90824/4.99011. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.90946/4.97167. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.91003/4.99389. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90790/4.97488. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.90602/4.98447. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90399/4.97904. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.90752/4.99002. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.89862/5.00158. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.90680/4.97763. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.90445/5.00167. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.90460/4.98597. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.90092/4.98925. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.90136/4.99762. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89978/4.99748. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.89661/5.00898. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.89866/4.99075. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.89893/5.00527. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.89785/5.01612. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.89559/5.01631. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.89035/5.01686. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.89836/5.00167. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.90354/4.99164. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89987/4.99386. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.90536/4.97724. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.89761/5.01818. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89910/4.99441. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.90373/4.99042. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.90092/5.00619. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.90492/4.99494. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.90186/4.98501. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.90367/4.98984. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.89783/5.00421. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.89858/5.01499. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.90197/4.98567. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.89788/5.01732. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.90211/4.97561. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.90159/4.99352. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.89971/5.00496. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 4.90367/4.98048. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.90419/4.98372. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.90158/5.00670. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.90218/4.98938. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.89575/5.01522. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.90166/4.99753. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.90213/4.97205. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.90204/4.99215. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.89912/4.98688. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.89778/5.00078. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.89823/5.01046. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.89915/5.00953. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.89465/5.00916. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.89683/5.00040. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.89890/5.01826. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.90218/4.99810. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.89601/5.01720. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.89190/5.01730. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.89862/5.02266. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.90099/5.01668. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.89645/5.01797. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.89630/5.01718. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.89845/5.00069. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.89478/5.04712. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.89996/5.01435. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.89651/5.03384. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.89296/5.01722. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.90138/4.99937. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.89327/5.02357. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.89575/5.01265. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.89667/4.99321. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88933/5.03760. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.89271/4.99480. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.89217/5.02288. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.89345/5.01628. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.89505/5.01411. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88977/5.01736. Took 0.20 sec\n",
      "ACC: 0.46875, MCC: -0.06262242910851495\n",
      "Epoch 0, Loss(train/val) 4.86120/4.84056. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.79715/4.81216. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.79056/4.81159. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79209/4.81002. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.78978/4.81015. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.78892/4.80883. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.78804/4.81245. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.78694/4.81168. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.78410/4.81115. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.78973/4.80700. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.78814/4.80774. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.78562/4.80857. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.78704/4.80777. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.78334/4.81091. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78489/4.81365. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78467/4.81045. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.78343/4.81195. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.78571/4.81379. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.78349/4.81675. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78289/4.81801. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.78229/4.82220. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.78401/4.81817. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78395/4.82676. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78201/4.82206. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78363/4.81715. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78250/4.81904. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78051/4.82317. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78242/4.82134. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.77978/4.82378. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78159/4.82114. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.77880/4.82699. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77840/4.82703. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77998/4.82711. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.78063/4.82430. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.78297/4.82693. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77965/4.82566. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78107/4.82228. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.77639/4.83316. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77706/4.82804. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77632/4.83683. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77837/4.82827. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.77365/4.83280. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.77416/4.83301. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.77304/4.83354. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77778/4.83568. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77659/4.83204. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77658/4.83682. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.77546/4.82477. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.77732/4.83258. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77770/4.83025. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.77487/4.82889. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.77787/4.82236. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.77516/4.83970. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.77247/4.84147. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.77811/4.83395. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.77183/4.84412. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.77247/4.82991. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.77569/4.84254. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.77356/4.83030. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.77657/4.81217. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.77242/4.84025. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.77428/4.82972. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.77393/4.83973. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.77279/4.84714. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.77194/4.84579. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.76975/4.84298. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.77097/4.84897. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.77706/4.84855. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.77336/4.84078. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77122/4.83916. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.77044/4.88141. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.77714/4.84447. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.77309/4.86180. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.77231/4.86099. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77481/4.85345. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.77220/4.82978. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.77311/4.85579. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77352/4.82768. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.77403/4.86213. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.76557/4.85413. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.76774/4.84478. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.76823/4.86513. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.76889/4.85314. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.77209/4.84468. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.77276/4.84852. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.77100/4.84788. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.76920/4.85042. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.76994/4.82991. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.77092/4.83218. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.76516/4.87186. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.76534/4.85491. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.76403/4.84291. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.76695/4.84237. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.77125/4.84173. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.76514/4.84489. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.76798/4.86441. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.76397/4.85548. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 4.76203/4.86000. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.76820/4.85134. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.76474/4.85344. Took 0.20 sec\n",
      "ACC: 0.4375, MCC: -0.11555252264060875\n",
      "Epoch 0, Loss(train/val) 5.05002/5.02461. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.02733/5.01742. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.03063/4.99059. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.00780/4.99149. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.99997/4.98600. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.00461/4.98623. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.00852/4.98603. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.00409/4.98800. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 5.00139/4.98518. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.00242/4.98735. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 5.00094/4.98762. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.00056/4.98708. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.00155/4.98826. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.00154/4.99136. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.99679/4.99012. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.99708/4.99539. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.99506/4.99991. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.99822/4.99310. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.99716/4.99027. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.99198/4.99292. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.99450/4.98492. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.99765/4.98404. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.99100/4.98728. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.99482/4.98902. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.99089/4.99150. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.98993/4.98636. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.99223/4.99840. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.98854/4.99649. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.98679/5.00072. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.98912/4.98618. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.98573/4.98981. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.98761/4.99475. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.98573/4.98282. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.98561/4.98265. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.98230/4.99556. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.98909/4.99591. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.98680/4.98552. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.98172/4.99561. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.98732/4.99107. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.98627/5.00178. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.98260/5.00001. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.98413/4.99333. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.98419/5.00564. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.98282/4.98110. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.98089/5.01008. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.98601/5.00190. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 4.98003/5.00265. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.98183/5.00619. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.98461/4.99458. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.98914/4.98999. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.98588/4.98927. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.97980/4.99400. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.97871/5.00464. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.98182/5.00985. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.97684/4.99830. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.98056/4.99562. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.98070/4.99538. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.97525/5.02536. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.97962/4.99777. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.98116/5.01637. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.97386/5.01191. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.97829/4.99925. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.97362/5.02756. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.97873/5.00747. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.97648/5.00885. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.97772/4.99770. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.97814/4.99474. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.97535/4.98987. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.97759/5.01075. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.97480/5.01978. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.97076/5.00489. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.98024/5.01421. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.97594/5.03124. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.97306/5.02522. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.97554/4.99899. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.96916/5.03739. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.97553/5.01343. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.97499/5.02031. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.97091/5.02435. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.97666/5.00729. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.97510/5.02415. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.96658/5.03098. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.96791/5.01593. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.97339/5.00881. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.97426/5.02803. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.97110/5.02685. Took 0.21 sec\n",
      "Epoch 86, Loss(train/val) 4.96887/5.02246. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.96838/5.04561. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.97102/5.03287. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.97246/5.00633. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.97079/5.02432. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.96842/5.03040. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.97402/5.00934. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.97317/5.02483. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.97462/5.02185. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.96837/5.03408. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.96544/5.03713. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.97283/5.05188. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.96573/5.04181. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.96773/5.03448. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.08304547985373996\n",
      "Epoch 0, Loss(train/val) 4.82136/4.82165. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.79315/4.79845. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.80028/4.82617. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79755/4.81943. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79975/4.79680. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.80281/4.78581. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.79980/4.78579. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79409/4.78409. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79441/4.78170. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79307/4.77880. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79433/4.77896. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.79319/4.77983. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79241/4.77523. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79181/4.77323. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 4.79092/4.76765. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78865/4.76744. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78951/4.76656. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.79024/4.76627. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.79194/4.76626. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78930/4.76497. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.78636/4.76396. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78221/4.75827. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78769/4.76921. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78390/4.75665. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78411/4.76329. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78580/4.76199. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78312/4.76158. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78616/4.76669. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.78028/4.76404. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.78045/4.77235. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.78259/4.76980. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77820/4.76871. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.78183/4.77523. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.77679/4.77251. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.77723/4.77206. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77872/4.77733. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77752/4.78904. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.77580/4.79442. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77743/4.77146. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77625/4.77146. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77405/4.76288. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.78010/4.76240. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.79498/4.78131. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.78932/4.78187. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.78736/4.77918. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.78757/4.77735. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78405/4.77706. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.78558/4.77981. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.78030/4.78246. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77911/4.78138. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.78409/4.78407. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.78311/4.79236. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.78217/4.78923. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.78199/4.77745. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.77968/4.76964. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.78361/4.77427. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.77999/4.76998. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.77806/4.77063. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.77880/4.77312. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.78112/4.76899. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.77826/4.76414. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.77955/4.76785. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.77878/4.77172. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.77763/4.77433. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.77858/4.77463. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.78044/4.76499. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.77808/4.76468. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.78083/4.77315. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77447/4.77546. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77832/4.77450. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.77858/4.77030. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.77550/4.77629. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.77723/4.77207. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.77347/4.77166. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.77952/4.77557. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.77704/4.77315. Took 0.21 sec\n",
      "Epoch 76, Loss(train/val) 4.77906/4.78132. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.78107/4.77363. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.77485/4.77565. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.77728/4.76482. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.77714/4.76750. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.77869/4.77968. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.77161/4.77693. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.77472/4.77972. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.77584/4.77073. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.77620/4.77962. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.77798/4.77829. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.77405/4.77878. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.77468/4.77076. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.77239/4.78240. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.77455/4.78409. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.77756/4.78701. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.76996/4.78553. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77671/4.77217. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77512/4.78471. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.77473/4.78216. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.77930/4.77354. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.77510/4.77351. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.77234/4.77003. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.77483/4.77305. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.08952126702661474\n",
      "Epoch 0, Loss(train/val) 4.94636/4.89922. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.88680/4.86240. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.88978/4.86031. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.89033/4.85776. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.88844/4.84492. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.88928/4.83677. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.88529/4.85014. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.87795/4.84535. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.87613/4.83829. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87381/4.84155. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.87782/4.84081. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.87095/4.84960. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.87018/4.84954. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.87391/4.84458. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87750/4.87399. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.88385/4.86268. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.87454/4.85903. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.87335/4.85311. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.87395/4.84568. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.87226/4.85068. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.86893/4.85009. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.87068/4.85860. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.86610/4.85503. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.86318/4.84118. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.86879/4.85731. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.86896/4.85704. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.86249/4.84846. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.86547/4.84845. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.86785/4.86519. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.85938/4.85210. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.86453/4.84059. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86309/4.86690. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.86085/4.85283. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.86877/4.85827. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85784/4.85372. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.85793/4.86402. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.86364/4.84927. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.88306/4.86094. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.87647/4.87509. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.86841/4.85782. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.86849/4.85977. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.87291/4.85159. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.86724/4.86621. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.86194/4.85333. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.86305/4.85523. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.86433/4.85252. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.87508/4.89427. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.87088/4.86919. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.87131/4.85850. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.87730/4.86474. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.86841/4.86455. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.86392/4.87378. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.86284/4.88694. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.86249/4.86771. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.86462/4.87725. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.86446/4.86813. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.85971/4.88643. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.86513/4.86218. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.85811/4.86617. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.85920/4.86087. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85740/4.86567. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.85979/4.86535. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85953/4.87367. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.85903/4.87333. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.86157/4.87181. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.85976/4.86036. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.85806/4.87617. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.86056/4.86877. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.85274/4.86089. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.85595/4.84909. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.85493/4.88249. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.85601/4.85592. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.85632/4.88298. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.85133/4.86334. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.85952/4.86526. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.85336/4.88043. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.85505/4.87846. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.85572/4.87109. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.85240/4.87594. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.85453/4.86713. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.85066/4.88310. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.85290/4.88369. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.85360/4.87780. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.85644/4.87933. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.84948/4.87554. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.85271/4.88649. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.84714/4.87516. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.84935/4.87184. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.84503/4.85971. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.85500/4.89807. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.85322/4.87739. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85111/4.88139. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.84725/4.87574. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.84671/4.88673. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.85084/4.86847. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.85400/4.88767. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.85306/4.90190. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.85693/4.86317. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.87387/4.91190. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.86570/4.89108. Took 0.19 sec\n",
      "ACC: 0.65625, MCC: 0.28877514330545034\n",
      "Epoch 0, Loss(train/val) 4.76521/4.78666. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.72687/4.67389. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.68384/4.67422. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.67712/4.67847. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.68361/4.67530. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.68118/4.67687. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.68271/4.67545. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.67945/4.67554. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.68062/4.67562. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.68152/4.67226. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.68042/4.67459. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.67835/4.67480. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.67879/4.67366. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.67761/4.67432. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.67716/4.67293. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.67499/4.67475. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.67494/4.67728. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.67427/4.68608. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.67551/4.69254. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.68046/4.68600. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.67663/4.67828. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.67707/4.67381. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.67538/4.67422. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.67383/4.67752. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.67422/4.67447. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.67633/4.67663. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.67013/4.68202. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.67390/4.66117. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.67244/4.67055. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.66725/4.67569. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.67032/4.67660. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.66853/4.67247. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.66358/4.69173. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.66498/4.66992. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.66724/4.66198. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.66559/4.66692. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.66232/4.66498. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.66348/4.67755. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.66387/4.65852. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.66168/4.65725. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.65460/4.69802. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.66369/4.65922. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.66283/4.67031. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.66103/4.67213. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.65802/4.66946. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.65934/4.67196. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.66010/4.66028. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.65609/4.67017. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.65818/4.67079. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.65523/4.67398. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.65531/4.67603. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.65370/4.67656. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.66093/4.66871. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.65592/4.65114. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.66911/4.65204. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.66148/4.64586. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.66033/4.65277. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.65613/4.65068. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.65749/4.65552. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.65695/4.65539. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.65445/4.66308. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.65823/4.67519. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.65808/4.65569. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.65029/4.65800. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.65922/4.66502. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.65415/4.68329. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.65689/4.66069. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.65417/4.66687. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.65903/4.67422. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.65469/4.65787. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.65568/4.66572. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.65104/4.66953. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.65510/4.66604. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.65118/4.66683. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.64743/4.67933. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.65572/4.67425. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.65460/4.67834. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.65724/4.66544. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.65134/4.67516. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.65397/4.65920. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.65056/4.66905. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.64994/4.69086. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.65690/4.69320. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.64984/4.68057. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.65417/4.68769. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.65260/4.69013. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.65294/4.69230. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.64928/4.71041. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.65324/4.69563. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.65099/4.68666. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.65045/4.68553. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.65045/4.69048. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.64698/4.69899. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.65187/4.68376. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.64822/4.69618. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.64167/4.69564. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.65428/4.70287. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.65555/4.69879. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.64908/4.67806. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.65255/4.68732. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.04350001325672093\n",
      "Epoch 0, Loss(train/val) 4.65428/4.66149. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.62329/4.62785. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.61645/4.63521. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.61437/4.64072. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.61351/4.64044. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.62135/4.63783. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.61603/4.63668. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.61726/4.63188. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.61418/4.63612. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.61694/4.63681. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.61655/4.62925. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.61233/4.63562. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.61628/4.63840. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.61266/4.63995. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.61259/4.63495. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.61041/4.63700. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.61331/4.63776. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.61470/4.64234. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 4.61369/4.63100. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.60926/4.63755. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.60751/4.64858. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.61040/4.63211. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.60543/4.65209. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.60788/4.63682. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.61382/4.63923. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.60696/4.63544. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.60840/4.64141. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.60066/4.64691. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.60333/4.66083. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.61479/4.65882. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.60869/4.65265. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.60713/4.66338. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.60682/4.67724. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.61537/4.63461. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 4.61040/4.66148. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.61009/4.62919. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.61004/4.64101. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.60776/4.64083. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.60872/4.63673. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.60257/4.66020. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.60507/4.64153. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.61303/4.63329. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.61040/4.65123. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.60750/4.66056. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.60859/4.64041. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.60497/4.65040. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.60321/4.63340. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.60297/4.65878. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.60702/4.64102. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.60517/4.63467. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.60160/4.63957. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.59930/4.63551. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.59953/4.62899. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.59847/4.63224. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.59982/4.65138. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.60807/4.63518. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.60304/4.64697. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.60617/4.65385. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 4.60329/4.65292. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.60304/4.64516. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.60307/4.63843. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.60065/4.65255. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.59897/4.65758. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.59813/4.66195. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.60037/4.65097. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.59736/4.65964. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.59693/4.65871. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.59120/4.70439. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.60086/4.64617. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.59405/4.66970. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.59706/4.65588. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.60001/4.65038. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.60348/4.64440. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 4.60391/4.63999. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.60496/4.66334. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.59874/4.67161. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.59612/4.67767. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.60070/4.68554. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.60052/4.64233. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.59793/4.64733. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.59456/4.65905. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.59227/4.67057. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.60096/4.65302. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.59953/4.64826. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.59412/4.66343. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.59721/4.66224. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.59904/4.67391. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.60018/4.66766. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.59734/4.66617. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.61035/4.64681. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.60124/4.65214. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.59509/4.65207. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.59661/4.66107. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.59571/4.64686. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.59242/4.64275. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.59194/4.65305. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.59163/4.65417. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.58738/4.66576. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.59181/4.65256. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.59355/4.65463. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.12267022872162016\n",
      "Epoch 0, Loss(train/val) 4.97092/4.96207. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.94550/4.94929. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.95503/4.98730. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.96547/4.93854. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.93715/4.94373. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.93019/4.94267. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.93215/4.94296. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.93507/4.93995. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.93296/4.94768. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.93496/4.94570. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.93171/4.94394. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.93078/4.94792. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.92793/4.95313. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.92956/4.95324. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.92539/4.95858. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.93076/4.95723. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.92343/4.97644. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.92202/4.97089. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.92966/4.95844. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.92329/4.96425. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.92279/4.97004. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.91884/4.96814. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.92394/4.97083. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.92189/4.96761. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.92095/4.96529. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.91835/4.99148. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.91872/4.98486. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.91709/4.98978. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.91649/4.99779. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.91668/5.00104. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.92090/4.98393. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.91742/4.98967. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.91526/5.00081. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.91514/4.99495. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.91443/5.00121. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.91161/5.00925. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.91569/5.00660. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.91053/5.01230. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.91531/4.99754. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.91315/5.00045. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.91543/4.99257. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.91354/5.00803. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.91149/5.00307. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.90876/4.99696. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.91227/5.01146. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.91163/5.00191. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.90891/5.01317. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.91031/5.01390. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.90783/5.01249. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.90807/5.00363. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.90880/5.00887. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.90727/5.01226. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.90760/5.03524. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.90909/4.99371. Took 0.21 sec\n",
      "Epoch 54, Loss(train/val) 4.92023/4.99100. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.91329/4.99431. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.91996/4.92630. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.93146/4.95538. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.92349/4.94580. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.92286/4.97845. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.91457/4.95733. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.92960/4.94595. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.92384/4.94504. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.92377/4.95412. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.91636/4.94774. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.92065/4.97332. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.91822/4.97049. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.91343/4.98271. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.91520/4.97923. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.91787/4.99453. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.91889/5.00512. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.91321/5.02436. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.91054/5.03439. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.91089/5.03595. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.90479/5.03658. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.90325/5.04185. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.90977/5.00592. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.91250/4.98912. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.91427/5.03059. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.90933/5.01440. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.91318/5.01209. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.90386/5.02565. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.90717/5.02379. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.90817/5.03666. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.90501/5.03268. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.90370/4.97188. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.92203/4.95656. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.92376/4.96057. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.91798/4.95150. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.91611/4.94716. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.91307/4.98543. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.91616/4.98985. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.91091/4.98300. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.91941/4.96883. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.91099/4.98789. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.91101/4.99995. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.90954/4.99459. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.91352/4.98524. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.91179/4.99763. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.90553/5.00735. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.02834008097165992\n",
      "Epoch 0, Loss(train/val) 5.05762/5.02170. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 5.02248/5.04570. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.01230/5.03906. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.01207/5.03574. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 5.01133/5.03667. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.00940/5.03908. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.01506/5.03183. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.00986/5.03825. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.01201/5.03537. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.00992/5.04137. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.01116/5.04095. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 5.01137/5.04383. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.00953/5.04389. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.01241/5.03230. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.01062/5.02805. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.00680/5.02853. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.00889/5.02604. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.00763/5.02460. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.00651/5.02472. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.00564/5.02550. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.00206/5.02447. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.00438/5.02631. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.00258/5.04425. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.00854/5.03076. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.00199/5.02654. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 5.00004/5.02942. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.00666/5.03226. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.00229/5.03727. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.00542/5.03730. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.00052/5.03550. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.99881/5.03243. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.99819/5.02597. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 5.00911/5.02321. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.00296/5.02014. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.00218/5.02349. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.00078/5.03689. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.00194/5.03797. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.00255/5.02865. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.99834/5.04114. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.99794/5.04032. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.99805/5.05757. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.99439/5.04538. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.99669/5.05881. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.99611/5.04483. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.99748/5.05216. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.99222/5.05099. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.99361/5.03659. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.98998/5.05279. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.99560/5.05597. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.99685/5.04966. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.99619/5.04517. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.98930/5.06639. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.99374/5.05316. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.99002/5.06292. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.98879/5.05703. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.99356/5.03910. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.98935/5.05733. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.99136/5.04832. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.99014/5.05957. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.98886/5.05265. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.99050/5.05246. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.99233/5.04610. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.99109/5.05638. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.98755/5.06886. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.98298/5.06274. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.99198/5.06724. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 4.99141/5.06094. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.98513/5.06957. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.98997/5.06609. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.98997/5.06148. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.98170/5.06479. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.98696/5.06549. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.98750/5.06095. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.98215/5.08749. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.98845/5.05663. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.98239/5.07675. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.99068/5.06458. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.98493/5.08011. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.98611/5.06683. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.97844/5.06627. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.98027/5.06753. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.98177/5.07923. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.98276/5.06941. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.98225/5.07930. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.98227/5.08125. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.97999/5.08551. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.98440/5.08372. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.98030/5.07840. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.98272/5.08556. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.97962/5.07825. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.97349/5.08958. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.98285/5.06960. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.98016/5.07621. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.98048/5.08822. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.98428/5.07315. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.97703/5.07707. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.98100/5.08367. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.98002/5.06587. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.98597/5.06196. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.97943/5.06685. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.13315005718713654\n",
      "Epoch 0, Loss(train/val) 4.94952/4.86287. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.85296/4.86916. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.84843/4.87257. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.84823/4.86795. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.84541/4.86690. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.84536/4.86322. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.84559/4.86157. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.84088/4.86876. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.84462/4.87050. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.84075/4.87287. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84624/4.87143. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.84028/4.87374. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.83682/4.87876. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.83471/4.87430. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.84039/4.87455. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.83580/4.88020. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.83258/4.88530. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.83479/4.87986. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83589/4.88630. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.83270/4.88709. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.82915/4.88557. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.83374/4.88523. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.83002/4.89817. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.83293/4.89284. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82678/4.89548. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82718/4.90318. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82301/4.91760. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.82458/4.91366. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.82696/4.89758. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.82558/4.91617. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.83038/4.90452. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.82843/4.91274. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.82648/4.90828. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.82084/4.93025. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.81814/4.92175. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82150/4.91797. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.82379/4.91596. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.82234/4.92264. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.81830/4.94475. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.81996/4.91981. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.82026/4.93423. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.81817/4.92619. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.81907/4.92159. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.81982/4.91619. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.82123/4.92995. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.81684/4.91564. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.82222/4.91799. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.81537/4.95267. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.82174/4.92049. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.81245/4.93292. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.81848/4.90935. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.81372/4.93332. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81220/4.93365. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.81305/4.92911. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81389/4.92931. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.82285/4.91991. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.82264/4.91138. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.81684/4.93979. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81654/4.92772. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.81319/4.95563. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.81400/4.96307. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81611/4.93483. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.81775/4.95093. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.81214/4.96708. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81528/4.93613. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81488/4.92757. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.81083/4.92600. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.81400/4.94905. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.81811/4.93091. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.81066/4.92692. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.81731/4.92982. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.81127/4.94509. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.81348/4.95329. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.81331/4.93431. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.81094/4.95173. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.81404/4.95767. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.81394/4.93414. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.81159/4.95904. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.80809/4.95052. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.80980/4.93209. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.80543/4.94199. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.80909/4.92776. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.80351/4.94979. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.81285/4.92487. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.81219/4.92892. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.80657/4.95292. Took 0.21 sec\n",
      "Epoch 86, Loss(train/val) 4.81133/4.92850. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.80869/4.98379. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.81964/4.87858. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82557/4.89664. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82357/4.90802. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.81666/4.91966. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.81508/4.91562. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.81680/4.91211. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.81464/4.93424. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.81844/4.93050. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.81769/4.92956. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81696/4.91505. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82064/4.90468. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.81676/4.90642. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.0710915894200724\n",
      "Epoch 0, Loss(train/val) 4.96103/4.92525. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.92496/4.97037. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.92507/4.94816. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.92530/4.95279. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.92382/4.95419. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.92111/4.95200. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.92132/4.94843. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.92768/4.95249. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.92234/4.95005. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.92226/4.94286. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.92002/4.95113. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.92064/4.95034. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.92026/4.94405. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.91970/4.94830. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.92284/4.94437. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.91892/4.93905. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.92041/4.94743. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.92096/4.95582. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.92013/4.94374. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.91795/4.93962. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.91877/4.94848. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 4.91568/4.95652. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.91861/4.95503. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.91488/4.95627. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.91360/4.96552. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.91464/4.95731. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.91129/4.96000. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.91159/4.96024. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.91265/4.96710. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.91291/4.96053. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.91117/4.97226. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.90673/4.97487. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.91150/4.96338. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.91192/4.95952. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.91041/4.96371. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90652/4.96882. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.90438/4.98078. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.90675/4.96518. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.90742/4.96045. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90314/4.96465. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90435/4.96580. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.91366/4.96382. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.90867/4.96818. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.91059/4.98232. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.90955/4.97057. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.90479/4.97911. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.90689/4.98382. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.91056/4.98391. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.90265/4.97292. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.90892/4.97148. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.90061/4.97584. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.90475/4.98504. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.90294/4.98188. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.90321/4.99117. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.90489/4.99164. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.90338/5.00242. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.90121/4.98898. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.90168/4.98513. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.89610/4.98738. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.90523/4.98091. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.90273/4.97922. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.89701/4.99340. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.90286/4.99234. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.90305/4.97496. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.89883/4.99509. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.89509/4.99959. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.89609/4.98600. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.89942/4.98398. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.90216/4.98989. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.90243/4.99258. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.89903/4.98806. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.89980/4.99701. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.89107/5.00244. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.89254/5.00220. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.89646/4.99113. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.90174/4.98643. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.90182/4.99062. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.89346/4.98487. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.89311/5.00351. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.89780/4.97662. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.89510/4.97722. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.89088/4.99786. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.89202/5.00018. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.89284/4.99373. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.89706/4.98588. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.89925/4.96768. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.88613/4.98846. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.88805/4.97797. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.91261/4.99967. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.90871/4.96265. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.91250/4.99229. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.90452/5.00708. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.90848/4.98435. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.90180/5.01176. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.90194/5.00841. Took 0.21 sec\n",
      "Epoch 95, Loss(train/val) 4.90295/4.99298. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.90082/4.97901. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.89706/5.00994. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.90079/4.97318. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.89843/4.99716. Took 0.20 sec\n",
      "ACC: 0.4375, MCC: -0.1252448582170299\n",
      "Epoch 0, Loss(train/val) 4.81262/4.78648. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.76518/4.77623. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.75690/4.77187. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.75056/4.77068. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.74917/4.77410. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.75100/4.77993. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.74921/4.77629. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.75264/4.77506. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.75125/4.77908. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.74891/4.78085. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.74868/4.77808. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.74761/4.77892. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.74680/4.78040. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.74628/4.78243. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.74705/4.78488. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.74811/4.78168. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.74724/4.77399. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.74399/4.78332. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.74358/4.78333. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.73942/4.78282. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.74249/4.78678. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.73866/4.78719. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.74092/4.78488. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.73569/4.78291. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.74162/4.79190. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.74789/4.78873. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.74466/4.78671. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.74055/4.79298. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.74012/4.77058. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.73968/4.77273. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.73804/4.77787. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.73861/4.77789. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.73828/4.78611. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.73631/4.77548. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.73861/4.77030. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.73662/4.77913. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.73719/4.78114. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.73669/4.78432. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.72880/4.78504. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.73535/4.78573. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.73079/4.79123. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.73614/4.78318. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.72886/4.78985. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.73330/4.80532. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.73620/4.80623. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.73269/4.79812. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.73619/4.79199. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.73651/4.77952. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.73227/4.78542. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.72729/4.79590. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.73686/4.78442. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.73190/4.78986. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.73358/4.80036. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.73497/4.80281. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.73749/4.78279. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.73558/4.79090. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.73282/4.78853. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.73202/4.79885. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.73111/4.79247. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.72787/4.80125. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.73077/4.79741. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.72781/4.80239. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.72970/4.82465. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.72945/4.79929. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.72543/4.80514. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.72478/4.79676. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.73199/4.80803. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.73072/4.79335. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.72626/4.81706. Took 0.21 sec\n",
      "Epoch 69, Loss(train/val) 4.72535/4.81421. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.72264/4.81689. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.72754/4.80652. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.72357/4.80974. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.72541/4.80939. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.72093/4.82378. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.72804/4.81028. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.73082/4.81312. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.72915/4.80633. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.72134/4.82998. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.72140/4.81125. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.72383/4.82061. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.71970/4.81887. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.72305/4.82994. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.72618/4.80993. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.72908/4.80835. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.71798/4.80973. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.72573/4.82430. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.71435/4.82292. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.72063/4.82436. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.71949/4.81657. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.71985/4.84654. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.71884/4.83915. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.72292/4.83159. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.71977/4.83771. Took 0.18 sec\n",
      "Epoch 94, Loss(train/val) 4.71878/4.82739. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.71678/4.80488. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.71822/4.81635. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.71632/4.81695. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.72352/4.83910. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.71793/4.80359. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.08952126702661474\n",
      "Epoch 0, Loss(train/val) 4.87381/4.86390. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86656/4.85434. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.85237/4.87518. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.86801/4.85406. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.86696/4.84081. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.85323/4.84078. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.84804/4.83989. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.85040/4.84096. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85262/4.84014. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.84951/4.83973. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84877/4.84106. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.84888/4.84280. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.85132/4.84082. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85112/4.83768. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.84627/4.83585. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.84587/4.83594. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.84679/4.83397. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.84849/4.83052. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.84667/4.82663. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.84239/4.82661. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84315/4.83048. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.84675/4.82779. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.84350/4.82564. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84081/4.82831. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.83762/4.83155. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.83975/4.82870. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84259/4.83480. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.83500/4.83680. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.83480/4.83885. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.84074/4.83423. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.83535/4.83659. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.83164/4.84182. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.84104/4.83930. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84377/4.82317. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84086/4.82096. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.84224/4.82071. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.83847/4.81967. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 4.83742/4.81340. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84074/4.81694. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.83684/4.81587. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.82929/4.80544. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.83532/4.80868. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83586/4.82317. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.83147/4.81703. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.82782/4.80923. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.83184/4.81772. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.83400/4.81902. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83122/4.81778. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.82893/4.80872. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.82745/4.81969. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.82876/4.81524. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.83661/4.82776. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.82983/4.83083. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.82554/4.82003. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.82682/4.80936. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.82406/4.82310. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.82407/4.82008. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.82388/4.82994. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81810/4.82457. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.82995/4.82222. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.82195/4.82175. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.82128/4.82636. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.81894/4.82125. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.81939/4.81863. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.82406/4.81733. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.82549/4.81677. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.82751/4.81575. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.82330/4.81484. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.82414/4.82257. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.82505/4.80862. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.82092/4.81639. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81803/4.81352. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.81434/4.80150. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.82310/4.83647. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81746/4.82928. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.82147/4.81114. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.82282/4.83363. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.81313/4.80421. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.81461/4.81185. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.81763/4.82588. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.81525/4.83451. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.80871/4.82572. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.81984/4.83245. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.81440/4.82329. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.81223/4.82919. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.81333/4.82474. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.81657/4.83608. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.81152/4.82159. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.80182/4.81363. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.81229/4.81819. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.80066/4.78838. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.80822/4.81369. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.81256/4.85148. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.81104/4.81683. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.81647/4.84510. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.80769/4.82359. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.81030/4.84335. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81222/4.82441. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.79882/4.84796. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.81124/4.87751. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.0629940788348712\n",
      "Epoch 0, Loss(train/val) 4.91355/4.86510. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.87515/4.84367. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.86824/4.84457. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.86254/4.84733. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.86432/4.84422. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.86101/4.84176. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.85785/4.84047. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.85794/4.84183. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85572/4.84089. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.86039/4.84263. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.85787/4.84885. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.85322/4.85128. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85545/4.85258. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85544/4.85628. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85137/4.85855. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.84961/4.85734. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85265/4.84279. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85325/4.85592. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85128/4.84987. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.84826/4.85086. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84587/4.84817. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.84824/4.85074. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.84815/4.85616. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84707/4.85778. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.84374/4.85211. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.84779/4.83646. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.85100/4.85338. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84566/4.84961. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.84749/4.84308. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84612/4.85240. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.84791/4.85298. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84317/4.84516. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.84524/4.84205. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.84279/4.85100. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.84351/4.84638. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84112/4.84994. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84516/4.84905. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.83955/4.84346. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.83674/4.84408. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84108/4.84763. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84456/4.86069. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84280/4.85412. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.84084/4.85562. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.83662/4.86439. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84335/4.85686. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.83642/4.85576. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84279/4.86038. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83665/4.86534. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.83500/4.86205. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.83593/4.87114. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.83950/4.86586. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.83494/4.86863. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.83321/4.87067. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.83725/4.85475. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.83330/4.87294. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.83737/4.85641. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.83655/4.86351. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.83357/4.85589. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.83059/4.86563. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.83839/4.86223. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.83538/4.86452. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.83358/4.85338. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.82748/4.85985. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.83179/4.85436. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.83721/4.86521. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.83311/4.85937. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.83234/4.85827. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83851/4.85829. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.84119/4.84447. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.83684/4.85883. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.83475/4.86584. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83022/4.86708. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83018/4.85909. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83021/4.84781. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.82647/4.84693. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.83484/4.84844. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83304/4.86382. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.83506/4.86568. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.83982/4.85794. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.84470/4.86772. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.83417/4.86765. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83290/4.87401. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.83230/4.86528. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.83713/4.85921. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.83808/4.86685. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.83027/4.87166. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.83468/4.85552. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83502/4.84714. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83383/4.85890. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83156/4.85635. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.83613/4.86046. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.83015/4.86433. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.83247/4.85260. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83596/4.85611. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83560/4.86742. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.82998/4.86570. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83559/4.85701. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 4.82737/4.86843. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82862/4.85801. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.83324/4.85668. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.1014574359634967\n",
      "Epoch 0, Loss(train/val) 4.91884/4.86501. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.85584/4.84026. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 4.85528/4.83962. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.85598/4.83799. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.85020/4.83532. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.84920/4.83199. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.85126/4.82964. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.85439/4.82811. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85016/4.83012. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.84578/4.82920. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84652/4.82684. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.84199/4.82359. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.84624/4.82329. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.83873/4.81472. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.84378/4.82153. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.83700/4.82128. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 4.84401/4.81909. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.83820/4.82241. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.84924/4.84742. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.84172/4.83651. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84159/4.83636. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.84374/4.84424. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.83861/4.83687. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84187/4.84373. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.84161/4.84841. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.84069/4.83665. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.83612/4.82949. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.83387/4.84148. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.83161/4.83825. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.83746/4.84448. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.83374/4.84559. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.83412/4.83559. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.83538/4.84014. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.83203/4.83248. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.83092/4.83842. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.83452/4.83561. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.83008/4.83209. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.83206/4.83925. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.83114/4.85263. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.83295/4.84091. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.82763/4.84874. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.82873/4.83337. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83264/4.85124. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.83001/4.83210. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.82689/4.85867. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.82994/4.83931. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.83026/4.84303. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.82965/4.84038. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.82724/4.84684. Took 0.22 sec\n",
      "Epoch 49, Loss(train/val) 4.82931/4.84919. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.82524/4.84589. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.83563/4.84809. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.82577/4.83871. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.82761/4.83858. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.82771/4.83441. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.82947/4.84497. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.83086/4.84935. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.82408/4.83653. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.82360/4.83165. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.82269/4.84196. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.82690/4.84020. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.82234/4.84460. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.82522/4.83805. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.82572/4.83688. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.82503/4.84420. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.82201/4.84834. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.82084/4.83294. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.81664/4.85152. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.82324/4.84732. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.82689/4.84448. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.82563/4.85834. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81951/4.84331. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.82315/4.85439. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.82128/4.85316. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.82521/4.86234. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.81786/4.85365. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.81906/4.85436. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.81834/4.84916. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.82777/4.85548. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.81982/4.84656. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.82446/4.84201. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.81431/4.85349. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.82309/4.85504. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.81989/4.84733. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.81934/4.85100. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.82237/4.86337. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.81619/4.85213. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.81498/4.87165. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.81479/4.85594. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.81787/4.86682. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.81434/4.85614. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.81738/4.87154. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.82178/4.84007. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82121/4.87086. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.81625/4.84831. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.81731/4.85551. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.81276/4.85914. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81076/4.85395. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.81675/4.85936. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.81489/4.87556. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.03202563076101743\n",
      "Epoch 0, Loss(train/val) 4.59127/4.56408. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.57274/4.56203. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.57770/4.58683. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.57656/4.58530. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.57144/4.56809. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.56423/4.56916. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.56647/4.57702. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.57091/4.57821. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.56731/4.57461. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.56502/4.57756. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.56395/4.57949. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.56283/4.57886. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.56285/4.57725. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.56695/4.57748. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.55833/4.57575. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.56284/4.56831. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.56088/4.56780. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.55921/4.56141. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.56099/4.56541. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.55945/4.56486. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.55373/4.55854. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.55643/4.55205. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.55572/4.57026. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.55510/4.57332. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.55251/4.56673. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.56141/4.58212. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.55843/4.56313. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.55496/4.56603. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.55104/4.57075. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.55252/4.56946. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.55148/4.57668. Took 0.22 sec\n",
      "Epoch 31, Loss(train/val) 4.55049/4.57111. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.55231/4.57335. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.55053/4.58717. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.54978/4.57721. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.54590/4.58006. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.54565/4.58376. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.55818/4.57994. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.54911/4.57367. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.54985/4.57816. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.54453/4.57499. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.54074/4.58987. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.54827/4.59216. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.54538/4.58040. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.54746/4.57706. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.54539/4.58809. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.54499/4.57603. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.54704/4.58616. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.54371/4.58178. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.54337/4.59002. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.54698/4.58663. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.54104/4.58966. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.53517/4.60536. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.54044/4.60463. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.54217/4.58869. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.54418/4.59106. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.54019/4.58106. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.53886/4.59576. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.54402/4.58000. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.54376/4.58603. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.53664/4.59322. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.53836/4.59372. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.54123/4.58119. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.54165/4.59615. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.54197/4.58241. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.53909/4.58667. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.53964/4.59536. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.53767/4.59362. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.53204/4.58825. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.53694/4.59767. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.53820/4.57373. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.53768/4.57843. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.52981/4.57659. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.54125/4.56939. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.53829/4.58729. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.53724/4.59396. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.53320/4.58155. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.53351/4.58873. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.53552/4.56664. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.53693/4.58862. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.53534/4.56122. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.53205/4.57609. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.53510/4.58436. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.53989/4.59242. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.53559/4.58628. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.53210/4.58833. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.53175/4.59358. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.53245/4.59085. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.53117/4.57254. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.53440/4.59002. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.53507/4.58706. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.52762/4.58073. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.52697/4.58986. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.53569/4.60042. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.53037/4.57941. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.52637/4.58649. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.53355/4.57450. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.52695/4.57499. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.53120/4.58555. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.53454/4.58435. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.019772733298509516\n",
      "Epoch 0, Loss(train/val) 5.07234/5.01579. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.01705/5.02303. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.01303/5.02205. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.00945/5.01892. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.01155/5.01879. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.00921/5.01933. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.00897/5.01692. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 5.00847/5.01321. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 5.01195/5.01134. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.00633/5.01788. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.00806/5.01765. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.00679/5.01693. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.00377/5.02150. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.00012/5.02906. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.00282/5.02210. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.00380/5.01598. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.00329/5.01541. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.00177/5.01780. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 5.00302/5.01952. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.99787/5.03026. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.00283/5.01621. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 5.00090/5.02542. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.99899/5.02751. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 5.00050/5.03221. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.99797/5.02869. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.99510/5.02806. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.99567/5.03884. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.99446/5.04044. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.99087/5.04348. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.99416/5.04411. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.99485/5.04620. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.00006/5.03689. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 4.99152/5.03707. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.99459/5.03716. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.99571/5.03652. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.99269/5.03746. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.99281/5.03865. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.99083/5.04459. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 4.99883/5.03525. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.99347/5.04226. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.99433/5.04272. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.98762/5.04896. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.98689/5.05804. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.99096/5.04619. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.98936/5.05334. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.98978/5.06263. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.98958/5.05338. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.98834/5.05759. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.98622/5.05234. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.98641/5.05137. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.98679/5.05625. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.98919/5.05424. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.98166/5.07032. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.98586/5.06732. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.98332/5.07821. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.98311/5.06442. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.98432/5.07671. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.98781/5.07386. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.98534/5.07075. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.98404/5.08998. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.98556/5.07897. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.98583/5.08086. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.98289/5.08047. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.97715/5.08368. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.98354/5.07895. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.98258/5.08855. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.98666/5.07828. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.97506/5.09781. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.98645/5.04715. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.98549/5.05548. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.97376/5.07265. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.98642/5.07320. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.97344/5.08848. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.98083/5.08011. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.98666/5.06401. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.97848/5.07525. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.97709/5.09455. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.97430/5.09262. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.97626/5.10563. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.97483/5.09935. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.97897/5.09782. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.98163/5.08944. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.98090/5.09959. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.97967/5.10441. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.97437/5.10658. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.97407/5.10728. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.97600/5.09455. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.97810/5.11545. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.97475/5.10388. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.97134/5.12316. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.97206/5.11553. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.97742/5.10668. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.97025/5.11753. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.97687/5.08563. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.97272/5.13168. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.97868/5.09931. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.97411/5.09163. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.96971/5.09861. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.97532/5.09431. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.97605/5.09659. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.03138824102871723\n",
      "Epoch 0, Loss(train/val) 4.99667/4.93127. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.93084/4.90893. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.92598/4.90836. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.92391/4.91344. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.92078/4.92128. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.92114/4.92548. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.92097/4.92281. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.91990/4.92560. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.92450/4.93064. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.91829/4.93057. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.92005/4.93240. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.92564/4.91451. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.92407/4.91037. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.92047/4.91629. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.91561/4.92423. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.91457/4.93086. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.91713/4.92355. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 4.91573/4.93144. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.91573/4.92597. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.91355/4.93822. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.91224/4.91985. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.91677/4.91754. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.91386/4.93779. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.91249/4.93184. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.91676/4.93356. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.91050/4.93199. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.91249/4.93491. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.91287/4.93958. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.91292/4.93667. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.90800/4.93754. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.91069/4.94319. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.90916/4.94934. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.90994/4.95685. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.90791/4.94164. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.90911/4.94715. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90812/4.95422. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.90719/4.96264. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.90774/4.92566. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.90739/4.94907. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90862/4.94081. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90431/4.97348. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.90844/4.95860. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.90304/4.95394. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.90233/4.95615. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.90277/4.95952. Took 0.18 sec\n",
      "Epoch 45, Loss(train/val) 4.90844/4.93779. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.90745/4.94835. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.90660/4.96073. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.90166/4.96351. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.90727/4.94635. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.90079/4.96545. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.90323/4.96403. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.89862/4.94385. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.90464/4.96039. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.90548/4.93361. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.90725/4.96848. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.90028/4.96103. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.90068/4.94661. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 4.90436/4.96862. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.90058/4.97026. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.89810/4.95560. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.89698/4.96952. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.89945/4.99040. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.90227/4.95931. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.89669/4.95628. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 4.89586/4.96718. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.89814/4.96298. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.89638/4.95906. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.89254/4.97354. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.89899/4.96016. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.90886/4.94194. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.90989/4.91553. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.90859/4.93638. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.90265/4.95446. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.89671/4.96674. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 4.89889/4.96498. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.89408/4.96268. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.89607/4.96542. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.89733/4.95889. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.89669/4.97105. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.89830/4.98498. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.91185/4.93736. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.90410/4.96115. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.90094/4.95822. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.89701/4.96185. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.89170/4.98007. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.89283/4.98558. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.90005/4.96056. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.89359/4.99481. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.89723/4.98825. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.89182/4.99408. Took 0.18 sec\n",
      "Epoch 91, Loss(train/val) 4.88792/4.97244. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.89171/4.99580. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.89676/4.98920. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88782/4.98201. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.88977/4.98575. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.89571/4.98985. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.89267/4.98105. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.89327/4.98560. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88963/4.99405. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.08952126702661474\n",
      "Epoch 0, Loss(train/val) 4.75529/4.69387. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.69905/4.69340. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.69667/4.70910. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.69696/4.70006. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.69936/4.69993. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.69333/4.70055. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.69368/4.70436. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.69526/4.70132. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.69500/4.71082. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.69112/4.71381. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.69153/4.71210. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.68855/4.70723. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.68759/4.71972. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.69174/4.71511. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.68882/4.71018. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.68910/4.71816. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.68663/4.71867. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.69263/4.71546. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.69123/4.70155. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.68848/4.70141. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.69424/4.70251. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.68830/4.70585. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.69363/4.70845. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.68397/4.71762. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.68107/4.72837. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.68082/4.73469. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.68389/4.72934. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.68251/4.72190. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.68042/4.71968. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.67654/4.74545. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.68158/4.74178. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.68027/4.73684. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.67974/4.73850. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.67947/4.75846. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.68634/4.73462. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.67650/4.74876. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.67575/4.75306. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.67580/4.77634. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.67871/4.75140. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.67778/4.74806. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.67245/4.75680. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.67375/4.74320. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.67565/4.77896. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.67570/4.76612. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.67680/4.75052. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.67558/4.76035. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.66951/4.75562. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.67213/4.77349. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.67317/4.75677. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.67363/4.76243. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.67041/4.76870. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.67083/4.78349. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.67575/4.75023. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.67114/4.75966. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.66661/4.76930. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.66921/4.76584. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.67057/4.76507. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.66587/4.78273. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.67206/4.77467. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.66884/4.76802. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.66423/4.79471. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.66520/4.78990. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.66929/4.76806. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.66849/4.76096. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.67153/4.75136. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.66459/4.76612. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 4.66814/4.78408. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.66139/4.77835. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 4.66226/4.80871. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.66669/4.79203. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.67158/4.78879. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.66375/4.79085. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.66708/4.78152. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.67064/4.76714. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.66683/4.78705. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.65895/4.78005. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.66504/4.80714. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.66012/4.79093. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.65894/4.78376. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.66552/4.79928. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.65750/4.79701. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.66061/4.80100. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.65748/4.79988. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.65524/4.80985. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.66026/4.79176. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.66226/4.79363. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.66061/4.79786. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.66749/4.75758. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.66163/4.80739. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.65902/4.80100. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.65767/4.80603. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.65235/4.80694. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.65876/4.79618. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.65602/4.80919. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.66102/4.79736. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.65917/4.80709. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.66202/4.80313. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.65551/4.79609. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.65683/4.79336. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.65204/4.79030. Took 0.19 sec\n",
      "ACC: 0.4375, MCC: -0.11834526708278773\n",
      "Epoch 0, Loss(train/val) 4.63761/4.60575. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.59986/4.60201. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.60131/4.60366. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.60308/4.60218. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.60552/4.60081. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.60198/4.60175. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.60010/4.59796. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.59756/4.60010. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.59688/4.60037. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.59641/4.59945. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.59818/4.59959. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.59741/4.59925. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.59720/4.59463. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.59581/4.59594. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.59547/4.59624. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.59412/4.59736. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.59111/4.60029. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.59187/4.59874. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.59034/4.60235. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.59101/4.60333. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.59175/4.60372. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.58852/4.60812. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.58867/4.60714. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.58946/4.60764. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.58841/4.60811. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.58698/4.60239. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.59306/4.59750. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.59054/4.60923. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.58532/4.60966. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.58684/4.61320. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.58500/4.61187. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.58397/4.62250. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.58211/4.61043. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.58726/4.59027. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.58175/4.61117. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.58422/4.60995. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.58197/4.61618. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.57931/4.61120. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.58517/4.59146. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.58364/4.60207. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.58615/4.60003. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.58409/4.60683. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.58560/4.59911. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.58636/4.59734. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.58828/4.60018. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.58366/4.60558. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.58431/4.60501. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.58312/4.60483. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.58288/4.61097. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 4.58375/4.60775. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.57948/4.60752. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.58272/4.60769. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.58184/4.61118. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.57889/4.62077. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.58418/4.60662. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.57876/4.61617. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.57874/4.61082. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.58264/4.60858. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.58000/4.61664. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.58040/4.61519. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.57819/4.61422. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.57858/4.61360. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.57735/4.61665. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.57681/4.61868. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.57611/4.62014. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.57710/4.62586. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.57655/4.63777. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.57462/4.63231. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.57868/4.61375. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.57521/4.61443. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.58427/4.60060. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.57470/4.62343. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.57925/4.61347. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.57334/4.61072. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.57699/4.61659. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.57399/4.62817. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.57313/4.63029. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.57327/4.62795. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.57262/4.63407. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.57953/4.63776. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.57423/4.63927. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.56930/4.64514. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.57454/4.62958. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.58221/4.61483. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.57690/4.64026. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.57903/4.63721. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.57837/4.63451. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.57290/4.63889. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.57826/4.62210. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.57677/4.62601. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.57292/4.63148. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.57385/4.64143. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.57685/4.63409. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.57719/4.62874. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.57773/4.63877. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.57414/4.63206. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.57551/4.64252. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.57620/4.63081. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.57455/4.63889. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.57967/4.63356. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.09593745047287525\n",
      "Epoch 0, Loss(train/val) 4.97105/4.95395. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.87654/4.84598. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.86776/4.84724. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.85662/4.85578. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.85516/4.85060. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.85750/4.84816. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.85970/4.84830. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.86029/4.84764. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85669/4.85016. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.85846/4.85119. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.85532/4.85202. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.85507/4.85278. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.85450/4.85327. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85472/4.85633. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85370/4.85942. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85306/4.85959. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85102/4.86206. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.84890/4.86422. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85077/4.86674. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.84717/4.86414. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84651/4.87193. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.84803/4.86974. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.84660/4.87403. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84549/4.87144. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.84358/4.86449. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.84581/4.86848. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84474/4.87362. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84534/4.87753. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.84424/4.87562. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84871/4.87181. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84667/4.87722. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84457/4.88031. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.84161/4.88309. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84126/4.88386. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.83999/4.88091. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.84304/4.87851. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84072/4.88161. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84447/4.87775. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84302/4.87885. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.83625/4.88672. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.83944/4.88105. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.83922/4.87851. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.84127/4.88145. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.83938/4.88647. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84151/4.87908. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.83865/4.88283. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.83984/4.88684. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83777/4.87163. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.84036/4.88884. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.83881/4.87652. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.83676/4.88711. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.83886/4.87578. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.84301/4.88202. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.83641/4.89727. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.83632/4.87257. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.84450/4.87841. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84610/4.86303. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.84090/4.86879. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.83910/4.87594. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.83881/4.87692. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.83636/4.87864. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.83565/4.88044. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.83492/4.88469. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83624/4.88234. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.83293/4.88489. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.83691/4.88272. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.83729/4.88059. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83548/4.88137. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.83554/4.88093. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.83429/4.88594. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.83467/4.88735. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83312/4.87859. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83347/4.88310. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83085/4.87435. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.83508/4.88824. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.83350/4.87589. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83202/4.88753. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.83457/4.88455. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.83364/4.88944. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.82859/4.88839. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.83248/4.88361. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83501/4.88686. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.83372/4.87902. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.83130/4.89660. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83107/4.89702. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83544/4.89421. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.83375/4.89153. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83092/4.88552. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83294/4.88914. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83579/4.88300. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.83478/4.88426. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82993/4.88445. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.83075/4.89530. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83134/4.88122. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82962/4.89383. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82968/4.88864. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83005/4.88834. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83318/4.88696. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.83074/4.89313. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.83287/4.88776. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.03126526997403612\n",
      "Epoch 0, Loss(train/val) 5.18645/5.10653. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.11553/5.11541. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.12056/5.13033. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.11866/5.13057. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.11987/5.11783. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.12389/5.09874. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.12123/5.09139. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.11712/5.09704. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.11643/5.09688. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.11392/5.09743. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.11112/5.09568. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 5.11196/5.09561. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.10897/5.09669. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.10900/5.09726. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.10912/5.09601. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.10746/5.09750. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.10954/5.09941. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.10714/5.10173. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.10680/5.10212. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.10956/5.10278. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.10879/5.10382. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.10647/5.10279. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.10530/5.10387. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.10450/5.10309. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.10481/5.10269. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 5.10343/5.10589. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.10183/5.10836. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 5.10594/5.11083. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.10298/5.10997. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.10478/5.11012. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.10234/5.11198. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.10030/5.11704. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.10436/5.11808. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.10179/5.11491. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.10108/5.11857. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 5.10364/5.11722. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.10623/5.11335. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.10170/5.11316. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 5.09994/5.11892. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.09706/5.12083. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.10006/5.11709. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.09914/5.12031. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.10469/5.11569. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.10055/5.11535. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.09540/5.11831. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.09795/5.11212. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.09846/5.11229. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.09831/5.11640. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.09990/5.10900. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.09680/5.11316. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.09554/5.11903. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.09743/5.11630. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 5.09398/5.12287. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.09635/5.11263. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 5.09830/5.10838. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.09375/5.11434. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.09890/5.10824. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.09640/5.11876. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.09376/5.11855. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.09364/5.11318. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 5.09470/5.11749. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.09596/5.11660. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.09196/5.12406. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.09184/5.11929. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.09479/5.11289. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.09007/5.11485. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.08966/5.12776. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.09514/5.12301. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 5.08920/5.12618. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.09134/5.12949. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.09411/5.12447. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.09234/5.12157. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.08652/5.13623. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.08726/5.12818. Took 0.21 sec\n",
      "Epoch 74, Loss(train/val) 5.08656/5.12580. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.09075/5.12949. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.09284/5.12286. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.08535/5.14151. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.09489/5.09698. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.10330/5.10406. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 5.09920/5.10992. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 5.09809/5.11200. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.09673/5.10618. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.09549/5.11577. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.09456/5.12721. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.09096/5.11665. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.08803/5.12479. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.08996/5.11894. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.09079/5.12678. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 5.08872/5.13722. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 5.08567/5.12679. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.09510/5.13372. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.08701/5.12351. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 5.09135/5.12516. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.08880/5.12742. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.08810/5.14644. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.09033/5.12569. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.09251/5.13459. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.08623/5.12574. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.08268/5.14066. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.049929648757959806\n",
      "Epoch 0, Loss(train/val) 4.85928/4.84057. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.82550/4.82184. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.81963/4.83685. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.82315/4.85048. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.82329/4.85272. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.82116/4.85835. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.82058/4.85714. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.82159/4.86160. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.82258/4.85373. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.82263/4.83415. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.81912/4.81484. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.81394/4.80471. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.81797/4.80816. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.81689/4.80960. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.81720/4.81238. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.81478/4.80549. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.81589/4.80486. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.81301/4.80832. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.81150/4.80945. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.81084/4.80800. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.81162/4.80924. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.81290/4.81439. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.80970/4.80693. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.80774/4.81479. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.81222/4.81390. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.80686/4.80898. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.81208/4.80884. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.80463/4.80728. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.80944/4.80300. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.80504/4.80255. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.80616/4.79729. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.80559/4.81030. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.80670/4.81333. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.80697/4.79977. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.80444/4.79636. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.80841/4.78906. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.80682/4.78790. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.80187/4.78064. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.80139/4.79122. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.80580/4.79733. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.80628/4.78849. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.79828/4.78209. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.80125/4.78783. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.80225/4.77069. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.80330/4.78547. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.79922/4.78648. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.79960/4.79661. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.79503/4.79295. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.79820/4.78029. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.79808/4.77808. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.79481/4.77766. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.79870/4.78481. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.79815/4.78886. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.79752/4.79311. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.80686/4.78976. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.80661/4.79724. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.80511/4.79863. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.80621/4.80105. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.79995/4.81043. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.79519/4.81812. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.80493/4.81647. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.79980/4.79174. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.79889/4.80332. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.79273/4.82195. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.80066/4.81825. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.79686/4.79620. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.79543/4.80545. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.79009/4.78925. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.79438/4.78298. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.79402/4.78917. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.79248/4.80264. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.79263/4.78155. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.79334/4.80047. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.79598/4.80364. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.79389/4.79736. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.78935/4.78705. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.79016/4.79355. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.79342/4.79486. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.79072/4.78840. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.79113/4.78527. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.79122/4.78830. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.78463/4.80412. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.79268/4.81751. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.78387/4.79821. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.78806/4.79255. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.78539/4.79067. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.78840/4.79189. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.78542/4.80977. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.78457/4.80316. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.78117/4.81492. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.78846/4.80313. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.78182/4.81890. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.78998/4.78673. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.78188/4.79293. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77864/4.79774. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79160/4.80229. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.78728/4.81240. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.77864/4.79736. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.78677/4.79859. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.78280/4.82505. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.0629940788348712\n",
      "Epoch 0, Loss(train/val) 5.09558/4.96637. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.98304/4.95665. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.97259/4.95662. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.96921/4.95725. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.97235/4.96232. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.97047/4.96998. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.96832/4.96833. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.96844/4.96606. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.96795/4.97420. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.96786/4.96998. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.96861/4.97308. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.96525/4.97903. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.96883/4.97266. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.96513/4.97719. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.96775/4.97733. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.96439/4.97803. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.96567/4.98479. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.96471/4.98376. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.96809/4.97923. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.96326/4.98036. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.96313/4.98936. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.96278/4.98361. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.95953/4.98091. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.95554/4.98639. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.96132/4.99554. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.95649/4.99557. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.95465/4.99858. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.96081/4.99853. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.95715/4.98617. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.95937/4.98962. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.96980/4.97480. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.96547/4.97092. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.97169/4.98052. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.97132/4.97316. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.96751/4.97531. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.96567/4.97421. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.96718/4.97613. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.96797/4.97268. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.96499/4.96853. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.96370/4.97169. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.96449/4.96389. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.96285/4.96387. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.96015/4.97441. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.96278/4.97407. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.95733/4.97942. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.96125/4.97237. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.95799/4.96998. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.95769/4.96702. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.95603/4.97241. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.95759/4.97616. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.95743/4.97127. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.95780/4.97236. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.95710/4.96932. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.95805/4.96841. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.95264/4.97433. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.95746/4.97220. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.95211/4.97169. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.95414/4.97412. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.95497/4.97340. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.95224/4.97453. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.95386/4.97286. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.95092/4.97094. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.94924/4.98033. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.95723/4.96614. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.95156/4.98113. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.95897/5.01577. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.96496/4.97121. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.96281/4.97777. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.95911/4.99245. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.96094/4.96973. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.95954/4.97136. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.95408/4.99184. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.95836/4.97247. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.95923/4.96645. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.95711/4.96950. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.94976/4.98306. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.94929/4.98828. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.94547/4.98507. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.95739/4.96767. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.94686/4.98018. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.95549/4.97033. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.94674/4.97609. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.94945/4.97451. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.94959/4.97135. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.95426/4.97885. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.95101/4.99274. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.96051/4.97535. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.95192/4.97911. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.94932/4.99328. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.95310/4.99731. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.95177/4.98358. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.94841/4.98638. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.94759/4.98745. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.94646/4.98077. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.94887/4.97919. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.94464/4.97723. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.94788/4.98806. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.94232/4.98128. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.94936/4.99055. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.94907/4.97495. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.013980301652228746\n",
      "Epoch 0, Loss(train/val) 4.88469/4.80832. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.83788/4.81183. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.82792/4.81036. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.82790/4.80838. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.82465/4.80318. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.82575/4.80210. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.82445/4.80152. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.82630/4.80092. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.82487/4.80470. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.82119/4.80510. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.82177/4.80320. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.82231/4.80431. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.82206/4.80862. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.82059/4.80792. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.82205/4.80642. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.81910/4.80342. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.82115/4.80319. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.82055/4.80148. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81906/4.79353. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.81669/4.79057. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.81890/4.78877. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.81481/4.78310. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.81540/4.78088. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.81339/4.77697. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.81204/4.77528. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.81053/4.78226. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.80565/4.78304. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.80630/4.78744. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.80377/4.80424. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.80830/4.79643. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.80929/4.77750. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.80309/4.77725. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.80062/4.77089. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.80120/4.78216. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.79866/4.77386. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.80799/4.77638. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.79715/4.76844. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.79829/4.77216. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.79923/4.75611. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.79647/4.76967. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.78987/4.76951. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.79322/4.78530. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.79175/4.78625. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.78673/4.77163. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.79600/4.77949. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.79324/4.79151. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78789/4.77938. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.78827/4.78916. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.78991/4.78314. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.78981/4.78541. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.79196/4.78071. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.78905/4.79439. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.78315/4.79695. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.78146/4.80558. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.78511/4.78845. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.78444/4.79425. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.79100/4.79973. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.79098/4.76502. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.78192/4.81520. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.77952/4.79734. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.78172/4.81242. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.78820/4.80382. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.78339/4.78334. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.77759/4.78716. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.77960/4.80212. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.78581/4.78913. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.77978/4.80159. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.78322/4.80658. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.78240/4.78974. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77839/4.82534. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.78128/4.81258. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.77263/4.81681. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.78225/4.82784. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.77968/4.82230. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77771/4.80205. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.78025/4.82194. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.77713/4.80435. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.78120/4.78349. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.77707/4.81189. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.77713/4.81487. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.77010/4.81672. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.77564/4.81435. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.77625/4.80774. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.77188/4.82783. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.77540/4.83638. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.77259/4.83576. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.77608/4.81297. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.77053/4.86248. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.77830/4.80602. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.77144/4.83042. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.77999/4.81949. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.77018/4.82342. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.77127/4.82772. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77465/4.82382. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77267/4.83624. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.76700/4.81684. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.76407/4.84945. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.76545/4.81536. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.77192/4.82273. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.76925/4.81904. Took 0.20 sec\n",
      "ACC: 0.515625, MCC: -0.02480694691784169\n",
      "Epoch 0, Loss(train/val) 4.73699/4.82625. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.71751/4.76050. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.71133/4.74371. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.70956/4.74778. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.70829/4.75708. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.70775/4.76803. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.70897/4.78172. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.70824/4.80179. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.70849/4.80825. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.70759/4.80221. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.70457/4.79461. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.70826/4.77921. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.70931/4.78064. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.70867/4.76805. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.70635/4.76422. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.70373/4.77271. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.70202/4.78006. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.70193/4.78804. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.70203/4.78172. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.69925/4.79343. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.69711/4.79880. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.69874/4.77675. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.69587/4.78679. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.69661/4.80265. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.69534/4.79252. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.69390/4.79577. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.68670/4.80223. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.69235/4.78806. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.69325/4.79490. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.69184/4.77808. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.70523/4.77436. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.69655/4.76360. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.69273/4.76229. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.68980/4.77190. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.69460/4.78949. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.69465/4.76640. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.69169/4.76121. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.69154/4.76463. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.69527/4.75320. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.70231/4.74041. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.70097/4.74037. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.69953/4.74987. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.68842/4.76753. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.70193/4.75551. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.69545/4.75517. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.69844/4.74839. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.69302/4.74453. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.69336/4.74668. Took 0.21 sec\n",
      "Epoch 48, Loss(train/val) 4.69318/4.75775. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.68901/4.75332. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.68958/4.75683. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.69566/4.74980. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.69262/4.77178. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.69154/4.75501. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.68864/4.74358. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.68425/4.74585. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.68540/4.75136. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.68649/4.78859. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.68803/4.78153. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.68632/4.74641. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.69313/4.76298. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.68805/4.73204. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.68783/4.75461. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.69205/4.75131. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.68555/4.77088. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.68434/4.75355. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.68488/4.76252. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.68992/4.72705. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.68561/4.75632. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.68372/4.75152. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.67828/4.75338. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.68219/4.75010. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.68093/4.75571. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.67834/4.75924. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.68605/4.75163. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.68540/4.74318. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.67319/4.77784. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.68000/4.74813. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.68010/4.75231. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.68210/4.74963. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.67965/4.75718. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.67710/4.76208. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.68208/4.74373. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.67992/4.76807. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.68073/4.75536. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.67610/4.75106. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.67943/4.73874. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.67626/4.76839. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.67109/4.76349. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.67952/4.74795. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.68196/4.75546. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.68243/4.74446. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.67431/4.74318. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.67197/4.75384. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.67935/4.76468. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.68157/4.75017. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.67496/4.77183. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.67986/4.74583. Took 0.21 sec\n",
      "Epoch 98, Loss(train/val) 4.67477/4.76547. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.67728/4.75578. Took 0.19 sec\n",
      "ACC: 0.390625, MCC: -0.20959097296420087\n",
      "Epoch 0, Loss(train/val) 5.06406/5.02814. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.03089/5.08854. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.02946/5.08813. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.02830/5.07215. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.02747/5.06397. Took 0.22 sec\n",
      "Epoch 5, Loss(train/val) 5.02626/5.04877. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.02140/5.04467. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.01971/5.04735. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.01686/5.04763. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.01805/5.04964. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.01575/5.04921. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.01658/5.05043. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.01635/5.04223. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.01498/5.04506. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.01486/5.04427. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.01409/5.05702. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.01260/5.05268. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 5.01446/5.05319. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.01390/5.05242. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 5.01006/5.04701. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 5.00544/5.05368. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 5.00644/5.05994. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.00893/5.05761. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 5.00790/5.05310. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.01072/5.03909. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.01158/5.04378. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.00659/5.04166. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.00724/5.04701. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.00685/5.05165. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 5.00783/5.04837. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.00556/5.04865. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.00298/5.05547. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.00136/5.05081. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 5.00886/5.05337. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.00619/5.05225. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.00535/5.04866. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.00292/5.05114. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.99939/5.05668. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.00443/5.06185. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.99496/5.05932. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.00704/5.05382. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.00477/5.04832. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.99803/5.06320. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.99833/5.05657. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.99660/5.06269. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.99951/5.06299. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.99926/5.05344. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.00010/5.05660. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.99925/5.05776. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.99873/5.06180. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.99977/5.06564. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.00852/5.05676. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.00206/5.05705. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.99364/5.06612. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.01667/5.01834. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.02748/5.01166. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.02180/5.01685. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.01801/5.01586. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.01643/5.01526. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 5.01174/5.02513. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 5.01110/5.02305. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.00794/5.03062. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.01180/5.03122. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.00803/5.03622. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.00751/5.04414. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.00513/5.07067. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.00258/5.07832. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.00066/5.07516. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.99920/5.06807. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 5.00002/5.07700. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.00167/5.07585. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.00021/5.07483. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.99648/5.08819. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.99857/5.09736. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.99641/5.05821. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.99830/5.08661. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.99764/5.08174. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.99472/5.06630. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.99290/5.09426. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.00470/5.07127. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.99945/5.04116. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.99213/5.07200. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.99864/5.07125. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.99555/5.08523. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.99027/5.08244. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.99600/5.08816. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.99969/5.06393. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.99281/5.06188. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.99868/5.03957. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.99820/5.07681. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.99035/5.10818. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.99353/5.07073. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.99786/5.07859. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.99743/5.08859. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.99199/5.10691. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.99397/5.08001. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.99476/5.04379. Took 0.18 sec\n",
      "Epoch 97, Loss(train/val) 4.99611/5.08633. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.99307/5.07821. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.99307/5.10110. Took 0.19 sec\n",
      "ACC: 0.4375, MCC: -0.12267022872162016\n",
      "Epoch 0, Loss(train/val) 4.54594/4.52656. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.50613/4.52014. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.50001/4.50960. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.49276/4.50496. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.49155/4.50821. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.49339/4.50985. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.49179/4.51277. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.49032/4.51379. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.49069/4.52042. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.48933/4.52060. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.48725/4.51916. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.48703/4.52934. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.49103/4.51075. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.48779/4.51663. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.48767/4.51581. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.48375/4.52325. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.48334/4.52325. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.48680/4.51824. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.48210/4.52363. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.48338/4.51348. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.48137/4.50368. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.48209/4.51039. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.48197/4.51310. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.47843/4.51391. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.48227/4.51295. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.48080/4.51156. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.48082/4.51403. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.47462/4.52697. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.48094/4.52404. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.49040/4.50980. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.48811/4.50565. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.48583/4.50920. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.48437/4.51907. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.47854/4.51540. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.47918/4.50855. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.47587/4.51708. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.47568/4.51196. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.47617/4.54080. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.48192/4.51605. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.47886/4.52561. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.47577/4.52480. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.47428/4.53400. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.47541/4.53341. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.47280/4.52571. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.47196/4.55741. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.47472/4.51987. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.47311/4.52371. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.47139/4.52973. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.46741/4.58629. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.47260/4.53590. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.47476/4.52800. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.47269/4.52349. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.46833/4.55600. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.46831/4.54041. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.46898/4.54068. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.46965/4.53630. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.47302/4.52994. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.46537/4.54594. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.46672/4.54104. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.46757/4.54938. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.47151/4.53579. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.47113/4.54299. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.46162/4.55682. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.46092/4.56242. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.46000/4.58401. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.47030/4.52752. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.46556/4.54606. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.46621/4.58048. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.46269/4.58018. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.45873/4.58951. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.46426/4.56001. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.45431/4.58258. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.46119/4.56707. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.45668/4.56552. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.45505/4.59315. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.45985/4.58024. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.45496/4.59328. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.45275/4.59189. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.46282/4.56460. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.45380/4.59353. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.45596/4.57933. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.45871/4.58350. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.44856/4.59124. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.45501/4.60137. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.45634/4.60347. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.45188/4.60433. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.46351/4.55523. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.46787/4.55815. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.46170/4.60246. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.46237/4.55381. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.46028/4.58135. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.46082/4.57750. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.45279/4.59231. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.45302/4.59596. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.45436/4.59471. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.46260/4.56656. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.45235/4.59354. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.44553/4.62212. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.45690/4.59824. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.45572/4.59621. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.0778902520624173\n",
      "Epoch 0, Loss(train/val) 4.83640/4.79730. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.81271/4.80029. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.80472/4.79949. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.80123/4.80122. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79866/4.80267. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79907/4.80127. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.79859/4.79986. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79526/4.79888. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79875/4.79574. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79543/4.79516. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.79761/4.79426. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.79402/4.79175. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79410/4.79101. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79390/4.79073. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.79412/4.79339. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.79443/4.79015. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78797/4.79178. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.79102/4.79350. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78917/4.79076. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78785/4.79559. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.79366/4.79137. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78953/4.79301. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78708/4.79450. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78828/4.79350. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78343/4.79584. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78152/4.79803. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78759/4.80140. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78275/4.79580. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78909/4.79667. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.78167/4.79469. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.78543/4.80221. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.78157/4.81667. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77865/4.80832. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.78497/4.80183. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.78133/4.79673. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.78387/4.79912. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78098/4.79874. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.77670/4.80770. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.77861/4.80379. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.77844/4.81125. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77766/4.81497. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 4.77764/4.81092. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.77362/4.81239. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.76961/4.83238. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77588/4.82328. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77483/4.83011. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78340/4.79905. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.77402/4.81972. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.77558/4.81785. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.77072/4.83351. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.77304/4.80690. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.77159/4.81622. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.77449/4.81959. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.77702/4.79702. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.77654/4.82183. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.78868/4.80910. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.79027/4.80285. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.78857/4.79574. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.78379/4.80406. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.78508/4.81039. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.78091/4.81195. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.77851/4.82837. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.77303/4.83616. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.77607/4.83539. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.77364/4.85409. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.77351/4.83517. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.77608/4.82862. Took 0.22 sec\n",
      "Epoch 67, Loss(train/val) 4.76846/4.83049. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.76660/4.87089. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77226/4.84495. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.76948/4.83868. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.77654/4.84320. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.79337/4.80855. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.78609/4.81830. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.78352/4.81657. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.77717/4.83186. Took 0.21 sec\n",
      "Epoch 76, Loss(train/val) 4.76831/4.86365. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77970/4.82608. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.77527/4.83245. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.77217/4.84043. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.77441/4.85041. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.76972/4.85287. Took 0.21 sec\n",
      "Epoch 82, Loss(train/val) 4.77196/4.82972. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.77171/4.84574. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.76346/4.85034. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.77387/4.83507. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.76964/4.83649. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.76562/4.87458. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.78189/4.81450. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.76508/4.85958. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.76891/4.83829. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.76546/4.84237. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.76979/4.83572. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.76041/4.84616. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.76502/4.84779. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.76589/4.85654. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.76670/4.84735. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.77027/4.83431. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.76793/4.83572. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75682/4.88969. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.04350001325672093\n",
      "Epoch 0, Loss(train/val) 5.05762/4.99829. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.99764/4.99378. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.98233/5.00177. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.97844/4.99312. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.97625/4.99453. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.97599/5.00278. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.97592/5.00532. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.97540/5.00908. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.97394/5.01970. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.97315/5.02455. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.97262/5.03124. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.97078/5.02289. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.96830/5.03333. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.96978/5.02706. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.96277/5.04254. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.96652/5.03402. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.96774/5.02935. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.96397/5.04176. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.96520/5.02149. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.95999/5.05161. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.96341/5.04193. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.96392/5.03452. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.95900/5.05810. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.95861/5.04623. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.95784/5.06719. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.95902/5.05119. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.95405/5.07277. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.95475/5.08978. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.96165/5.05637. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.96417/5.01725. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.96203/5.02503. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.96599/5.02547. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.96152/4.99739. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.96558/5.00219. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.96538/5.00824. Took 0.22 sec\n",
      "Epoch 35, Loss(train/val) 4.96468/5.00953. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.96057/5.02511. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.96110/5.04061. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.96174/5.03058. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.96165/5.02845. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.95764/5.05047. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.95866/5.06190. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.96178/5.02173. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.95029/5.06280. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.96391/5.00952. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.97154/4.96025. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.97227/4.97869. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.96781/4.98649. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.97070/4.99456. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.96913/4.99728. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.96716/4.99738. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.96907/4.99780. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.96734/4.99564. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.96903/5.00363. Took 0.21 sec\n",
      "Epoch 54, Loss(train/val) 4.96808/5.00360. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.96632/5.00677. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.96626/5.01141. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.96122/5.01177. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.96065/5.02427. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.96454/5.01038. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.96121/5.00778. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.96076/5.01298. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.95595/5.01746. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.96708/4.98331. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.96627/4.98969. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.96275/4.99356. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.95629/5.00746. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.96135/5.01051. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.96044/5.01224. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.95538/5.01011. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.95940/5.01208. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.95803/5.00557. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.96150/5.00811. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.95448/5.01736. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.95343/5.01102. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.95265/5.01886. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.95767/5.02972. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.95441/5.01102. Took 0.22 sec\n",
      "Epoch 78, Loss(train/val) 4.95758/5.01253. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.95645/5.02092. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.95265/5.02350. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.95342/5.02520. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.95271/5.02472. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.95268/5.04130. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.94369/5.04463. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.95468/5.04192. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.95491/5.02290. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.95568/5.02231. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.94651/5.04951. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.95137/5.04321. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.94789/5.03210. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.94771/5.03884. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.94953/5.05420. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.95309/5.01887. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.95068/4.97848. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.97571/5.00092. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.97295/4.99693. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.96864/4.99585. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.96943/5.00493. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.96634/5.00893. Took 0.20 sec\n",
      "ACC: 0.4375, MCC: -0.1241446725317693\n",
      "Epoch 0, Loss(train/val) 4.90135/4.80060. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.81897/4.81668. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.81324/4.80252. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.80216/4.80542. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.80165/4.81121. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.80411/4.81752. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.80550/4.81680. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.80559/4.81669. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.80170/4.82048. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.80029/4.82328. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.80022/4.82434. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.79643/4.82710. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.79931/4.82293. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.80103/4.83151. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.79626/4.82300. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.79643/4.84508. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.79377/4.82951. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.79446/4.83401. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.79841/4.84333. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.79396/4.84156. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.79015/4.85650. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.79077/4.85879. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.78806/4.84920. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78941/4.85772. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.78345/4.86890. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78734/4.86068. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.78833/4.86965. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78403/4.85614. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78900/4.85810. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78797/4.85462. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.78042/4.86795. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.78703/4.87102. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.78049/4.86473. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 4.78296/4.87078. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.78014/4.88097. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.78258/4.89463. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78167/4.88308. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.77896/4.86095. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77995/4.87976. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.78060/4.89276. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77927/4.85663. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.77971/4.88259. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.78215/4.89030. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 4.77793/4.90539. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.77842/4.86702. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77751/4.86679. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.78029/4.88591. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.77769/4.87041. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.78118/4.88198. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77627/4.87786. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.77530/4.89614. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.77698/4.88988. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.77900/4.87218. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.77564/4.89752. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.77469/4.90014. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.77595/4.89450. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.77944/4.88888. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.78247/4.87870. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.77680/4.89398. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.77071/4.92308. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.77968/4.86298. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.78864/4.82909. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.78822/4.85220. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 4.78324/4.86816. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.77562/4.89059. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.77873/4.87694. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 4.78111/4.88938. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.77322/4.89237. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77748/4.88532. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77938/4.87755. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.77716/4.87952. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.77839/4.89139. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.77153/4.89235. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.78011/4.87199. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77647/4.90626. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.77296/4.89923. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.77575/4.86342. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77740/4.84712. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.77597/4.86030. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.78498/4.84058. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.77870/4.88097. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.78226/4.83893. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.78826/4.84196. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.78363/4.84833. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.78226/4.86933. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.78321/4.86849. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.78006/4.86721. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.77845/4.87616. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.78344/4.87862. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.77430/4.90869. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.77857/4.88918. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.77823/4.87196. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.77836/4.88056. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.78482/4.90289. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.78107/4.89437. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.77968/4.90254. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.77849/4.89109. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.77504/4.90781. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.77092/4.90119. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.77355/4.90733. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.06262242910851495\n",
      "Epoch 0, Loss(train/val) 4.85034/4.84232. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.81467/4.81215. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.81178/4.82083. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.80973/4.82121. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.80843/4.81024. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.80272/4.80933. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.80243/4.80996. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.80539/4.80960. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.80273/4.80775. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.80225/4.80683. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.80077/4.80708. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.80117/4.80597. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.80054/4.80733. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.80089/4.80799. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.80095/4.80628. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.79941/4.80737. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.79881/4.81122. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.80035/4.81119. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.80001/4.80627. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.79841/4.80695. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.79842/4.81098. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.79832/4.81374. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.79710/4.81367. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.79810/4.81102. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.79751/4.81012. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.79297/4.82053. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.79522/4.81656. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.79296/4.81545. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.79299/4.81678. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78904/4.81522. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.79334/4.81399. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.79147/4.81611. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.78617/4.81651. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.78940/4.81837. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.79387/4.81276. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.78975/4.82121. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.79021/4.81648. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.78681/4.82470. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.78953/4.80923. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.78784/4.81675. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.78456/4.81632. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.78575/4.82471. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.78639/4.82606. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.78563/4.83598. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.78302/4.84626. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.78333/4.84598. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78998/4.82481. Took 0.21 sec\n",
      "Epoch 47, Loss(train/val) 4.78957/4.82288. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.78540/4.83741. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.78666/4.84152. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.78362/4.83913. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.78528/4.84087. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.78149/4.84517. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.79624/4.80387. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.79696/4.80221. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.79279/4.81479. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.79008/4.81934. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.78590/4.82970. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.79389/4.82041. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.78630/4.83222. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.78936/4.81698. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.78716/4.84186. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.78551/4.83721. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.78648/4.83215. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.78187/4.84254. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.78065/4.83389. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.78160/4.83024. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.78268/4.83986. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.77967/4.84184. Took 0.22 sec\n",
      "Epoch 69, Loss(train/val) 4.77979/4.83566. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.78352/4.82792. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.77812/4.83008. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.77652/4.83703. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.77791/4.83387. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.77653/4.84734. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.77884/4.85260. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.77558/4.84037. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77712/4.83768. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.77852/4.84537. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.77486/4.84455. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.78038/4.83755. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.77730/4.84222. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.77679/4.84253. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.77238/4.84451. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.77674/4.84510. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.77709/4.84683. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.77429/4.85079. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.77688/4.84342. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.77660/4.84961. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.77962/4.84089. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.77411/4.85335. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.77946/4.86086. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.77200/4.85480. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77376/4.85988. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77417/4.85715. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.77206/4.87553. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.77427/4.88886. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.77638/4.84951. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.76982/4.85581. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.77628/4.87342. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.06362847629757777\n",
      "Epoch 0, Loss(train/val) 4.99551/4.96683. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.97344/4.96381. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.96624/4.96530. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.96501/4.96552. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.96449/4.96452. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.96331/4.96306. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.96234/4.96437. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.95770/4.96234. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.96213/4.96177. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.95991/4.96154. Took 0.22 sec\n",
      "Epoch 10, Loss(train/val) 4.96325/4.96170. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.95956/4.96203. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.96147/4.96211. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.95742/4.96286. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.95774/4.96269. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.95495/4.96625. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.95817/4.96973. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.95715/4.96787. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.95840/4.96872. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.95720/4.96606. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.95453/4.96715. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.95423/4.95681. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.95978/4.96262. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.95715/4.96430. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.95385/4.96191. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.95492/4.96115. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.95126/4.96278. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 4.95229/4.96653. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.95003/4.96530. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.94840/4.96527. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.94669/4.96310. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.94986/4.95988. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.95258/4.95947. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.94793/4.95438. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.94439/4.95785. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.94585/4.95987. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.94621/4.95432. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.94397/4.95696. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.93803/4.95695. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.94763/4.95412. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.94544/4.96021. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.94615/4.95494. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.94084/4.95764. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.94501/4.95056. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.94347/4.94903. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.94338/4.95832. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.93884/4.95591. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.94400/4.96062. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.93925/4.96042. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.94032/4.96590. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.93820/4.95773. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.93920/4.96171. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.93902/4.96118. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.93960/4.96192. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.94071/4.95656. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.93857/4.96020. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.93757/4.96155. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.93790/4.95427. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.93886/4.95412. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.94261/4.95514. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.93465/4.96104. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.93659/4.96004. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.93599/4.95396. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.93060/4.95211. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.93440/4.96433. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.93727/4.95214. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.93737/4.95235. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.93353/4.96121. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.93444/4.96157. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.93386/4.95502. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.93694/4.95723. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.93309/4.95335. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.93099/4.96435. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.93009/4.95117. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.93869/4.96548. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.92807/4.97081. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.93221/4.97596. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.93692/4.96006. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.93155/4.95255. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.92659/4.97592. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.93426/4.96306. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.92702/4.97566. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.93000/4.96406. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.93260/4.96784. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.93051/4.97021. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.92758/4.96525. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.92946/4.94960. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.92792/4.95768. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.93051/4.95137. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.92711/4.95551. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.92554/4.96573. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.92369/4.95727. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.92747/4.97290. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.92540/4.97393. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.92754/4.96197. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.92554/4.97329. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.92805/4.95984. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.92720/4.97389. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.92677/4.95700. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.92412/4.97458. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.1111111111111111\n",
      "Epoch 0, Loss(train/val) 4.90143/4.84207. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.85587/4.84400. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.85401/4.83761. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.84575/4.83816. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.84784/4.83745. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.84602/4.84363. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.84517/4.84432. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.84593/4.85044. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.84301/4.85346. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.84293/4.85161. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84107/4.85695. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.83791/4.85311. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.84074/4.84006. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.84126/4.84520. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.83857/4.84349. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.83871/4.84146. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.83351/4.84974. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.83541/4.85169. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83657/4.84659. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.83373/4.84820. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.83369/4.84455. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.83374/4.85292. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.82985/4.84603. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.83387/4.84631. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.83548/4.82823. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.84143/4.83208. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.83786/4.83527. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.83663/4.82880. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.83585/4.83165. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.83321/4.84722. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.83805/4.83592. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.83673/4.84169. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.83287/4.83549. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.83391/4.84562. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.83523/4.83368. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.83097/4.83385. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.83444/4.83690. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.83005/4.83798. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.82866/4.84715. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.82833/4.85215. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.83583/4.84400. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.83083/4.84446. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83006/4.84083. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.82449/4.85050. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.82420/4.84722. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.82257/4.84127. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.82563/4.84028. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.82262/4.84298. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.82683/4.84598. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.82341/4.83065. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.82452/4.84355. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.82636/4.82937. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.82203/4.83517. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.82035/4.84272. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.82313/4.84280. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.82022/4.84223. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.82361/4.82989. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.82008/4.84000. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.82041/4.83484. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.82041/4.82777. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.82254/4.83955. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81669/4.84164. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.81994/4.83677. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.82266/4.83557. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.82129/4.83137. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81937/4.83600. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.82100/4.82650. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.81963/4.83891. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.81446/4.84392. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.81895/4.83354. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.81664/4.83604. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81857/4.83525. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.82190/4.83099. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.81315/4.83735. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81584/4.82824. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.81615/4.83847. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.81528/4.83080. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.81908/4.83376. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.81641/4.82038. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.81377/4.83492. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.81744/4.82681. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.81467/4.83353. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.82385/4.90053. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.83622/4.85096. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.82723/4.84266. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.81859/4.83874. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.81572/4.82959. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.81632/4.83150. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.81573/4.83158. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.81446/4.86604. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.81309/4.84199. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.83204/4.86790. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82812/4.84465. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.82068/4.82830. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.81611/4.83250. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.81232/4.83471. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.81337/4.82244. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81831/4.82544. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.81400/4.82697. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.80923/4.84324. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.14180020247260586\n",
      "Epoch 0, Loss(train/val) 4.73159/4.78593. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.71158/4.71618. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.71236/4.68063. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.70992/4.67282. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.70661/4.68141. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.70168/4.68433. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.70299/4.68078. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.69951/4.68028. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.70150/4.67573. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.70123/4.67479. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.70013/4.67514. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.69707/4.67870. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.69931/4.67941. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.69503/4.68460. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.69673/4.69086. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.69339/4.70579. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.69546/4.68546. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.69706/4.69189. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.69181/4.69763. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.69195/4.70063. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.68893/4.69962. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.68993/4.70066. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.69395/4.69571. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.69398/4.69124. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.69446/4.69373. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.69185/4.68910. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.69218/4.68827. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.68949/4.69127. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.68940/4.69983. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.69167/4.69952. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.68472/4.69796. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.68661/4.69205. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.68952/4.69939. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.68277/4.69823. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.68843/4.70813. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.68230/4.70484. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.68386/4.69504. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.68133/4.70447. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.68224/4.69895. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.68291/4.69398. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.68475/4.70193. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.68249/4.70446. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.68214/4.69779. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.67592/4.71287. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.67653/4.71079. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.68078/4.70591. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.67158/4.73340. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.68310/4.69323. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.67874/4.70585. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.67555/4.70992. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.67299/4.71380. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.67189/4.70766. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.66881/4.71373. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.67507/4.70930. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.67423/4.70986. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.67354/4.71865. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.67242/4.71660. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.67278/4.70934. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.67195/4.71532. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.67066/4.70037. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.66542/4.73302. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.67178/4.70705. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.67049/4.69917. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.66680/4.71862. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.66462/4.71996. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.66593/4.70573. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.66556/4.71287. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.67051/4.69600. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.66677/4.71690. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.66430/4.71137. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.66411/4.70583. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.66290/4.72143. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.65865/4.70586. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.66449/4.70014. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.66400/4.71619. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.66203/4.70238. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.65700/4.70587. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.66161/4.71612. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.66684/4.72221. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.65787/4.71047. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.66091/4.71320. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.66342/4.71515. Took 0.21 sec\n",
      "Epoch 82, Loss(train/val) 4.65731/4.70909. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.65902/4.70453. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.65347/4.72249. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.65861/4.73577. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.69910/4.69675. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.69173/4.69649. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.68349/4.66962. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.68168/4.69745. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.67981/4.69047. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.67892/4.71251. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.67709/4.67835. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.67295/4.68477. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.67039/4.66798. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.67498/4.67793. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.67845/4.67301. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.66377/4.69174. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.66457/4.71193. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.66555/4.69281. Took 0.20 sec\n",
      "ACC: 0.625, MCC: 0.2123076923076923\n",
      "Epoch 0, Loss(train/val) 4.83389/4.80677. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.78402/4.79231. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.78060/4.78376. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.77783/4.78168. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.77839/4.78403. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.77900/4.78576. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.77447/4.78901. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.77455/4.79228. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.77392/4.79468. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.77580/4.79031. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.77450/4.79011. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.77294/4.78879. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.77116/4.79466. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.77374/4.79041. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.77071/4.79519. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.76960/4.79856. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.76794/4.80103. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.76825/4.79657. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.76870/4.80207. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.76763/4.79582. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.76837/4.79484. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.76753/4.79190. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.76766/4.79957. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.76291/4.80283. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.76370/4.79514. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.76266/4.79248. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.75929/4.79940. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.76164/4.79168. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.75948/4.79461. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.76053/4.80167. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.75430/4.79783. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.75310/4.79827. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.75699/4.80190. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.75917/4.79251. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.75316/4.81229. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.75246/4.80191. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.75402/4.79287. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.75631/4.79762. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.75791/4.79984. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.75543/4.79523. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.75107/4.79795. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.75408/4.79674. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.74969/4.79488. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.74915/4.79252. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.74782/4.80178. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.74679/4.79764. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.74357/4.80361. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.75193/4.80081. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.74472/4.79804. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.74814/4.79042. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.74031/4.80153. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.75106/4.80224. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.74496/4.78466. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.73475/4.80580. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.74147/4.79181. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.73917/4.81266. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.74573/4.79759. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.74824/4.79635. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.73999/4.81403. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.74063/4.80754. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.73697/4.80284. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.74258/4.80339. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.74430/4.79908. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.73239/4.82944. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 4.74392/4.79406. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.74546/4.80237. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.73011/4.81169. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.73621/4.80350. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.73417/4.82788. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.74073/4.80678. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.74003/4.80826. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.73168/4.81765. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.73807/4.79498. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.74167/4.79495. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.72923/4.80990. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.73343/4.82109. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.73661/4.80790. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.72404/4.80028. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.72758/4.81215. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.72311/4.83682. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.73862/4.81830. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.73420/4.80683. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.72990/4.79553. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.73378/4.80902. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.73297/4.80379. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.73038/4.81382. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.72696/4.79374. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.72748/4.82795. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.73173/4.81059. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.73414/4.80464. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.72525/4.82572. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.73251/4.82206. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.73901/4.80259. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.72952/4.81653. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.72251/4.83376. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.73044/4.82686. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.72510/4.82026. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.71640/4.84168. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.73317/4.80203. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.73296/4.81057. Took 0.19 sec\n",
      "ACC: 0.609375, MCC: 0.21971768720102058\n",
      "Epoch 0, Loss(train/val) 4.98153/4.92992. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.95532/4.93145. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.95352/4.94058. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.94685/4.94633. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.94729/4.95061. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.93967/4.96163. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.94204/4.96277. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.94199/4.94346. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.94218/4.94060. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.94420/4.94291. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.94452/4.94875. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.94434/4.95130. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.94176/4.95317. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.94234/4.95552. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 4.94194/4.95953. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.94065/4.96462. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.93773/4.96689. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.93737/4.96654. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.93449/4.96389. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.93669/4.96760. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.93652/4.95869. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.93812/4.96775. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.93566/4.97233. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.93565/4.97197. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.93567/4.96714. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.93159/4.97241. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.93499/4.96919. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.93108/4.99542. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.93068/4.98152. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.93344/4.97871. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.93473/4.95869. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.94006/4.95810. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.93631/4.94671. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.93412/4.94974. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.94094/4.94910. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.93464/4.94598. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.93311/4.95088. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.93628/4.95301. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.93499/4.96335. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.93519/4.95945. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.93395/4.96922. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.93266/4.96811. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.93435/4.96579. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.93408/4.97170. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.93297/4.96507. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.93323/4.96664. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.93276/4.94503. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.93113/4.97480. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.93158/4.96295. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.93443/4.96040. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.92865/4.96337. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.93016/4.96413. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.92651/4.95551. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.92395/4.97892. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.92638/4.98682. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.92716/4.97097. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.92916/4.98178. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.92827/4.96697. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.92101/4.97405. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.92995/4.96982. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.92270/4.98421. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.92473/4.98626. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.92584/4.97025. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.92607/4.98216. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 4.92409/4.97828. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.93151/4.97772. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.92324/4.96034. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.92842/4.96546. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.91892/4.99019. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.92679/4.98072. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.92275/4.97256. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.91692/4.98005. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.92864/4.99252. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.92089/4.98843. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.92243/4.98866. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.92275/4.98248. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.91936/5.00042. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.92286/4.99759. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.91936/4.98203. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.92044/4.98639. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.91648/4.99593. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.91435/5.01038. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.91820/4.98129. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.91721/4.98903. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.92026/4.98183. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.91387/5.00834. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.92131/4.99745. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.91383/4.98734. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.91916/4.99549. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.91579/4.99586. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.91237/4.99739. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.91681/5.00553. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.91521/4.98358. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.91159/5.00740. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.91679/4.99467. Took 0.21 sec\n",
      "Epoch 95, Loss(train/val) 4.91659/5.00848. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.91816/4.98700. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.91475/4.99636. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.90582/4.98186. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.91433/4.99223. Took 0.19 sec\n",
      "ACC: 0.5, MCC: -0.008866995073891626\n",
      "Epoch 0, Loss(train/val) 4.74960/4.71452. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.71201/4.70455. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.70258/4.71206. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.70156/4.72462. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.70077/4.72539. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.70098/4.72005. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.70098/4.72073. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.69511/4.73241. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.69480/4.72941. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.69361/4.72908. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.69697/4.73685. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.69216/4.73709. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.69590/4.72753. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.69580/4.73643. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.69727/4.73501. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.69670/4.73499. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.69195/4.74718. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.69538/4.74340. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.69255/4.74810. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.69522/4.74716. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.69132/4.75589. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.69310/4.75880. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.69239/4.76644. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.68989/4.77685. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.69105/4.76127. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.69317/4.77292. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.69101/4.76163. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.69141/4.75836. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.68728/4.76402. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.68855/4.76111. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.69006/4.76305. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.68724/4.76053. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.68771/4.75261. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.68597/4.76834. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.68730/4.76201. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.68654/4.75794. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.69026/4.76858. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.68215/4.75789. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.68829/4.76952. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.68653/4.76858. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.68775/4.75678. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.69044/4.76441. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.68840/4.76215. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.68591/4.76117. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.68893/4.77953. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.68348/4.77619. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.68126/4.77341. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.68361/4.76770. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.68531/4.78460. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.68757/4.76853. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.68608/4.76821. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.68222/4.76944. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.68473/4.75566. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.68110/4.77399. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.67903/4.77779. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.68350/4.77132. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.68158/4.77098. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.67951/4.79324. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.68279/4.76815. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.69821/4.76068. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.69770/4.73725. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.69164/4.74918. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.68847/4.75062. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.68709/4.74427. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.69029/4.75418. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.68524/4.74360. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.68646/4.75634. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.67874/4.77028. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.68602/4.75359. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.69060/4.75502. Took 0.21 sec\n",
      "Epoch 70, Loss(train/val) 4.68598/4.75086. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.68280/4.76891. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.68423/4.76573. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.68079/4.80290. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.69047/4.76947. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.68326/4.76718. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.68402/4.78128. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.68177/4.79008. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.68411/4.77992. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.68561/4.78458. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.68202/4.78312. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 4.67888/4.80764. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.68000/4.78635. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.67784/4.80161. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.68138/4.78117. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.67854/4.78287. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.68025/4.77741. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.67770/4.77906. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.68151/4.79048. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.67461/4.77402. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.67937/4.77496. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.67729/4.76874. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.67822/4.77637. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.67811/4.77592. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.67583/4.78004. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.67863/4.77392. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.67641/4.79718. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.67478/4.79761. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.67941/4.77486. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.67757/4.77401. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.11383959916814836\n",
      "Epoch 0, Loss(train/val) 5.06467/5.01842. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.97694/4.98437. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.97103/4.97431. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.97223/4.97469. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.97256/4.97194. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.97168/4.97036. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.96990/4.97379. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.97364/4.97410. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.97007/4.97219. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.96749/4.97350. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.96896/4.96811. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.96839/4.97010. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.96983/4.96916. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.96799/4.96696. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.96548/4.96731. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.96752/4.96786. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.96543/4.96855. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.96534/4.96305. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.96750/4.96763. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.96644/4.96426. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.96619/4.96606. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.96590/4.96873. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.96306/4.97072. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.96393/4.97421. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.96560/4.98380. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.96487/4.98160. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.95947/4.97755. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.96225/4.97560. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.96091/4.98015. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.96141/4.97511. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.95984/4.97694. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.95612/4.98046. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.96066/4.97607. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.96092/4.98090. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.96040/4.97828. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.95646/4.98266. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.95769/4.97290. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.95419/4.98256. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.96185/4.97907. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.95657/4.98843. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.96060/4.97926. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.94985/4.98323. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.95743/4.98213. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.94888/4.99890. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.95754/4.97880. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.96004/4.97247. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 4.96212/4.97410. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.95186/4.98258. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.95800/4.97088. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.95142/4.97625. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.95712/4.97681. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.95405/4.99473. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.95375/4.99840. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.95600/4.97058. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.96476/4.96784. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.95854/4.96877. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.95496/4.98340. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.95587/4.98603. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.95287/4.99604. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.94966/4.99554. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.95350/4.98600. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.94865/5.00637. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.94686/5.01720. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.95083/5.01892. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.94704/5.00776. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 4.95076/5.00892. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.95120/4.99250. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.94422/5.00744. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.94802/5.00197. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.94749/5.00764. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.96104/4.97081. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.95627/4.98300. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.95291/4.99311. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.94890/5.00159. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.95031/4.99995. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.94843/4.98755. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.94479/5.01511. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.94568/5.00527. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.94627/5.00422. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.94909/4.98948. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.93923/5.03790. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.94945/4.97387. Took 0.21 sec\n",
      "Epoch 82, Loss(train/val) 4.94153/4.99597. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.94006/5.00271. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.94034/5.01118. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.95055/5.02608. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.95131/5.00671. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.95193/4.97432. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.95377/4.99876. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.94576/5.01180. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.94937/5.00096. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.94711/5.00948. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.94689/5.01171. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.94559/5.00277. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.94267/4.99876. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.94421/5.01010. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.94045/5.00784. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.94810/5.01630. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.94058/5.01532. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.94014/5.02973. Took 0.20 sec\n",
      "ACC: 0.40625, MCC: -0.1746031746031746\n",
      "Epoch 0, Loss(train/val) 5.00919/5.01836. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.97145/5.02649. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.96291/5.02991. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.96670/4.99248. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.95847/4.97860. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.95253/4.98884. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.95356/4.97338. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.95992/4.97981. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.95879/4.97480. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.95565/4.96641. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.95275/4.97158. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.95261/4.97621. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 4.94802/4.98531. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.94936/4.97928. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.94881/4.98189. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.94665/4.97486. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.94194/4.99501. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.94686/4.98337. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.94555/4.99173. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.93504/5.00082. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.94310/4.99043. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.93993/4.98937. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.94368/4.99328. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.94190/4.99011. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.94096/4.98621. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.93766/4.99599. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.93783/4.99483. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.93555/5.00863. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.93502/5.00207. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.93740/5.00390. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.93521/5.01343. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.93437/5.00432. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.93219/5.00932. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.93932/5.00570. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.93251/5.01602. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.93021/5.01570. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.93530/5.01598. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.93271/5.00599. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.93219/5.02380. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.92869/5.01872. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.92755/5.02678. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.93438/5.02293. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.92901/5.02199. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.92705/5.02017. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.92943/5.02704. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.92681/5.02413. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.93318/5.04132. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.92436/5.01864. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.92936/5.03342. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.92640/5.02243. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.92706/5.02119. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.92875/5.02416. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.92746/5.01845. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.92251/5.03366. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.92618/5.02831. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.92499/5.05405. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.92719/5.03385. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.92977/5.02765. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.92525/5.03074. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.92649/5.03671. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.92489/5.03978. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.92288/5.03610. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.92553/5.05253. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.92920/5.02365. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.92718/5.02318. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.92231/5.04061. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 4.92557/5.02588. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.91761/5.05417. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.92777/5.02182. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.92416/5.02539. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.92585/5.03370. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.91887/5.04086. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.92300/5.05035. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.91973/5.04071. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.92715/5.02473. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.92989/5.02903. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.92136/5.02702. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.92010/5.04669. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.91963/5.03299. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.92451/5.03930. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.92507/5.03460. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.91921/5.04211. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.91856/5.04436. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.92684/5.01856. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.92379/5.02889. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.92280/5.02521. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.92271/5.02484. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.92181/5.02282. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.91999/5.03371. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.92119/5.03120. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.92647/5.02866. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.91875/5.04236. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.91890/5.04132. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.92112/5.04367. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.91999/5.05333. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.92370/5.03980. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.92014/5.04585. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.91989/5.02540. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.91965/5.02860. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.92417/5.03709. Took 0.20 sec\n",
      "ACC: 0.515625, MCC: 0.03126526997403612\n",
      "Epoch 0, Loss(train/val) 4.67869/4.62422. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.63728/4.64281. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.62816/4.66302. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.63155/4.66489. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.62886/4.66497. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.62832/4.67446. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.62965/4.68373. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.63078/4.69542. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.62911/4.70172. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.62691/4.70084. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.62541/4.69088. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.62305/4.68063. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 4.62046/4.67883. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.62110/4.67707. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.62309/4.68094. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 4.62133/4.70745. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.62554/4.69134. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.61933/4.69833. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.62361/4.68082. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.62091/4.67841. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.61359/4.69100. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.61427/4.69858. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.61969/4.69965. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.61801/4.71785. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.61560/4.70660. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.61606/4.70318. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.61434/4.69682. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 4.61471/4.70325. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.60990/4.70894. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.61274/4.71391. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.61645/4.70191. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.60890/4.72005. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.61241/4.71954. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.61549/4.70126. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.61284/4.70815. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.61692/4.70587. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.62132/4.67681. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.62385/4.69028. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.61942/4.67501. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.61806/4.67962. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.61473/4.67520. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.60891/4.70015. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.61371/4.68021. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.60957/4.69243. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.61296/4.69546. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.61805/4.64324. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.61688/4.70000. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.61544/4.69371. Took 0.22 sec\n",
      "Epoch 48, Loss(train/val) 4.60784/4.71023. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.60973/4.70145. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.60558/4.71718. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.61043/4.72043. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.60597/4.72768. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.60896/4.70103. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.60510/4.74848. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.61171/4.72843. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.60531/4.69625. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 4.60962/4.72100. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.60485/4.74347. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.60746/4.70779. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.60544/4.75001. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.60915/4.73764. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.60528/4.71458. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.59971/4.73709. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.60477/4.70440. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.60493/4.71429. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.59962/4.73399. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.60493/4.73572. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.60375/4.70973. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.59587/4.74839. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.59687/4.72015. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.60082/4.73526. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.59988/4.71456. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.60329/4.72726. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.59709/4.70141. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.59967/4.71177. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.59561/4.72290. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.59365/4.75891. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.59113/4.74451. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.60184/4.71828. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.59550/4.70925. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.59620/4.71296. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.59552/4.72344. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.59847/4.74975. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.60396/4.72558. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.59689/4.71965. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.59945/4.72107. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.60092/4.73037. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.59286/4.73030. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.59241/4.76933. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.59639/4.71790. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.59272/4.72885. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.59518/4.73452. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.59865/4.70154. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.59574/4.71456. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.59381/4.72757. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.59141/4.72761. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.59502/4.72153. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.58988/4.76333. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.59493/4.72991. Took 0.20 sec\n",
      "ACC: 0.578125, MCC: 0.16282240225645903\n",
      "Epoch 0, Loss(train/val) 4.77737/4.70901. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.72586/4.71985. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.72580/4.72744. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 4.72747/4.72943. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.72357/4.72389. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.72361/4.72083. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.72532/4.71789. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.72193/4.71416. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.72445/4.71399. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.72389/4.71182. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.72102/4.71296. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.71964/4.71316. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.72227/4.71515. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.72101/4.71214. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.71818/4.71314. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.71900/4.70936. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.72055/4.71230. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.71901/4.71220. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.71870/4.71370. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.72022/4.71700. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.71505/4.71712. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.71808/4.71692. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.71706/4.70993. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.71791/4.71361. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.71525/4.71523. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.71591/4.71498. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.71346/4.71354. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.71659/4.71646. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.71472/4.71482. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.71349/4.71333. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.71286/4.72016. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.71214/4.71743. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.71373/4.71521. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.71228/4.71345. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.71395/4.72044. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.71371/4.72323. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.71063/4.71831. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.71142/4.72133. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.71287/4.72731. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.71096/4.72690. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.71043/4.72659. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.71382/4.72198. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.70871/4.72740. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.71516/4.73435. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.71255/4.73102. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.71230/4.72400. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.70970/4.72893. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.70921/4.72581. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.71004/4.73018. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.70916/4.73606. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.71025/4.73102. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.70786/4.73542. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.70922/4.73349. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.71067/4.72577. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.70743/4.73270. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.70623/4.74417. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.70714/4.73055. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.70933/4.73755. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.70614/4.73558. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.70458/4.73322. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.71029/4.73190. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.70591/4.73507. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.70588/4.73501. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.70398/4.73920. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.70670/4.73145. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.70712/4.73920. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.70359/4.73535. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.70735/4.74122. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.70550/4.74635. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.70508/4.74429. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.70099/4.74810. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.70658/4.74403. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.70205/4.75184. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.70300/4.75736. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.69970/4.75402. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.70030/4.74925. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.70271/4.74803. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.70673/4.73980. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.70294/4.74453. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.70145/4.75171. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.70301/4.74962. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.69916/4.74786. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.70126/4.75565. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.69733/4.76381. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.70310/4.75335. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.70091/4.75024. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.70140/4.74329. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.69696/4.74695. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.69860/4.76017. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.70469/4.74653. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.69942/4.75030. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.69908/4.74373. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.69869/4.75403. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.69847/4.75160. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.69999/4.74966. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.70141/4.74909. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.69528/4.75807. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.69696/4.75755. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.69832/4.77008. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.69174/4.75853. Took 0.19 sec\n",
      "ACC: 0.5, MCC: -0.011953709238683663\n",
      "Epoch 0, Loss(train/val) 5.10073/5.09545. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.03106/5.04343. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.02927/5.03366. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.03305/5.03452. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.03774/5.03071. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.03293/5.02647. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.03083/5.02655. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.03019/5.02733. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.02653/5.02924. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.02623/5.02947. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 5.02637/5.03105. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.02588/5.03379. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 5.02819/5.03543. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.02608/5.03555. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.02389/5.03753. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.01991/5.03950. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.02217/5.03916. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.02243/5.03844. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.02047/5.04189. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.02029/5.04145. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.02189/5.04085. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.01835/5.04094. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.02093/5.04017. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.01822/5.03501. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.01958/5.03714. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.01668/5.03900. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.01697/5.04224. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.01674/5.05906. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.02013/5.05057. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.01817/5.05031. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.01515/5.06717. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.01631/5.04432. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.01345/5.05049. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.01910/5.04037. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.01418/5.05435. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.01331/5.05493. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.01048/5.05838. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 5.00977/5.05200. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.01364/5.05283. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.00899/5.05732. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.01319/5.04132. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.00769/5.06242. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 5.00756/5.05329. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.00532/5.05548. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.01201/5.04475. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.01601/5.05616. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.00852/5.03656. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.00642/5.05017. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.00506/5.06531. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 5.00713/5.05095. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 5.00556/5.05760. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.00365/5.06128. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.00788/5.04601. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.00248/5.06283. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.00153/5.08610. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 5.00270/5.06554. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.00232/5.06365. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.00028/5.06621. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.00377/5.06148. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.99918/5.07136. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.00075/5.07593. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.00614/5.05624. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.00196/5.06096. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.99896/5.05619. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.99977/5.05623. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.00615/5.05144. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.00334/5.07152. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.00352/5.04876. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.00484/5.06647. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.00478/5.04656. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.00549/5.06629. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.00082/5.05471. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.99889/5.06056. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.99820/5.06517. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.00040/5.06665. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.00007/5.07211. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.99744/5.05963. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.00602/5.06240. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.99995/5.06929. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.00281/5.06602. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.99661/5.09842. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.99981/5.05645. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.99748/5.07140. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.99901/5.06430. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 5.00294/5.06726. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.99119/5.06239. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.00392/5.05133. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 5.00033/5.06972. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.99705/5.06829. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.00057/5.05673. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.99229/5.08613. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.99977/5.06288. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.99817/5.07960. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.99775/5.05758. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.99103/5.08780. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.99611/5.08976. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.99717/5.06846. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.99582/5.06633. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.99979/5.07792. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.99488/5.06225. Took 0.19 sec\n",
      "ACC: 0.390625, MCC: -0.20959097296420087\n",
      "Epoch 0, Loss(train/val) 5.08958/5.04769. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.04526/5.02653. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.03752/5.02671. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.03080/5.01774. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.02726/5.01611. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.02097/5.01617. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.02505/5.01756. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.02442/5.01723. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.02256/5.01779. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.02318/5.01703. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.02100/5.01779. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.02066/5.01705. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.02119/5.01655. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.01821/5.01607. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.01821/5.01562. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.01912/5.01719. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 5.01733/5.01626. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.01704/5.01836. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.01518/5.01427. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.01311/5.01589. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.01602/5.01173. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.01336/5.01378. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.01367/5.01067. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.01201/5.00670. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.01288/5.01258. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.01074/5.01633. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.01149/5.01074. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.00929/5.00450. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.00596/5.00466. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.00828/5.02101. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.00987/4.99854. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.01267/4.99789. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.01332/5.00796. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.00816/5.00368. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.00798/4.99827. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.00497/5.00586. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.99989/5.00437. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.00305/4.99573. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 5.00094/5.00519. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.00161/5.01068. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.01017/5.00658. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.00606/5.00314. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.00469/5.01050. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.00005/5.00910. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.99798/5.01066. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.99955/5.00473. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.99590/5.01548. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.00440/5.01950. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.99542/5.01548. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.99705/5.00395. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.99905/5.00984. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.99765/5.01110. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.99845/5.01247. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.98915/5.01444. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.99774/5.00667. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 5.01365/5.00726. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.00494/5.01199. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.00046/5.01650. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.99967/5.01599. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.99928/5.00854. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.99702/5.00176. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.99771/5.00291. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.99612/5.00725. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.99822/5.01788. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.99715/5.01594. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.99819/5.01139. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.99335/5.01043. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.00185/5.02267. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.99478/5.01848. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.99229/5.01470. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.99623/5.01810. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.98850/5.01415. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.99152/5.01450. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.99200/5.01575. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.99676/5.01520. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.99221/5.01596. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.99478/5.00884. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.98984/5.01129. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.99285/5.00723. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.99330/5.01112. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.98883/5.01585. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.98580/5.01543. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.99269/5.02575. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.98734/5.02865. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.99093/5.02287. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.99112/5.02781. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.98817/5.02513. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.98901/5.02522. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.98417/5.03047. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.98848/5.02801. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.99747/5.02046. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.98589/5.01652. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.98295/5.02415. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.99312/5.01567. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.98004/5.02723. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.99279/5.01232. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.99129/5.02127. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.98592/5.01749. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.98368/5.01538. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.98659/5.02394. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.0778902520624173\n",
      "Epoch 0, Loss(train/val) 4.96933/4.90150. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.91242/4.89767. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.90941/4.89722. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.91129/4.89598. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.90893/4.89756. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.91191/4.89678. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.90951/4.89553. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.90929/4.89408. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.90355/4.89291. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.90395/4.89339. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.90312/4.89697. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.90225/4.89890. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.90268/4.89864. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.90344/4.90444. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.89809/4.89933. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.89797/4.89839. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.89711/4.89805. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.89679/4.90802. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.89367/4.90408. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.89711/4.91689. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.90191/4.89081. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.90061/4.89375. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.89802/4.88811. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.89942/4.89248. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.89185/4.88540. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.89196/4.88757. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.89411/4.89043. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.89262/4.88956. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.88998/4.89445. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.88686/4.90016. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.88795/4.90015. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.88819/4.89754. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.88513/4.89782. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.88363/4.90876. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.88987/4.90229. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.88200/4.90558. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.88620/4.89606. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.88590/4.89597. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.88805/4.88791. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.88376/4.89933. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.88106/4.89183. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.87884/4.89155. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.88093/4.89230. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.87837/4.89591. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.88822/4.90359. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.88867/4.90651. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.88400/4.89837. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.88330/4.89272. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.88423/4.90010. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.88360/4.89681. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.87196/4.90860. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.88483/4.89972. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 4.87955/4.90808. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.88294/4.89359. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.87868/4.89576. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.87308/4.90561. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.87024/4.90658. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.87630/4.89417. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.88388/4.89082. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.87603/4.90546. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.87276/4.90076. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.87368/4.88956. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.87237/4.90829. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.86958/4.90630. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.87594/4.90674. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.87790/4.90453. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.87550/4.89721. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.86853/4.90722. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.87355/4.89307. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.87106/4.89491. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.87081/4.91253. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.86787/4.91116. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.86959/4.90132. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.86674/4.90779. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.86905/4.89724. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 4.86972/4.89399. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.87141/4.90191. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.86803/4.89950. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.86634/4.92776. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.86842/4.89993. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.87278/4.90393. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.86986/4.90686. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.86283/4.90929. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.86455/4.90468. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.87019/4.91052. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.86495/4.90951. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.86426/4.88845. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.86697/4.91339. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.86100/4.91258. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.85875/4.90389. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.86653/4.88137. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.86480/4.89176. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.87565/4.90251. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.86328/4.91925. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.86729/4.91155. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.86102/4.92965. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.85188/4.93513. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 4.85415/4.90289. Took 0.21 sec\n",
      "Epoch 98, Loss(train/val) 4.85339/4.91412. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.86871/4.89478. Took 0.19 sec\n",
      "ACC: 0.390625, MCC: -0.23865721224648745\n",
      "Epoch 0, Loss(train/val) 4.99660/4.98856. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.92626/4.93992. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.91835/4.92678. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.91429/4.92408. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.91303/4.92149. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.91349/4.92176. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.91034/4.91292. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.91163/4.90711. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.91014/4.91384. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.90794/4.90536. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.91056/4.91610. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.90883/4.89041. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.90341/4.89882. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.90321/4.89874. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.90500/4.89606. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.90618/4.90041. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.90553/4.89634. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.90151/4.89635. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.90318/4.90345. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.90442/4.90425. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.90317/4.90171. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.90255/4.89951. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.90075/4.90221. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.89968/4.90207. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.89930/4.90928. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.90077/4.90808. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.90212/4.90833. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.89895/4.91034. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.89978/4.91720. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.89415/4.91837. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.89824/4.91473. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.90333/4.92063. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.89678/4.91997. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.89580/4.93051. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.89256/4.93322. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.89751/4.92855. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.88954/4.93724. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.89384/4.93157. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89391/4.93738. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90514/4.92237. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.89230/4.93564. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.89344/4.93677. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89010/4.93520. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.89584/4.93507. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.89081/4.94000. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.89172/4.93721. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.89071/4.93967. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.88868/4.94817. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.89315/4.93474. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.88965/4.94774. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.88730/4.94961. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.89227/4.94189. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.90121/4.94140. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.89095/4.94362. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.89133/4.96401. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 4.89829/4.94419. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.89282/4.93219. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.89347/4.93848. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.89349/4.95720. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89451/4.94381. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.88904/4.95200. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.88862/4.95406. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.89327/4.93988. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.88598/4.97292. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.88986/4.96140. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.88732/4.95248. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.89865/4.93177. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.88851/4.95109. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.89163/4.94559. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.88386/4.95359. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.88369/4.96530. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.88725/4.95750. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.88414/4.96052. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88719/4.97170. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.88531/4.96873. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.88989/4.94476. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.88482/4.95195. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.88843/4.94435. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.88176/4.96293. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.88471/4.97088. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88528/4.94403. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.88879/4.94470. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.88374/4.95128. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.88123/4.96995. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88123/4.96298. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.88262/4.96566. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.88932/4.93927. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.87786/4.96534. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.88291/4.94440. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.87980/4.93823. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.88404/4.94350. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.88333/4.95277. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.87495/4.93852. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.87450/4.96269. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.87580/4.94969. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.88016/4.95582. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.88141/4.96422. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.87880/4.94334. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.87748/4.95722. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88236/4.95037. Took 0.20 sec\n",
      "ACC: 0.484375, MCC: -0.03734655007457467\n",
      "Epoch 0, Loss(train/val) 4.77878/4.70895. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.71537/4.70576. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.71392/4.70702. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.71511/4.70315. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.71203/4.70300. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.71280/4.70291. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.70959/4.70226. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.70987/4.70171. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.71314/4.70069. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.70726/4.70395. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.71321/4.70969. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.70805/4.70445. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.70914/4.70512. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.70857/4.70476. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.70827/4.70642. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.70766/4.70218. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.70703/4.71336. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.70561/4.71043. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.70468/4.70872. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.70618/4.71418. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.70744/4.71274. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.70344/4.71251. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.70883/4.71407. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.70376/4.71529. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.70525/4.71967. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.70295/4.71944. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.70429/4.72038. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.70338/4.72236. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.69777/4.72049. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.70037/4.72375. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.69977/4.72534. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.70482/4.71859. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.70214/4.72590. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.69852/4.73122. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.69901/4.72927. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.70117/4.73581. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 4.70019/4.73360. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.69416/4.73288. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.69832/4.72919. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.69614/4.72546. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.69372/4.74504. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.69757/4.72554. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.69917/4.73485. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.69876/4.73269. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.69408/4.73223. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 4.69540/4.72726. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.69227/4.74163. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.69272/4.74267. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.68996/4.75161. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.69426/4.74185. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.69157/4.76743. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.69619/4.75520. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.69268/4.76305. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.69305/4.77282. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.69481/4.75972. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.68997/4.75588. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.69491/4.75073. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.69309/4.75408. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.68762/4.75446. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.69778/4.74787. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.69081/4.76003. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.68578/4.76044. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.68921/4.76179. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.69129/4.76031. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.68382/4.76479. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.68765/4.78070. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.68711/4.76164. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.68499/4.77692. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.69138/4.76066. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.67947/4.75411. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.68003/4.79700. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.68720/4.76375. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.68561/4.76967. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.68471/4.78992. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.69080/4.76979. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.67911/4.78905. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.68186/4.78800. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.68645/4.76723. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.67919/4.77716. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.68661/4.77047. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.67814/4.78528. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.67235/4.78814. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.68315/4.78193. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.67302/4.77907. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.68047/4.76222. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.67356/4.78815. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.67019/4.80854. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.67569/4.81430. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.67297/4.79573. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.67594/4.81047. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.67916/4.80945. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.67854/4.78298. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.68007/4.79040. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.66855/4.80783. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.67557/4.81266. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.67143/4.80147. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.67147/4.79957. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.67383/4.77572. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.67147/4.79140. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.66755/4.79749. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.056730862893117545\n",
      "Epoch 0, Loss(train/val) 4.77994/4.73958. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.73203/4.74110. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.72637/4.73831. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.72769/4.74066. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.72769/4.74097. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.72357/4.74158. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.72722/4.74043. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 4.72818/4.73859. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.72534/4.74573. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.72722/4.74079. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.72265/4.74364. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.72084/4.74996. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.72060/4.75799. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.71619/4.76304. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.71855/4.74929. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.71145/4.76724. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.71604/4.76061. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.70927/4.76911. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.70705/4.78688. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.70983/4.76965. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.71270/4.77149. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.71446/4.76312. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.71596/4.75910. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.70837/4.77840. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.71162/4.76169. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.70907/4.78598. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.70939/4.77962. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.70578/4.77852. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.70461/4.80743. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.70175/4.78655. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.71530/4.77934. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.70823/4.79333. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.70730/4.79365. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.70353/4.81589. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.70644/4.80269. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 4.70578/4.79179. Took 0.22 sec\n",
      "Epoch 36, Loss(train/val) 4.69948/4.80692. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.70480/4.78968. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.70315/4.81187. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.70131/4.80278. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.70157/4.78842. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.69914/4.79826. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.69548/4.82440. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.69990/4.81141. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.70263/4.79572. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.70014/4.80302. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.70173/4.80865. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.69981/4.79722. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.69140/4.81989. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.69683/4.80277. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.69703/4.81296. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.70398/4.79858. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.69663/4.79427. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.70027/4.82079. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.69567/4.83344. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.69700/4.80998. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.69455/4.83505. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.69324/4.81549. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.69406/4.80370. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.69477/4.84023. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.70053/4.82419. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.69574/4.83502. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.69430/4.81691. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.70062/4.84277. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.69244/4.83239. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.69195/4.83715. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.68763/4.85898. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.69365/4.83095. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.69030/4.82480. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.68867/4.81237. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.68652/4.83718. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.70114/4.81037. Took 0.22 sec\n",
      "Epoch 72, Loss(train/val) 4.69594/4.81761. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.69884/4.82460. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.69698/4.80701. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.69375/4.82121. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.69400/4.81930. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.69202/4.81202. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.68707/4.84770. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.69207/4.82048. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.68970/4.80306. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.69244/4.84313. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.69092/4.82867. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.69188/4.80228. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.69467/4.80971. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.68935/4.82235. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.69260/4.81710. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.67819/4.84629. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.69099/4.81206. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.68204/4.83135. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.68402/4.82443. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.68964/4.79582. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.68952/4.83770. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.69230/4.80687. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.68694/4.84972. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.68654/4.81798. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.68299/4.84463. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.68300/4.83942. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.68119/4.85744. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.67668/4.84188. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.17930563858025494\n",
      "Epoch 0, Loss(train/val) 4.77398/4.71734. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.70384/4.71148. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 4.70253/4.70359. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.69981/4.69710. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.69283/4.69862. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.69262/4.69827. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.69143/4.69685. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.69296/4.69684. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.69036/4.69892. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.68979/4.69997. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.68937/4.69926. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.68809/4.69963. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.68699/4.69580. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.68191/4.70224. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.68546/4.70087. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.68373/4.70454. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.67923/4.71170. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.68427/4.70348. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.68096/4.70664. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.68244/4.70500. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.68243/4.70671. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.67822/4.69569. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.68309/4.68731. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.68615/4.68705. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.68248/4.68019. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.68196/4.68426. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.67970/4.68362. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.68208/4.67635. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.68216/4.67709. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.67934/4.68495. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.68046/4.68042. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.67654/4.68093. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.68137/4.69238. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.68987/4.68275. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.68809/4.68132. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.68487/4.68464. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.68531/4.68717. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.68279/4.68731. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.68468/4.68456. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.68518/4.68946. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.68289/4.68188. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.68255/4.68362. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.68356/4.68664. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.68373/4.68363. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.68312/4.68674. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.68243/4.68575. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.68191/4.68441. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.68288/4.67929. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.68469/4.68382. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.68178/4.68582. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.68038/4.68678. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.68487/4.68397. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.68165/4.68814. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.67863/4.68408. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.68158/4.68410. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.68064/4.68826. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.67868/4.69237. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.68112/4.69399. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.67862/4.70069. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.68107/4.69978. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.67795/4.70375. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.67827/4.70141. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.68206/4.69487. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.67086/4.71614. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.67667/4.71016. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.68388/4.71244. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.67762/4.69853. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.67672/4.70488. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.67519/4.70164. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.67290/4.70273. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.67351/4.72638. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.68015/4.69283. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.67683/4.70635. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.67407/4.71175. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.67023/4.72276. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.67649/4.70899. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.67736/4.70769. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.67173/4.72218. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.67557/4.71676. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 4.67148/4.71329. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.67264/4.71842. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.67419/4.70611. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.66889/4.71838. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.66794/4.72030. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.66711/4.72422. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.66836/4.72139. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.67177/4.71305. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.67231/4.72064. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.66912/4.73273. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.67228/4.73694. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.67924/4.71328. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.67386/4.71776. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.67270/4.72405. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.66732/4.72737. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.66921/4.72039. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.66625/4.72848. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.67059/4.71974. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.66745/4.71038. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.66900/4.71322. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.66732/4.71290. Took 0.20 sec\n",
      "ACC: 0.359375, MCC: -0.3069288458707305\n",
      "Epoch 0, Loss(train/val) 5.24958/5.17995. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.18721/5.20279. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.18839/5.21646. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.19172/5.22221. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.19310/5.21271. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 5.19218/5.19065. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.18710/5.18658. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 5.18362/5.18756. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.18327/5.19392. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.18201/5.19373. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.18209/5.19424. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 5.18283/5.19765. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.18212/5.19017. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.18031/5.19802. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 5.17908/5.20339. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.17943/5.19537. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.17925/5.19914. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.18085/5.19739. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.18036/5.19550. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.17519/5.19754. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.17526/5.20281. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.17704/5.20450. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.17591/5.20387. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.18313/5.21277. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.17839/5.20136. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.17831/5.20351. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.17886/5.19134. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.17720/5.20268. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.17906/5.20582. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.17264/5.21697. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 5.17544/5.21685. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 5.17464/5.21354. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 5.16930/5.23401. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.17638/5.22458. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.16762/5.23363. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.17232/5.21646. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.18186/5.21210. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 5.17663/5.20025. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 5.17376/5.20281. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.17043/5.20791. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.17538/5.20898. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 5.17323/5.20617. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.16966/5.22108. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.16881/5.22359. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.16645/5.23622. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.16969/5.22627. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.16451/5.23846. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.16730/5.20492. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.16193/5.24610. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 5.17148/5.20652. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 5.17231/5.21093. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.17240/5.21383. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 5.16556/5.23515. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.17317/5.21142. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 5.16900/5.20432. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.16938/5.21129. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.16449/5.22087. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.16611/5.22237. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.16198/5.23081. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.16190/5.23858. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.16444/5.23724. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 5.16388/5.22197. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 5.15698/5.23403. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.16082/5.25042. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 5.16581/5.22927. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.15446/5.25262. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.15910/5.22356. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.15670/5.22866. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 5.15519/5.24045. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.15714/5.23761. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.15677/5.25565. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.15568/5.25562. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.15361/5.25950. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.15312/5.24989. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 5.15074/5.26937. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 5.15065/5.26879. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.15621/5.23358. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 5.15472/5.25074. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.15246/5.24491. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.14938/5.26881. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.15908/5.24125. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.14875/5.24934. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.15051/5.24074. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.15054/5.27266. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.14816/5.26131. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.15331/5.23944. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.14700/5.28608. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.14984/5.26513. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.15290/5.23970. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.14930/5.26446. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.15499/5.26208. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.15269/5.25681. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.15109/5.25974. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.15829/5.25053. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.15235/5.24616. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 5.14861/5.24169. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.15085/5.26959. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.13907/5.29707. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.14362/5.27425. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.14654/5.26864. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.05707352953442433\n",
      "Epoch 0, Loss(train/val) 4.64862/4.62224. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.59916/4.59230. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.59145/4.59127. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.59210/4.59370. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.59281/4.59876. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.59082/4.60332. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.58571/4.60871. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.58874/4.60636. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.58692/4.60845. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.58893/4.60381. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.58751/4.58906. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.58841/4.59404. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.58867/4.59752. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.58556/4.59972. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.58234/4.60535. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.58023/4.60908. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.58310/4.61658. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.58830/4.61070. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.57900/4.61271. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.57929/4.61496. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.57928/4.62356. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.57649/4.62744. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.57940/4.62707. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.57920/4.62559. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.58042/4.61971. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.57162/4.63118. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.57728/4.62488. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.57912/4.62235. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.57873/4.62510. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.57618/4.62929. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.57560/4.63351. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.57341/4.63825. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.57862/4.62597. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.57392/4.63403. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.57110/4.64259. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.56714/4.64704. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.57386/4.64655. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.57122/4.64377. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.56978/4.65476. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.57208/4.65429. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.57401/4.64303. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.57074/4.65073. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.57095/4.65981. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.56806/4.66904. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.57117/4.66118. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.56912/4.66172. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.57136/4.66145. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.56260/4.67187. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.56584/4.67057. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.57281/4.65482. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.56267/4.66414. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.57028/4.66474. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.56524/4.66183. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.57122/4.65770. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.56724/4.66713. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.56374/4.66920. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.56453/4.67703. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.56615/4.67815. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.56395/4.66713. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.56611/4.66084. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.56481/4.66749. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.56606/4.66968. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.56539/4.67423. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.56553/4.67254. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.56724/4.67182. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.56266/4.68078. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.56395/4.68579. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.56603/4.67190. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.56057/4.69111. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.56705/4.67223. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.56498/4.67005. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.56411/4.68030. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.56173/4.67450. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.56207/4.67807. Took 0.21 sec\n",
      "Epoch 74, Loss(train/val) 4.56555/4.67553. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.56433/4.67980. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.56361/4.67026. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 4.56192/4.67767. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.56356/4.67521. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.56391/4.68768. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.56329/4.67722. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.56064/4.67910. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.55883/4.68372. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.56439/4.67111. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.55747/4.68580. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.55910/4.69420. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.55692/4.70642. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.55777/4.70561. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.55718/4.71138. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.55685/4.69581. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.56231/4.67840. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.55955/4.68401. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.55622/4.70062. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.55732/4.71179. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.55820/4.69118. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.55646/4.69479. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.54850/4.74044. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.56579/4.68494. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.55584/4.70500. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.57622/4.60167. Took 0.20 sec\n",
      "ACC: 0.46875, MCC: -0.06362847629757777\n",
      "Epoch 0, Loss(train/val) 4.61938/4.55760. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.56786/4.58524. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.58500/4.57561. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.58845/4.55632. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.57050/4.55669. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.56130/4.56274. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.56632/4.56317. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.56566/4.56168. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.56415/4.56457. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.56243/4.56487. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.55959/4.56491. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.55904/4.56645. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.56033/4.56698. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.55777/4.57169. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.55897/4.56897. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.55642/4.57502. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.55400/4.57586. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.55620/4.57814. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.55555/4.57613. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.55740/4.57396. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.55150/4.58730. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.54936/4.58906. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.55250/4.58260. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.55183/4.58355. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.55072/4.59112. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.55033/4.58871. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.55012/4.58782. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.54648/4.59639. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.54950/4.59032. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.54882/4.59381. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.54885/4.59157. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.54593/4.59468. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.54770/4.58317. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.54642/4.60048. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.54433/4.60417. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.54555/4.60137. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.54489/4.59192. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.54941/4.58505. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.54475/4.59728. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.54143/4.61179. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.54185/4.60699. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.54681/4.59128. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.54596/4.59314. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.54378/4.59107. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.54648/4.59891. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.54539/4.58660. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.54514/4.59651. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.54020/4.61565. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.54474/4.59712. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.54278/4.59884. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.54083/4.60088. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.53988/4.62153. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.54062/4.60754. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.53961/4.60871. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.54003/4.59516. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.54429/4.59869. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.54263/4.60285. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.54296/4.59617. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.53864/4.60328. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.53864/4.60827. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.54343/4.60928. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.54267/4.60596. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.54069/4.60050. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.53643/4.62465. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.54055/4.60326. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.54142/4.61599. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.54015/4.60543. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.54051/4.60420. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.54364/4.60863. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.54537/4.58203. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.54604/4.59071. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.54313/4.60043. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.54568/4.59902. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.54183/4.59848. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.54303/4.60816. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.53994/4.60207. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.53991/4.61820. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.53766/4.61713. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.54055/4.61649. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.53919/4.61772. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.54044/4.61825. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.53520/4.62010. Took 0.21 sec\n",
      "Epoch 82, Loss(train/val) 4.53278/4.63138. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.54984/4.57444. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.54162/4.59879. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.54198/4.61099. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.53328/4.62364. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.53734/4.62133. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.53841/4.62916. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.54076/4.62204. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.53864/4.63146. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.53610/4.61660. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.53980/4.62110. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.53838/4.61663. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.53654/4.64059. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.53396/4.63329. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.53686/4.61338. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.53743/4.63514. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.54220/4.61646. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.53841/4.62412. Took 0.19 sec\n",
      "ACC: 0.4375, MCC: -0.0931174089068826\n",
      "Epoch 0, Loss(train/val) 4.98428/4.86118. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.88373/4.87396. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.87312/4.86620. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87189/4.85998. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.87104/4.85703. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.87406/4.85672. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.87530/4.85670. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.87365/4.85546. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.87235/4.85759. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87027/4.85095. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.86918/4.85685. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.86778/4.85641. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.87020/4.85626. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.86853/4.85834. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.86969/4.85921. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.86776/4.86109. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.86654/4.86149. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.86387/4.86189. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.86365/4.86222. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.86738/4.86144. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.86502/4.86137. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.86135/4.86163. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.86852/4.86056. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.86373/4.86345. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.86221/4.86775. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.86198/4.87074. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.86260/4.86876. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.86237/4.86883. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.86091/4.86912. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.85950/4.87766. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.86447/4.86714. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.85781/4.86800. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.86034/4.87168. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.85682/4.87238. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.86121/4.86943. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.85511/4.87001. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.86146/4.86862. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.85988/4.87194. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.86133/4.87482. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.85942/4.86766. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.85893/4.86784. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.85414/4.86764. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.85617/4.87099. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.85450/4.87044. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.85750/4.87058. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.85675/4.86509. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.85590/4.86318. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.85691/4.86598. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.84510/4.87926. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.85545/4.87794. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.85439/4.87486. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.85314/4.86754. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.85416/4.87441. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.84871/4.87989. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.85095/4.86892. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.85518/4.86982. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.85222/4.86592. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.85191/4.87247. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.84624/4.88121. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.85113/4.88010. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.84864/4.87624. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84690/4.87962. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85064/4.87569. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.85200/4.86924. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.84573/4.87197. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.84593/4.87574. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.84814/4.87276. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.84368/4.88404. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.84753/4.87505. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84634/4.86887. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.84934/4.86830. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.84602/4.87151. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.84480/4.87716. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84482/4.87160. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.84419/4.87737. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.84509/4.86629. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.85857/4.88666. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.85832/4.88299. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.84960/4.88404. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.85064/4.88360. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.84034/4.89865. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.85008/4.88470. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.84910/4.88255. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84841/4.87760. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.84477/4.87623. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.84764/4.87284. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.83986/4.89539. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.84501/4.88543. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.84259/4.88014. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.84503/4.89476. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.84254/4.88987. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.84315/4.88003. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.83871/4.89352. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.84198/4.90860. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.84064/4.89484. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.84082/4.90219. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83946/4.88880. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.84171/4.88700. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.83737/4.90527. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.83275/4.88683. Took 0.19 sec\n",
      "ACC: 0.65625, MCC: 0.3076976944152481\n",
      "Epoch 0, Loss(train/val) 4.74311/4.72658. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.72957/4.71404. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.71766/4.70765. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.71301/4.71005. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.71701/4.70912. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.71180/4.70483. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.70992/4.70549. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.70761/4.69660. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.71340/4.70101. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.71227/4.69933. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.71106/4.69938. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.70892/4.69731. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.70994/4.69865. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.70833/4.69738. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.70986/4.69653. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.71223/4.69746. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.70778/4.69035. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.70703/4.72269. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.70715/4.70645. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.70503/4.70309. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.70511/4.70091. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.70526/4.70605. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.70613/4.70144. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.70321/4.70794. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.70291/4.70202. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.70037/4.70520. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.70355/4.70215. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.70426/4.70725. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.70216/4.70135. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.70338/4.70731. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.70300/4.70198. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.70501/4.70536. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.70283/4.70333. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.70295/4.70359. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.70307/4.70129. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.69860/4.70595. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.70344/4.70320. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.70412/4.70722. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.70104/4.70670. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.70022/4.70751. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.70045/4.70578. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.70057/4.70472. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.69921/4.70609. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.70152/4.70328. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.70602/4.70320. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.70055/4.69908. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.70313/4.70316. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.70078/4.70416. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.70358/4.70706. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.70375/4.70251. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.70039/4.69995. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.70039/4.70173. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.69780/4.71693. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.70088/4.70514. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.70002/4.70624. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.69685/4.69825. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.69764/4.70844. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.69732/4.70543. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.69572/4.71191. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.69683/4.70193. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.69429/4.71058. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.69675/4.70470. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.68933/4.71249. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.69416/4.69944. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.69280/4.71930. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.69731/4.70323. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.69148/4.71228. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.69562/4.70662. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.69085/4.70088. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.68887/4.70344. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.69320/4.72265. Took 0.21 sec\n",
      "Epoch 71, Loss(train/val) 4.69468/4.70243. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.68959/4.70059. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.70038/4.71150. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.68971/4.70345. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.68927/4.70432. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.68878/4.70522. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.68941/4.70391. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.68951/4.71200. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.68793/4.70447. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.68530/4.71155. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.68895/4.70364. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.68503/4.71335. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.69604/4.70273. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.68276/4.70490. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.68931/4.70564. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.68265/4.70782. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.68647/4.70361. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.68517/4.70414. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.68483/4.69987. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.68616/4.70155. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.68429/4.70258. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.68534/4.70437. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.68174/4.69901. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.68289/4.70868. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.68494/4.70364. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.68419/4.71063. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.67605/4.69969. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.68363/4.70247. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.68830/4.70083. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.10421364855284106\n",
      "Epoch 0, Loss(train/val) 4.94389/4.91397. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.88707/4.87289. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.87917/4.86403. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.88003/4.86646. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.87954/4.86790. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.87738/4.87221. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.88015/4.86962. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.88019/4.87182. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.87589/4.87462. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87978/4.87766. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.87976/4.87903. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.87273/4.88190. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.87241/4.88777. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.87491/4.87902. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87349/4.88501. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.87693/4.87776. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.87032/4.87980. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.87093/4.87521. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 4.86895/4.88746. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.87299/4.87000. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.87120/4.87799. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.86405/4.87503. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.86209/4.89351. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.85904/4.88134. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.86500/4.86854. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.86183/4.87478. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.86024/4.86794. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.86120/4.86945. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.86247/4.86389. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.86939/4.85613. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.86124/4.86327. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86351/4.86601. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.86160/4.87141. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.85783/4.86217. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85995/4.86499. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.85811/4.86796. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.85564/4.87631. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.86054/4.86509. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.86319/4.86340. Took 0.22 sec\n",
      "Epoch 39, Loss(train/val) 4.87048/4.86883. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.85664/4.87699. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.85670/4.85526. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.85715/4.86556. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.85570/4.86624. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.85150/4.87204. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.85396/4.87275. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.85687/4.87407. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84815/4.86233. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.85154/4.86707. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.85647/4.86859. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.85676/4.87873. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.84780/4.87803. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.85546/4.87643. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.85173/4.87976. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.84567/4.88336. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 4.85172/4.87779. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84637/4.87008. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.85165/4.89529. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.85009/4.88010. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.84428/4.87430. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.84863/4.87658. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.84832/4.88480. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85082/4.87959. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.85350/4.88511. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.85947/4.86436. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.84916/4.87866. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.84425/4.87291. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.85622/4.87348. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.84896/4.88675. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84762/4.87725. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.85413/4.88316. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.85272/4.88146. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.87286/4.84955. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.86899/4.86459. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.86264/4.86301. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.85875/4.86371. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.84802/4.88890. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.85807/4.87452. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.85454/4.87909. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.84937/4.86291. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.85057/4.88046. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83952/4.89124. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.84961/4.87871. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84335/4.89953. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.84427/4.88788. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.84121/4.88540. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.84513/4.86991. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83928/4.87861. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.85122/4.86432. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.84978/4.87734. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.85136/4.87635. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85252/4.85569. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.84718/4.88080. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.84919/4.87726. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.84924/4.89734. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.84449/4.88241. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83776/4.89363. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83728/4.88585. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.84769/4.88963. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.83423/4.87938. Took 0.20 sec\n",
      "ACC: 0.484375, MCC: 0.02480694691784169\n",
      "Epoch 0, Loss(train/val) 5.13385/5.12386. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.05418/5.09331. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.04646/5.08814. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.04667/5.07967. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 5.04506/5.07790. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.04443/5.07539. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.04303/5.07224. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.04514/5.07363. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.03952/5.07071. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.04009/5.07036. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.03969/5.06382. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.03757/5.06058. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.03958/5.06929. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.03635/5.06802. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.03542/5.06263. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.03245/5.06917. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.03490/5.07583. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.03210/5.06078. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.03152/5.05236. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.02995/5.05616. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 5.02800/5.05666. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.03027/5.04925. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.02716/5.05323. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.02641/5.04710. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.02439/5.04911. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.02676/5.05647. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.02257/5.05185. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.02331/5.03773. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.02351/5.04165. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.01801/5.04824. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.02408/5.04795. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.01961/5.04921. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.01905/5.04405. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.02319/5.04443. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.02210/5.04157. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.01438/5.04903. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.02150/5.04447. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.02279/5.03890. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 5.02205/5.03752. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.02171/5.03418. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 5.01910/5.04079. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.01071/5.02910. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.01927/5.02835. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.01372/5.03038. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.01793/5.02959. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.01805/5.02547. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.01530/5.02357. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.01530/5.02899. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.01476/5.03049. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 5.01414/5.03813. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.01957/5.03146. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 5.00945/5.04156. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.00935/5.05414. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 5.01149/5.05694. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.01324/5.03744. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.01400/5.01983. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 5.01562/5.03706. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.01053/5.04423. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 5.00658/5.03785. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.00915/5.05743. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.01298/5.04739. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.01309/5.04817. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.01631/5.03570. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 5.00789/5.03447. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.01127/5.03991. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 5.02238/5.04106. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.02059/5.04092. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.01361/5.03222. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.01227/5.03597. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.01137/5.02243. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.00673/5.02391. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.01518/5.02534. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.01143/5.04132. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.01420/5.01743. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.00947/5.01688. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.00498/5.01576. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.00226/5.02844. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.01014/5.02819. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.00493/5.01984. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.00030/5.01190. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.00018/5.01760. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.00642/5.01920. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.00971/5.02018. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.00247/5.02367. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.00401/4.99939. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.00514/5.00528. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 5.00604/5.00345. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.00275/5.00571. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 5.00161/5.00531. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.00639/5.01822. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.00275/4.98527. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 5.00612/5.00832. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.00709/5.00215. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 5.00645/5.00213. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 5.00479/5.01698. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.00057/5.00077. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.99672/5.02406. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.00646/5.01466. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.00486/5.00526. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.00310/5.00046. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.08845235543978211\n",
      "Epoch 0, Loss(train/val) 4.87709/4.84378. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.80259/4.81983. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.80092/4.80693. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.80052/4.80433. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.79745/4.80724. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.79921/4.80785. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.79942/4.80743. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79737/4.81004. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79689/4.80850. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79500/4.81380. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79656/4.81055. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.79501/4.80895. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79701/4.80976. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79167/4.80995. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.79503/4.81144. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78796/4.81895. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.79462/4.81542. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.79255/4.81281. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.79000/4.81399. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78980/4.81921. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.79157/4.81480. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.79044/4.81490. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78816/4.81624. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78799/4.81915. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78821/4.81688. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78842/4.81760. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78776/4.81472. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78682/4.81748. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78912/4.80921. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78393/4.80890. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.78838/4.81543. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.78997/4.80967. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.78213/4.81551. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.78490/4.80936. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.78529/4.80997. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.78549/4.80997. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78070/4.81050. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.78643/4.81274. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 4.78076/4.81513. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77897/4.80773. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77974/4.81334. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.77985/4.81184. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.78092/4.80780. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.78029/4.80913. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.78087/4.81032. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77893/4.80727. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.77732/4.81529. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.78042/4.81235. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.77877/4.81212. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77948/4.81048. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.77712/4.81319. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.77920/4.81507. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.77536/4.81919. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.77361/4.83382. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.78817/4.81751. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.78212/4.82454. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.78276/4.81648. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.77958/4.82777. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.77618/4.81617. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.77742/4.81698. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.77994/4.81048. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.78222/4.81706. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.77571/4.82617. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.77752/4.82621. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.77451/4.81549. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.76615/4.82263. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.77439/4.82296. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.77011/4.81581. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77380/4.81008. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77123/4.81534. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.76803/4.83027. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.77070/4.81471. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.77051/4.81075. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.76973/4.83769. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77255/4.81574. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 4.76953/4.84021. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.76612/4.81915. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.76797/4.83263. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.77002/4.81103. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.76502/4.81397. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.77130/4.82582. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.76817/4.81547. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.78018/4.80160. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.76893/4.83118. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.76485/4.80308. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.76415/4.82248. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.76810/4.82930. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.76483/4.83404. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.76629/4.85578. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.77909/4.83152. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.77271/4.82279. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.77271/4.83559. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.77608/4.84043. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77740/4.84080. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77527/4.83828. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.77527/4.83368. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.77467/4.83655. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.76639/4.85965. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.76833/4.85020. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.77638/4.82200. Took 0.20 sec\n",
      "ACC: 0.453125, MCC: -0.08845235543978211\n",
      "Epoch 0, Loss(train/val) 5.12203/5.07527. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.05324/5.06233. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.05648/5.06177. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.05986/5.05960. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 5.06092/5.06023. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.05940/5.05958. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.05667/5.05805. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.05011/5.06362. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 5.04907/5.06674. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 5.04678/5.05984. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.04903/5.05995. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.04942/5.06383. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.04712/5.06700. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.04659/5.05781. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.04877/5.06651. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.04717/5.07116. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.04554/5.06868. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.04349/5.05881. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.05193/5.05206. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 5.05040/5.04945. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 5.04734/5.04699. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 5.04421/5.05002. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.04521/5.05383. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.04696/5.05457. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.04695/5.06480. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.04304/5.06968. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.04417/5.05310. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.04293/5.07298. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.04138/5.05657. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.04828/5.04937. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 5.04880/5.04852. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.04523/5.05202. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 5.04590/5.06254. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.04960/5.05569. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.04801/5.05892. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.04373/5.06379. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.04274/5.06413. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.04045/5.06756. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.04456/5.06082. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.04550/5.05742. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.04419/5.06210. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.03879/5.07186. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.03993/5.06446. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.04794/5.05266. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.05199/5.06388. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.04416/5.05552. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.04027/5.05856. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.04891/5.06375. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.04034/5.07712. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.03590/5.07631. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.03461/5.07822. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.03802/5.07916. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.03479/5.08361. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.03781/5.07955. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 5.03408/5.08268. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 5.03250/5.07228. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.03620/5.07567. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.03771/5.07357. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.03543/5.05459. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.03200/5.05308. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.04017/5.02391. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.04395/5.04815. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.03512/5.08891. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.03063/5.08463. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.03244/5.08425. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 5.02808/5.08425. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.02932/5.08377. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.02930/5.08094. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.02737/5.09267. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.02557/5.08602. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.02524/5.10593. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.03052/5.09920. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.02592/5.07817. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.03017/5.09655. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.02717/5.09750. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 5.02804/5.09884. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.02575/5.10321. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.02433/5.11216. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.02174/5.09105. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.02802/5.08641. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.02118/5.11691. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.02049/5.09638. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.02778/5.08033. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.02124/5.10417. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.03007/5.08992. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.02862/5.10055. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.01984/5.10938. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.02107/5.11545. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.02389/5.11606. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.02351/5.11425. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.02306/5.11083. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.02304/5.09924. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.02235/5.10813. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.03761/5.03115. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.05183/5.05260. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 5.04181/5.06797. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.03189/5.08407. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 5.04530/5.07210. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.04232/5.07055. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.04056/5.07667. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.07539645724831788\n",
      "Epoch 0, Loss(train/val) 4.70058/4.66313. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.67150/4.66389. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.66339/4.66415. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.66551/4.66603. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.66578/4.66740. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.66465/4.66573. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.66264/4.66623. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.66615/4.67150. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.66596/4.66871. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.66583/4.66538. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.66216/4.66742. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.66108/4.67077. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.66096/4.67102. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.65891/4.67162. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.65726/4.67703. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.65604/4.67396. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.65555/4.67082. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.65373/4.67209. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.65744/4.66848. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.66024/4.66609. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.65738/4.67173. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.65650/4.66997. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.66094/4.66895. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.65822/4.67814. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.65575/4.67224. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.65649/4.67364. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.65421/4.67394. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.65149/4.68456. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.65279/4.68312. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.65894/4.66530. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.65922/4.65907. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.65147/4.67202. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.65130/4.67499. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.65418/4.68043. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.64630/4.68498. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.65443/4.66652. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.64694/4.68795. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.65111/4.68303. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.64009/4.72313. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.64601/4.68880. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.64533/4.68938. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.64402/4.70118. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.64203/4.69692. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.64673/4.68675. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.64556/4.69018. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.64321/4.68119. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.64556/4.68682. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.64214/4.71507. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.64394/4.69055. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.63834/4.69568. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.63727/4.70977. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.64882/4.67802. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.63953/4.72603. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.64048/4.69356. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.64076/4.68529. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.63529/4.71551. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.63865/4.70171. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.64219/4.67304. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.64549/4.68203. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.64885/4.66103. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.66341/4.66803. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.66298/4.66580. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.65949/4.66585. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.66021/4.66684. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 4.65983/4.66930. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.66182/4.67220. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.65825/4.67370. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.65701/4.67512. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.65591/4.67678. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.66035/4.68012. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.65770/4.68157. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.65532/4.67900. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.65912/4.67485. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.65991/4.67310. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.65602/4.68103. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.65322/4.68571. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.65013/4.69313. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.64850/4.69904. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.65035/4.70431. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.64924/4.70398. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.64621/4.71618. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.65187/4.69916. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.64552/4.73020. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.64515/4.73778. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.64682/4.72642. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.64395/4.72089. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.64181/4.71933. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.64629/4.73667. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.64340/4.71412. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.64304/4.70740. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.64387/4.75130. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.64313/4.71702. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.64602/4.72380. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.64520/4.70548. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.63845/4.72510. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.64243/4.72819. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.63616/4.73006. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.63809/4.75313. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.64010/4.73398. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.63573/4.74826. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.08818077954471167\n",
      "Epoch 0, Loss(train/val) 4.96224/4.85889. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.87531/4.86954. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.87314/4.87694. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87394/4.87770. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.87188/4.87528. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.86939/4.87489. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.87540/4.87077. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.87628/4.86697. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.87311/4.86793. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87167/4.86616. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.86960/4.86592. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.86828/4.87193. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.86811/4.87055. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.86260/4.87293. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.86545/4.87441. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.86487/4.87814. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.86253/4.87461. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85970/4.87525. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.86037/4.86979. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.85988/4.87714. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.85826/4.87402. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85756/4.87363. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85829/4.87890. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85816/4.87670. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.85810/4.88261. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.85442/4.87952. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.85219/4.88860. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.85370/4.87366. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.85644/4.87888. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.85279/4.87709. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.85137/4.89716. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.85196/4.88660. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.84696/4.90436. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.85019/4.89871. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84595/4.92103. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84943/4.88999. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.84746/4.91084. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.85376/4.89054. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84595/4.92752. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84393/4.91382. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.85339/4.89172. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84187/4.92987. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.84311/4.91057. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.84397/4.90481. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84069/4.92266. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84269/4.89133. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.83794/4.94674. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84144/4.92786. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.84164/4.91691. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.83521/4.92732. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.85407/4.89770. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.85226/4.88544. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.85150/4.89784. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.85252/4.89845. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.84958/4.91324. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.85207/4.90577. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84134/4.92979. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.85509/4.87783. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.84838/4.92948. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.83753/4.95519. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85092/4.88370. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84646/4.91429. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85018/4.90469. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.84347/4.91957. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.84045/4.93002. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.84253/4.91307. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.84919/4.90635. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.84056/4.93958. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.84959/4.91412. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.84564/4.94193. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.84427/4.93829. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.84253/4.94598. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.84447/4.94717. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83819/4.93071. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.84220/4.94205. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.84182/4.92197. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.84133/4.93879. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.84133/4.95567. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.83340/4.96158. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.83935/4.96894. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.84026/4.94377. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.83102/4.96463. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.83841/4.93154. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.83245/4.95274. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.83659/4.95376. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83574/4.92016. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.83297/4.95556. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83581/4.93001. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83530/4.96240. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83585/4.92589. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.83050/4.96026. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.83275/4.91859. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.83913/4.93180. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.83656/4.93025. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83540/4.93176. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.83343/4.92584. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83188/4.94316. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.84224/4.91880. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.83035/4.93118. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.82532/4.95619. Took 0.20 sec\n",
      "ACC: 0.421875, MCC: -0.15467466504810498\n",
      "Epoch 0, Loss(train/val) 4.81802/4.73373. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.72254/4.72916. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.72268/4.72734. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.72020/4.72401. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.71942/4.73075. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.72183/4.72568. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.72067/4.72597. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.72011/4.72469. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.71805/4.72409. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.71996/4.72416. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.71747/4.72335. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.71591/4.72417. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.71872/4.72652. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.71832/4.72902. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.71374/4.72998. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.71257/4.73116. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.71526/4.72921. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.71461/4.72787. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.71328/4.73061. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.70709/4.72844. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.70771/4.73614. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.70677/4.73787. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.70649/4.74910. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.69985/4.75260. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.70229/4.71908. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.71120/4.72902. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.70925/4.73145. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.70330/4.72821. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.69921/4.74979. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.69908/4.73863. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.70409/4.74113. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.70122/4.75588. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.70153/4.75520. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.69750/4.75845. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.70153/4.75635. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.69852/4.77115. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.69808/4.76646. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.69836/4.76979. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.69911/4.76194. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.70028/4.76047. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.69418/4.76643. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.69971/4.76284. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.69543/4.76039. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.69753/4.73658. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.70288/4.74084. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.69863/4.75915. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.69401/4.77303. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.69868/4.76680. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.69370/4.77382. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.69741/4.77321. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.69546/4.77519. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.69454/4.77017. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.69832/4.76104. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.69688/4.77983. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.68714/4.78258. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.69468/4.80056. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.69583/4.80491. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.69166/4.79701. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.69654/4.77838. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.69046/4.79142. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.69016/4.80834. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.69735/4.78919. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.69131/4.79141. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.68931/4.80451. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.69211/4.79771. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.69191/4.79879. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.68665/4.79168. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.68924/4.80708. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.68829/4.81094. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.69565/4.80177. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.68749/4.81491. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.68376/4.81462. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.69208/4.78322. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.69065/4.80704. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.68625/4.80971. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.68202/4.79361. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.68348/4.79456. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.69307/4.76397. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.69971/4.77939. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.69490/4.78383. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.69051/4.78623. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.68524/4.82111. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.69148/4.81484. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.68814/4.80398. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.68766/4.81974. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.68271/4.82679. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.69289/4.78847. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.68683/4.81861. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.69283/4.80473. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.68377/4.81151. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.68000/4.81712. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.68400/4.80980. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.68638/4.81442. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.68664/4.80996. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.68397/4.81049. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.68198/4.81211. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.68374/4.84351. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.69323/4.79191. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.68776/4.82192. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.68156/4.83547. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.1563263498701806\n",
      "Epoch 0, Loss(train/val) 5.03644/5.06074. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.02987/5.02523. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.01160/5.02387. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.01402/5.02992. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.01393/5.03242. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.01405/5.02820. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.01259/5.02738. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.01224/5.02700. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.01769/5.02296. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.01584/5.01699. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 5.01189/5.01779. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 5.01192/5.01855. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 5.01428/5.01807. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.01005/5.01950. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.01157/5.02028. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.01365/5.02033. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.01109/5.02029. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 5.00800/5.02295. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.00922/5.02702. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.00922/5.02805. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.01058/5.02539. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.00617/5.02771. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 5.01135/5.02643. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.00799/5.02772. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.00688/5.03161. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.00608/5.03815. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.00604/5.03582. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.01094/5.03031. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.00863/5.03161. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.00553/5.03522. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.00599/5.03683. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 5.00666/5.03556. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.00383/5.03772. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.00557/5.04149. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 5.00259/5.03956. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.00274/5.04311. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.00410/5.04194. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 5.00164/5.04872. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.00190/5.05411. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.99954/5.04950. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 5.00419/5.04305. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.99956/5.04843. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.00324/5.04610. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.00184/5.05196. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.00249/5.04784. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.00137/5.05221. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.99937/5.05089. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 5.00080/5.04974. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.00630/5.03910. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.99760/5.05683. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 5.00213/5.04555. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.99910/5.06504. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.99676/5.06986. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.00243/5.04545. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.99869/5.06002. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.00215/5.04460. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.99955/5.05402. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.00342/5.06137. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 4.99832/5.06367. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.99862/5.07327. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.99630/5.07423. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.99711/5.05692. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.00014/5.05372. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.99985/5.06276. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.99752/5.08555. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.99666/5.08661. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.99287/5.08259. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.99552/5.06467. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.99668/5.08784. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.99491/5.06324. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.99297/5.07194. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.99884/5.05667. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.99585/5.05268. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.99619/5.06039. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 5.00122/5.06142. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.99671/5.07416. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.99622/5.07659. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.99387/5.07291. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.99409/5.07401. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.99445/5.08260. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.99707/5.07295. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.99445/5.08774. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.99229/5.11472. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.99739/5.07953. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.99185/5.08944. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.98395/5.08275. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.99517/5.08129. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.99589/5.06535. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.98970/5.08017. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.99024/5.08008. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.99379/5.07838. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.99574/5.07619. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.98907/5.09779. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.98841/5.08869. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.99540/5.06517. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.99217/5.05818. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.99057/5.06959. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.99012/5.08297. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.99487/5.06837. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.98373/5.11332. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.07539645724831788\n",
      "Epoch 0, Loss(train/val) 4.84780/4.77003. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.78492/4.77336. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.78174/4.77218. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.78016/4.77263. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 4.77994/4.77101. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.77871/4.77087. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.78082/4.77090. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.78048/4.77297. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.77961/4.77193. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.77930/4.77409. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.77683/4.77469. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.77816/4.77654. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.77965/4.77449. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.78072/4.77273. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.77691/4.76941. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.77592/4.76970. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.77480/4.77272. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.77466/4.77460. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.77594/4.77524. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.77430/4.77446. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.77172/4.77093. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.77711/4.76928. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.77258/4.77465. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.77407/4.77318. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.77418/4.77231. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.77160/4.77203. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.77439/4.76780. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.76937/4.77269. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.77014/4.77324. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.76750/4.77229. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.76703/4.76858. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77498/4.76235. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.76963/4.76887. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.76877/4.77264. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.76723/4.77006. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.76844/4.77227. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.76775/4.77311. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.76724/4.76650. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.76782/4.76962. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.76739/4.77101. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.76201/4.76689. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.76482/4.77712. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76824/4.78544. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 4.77249/4.77072. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.76589/4.77438. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.76112/4.76844. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.76589/4.76284. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.75973/4.77504. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.76531/4.76553. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.76762/4.76833. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76068/4.77021. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.76021/4.77086. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.76103/4.77959. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76333/4.78735. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.76293/4.77945. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.75910/4.78046. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.76440/4.78314. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.75305/4.78106. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.75795/4.78363. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 4.75229/4.77538. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.75678/4.79181. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.75718/4.77977. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.75458/4.79306. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.75632/4.78274. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76039/4.78760. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.75828/4.78323. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75754/4.79547. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.75299/4.77583. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.75519/4.78820. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.75594/4.78066. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.75812/4.77847. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.75455/4.79040. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.75503/4.79608. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.75580/4.77469. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.75349/4.80442. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.75277/4.78524. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.75440/4.78966. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.75310/4.77864. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.75216/4.78488. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.75074/4.78060. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.75439/4.78308. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.75151/4.78416. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.75472/4.79636. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.76389/4.79087. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.76313/4.77751. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.76239/4.75956. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.75980/4.78448. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.75461/4.77800. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.75524/4.77697. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.75724/4.75932. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.76251/4.78907. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.75823/4.76600. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.75243/4.78070. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.74383/4.77587. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.75270/4.79651. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75052/4.77357. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.74988/4.78679. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.74915/4.77750. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.75585/4.78457. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75568/4.80437. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.16265001215808886\n",
      "Epoch 0, Loss(train/val) 4.85628/4.82612. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.82976/4.82305. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.82816/4.82091. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.82708/4.81722. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.82750/4.81640. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.82773/4.81632. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.82809/4.81488. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.82534/4.81736. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.82084/4.81595. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.81915/4.81321. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.81983/4.80878. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.82012/4.80720. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.81481/4.80620. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.81855/4.81163. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.81930/4.81546. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.81496/4.81042. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.81751/4.80898. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.81330/4.80804. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81740/4.81078. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.81521/4.81107. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.81479/4.81184. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.81211/4.81563. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.80986/4.81973. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.80965/4.81955. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.80703/4.81917. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.80512/4.82402. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.80149/4.82902. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.80428/4.83343. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.80310/4.82420. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.80032/4.81609. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.79968/4.83262. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.80309/4.82452. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.80031/4.82664. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.79892/4.82416. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.79750/4.80787. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 4.79924/4.83172. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.79145/4.83485. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.80070/4.82369. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.79517/4.80996. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.79927/4.83214. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.79700/4.82976. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.79595/4.81458. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.79889/4.83010. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.79562/4.82965. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.79102/4.82706. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.79585/4.82708. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.79058/4.81360. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.79424/4.82254. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.79706/4.82948. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.79251/4.82208. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.79481/4.81841. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.79744/4.82020. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.79182/4.83687. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.79430/4.82823. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.79301/4.82642. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.79815/4.83901. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.80192/4.83770. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.79777/4.84178. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.79934/4.82818. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.79121/4.83596. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.79065/4.82232. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.79665/4.83590. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.79456/4.83605. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.79711/4.82539. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.79412/4.82079. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.79150/4.82542. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.78967/4.81815. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.79525/4.82118. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.78762/4.82850. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.78879/4.82485. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.79049/4.82104. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.78924/4.81823. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.79111/4.82912. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.78699/4.82927. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.79095/4.82538. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.79011/4.82316. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.78902/4.82243. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.79287/4.81799. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.78841/4.82400. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.78718/4.81252. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.78880/4.83296. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.78844/4.82237. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.78867/4.81681. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.78852/4.82697. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.78596/4.81495. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.78174/4.82802. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.78555/4.81980. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.78629/4.82186. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.78801/4.82616. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.78663/4.82699. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.78820/4.82212. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.79019/4.82643. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.78690/4.81956. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.79318/4.81620. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.79205/4.82920. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.78947/4.82126. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.78824/4.82243. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.78859/4.82188. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.78931/4.82118. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.78882/4.83049. Took 0.19 sec\n",
      "ACC: 0.4375, MCC: -0.0931174089068826\n",
      "Epoch 0, Loss(train/val) 4.89062/4.84227. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.82842/4.82442. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.82681/4.82658. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.82790/4.83300. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.82826/4.81869. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.82229/4.82677. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.82242/4.82834. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.82271/4.81564. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.82399/4.81812. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.82440/4.81995. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.82339/4.81607. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.82137/4.81914. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.81981/4.82196. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.81773/4.81770. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.82140/4.81517. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.81463/4.81989. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.81597/4.81266. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.81668/4.80173. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81746/4.81717. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.81287/4.80259. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.81896/4.80483. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.80832/4.80124. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.81339/4.81946. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.81542/4.79998. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.81096/4.81288. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.80824/4.81016. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.80519/4.80528. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.80846/4.80536. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.80315/4.79063. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.81070/4.80782. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.80456/4.80173. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.80670/4.81395. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.80747/4.80766. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.81063/4.81561. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.80948/4.80294. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.80769/4.80722. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 4.80844/4.81357. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.79633/4.80447. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.79804/4.81009. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.80093/4.81587. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.80149/4.82676. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.80363/4.82190. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.80369/4.82867. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.80257/4.81927. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.81649/4.78854. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.81533/4.80145. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.80676/4.80064. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.80428/4.80299. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.80397/4.81580. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.80079/4.80938. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.80368/4.81722. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.80397/4.79829. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.80190/4.80548. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.80324/4.80144. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.80272/4.81621. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.80184/4.81034. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.79699/4.81277. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.79613/4.83423. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.80058/4.80411. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.80284/4.81236. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.80153/4.81600. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.79817/4.80956. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.79294/4.81894. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.79970/4.80773. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.79569/4.83177. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 4.80263/4.80653. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.80038/4.80165. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.79693/4.80717. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.79980/4.81693. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.79617/4.81488. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.79490/4.82299. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.79605/4.81895. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.79444/4.82519. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.79391/4.81250. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.80414/4.82833. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.79651/4.82500. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.79434/4.82603. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.79071/4.82893. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.79307/4.82823. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.79421/4.81690. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.80497/4.79448. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.78816/4.81685. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.78129/4.82634. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.79489/4.80276. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.80364/4.81812. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.81327/4.81191. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.80431/4.79747. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.79977/4.81047. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.79801/4.81974. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.79615/4.81734. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.79945/4.82154. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.80353/4.82076. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.79499/4.81817. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.80235/4.81915. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.79367/4.82492. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79839/4.81155. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.79312/4.81728. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.79267/4.81222. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.79390/4.82096. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.79391/4.82198. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.004034850217792016\n",
      "Epoch 0, Loss(train/val) 4.71504/4.79293. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.71055/4.75516. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.69732/4.75216. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.69565/4.72551. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.68939/4.72012. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.68660/4.72953. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.68787/4.73497. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.68732/4.73186. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.68593/4.73385. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.68616/4.73235. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.68293/4.74267. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.68264/4.73182. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.68297/4.72972. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.68056/4.74175. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.68185/4.74450. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.67832/4.74998. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.68329/4.74257. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.67888/4.73079. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.68177/4.71948. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.68375/4.71500. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.68275/4.72521. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.68152/4.73392. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.68027/4.73571. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.67788/4.73906. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.67949/4.73725. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.67632/4.74194. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.67502/4.73884. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.67579/4.74497. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.67809/4.74261. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.67489/4.72982. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.67079/4.73880. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.67442/4.74193. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.67027/4.74900. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.67612/4.73273. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.67060/4.74532. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.67166/4.72688. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.67071/4.74074. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.66634/4.74408. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.66811/4.75254. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.67148/4.72971. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.66706/4.73909. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.66919/4.73532. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.66820/4.74418. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.66444/4.72831. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.66849/4.73761. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.66620/4.73213. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.66755/4.72838. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.66537/4.73473. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.66701/4.72998. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.66240/4.74178. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.66782/4.72940. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.66289/4.73473. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.66513/4.72413. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.65961/4.74350. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.66708/4.73401. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.66568/4.73392. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.66655/4.71926. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.66265/4.72213. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.66234/4.73367. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.66619/4.73530. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.66262/4.73135. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.65751/4.72965. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.65696/4.74975. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.65889/4.72951. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.65616/4.72629. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.66298/4.72153. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.66482/4.71588. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.66324/4.71883. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.65998/4.73333. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.66217/4.71932. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.65873/4.72967. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.65823/4.72556. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.66118/4.72424. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.65885/4.72391. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.65558/4.74239. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.65752/4.72172. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.65631/4.72416. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.65584/4.73231. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.65639/4.73525. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.65643/4.71841. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.65124/4.74681. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.66677/4.71465. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.65826/4.72959. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.66113/4.71722. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.65745/4.71900. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.65738/4.73532. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.65747/4.73346. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.65337/4.73107. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.65463/4.73143. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.65796/4.72440. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.65025/4.72147. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.65168/4.73076. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.65631/4.74495. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.65378/4.70270. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.65090/4.72837. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.64999/4.72938. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.65222/4.74474. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.65396/4.72458. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.64698/4.74207. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.64721/4.74498. Took 0.19 sec\n",
      "ACC: 0.4375, MCC: -0.10323631789938985\n",
      "Epoch 0, Loss(train/val) 4.97657/4.89609. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.85431/4.91578. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.86202/4.87434. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.84960/4.85744. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.84326/4.86261. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.84387/4.87250. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.84626/4.87618. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.84717/4.87504. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.84713/4.87315. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.84366/4.87517. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84208/4.87866. Took 0.23 sec\n",
      "Epoch 11, Loss(train/val) 4.84461/4.86786. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 4.84340/4.86192. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.83827/4.86878. Took 0.22 sec\n",
      "Epoch 14, Loss(train/val) 4.84017/4.86841. Took 0.24 sec\n",
      "Epoch 15, Loss(train/val) 4.84002/4.87459. Took 0.23 sec\n",
      "Epoch 16, Loss(train/val) 4.84157/4.86954. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 4.83941/4.86982. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 4.83618/4.87327. Took 0.25 sec\n",
      "Epoch 19, Loss(train/val) 4.83759/4.86882. Took 0.22 sec\n",
      "Epoch 20, Loss(train/val) 4.83990/4.86838. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.83754/4.87233. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 4.83866/4.86411. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 4.83730/4.87733. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 4.83938/4.86473. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 4.83538/4.87043. Took 0.21 sec\n",
      "Epoch 26, Loss(train/val) 4.83478/4.87235. Took 0.22 sec\n",
      "Epoch 27, Loss(train/val) 4.83708/4.86626. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 4.83411/4.87146. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 4.83779/4.86987. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 4.83582/4.88428. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.83601/4.86990. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.83518/4.88102. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 4.83445/4.86895. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.83370/4.86333. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 4.83360/4.87311. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.83227/4.87452. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 4.83203/4.86980. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 4.83408/4.87788. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.83644/4.86071. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 4.83387/4.85407. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.83068/4.85594. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.83170/4.86302. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.83285/4.85891. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 4.83078/4.88141. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.83128/4.86459. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.83167/4.86944. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.82947/4.87365. Took 0.21 sec\n",
      "Epoch 48, Loss(train/val) 4.82514/4.87230. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 4.83223/4.86458. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.83036/4.86368. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.83192/4.86644. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.82885/4.85250. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.82938/4.86999. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.82998/4.85570. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.82375/4.87100. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.82970/4.87066. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.82980/4.86680. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.82773/4.87263. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.82287/4.86075. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.82498/4.87755. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.81980/4.88390. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.82583/4.86800. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.82605/4.87588. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.82520/4.87788. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.82492/4.85758. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.82193/4.88969. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.82001/4.86905. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.82022/4.87116. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.82062/4.87980. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.81239/4.88962. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.82603/4.89228. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.83056/4.84871. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83105/4.87907. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.81805/4.87018. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.81916/4.88137. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.82379/4.86353. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.82484/4.90464. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.82963/4.87567. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.82357/4.88260. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.82233/4.86968. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.82092/4.88248. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.81976/4.88511. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.82156/4.88711. Took 0.23 sec\n",
      "Epoch 84, Loss(train/val) 4.82174/4.86358. Took 0.22 sec\n",
      "Epoch 85, Loss(train/val) 4.81460/4.89894. Took 0.22 sec\n",
      "Epoch 86, Loss(train/val) 4.81383/4.88998. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.82207/4.87770. Took 0.22 sec\n",
      "Epoch 88, Loss(train/val) 4.81898/4.87045. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 4.81869/4.87943. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.81636/4.87813. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.81721/4.87475. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.81806/4.85877. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.81168/4.88447. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.82267/4.88455. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.81941/4.86421. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.81090/4.88913. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.81755/4.87839. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.81600/4.87720. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.81486/4.88243. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.03734655007457467\n",
      "Epoch 0, Loss(train/val) 4.94022/4.92160. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.93865/4.92316. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.91606/4.91298. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.92089/4.91465. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.91907/4.91290. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.91730/4.91334. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.91446/4.91427. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.91255/4.91604. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.91027/4.91593. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.91108/4.91676. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.91676/4.91810. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.91722/4.93128. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.91597/4.93320. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.91448/4.92060. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.91146/4.91850. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.90764/4.92158. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90871/4.92463. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.90725/4.92728. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.90489/4.93039. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.90933/4.92071. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.91020/4.92591. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.90988/4.92605. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.90628/4.93075. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.90590/4.93446. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.90787/4.93345. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.90785/4.93118. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.90442/4.93955. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.90196/4.94571. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90660/4.94046. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.90304/4.93832. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90776/4.93717. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.90297/4.94366. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.90112/4.94316. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.90645/4.94606. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.90485/4.94448. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90284/4.94292. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.89879/4.94722. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.90123/4.94641. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.90416/4.95015. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90692/4.94076. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90383/4.94368. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.90068/4.96207. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89901/4.96088. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.90065/4.96221. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.90280/4.93790. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.90255/4.94843. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.90043/4.95264. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89463/4.95857. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89803/4.95689. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.90160/4.94444. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89724/4.96929. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.89558/4.98013. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89241/4.97026. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.89670/4.97810. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.89846/4.96782. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.89895/4.95635. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.89317/4.96856. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.89677/4.97377. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.89729/4.97253. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89098/4.98488. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.89595/4.96586. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 4.89727/4.97511. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.89878/4.96247. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.88811/4.96849. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.89369/4.97900. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.88951/4.98331. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.89323/4.98320. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.89408/4.97863. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.89119/4.99624. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.89167/4.97702. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.89049/4.98302. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.89264/4.98501. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.88693/5.00677. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.89086/4.99167. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.89246/4.99651. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.88861/4.99991. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.88953/4.97945. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.88997/5.01900. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.88738/4.99849. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.88462/5.01440. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88707/5.00561. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.88454/5.01185. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.89070/5.00967. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.89230/4.99633. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.88800/5.00657. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.88374/5.03152. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.88985/5.02782. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.88330/5.01283. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.88194/5.04573. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.88210/5.02701. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.88412/4.98873. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.88649/5.00011. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.88021/5.04562. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.88310/5.02262. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.87873/5.03631. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.88430/5.03078. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.87823/5.03346. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.88557/5.01963. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.88276/5.01486. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88537/5.01943. Took 0.19 sec\n",
      "ACC: 0.390625, MCC: -0.2057101528827944\n",
      "Epoch 0, Loss(train/val) 5.03088/5.01090. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.94779/4.94799. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.94179/4.92474. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.93642/4.92039. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.93400/4.92171. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.93048/4.92763. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.92647/4.93173. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.92846/4.93408. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.92812/4.93366. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.92369/4.94270. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.92110/4.94114. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.91973/4.94198. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.92042/4.94638. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.91577/4.95957. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.91668/4.94123. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.91540/4.96861. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.91198/4.94555. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.91184/4.96092. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.91269/4.95085. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.90979/4.95457. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.91434/4.96303. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.90935/4.95056. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.90996/4.95955. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.90841/4.95260. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.90414/4.97246. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.91287/4.94356. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.90619/4.96599. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.90366/4.96227. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90401/4.96647. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.90653/4.95802. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90569/4.95744. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.91310/4.94907. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.90880/4.96389. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.90724/4.97618. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.90278/4.97197. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90528/4.97548. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.90187/4.96883. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.90268/4.98539. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.90079/4.99026. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90242/4.99279. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90115/4.99656. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.89801/4.99993. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89874/4.98627. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.90186/4.98118. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.90551/4.97150. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.89619/5.00276. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.90432/4.98492. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89801/4.97916. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.90193/4.97876. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.89897/4.98601. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89711/4.99011. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.89885/4.99537. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89283/4.99968. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.89703/4.99774. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.89824/4.98926. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.89738/4.99471. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.89580/4.96209. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.91130/4.96936. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.90046/4.98433. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89212/4.99731. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.89771/4.98492. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.89552/4.98471. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.89470/4.99153. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.89569/4.99853. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.89480/5.01314. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.89898/4.99152. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.89702/4.99770. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.89301/5.02206. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.89450/4.99178. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.89162/4.98540. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.89467/4.99809. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.89328/4.99513. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.88678/5.01257. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.89177/5.01292. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.89055/4.99472. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.89474/4.99601. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.89194/5.02008. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.89206/4.99572. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.89609/4.99366. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.88785/5.00551. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.89344/5.00979. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.89141/5.00275. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.89706/5.00395. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.89561/4.99461. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88858/5.00261. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.88552/5.01190. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.89199/4.99975. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.88875/5.00111. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.89004/5.00626. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.89288/5.02405. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.88373/5.01248. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.88704/5.00900. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.88830/5.01322. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.88597/5.02272. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88484/5.03342. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.88903/5.01079. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.90464/4.99010. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.90788/4.95509. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.90033/4.97682. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.90199/4.97548. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.10677275981195139\n",
      "Epoch 0, Loss(train/val) 4.80829/4.71124. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.74754/4.75212. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.73902/4.77655. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.73490/4.76144. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.72993/4.75631. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.73161/4.75931. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.73387/4.76065. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.73444/4.76601. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.73329/4.76438. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.73045/4.75997. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.73128/4.75990. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.72875/4.76083. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.73129/4.75958. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.72691/4.75983. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.72651/4.76357. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.72832/4.76747. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.72661/4.77290. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.72635/4.76915. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.72414/4.76900. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.72636/4.76790. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.72541/4.77175. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.72573/4.77022. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 4.72538/4.76975. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.72427/4.77083. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.72397/4.77072. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.72276/4.76483. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.72318/4.76649. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.72337/4.77184. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.72480/4.77151. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.72388/4.76979. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.72339/4.77423. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.72316/4.77366. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 4.72172/4.77185. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.72112/4.77483. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.71977/4.77454. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.71998/4.77508. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.71945/4.76794. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.72254/4.76080. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.72504/4.75758. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.72411/4.75708. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.72450/4.76775. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.72185/4.77140. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.72000/4.78190. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.72264/4.77077. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.72087/4.77279. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.72512/4.75212. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.72104/4.76399. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.72012/4.77074. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.72227/4.76179. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.72120/4.76624. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.71964/4.76894. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.72204/4.76675. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.72021/4.77061. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.72145/4.78122. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.72136/4.77623. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.72271/4.76017. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.72979/4.74784. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.71924/4.75815. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.71284/4.76273. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.72067/4.76593. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.72248/4.75064. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.72455/4.75734. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.72441/4.75324. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.71892/4.75450. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.72359/4.74792. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.72427/4.74899. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.72425/4.74729. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.72214/4.75946. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.71992/4.76040. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.72062/4.75206. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.71861/4.75315. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.72279/4.74838. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.71825/4.75333. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.72345/4.75103. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.72177/4.75373. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.72036/4.75631. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.71942/4.76670. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.71928/4.76917. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.71644/4.77302. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.71998/4.77186. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.71780/4.77900. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.71582/4.76804. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.71676/4.77701. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.71631/4.77348. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.71418/4.77912. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.71644/4.77984. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.71453/4.78970. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.71159/4.77836. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.71795/4.78920. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.71402/4.78015. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.71284/4.81076. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.71530/4.76999. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.71285/4.79460. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.71163/4.76341. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.71148/4.78686. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.70991/4.78490. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.71096/4.79835. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.71010/4.81780. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.70752/4.78640. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.71778/4.76485. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.035451483867683334\n",
      "Epoch 0, Loss(train/val) 5.05567/5.03780. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.95358/5.00295. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.94614/4.99933. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.94733/4.99597. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.94731/5.00319. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.94362/5.00327. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.94797/5.00201. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.94372/5.00062. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.94527/5.00904. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.94452/5.00668. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.94319/5.00660. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.94848/5.00852. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.94314/5.00920. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.94466/5.01017. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.94372/5.01269. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.94205/5.01316. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.94162/5.01898. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.93947/5.01333. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.93919/5.01751. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.93906/5.00581. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.93650/5.01228. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.93609/5.00956. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.93933/5.04289. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.94112/5.04123. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.93518/5.04769. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.93630/5.03786. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.93377/5.04985. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.93534/5.03733. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.95241/4.99368. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.94248/5.00594. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.93945/5.02010. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.94044/5.00988. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.93963/5.00971. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.93724/5.03106. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.93352/5.04052. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.93777/5.01788. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.93433/5.03325. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.93520/5.02733. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.93304/5.04115. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.93562/5.03847. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.93062/5.04468. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.92832/5.05403. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.93123/5.05903. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.92352/5.06995. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.92830/5.05653. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.93040/5.07443. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.92566/5.04564. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.92145/5.09431. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.92377/5.07048. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.93028/5.05597. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.92483/5.06549. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.92696/5.05145. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.92814/5.09004. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.91878/5.08093. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.92685/5.06965. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.92837/5.06653. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.92300/5.07311. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.92013/5.08944. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.92309/5.09083. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.92796/5.06633. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.91712/5.12406. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.91640/5.08299. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.91581/5.11957. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.93049/5.07422. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.91632/5.13227. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.91754/5.06432. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.91725/5.10869. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.91754/5.13812. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.91842/5.11545. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.91677/5.11327. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.91325/5.06271. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.91310/5.11466. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.91141/5.14329. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.91551/5.09174. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.91996/5.08456. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.91636/5.10862. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.90916/5.12724. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.91222/5.08028. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.91630/5.11651. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.91407/5.13493. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.91241/5.13872. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.90650/5.16930. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.91607/5.13374. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.91362/5.16589. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.92329/5.13220. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.91989/5.13806. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.93188/5.06623. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.92559/5.08529. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.92351/5.10150. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.91616/5.13131. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.92011/5.09659. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.91596/5.13004. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.91503/5.10021. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.91879/5.10124. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.91801/5.07396. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.91803/5.12552. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.91660/5.10071. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.91475/5.12066. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.92262/5.08726. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.92212/5.07232. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.1563263498701806\n",
      "Epoch 0, Loss(train/val) 4.74909/4.73600. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.71450/4.71880. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.70903/4.72493. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.70345/4.72481. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.70428/4.72903. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.70430/4.73000. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.70473/4.72399. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.69781/4.72429. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.69906/4.72349. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.69372/4.72124. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.69853/4.71580. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.69407/4.71440. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.69439/4.71732. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.69630/4.72235. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.69409/4.71908. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.69150/4.72688. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.69334/4.72410. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.69298/4.72421. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.69403/4.71991. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.69233/4.72003. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.69165/4.71798. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.69083/4.72694. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.69224/4.71873. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.68923/4.71971. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.69070/4.71926. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.68947/4.73189. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.68873/4.73581. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.68767/4.74109. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.69424/4.72129. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.69243/4.72679. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.68978/4.72772. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.68859/4.72860. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.68920/4.73601. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.68953/4.73258. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.68633/4.73682. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.68523/4.73990. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.68747/4.72970. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.68578/4.75549. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.68964/4.73097. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.69034/4.73552. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.68517/4.74275. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.68779/4.72824. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.68258/4.74436. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.68493/4.74489. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.68480/4.75704. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.68661/4.73960. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.68288/4.76716. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.68517/4.73534. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.68621/4.73954. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.68438/4.73296. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.68693/4.75476. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.68400/4.73563. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.68697/4.74441. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.68401/4.74512. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.68118/4.75136. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.68332/4.74796. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.68345/4.74985. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.68860/4.73402. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.68077/4.74993. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.68086/4.74684. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.67794/4.78211. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.68397/4.73608. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.68095/4.76149. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.68469/4.74855. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.67728/4.75782. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.67798/4.78392. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.68720/4.74209. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.69317/4.72478. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.68512/4.74967. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.68782/4.73595. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.68192/4.74687. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.68293/4.76483. Took 0.21 sec\n",
      "Epoch 72, Loss(train/val) 4.68467/4.75268. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.68144/4.75114. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.68431/4.73309. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.67935/4.75013. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.67901/4.76122. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.68264/4.74832. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.67928/4.76224. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.67612/4.75701. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.67629/4.77941. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.67781/4.75556. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.67883/4.77226. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.67580/4.75403. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.68513/4.74071. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.67664/4.76074. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.67488/4.74086. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.68066/4.74838. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.68016/4.76745. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.67842/4.74202. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.67605/4.77787. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.67568/4.75790. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.67961/4.77107. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.68202/4.75233. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.67172/4.78284. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.67807/4.76830. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.67317/4.75437. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.67246/4.76497. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.67585/4.75659. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.67768/4.74406. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.1014574359634967\n",
      "Epoch 0, Loss(train/val) 4.88692/4.85334. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.86844/4.84848. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.85874/4.86409. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.85665/4.86494. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.85809/4.86318. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.85524/4.86975. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.85534/4.86689. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.85825/4.86822. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85802/4.86701. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.85261/4.85817. Took 0.21 sec\n",
      "Epoch 10, Loss(train/val) 4.85531/4.87063. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.85841/4.87058. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85369/4.86632. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85404/4.86725. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85733/4.86474. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.85299/4.86152. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85204/4.85919. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85129/4.86228. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85404/4.86076. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.85033/4.85432. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.85049/4.85672. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85266/4.84674. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85069/4.84716. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84725/4.84454. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.84854/4.84035. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.84905/4.84558. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.84472/4.83958. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84454/4.83831. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.84576/4.83565. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84997/4.83301. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.85036/4.83552. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84665/4.84595. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.85056/4.85070. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84833/4.85190. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84567/4.85123. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84186/4.85225. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.84504/4.84458. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84507/4.84239. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84325/4.84220. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84249/4.83448. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84432/4.84527. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84352/4.84459. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.84454/4.85051. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.83948/4.85004. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.84264/4.84602. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.84105/4.84998. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84489/4.84046. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.83898/4.84233. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.84515/4.86473. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.83970/4.83683. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.83677/4.84357. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.84047/4.84391. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.83696/4.83731. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.83711/4.84751. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.83299/4.85468. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.83569/4.84512. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.83868/4.85150. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.83381/4.85377. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.83896/4.85418. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.83598/4.85374. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.83478/4.85876. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.83326/4.85736. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.83804/4.85399. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.82762/4.84539. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.83643/4.85437. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.83024/4.85438. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.83333/4.85433. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83248/4.84760. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.83348/4.84023. Took 0.18 sec\n",
      "Epoch 69, Loss(train/val) 4.82922/4.85003. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.82936/4.85161. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83366/4.84898. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83128/4.86262. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83387/4.85109. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.82994/4.84655. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.83169/4.85199. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.82745/4.86345. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.82994/4.86373. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.83240/4.86411. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.82426/4.87051. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.82388/4.87019. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83331/4.85369. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.82555/4.86955. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.83729/4.85583. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.82362/4.86746. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.82925/4.85248. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.82414/4.86704. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.82616/4.85850. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.82505/4.86742. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82097/4.87056. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82969/4.85949. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82014/4.85541. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82877/4.84307. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82143/4.86180. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82548/4.88622. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82221/4.88015. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82305/4.87182. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82505/4.86935. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82756/4.87904. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.82255/4.88730. Took 0.20 sec\n",
      "ACC: 0.484375, MCC: -0.011958266722236254\n",
      "Epoch 0, Loss(train/val) 4.74322/4.77489. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.72983/4.72055. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.71506/4.73636. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.70352/4.72619. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.70344/4.73210. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.70491/4.73512. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.70615/4.72763. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.70617/4.72324. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.70440/4.72624. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.70172/4.72924. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.70352/4.73136. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.69943/4.73336. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.70135/4.72531. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.69685/4.72632. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.69826/4.72644. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.69673/4.72816. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.69793/4.72936. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.69763/4.72635. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.69656/4.72241. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.69365/4.73246. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.69525/4.72218. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.69448/4.71987. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.69480/4.72436. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.69433/4.73149. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.69671/4.71781. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.69228/4.73308. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.69289/4.72054. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.70021/4.70566. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.69899/4.70825. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.69332/4.71571. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.69688/4.71268. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.69609/4.71150. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.68942/4.72272. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.69164/4.72111. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.69383/4.71522. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.69268/4.72087. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.69674/4.71414. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.69196/4.71909. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.69192/4.71848. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.69344/4.70770. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.69283/4.71718. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.69522/4.71270. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.68797/4.71561. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.68830/4.72355. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.68733/4.71831. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.69191/4.71283. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.69580/4.70894. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.68827/4.71158. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.68973/4.70968. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.68859/4.70933. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.69340/4.70559. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.68728/4.71253. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.69021/4.71395. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.68597/4.70881. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.68812/4.70901. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.68839/4.71181. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.68625/4.71036. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.68931/4.70817. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.68555/4.70996. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.69003/4.70310. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.68783/4.71446. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.68557/4.70756. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.68992/4.70662. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.68976/4.70966. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.68787/4.70635. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.68699/4.70508. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.68993/4.70039. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.68477/4.70466. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.68515/4.70191. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.68598/4.70519. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.68946/4.70721. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.67951/4.70901. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.68951/4.70248. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.68455/4.70496. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.68401/4.71162. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.68346/4.71039. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.68602/4.70198. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.68604/4.70424. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.68762/4.70185. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.68355/4.70146. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.68857/4.70251. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.68450/4.70584. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.68540/4.70170. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.67935/4.72012. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.68005/4.70110. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.68343/4.70096. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.68516/4.71135. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.68762/4.70822. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.68447/4.70700. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.67943/4.70536. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.68683/4.69749. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.68633/4.69664. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.68458/4.70165. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.67890/4.70117. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.68170/4.69665. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.69008/4.70060. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.68201/4.70007. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.67853/4.71580. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.68254/4.70161. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.67992/4.69739. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.07570682517832987\n",
      "Epoch 0, Loss(train/val) 4.82579/4.80045. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.80508/4.78366. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.80030/4.78177. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.79562/4.78244. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79388/4.78337. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79283/4.77938. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.78916/4.77961. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.78685/4.78218. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.78826/4.78490. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.78684/4.78829. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.78487/4.79047. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78263/4.79201. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.78432/4.79220. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.78213/4.78868. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78306/4.79153. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.77945/4.79555. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.78683/4.77813. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78399/4.78399. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.77863/4.78736. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78114/4.78808. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.77647/4.78744. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78456/4.78885. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.77515/4.80240. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.77395/4.79995. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.77519/4.79855. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.77749/4.79850. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.77588/4.79974. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.77505/4.80051. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.77649/4.80125. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.77461/4.80833. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.77533/4.80390. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77089/4.81388. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77207/4.80351. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.77312/4.79526. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.77086/4.80085. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.76913/4.80791. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77534/4.78835. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.76736/4.80709. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.76504/4.80201. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77474/4.79996. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.76846/4.80137. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.76719/4.79244. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76675/4.81523. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.76757/4.80457. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.76436/4.80307. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.76414/4.81876. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.76606/4.79989. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.76681/4.80797. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.76508/4.80455. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.76734/4.81075. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76493/4.79692. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.76836/4.79167. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.76689/4.79413. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76114/4.78945. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76480/4.80714. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.76301/4.81056. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.75629/4.82112. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76048/4.80475. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.75763/4.80529. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.75573/4.81899. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.75546/4.82109. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.76062/4.80267. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.75440/4.80469. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.76005/4.79823. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.75958/4.80048. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.76411/4.80799. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.76666/4.79707. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.76024/4.81484. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.76230/4.81250. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.75916/4.81156. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.75702/4.80180. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.76093/4.78626. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.75883/4.81341. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.75202/4.80636. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.75857/4.81488. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.75515/4.81245. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.75217/4.81035. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.75104/4.82974. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.75556/4.83372. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.74875/4.82284. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.75752/4.82006. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.75309/4.83892. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.75687/4.80828. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.75008/4.82467. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.75421/4.80679. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.75110/4.80831. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.74841/4.82110. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.75464/4.82330. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.74933/4.82294. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.75119/4.84559. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.75220/4.80996. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.75308/4.83340. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.74625/4.80666. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.74888/4.79737. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.74366/4.84015. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75342/4.81151. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.75198/4.82877. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.74879/4.83272. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.74380/4.83766. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75248/4.79822. Took 0.20 sec\n",
      "ACC: 0.5625, MCC: 0.18153846153846154\n",
      "Epoch 0, Loss(train/val) 4.90893/4.89406. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.86851/4.87097. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.86716/4.87593. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.86763/4.87948. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.86872/4.87490. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.87055/4.87346. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.87278/4.88115. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.86552/4.88548. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.86245/4.89202. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.85930/4.88912. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.85911/4.89578. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.85775/4.89971. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.85396/4.91052. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.85503/4.91429. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85353/4.91242. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.84938/4.91224. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85007/4.91077. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.84832/4.90768. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.84749/4.93693. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.84825/4.92121. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84967/4.90703. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.84226/4.91603. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.84750/4.91362. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85753/4.85351. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85879/4.86755. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.85461/4.89480. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.85224/4.89430. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84891/4.90566. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.84663/4.90482. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84855/4.90995. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.85025/4.90204. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84518/4.90511. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.84631/4.89762. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84309/4.90149. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84342/4.90087. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84274/4.90067. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84242/4.90092. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84128/4.90589. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84123/4.89969. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.84006/4.89715. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84054/4.90908. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.83463/4.91305. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83704/4.93291. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.84502/4.90243. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84090/4.89099. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.83699/4.90273. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.83798/4.90564. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84072/4.90023. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.83862/4.89592. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.83481/4.89608. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.84212/4.89553. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.83656/4.91146. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.83404/4.91038. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.83783/4.89156. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.83224/4.89620. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 4.83675/4.89831. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.83311/4.90828. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.83380/4.89472. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.83256/4.90267. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.83920/4.90991. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.83719/4.89263. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.83177/4.90416. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.83455/4.90208. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83514/4.90227. Took 0.22 sec\n",
      "Epoch 64, Loss(train/val) 4.83357/4.93003. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.83116/4.92117. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.83314/4.88447. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.82903/4.90208. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.83255/4.94004. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.83666/4.89201. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 4.83741/4.90039. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83290/4.90167. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83611/4.89935. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.82807/4.90091. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.83302/4.90378. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.83338/4.89838. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83260/4.90255. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.83059/4.89784. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.83534/4.92086. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.83126/4.89073. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.83090/4.94103. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.82838/4.89598. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.82473/4.92616. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.82728/4.91152. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83249/4.89755. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.82638/4.93050. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.82754/4.91677. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.82569/4.92681. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.82416/4.93145. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83295/4.89139. Took 0.18 sec\n",
      "Epoch 90, Loss(train/val) 4.82855/4.92150. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.82527/4.94374. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.82779/4.91466. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83021/4.92702. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82799/4.91822. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82410/4.92586. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82656/4.92098. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82809/4.90115. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.83037/4.92539. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.82446/4.93416. Took 0.19 sec\n",
      "ACC: 0.6875, MCC: 0.37389885714349824\n",
      "Epoch 0, Loss(train/val) 4.85068/4.79564. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.79547/4.80481. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.79447/4.81314. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79579/4.81101. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79584/4.80494. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79496/4.80308. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.79278/4.80453. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.78738/4.80750. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.78936/4.80909. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.78700/4.81633. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.78750/4.81463. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 4.78747/4.81203. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.78803/4.81298. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.78884/4.81193. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78273/4.82394. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78469/4.81814. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78554/4.81592. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78221/4.81797. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78308/4.80862. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78040/4.82008. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.77882/4.83013. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78679/4.80460. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.78003/4.79704. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78543/4.79194. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78163/4.80161. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.77831/4.80771. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.77906/4.81114. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78738/4.82372. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78245/4.82381. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.77976/4.82159. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.78023/4.82329. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.78013/4.82791. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77624/4.82645. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.78032/4.81241. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.77599/4.82660. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77688/4.83477. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77540/4.83092. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.77523/4.82582. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.77477/4.83132. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77548/4.82060. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77148/4.84999. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 4.77164/4.82698. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.77358/4.83459. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.77404/4.84398. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77014/4.83334. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.76651/4.85120. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77241/4.84178. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.76757/4.82348. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.77070/4.82985. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.76464/4.83629. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.76790/4.83250. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.76409/4.84004. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.76679/4.84152. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76739/4.82326. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76637/4.84318. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.77060/4.82777. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.76269/4.84526. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76031/4.83612. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.76355/4.84348. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.76592/4.83970. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.75858/4.84923. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.76837/4.84007. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.76279/4.84112. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.75903/4.83713. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76463/4.83240. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.75985/4.83698. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75720/4.83785. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.76504/4.83738. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.76368/4.83705. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.75898/4.83409. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.76210/4.81786. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.75678/4.83365. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.76199/4.83404. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.75750/4.84645. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.75485/4.84338. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.76170/4.83469. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.75657/4.84088. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.75690/4.85528. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.75716/4.83318. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.77040/4.84895. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.78239/4.80887. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.77256/4.82434. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.77079/4.82986. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.76472/4.83841. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.77405/4.84475. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.76205/4.84303. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.76054/4.84463. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.76113/4.83662. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.76487/4.80874. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.76134/4.82320. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.75515/4.84174. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.76003/4.82304. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.75373/4.83568. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.75884/4.83768. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.75939/4.82842. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75450/4.85345. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.75566/4.84864. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.75936/4.82656. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.75554/4.85725. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75670/4.82459. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.04350001325672093\n",
      "Epoch 0, Loss(train/val) 5.14637/5.03815. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.04827/5.03553. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.04964/5.03584. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.04763/5.03610. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.04901/5.03669. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.04665/5.03835. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.04638/5.03966. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.04196/5.04307. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.04165/5.04434. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.03982/5.04704. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.04095/5.04581. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.04152/5.05092. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.03765/5.05141. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.03897/5.05323. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.03625/5.05624. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.03638/5.05393. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.03960/5.05246. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.03471/5.05869. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.03505/5.05571. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.02887/5.07072. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.03097/5.05822. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.03193/5.06714. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.02927/5.07317. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.03194/5.05961. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 5.02539/5.09052. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.02747/5.07988. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.02561/5.08307. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.02673/5.07461. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 5.02355/5.09985. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.02172/5.08480. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.02135/5.11521. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.01924/5.09197. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.01600/5.11569. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.02401/5.08877. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.02184/5.10486. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.01654/5.12548. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.01697/5.10958. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 5.01324/5.12923. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.01842/5.11811. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.01867/5.10111. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.01449/5.13250. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.01583/5.11662. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 5.01076/5.13303. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 5.01978/5.10191. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 5.01993/5.07472. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.02184/5.11303. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.01704/5.10187. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 5.01324/5.11799. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.01595/5.10972. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 5.01218/5.16446. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 5.01270/5.12540. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 5.00618/5.13515. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.01390/5.12361. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.00892/5.13849. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 5.00858/5.13953. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.00695/5.15032. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 5.01988/5.06989. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 5.00916/5.14764. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.01501/5.10468. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.01463/5.11477. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.01198/5.12821. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 5.00945/5.14375. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.00723/5.13380. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 5.00649/5.09802. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.00959/5.13021. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 5.00875/5.13266. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 5.00615/5.15145. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 5.00811/5.09074. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 5.01052/5.13474. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.00514/5.13354. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.00299/5.14380. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.00318/5.15397. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 5.00737/5.13508. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.00970/5.11249. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.00755/5.16763. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.00373/5.12608. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.99942/5.14502. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 5.00554/5.11852. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 5.00845/5.12705. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.99801/5.14056. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.00089/5.13839. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.99822/5.12065. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.00441/5.13323. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 5.00360/5.10885. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.00486/5.11503. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 5.01740/5.06671. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 5.01588/5.11717. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.01396/5.10503. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.00963/5.12720. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.00319/5.14794. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 5.00686/5.11432. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.00691/5.14868. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.00566/5.13247. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 5.00833/5.14833. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 5.01028/5.14838. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.00583/5.12423. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.00952/5.12673. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.00854/5.12772. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.00907/5.10140. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 5.00392/5.15757. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.02834008097165992\n",
      "Epoch 0, Loss(train/val) 5.27027/5.19174. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.21305/5.19280. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.20464/5.19603. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.20314/5.19389. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 5.20440/5.19567. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.20099/5.19753. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 5.20584/5.19645. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 5.20173/5.20144. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 5.19658/5.20469. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.19691/5.20848. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.19447/5.21719. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.19501/5.21482. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.19579/5.22394. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.19579/5.22408. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 5.19523/5.21991. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.19282/5.22646. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 5.19357/5.23204. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.19129/5.23588. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.19339/5.21855. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 5.19061/5.23979. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 5.18874/5.23195. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.18611/5.23898. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.19002/5.23327. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.19090/5.23420. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.18786/5.22104. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.18663/5.24001. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 5.18623/5.22924. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.18299/5.24248. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.18614/5.23875. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.18707/5.23410. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.18176/5.24535. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.17894/5.24586. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.18347/5.23293. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.17793/5.24237. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.17750/5.23280. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.18264/5.23033. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 5.18528/5.25384. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.18121/5.25299. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.18328/5.25157. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.18315/5.24155. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.18301/5.22630. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 5.18260/5.24861. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 5.18126/5.24634. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.17347/5.25748. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.17661/5.24595. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.18183/5.26349. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.19418/5.22265. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.17582/5.26418. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 5.18056/5.24212. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.17681/5.25574. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 5.17763/5.25457. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.17154/5.26003. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.17295/5.26405. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.17004/5.28751. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.17164/5.26596. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.17330/5.24278. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.18305/5.24588. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 5.17624/5.26050. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.16808/5.27549. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.17255/5.24707. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.17325/5.24295. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.17603/5.28004. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 5.17165/5.25253. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.16758/5.27713. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.16553/5.27809. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.16579/5.26609. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.16730/5.26628. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 5.16578/5.26757. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 5.17250/5.26434. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 5.16335/5.26899. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.16445/5.28513. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.16488/5.32358. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.16905/5.25585. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.16920/5.26162. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.16403/5.29662. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.17014/5.29449. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.17006/5.26940. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.18617/5.22957. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.17647/5.24366. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.17986/5.23786. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.16932/5.24238. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 5.17647/5.24094. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.17301/5.23454. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.16932/5.25055. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.17524/5.23484. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.17932/5.25365. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.17070/5.21507. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.18722/5.23387. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 5.17983/5.23648. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.17308/5.25023. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.17773/5.23644. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.17101/5.24549. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.17146/5.24754. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.17104/5.25896. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.17203/5.24722. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.17732/5.24234. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.17760/5.23886. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.17220/5.23720. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.16546/5.25216. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.16932/5.25755. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.03253000243161777\n",
      "Epoch 0, Loss(train/val) 5.08238/5.03210. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.99583/4.99004. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.98532/4.99026. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.97910/5.00981. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.97926/5.01821. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.98270/5.02549. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.98286/5.01136. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.98051/5.01482. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.98033/5.01143. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.97987/5.01487. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.97191/5.01902. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.97443/5.01878. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.97806/5.00967. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.97439/5.01378. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.96932/5.02934. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.97164/5.02621. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.97006/5.03666. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.97169/5.02099. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.97063/5.02680. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.97044/5.01957. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.97028/5.01495. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.96577/5.03239. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.96824/5.02759. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.96880/5.02336. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.96613/5.02463. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.96903/5.02792. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.96469/5.02800. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.96689/5.02846. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.96608/5.02837. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.96349/5.04392. Took 0.18 sec\n",
      "Epoch 30, Loss(train/val) 4.96465/5.04170. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.96509/5.03953. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.96433/5.04177. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.96058/5.04543. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.96056/5.04566. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.95840/5.05728. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.95766/5.06925. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.96088/5.07647. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.95757/5.07232. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.97118/5.02500. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.96344/5.05571. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.96070/5.06209. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.96488/5.05688. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.96239/5.04461. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.95731/5.06231. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.95600/5.07399. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.95389/5.07120. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.96284/5.06195. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.95977/5.04548. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.95991/5.05754. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.95597/5.07280. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.95493/5.06283. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.95348/5.06249. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.95425/5.06733. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.95814/5.05824. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.95242/5.04871. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.95442/5.06249. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.95582/5.06157. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.95265/5.07038. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.95142/5.07559. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.95177/5.07018. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.94446/5.07681. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.95642/5.05667. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.95187/5.05744. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.95158/5.07382. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.94650/5.07953. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.94947/5.09770. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.95772/5.06845. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.94754/5.06321. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.94946/5.05647. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.94559/5.06707. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.94252/5.09594. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.95143/5.06941. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.94314/5.10783. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.94668/5.08339. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 4.94674/5.04644. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.95455/5.06439. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.94222/5.08326. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.94634/5.04997. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.94079/5.10634. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.94482/5.08234. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.94546/5.06636. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.94167/5.08225. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.94668/5.11131. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.94623/5.08422. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.94338/5.05630. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.94561/5.07052. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.94375/5.09152. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.94819/5.06395. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.94355/5.07742. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.94407/5.07258. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.94366/5.07159. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.94273/5.11022. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.94165/5.07286. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.94248/5.09278. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.93812/5.07800. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.94290/5.07072. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.93788/5.08550. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.93404/5.07088. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.94001/5.07220. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.19136555680572745\n",
      "Epoch 0, Loss(train/val) 4.83013/4.84333. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.78770/4.79508. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.78601/4.80475. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.78737/4.80838. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.78420/4.81107. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.77920/4.81173. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.77996/4.82546. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.77862/4.82388. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.77884/4.82424. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.77900/4.81903. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.77831/4.81224. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.77998/4.81775. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.77825/4.80788. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.77744/4.81638. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.77531/4.83041. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.77757/4.81491. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.77574/4.81084. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.77351/4.81715. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.77615/4.80266. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78126/4.79197. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.77642/4.80228. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.77775/4.80690. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.77410/4.81878. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.77222/4.80879. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.77211/4.82047. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.77286/4.81103. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.77628/4.78259. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.77292/4.81248. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.76919/4.81878. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.76806/4.80972. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.76860/4.82659. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77007/4.82307. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.76801/4.82409. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.76861/4.82073. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.76731/4.83018. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.76584/4.82576. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.76859/4.80789. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.76633/4.82992. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.76477/4.82500. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.76509/4.81863. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.76671/4.82809. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.76381/4.83046. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76291/4.83242. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.76534/4.82686. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.76273/4.83499. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.75609/4.84713. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.76289/4.83656. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.75617/4.83780. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.76055/4.82156. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.76151/4.83528. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76585/4.83382. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.77279/4.83039. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.76968/4.83346. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76766/4.83242. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.76555/4.84420. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.76841/4.82747. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.76419/4.84651. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76355/4.84136. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.76204/4.85628. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.75982/4.85627. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.75873/4.87106. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.76027/4.86902. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.75766/4.84696. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.75785/4.83240. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.75448/4.86783. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.75146/4.87343. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75433/4.85783. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.75472/4.84900. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.75843/4.86637. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.75397/4.88459. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.74664/4.88436. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.75133/4.87359. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.75155/4.89147. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.75283/4.85635. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.75246/4.89093. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.74660/4.90554. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.74362/4.90638. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.74763/4.90208. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.75043/4.88054. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.74580/4.86789. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.74507/4.85657. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.74431/4.91211. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.73661/4.93666. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.75171/4.89569. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.74618/4.92528. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.74343/4.91250. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.73896/4.92630. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.75385/4.84155. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.75067/4.86812. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.75053/4.89617. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.75839/4.85844. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.75371/4.85118. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.74325/4.85881. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.74985/4.87372. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.75483/4.88089. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.73985/4.89726. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.74773/4.90803. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.74172/4.90113. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.74613/4.88563. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75018/4.87236. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.06262242910851495\n",
      "Epoch 0, Loss(train/val) 4.79851/4.66822. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.73294/4.68258. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.72466/4.68622. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.72599/4.68780. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.72517/4.69037. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.72559/4.69183. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.72511/4.68881. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.72321/4.69429. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.72156/4.69359. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.72219/4.69874. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.72032/4.70031. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.72129/4.70184. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.71820/4.70105. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.72170/4.70211. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.71844/4.70507. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.71707/4.70543. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.72110/4.70709. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.71978/4.71009. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.71653/4.71272. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.71538/4.71367. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.71871/4.71360. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.71358/4.71585. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.71452/4.71964. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.71487/4.71726. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.71744/4.71319. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.71430/4.71814. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.71442/4.72625. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.71376/4.72340. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.71731/4.72275. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.71232/4.73205. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.71266/4.72833. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.71169/4.74554. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.71582/4.74279. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.71499/4.72856. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.71435/4.73554. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.71206/4.74586. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.71342/4.74220. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.71145/4.74475. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.71025/4.75615. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.70909/4.74788. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.70934/4.75442. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.71031/4.75412. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.71120/4.74128. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.70935/4.75139. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.70942/4.71951. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.71372/4.71471. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.71263/4.73112. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.70915/4.74443. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.71137/4.72981. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.70824/4.74132. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.71148/4.73946. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.70807/4.74607. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.70998/4.74928. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.70746/4.74828. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.70830/4.75655. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.70669/4.75399. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.70547/4.74840. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.70459/4.76017. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.70919/4.73931. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.70779/4.75257. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.70645/4.76696. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.70600/4.75123. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.70582/4.75563. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.70406/4.74656. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.70657/4.75390. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.70292/4.76840. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.70545/4.76018. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.70224/4.77477. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.70493/4.78380. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.70000/4.75657. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.70473/4.75039. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.70395/4.76013. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.70120/4.79395. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.69624/4.76844. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.70904/4.74665. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.70294/4.74952. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.69947/4.77521. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.69942/4.76278. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.69789/4.77215. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.70111/4.76359. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.70042/4.76797. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.70253/4.77159. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.69767/4.77348. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.69765/4.76734. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.70076/4.75557. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.69665/4.75337. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.69920/4.75192. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.69351/4.76631. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.69276/4.78702. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.69539/4.76588. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.69954/4.75042. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.69652/4.75459. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.69217/4.76967. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.69132/4.77711. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.69510/4.80342. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.69345/4.77530. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.69631/4.76113. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.69260/4.78447. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.69793/4.80399. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.68795/4.77644. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.03779786830012622\n",
      "Epoch 0, Loss(train/val) 4.86480/4.81895. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.81154/4.79014. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.80717/4.78308. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.80767/4.78135. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.80150/4.78096. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.80327/4.78019. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.80292/4.77946. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.80407/4.77891. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.80237/4.77980. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.80208/4.77473. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79757/4.76683. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.79847/4.76940. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79303/4.76736. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79431/4.77897. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.79351/4.76915. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.79478/4.77094. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.79228/4.76618. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.79414/4.76771. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.79227/4.76928. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.79068/4.77217. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.79133/4.77016. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.78976/4.76778. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.79036/4.76626. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78715/4.76343. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.79017/4.76582. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.78822/4.76782. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78460/4.75898. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78667/4.76040. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78671/4.75766. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78502/4.75825. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.78828/4.76338. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.78357/4.75901. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.78636/4.76025. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.78496/4.75580. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.78381/4.75652. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.78136/4.75778. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78314/4.75976. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.78336/4.75919. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.78285/4.75703. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.78333/4.75738. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.78164/4.75234. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.78070/4.75821. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.77824/4.75742. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.78265/4.76012. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.78102/4.75775. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.78414/4.76354. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78174/4.75496. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.77953/4.75460. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.77942/4.76266. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.78335/4.75941. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.78181/4.76183. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.77567/4.75666. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.77545/4.77061. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.77871/4.76748. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.77630/4.76143. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.77496/4.76143. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.77840/4.76826. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.77949/4.76361. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.77808/4.76453. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.77681/4.75987. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.77551/4.76200. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.77686/4.76727. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.77833/4.76267. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.77635/4.76467. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.77602/4.76706. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.77652/4.76725. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.77391/4.76424. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.77548/4.76427. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77512/4.76992. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.76936/4.76539. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.77473/4.78508. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.77582/4.76682. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.77525/4.77195. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.77689/4.77388. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77490/4.77175. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.77254/4.76938. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.77073/4.77921. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.76787/4.78072. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.77038/4.77404. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.76841/4.77748. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.76974/4.77467. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.77485/4.77115. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.77489/4.76735. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.77477/4.76558. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 4.77063/4.77281. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.76470/4.78184. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.77833/4.77494. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.76870/4.77184. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.77330/4.77617. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.77206/4.77332. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.77204/4.77234. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.76951/4.77040. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.77137/4.77790. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.77175/4.77459. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.78124/4.79392. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.78896/4.76973. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.78171/4.75757. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.78062/4.75410. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.77123/4.75130. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.77623/4.76298. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.1889822365046136\n",
      "Epoch 0, Loss(train/val) 4.74158/4.78390. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.75095/4.73858. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.73438/4.75297. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.73412/4.75260. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.73413/4.75247. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.73922/4.74075. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.73947/4.72548. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.73661/4.72253. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.73182/4.72235. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.72785/4.72351. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.72859/4.72414. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.72958/4.72393. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.73056/4.72370. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.72954/4.72680. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.73270/4.71801. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.72817/4.71837. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.72551/4.72060. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.72765/4.72131. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.72583/4.72045. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.72744/4.71961. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.72521/4.71991. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.72309/4.71911. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.72537/4.71546. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.72279/4.71612. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.72367/4.71675. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.72446/4.71736. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.72145/4.71443. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.72094/4.71487. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.72348/4.71344. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.72131/4.71440. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.72065/4.71665. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.72612/4.71373. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.72079/4.71777. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.71816/4.72291. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.72146/4.71900. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.71940/4.71826. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.72128/4.71416. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.71923/4.71257. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.72103/4.71173. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.71767/4.71843. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.72204/4.71541. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.71793/4.72925. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.72222/4.72103. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.72029/4.72178. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.71804/4.72243. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.71868/4.72263. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.71485/4.71700. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.72171/4.71935. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.71784/4.72270. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.71785/4.72896. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.72104/4.71754. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.71707/4.71673. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.71843/4.72136. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.71488/4.72171. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.71428/4.72454. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.71888/4.72507. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.71369/4.71961. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.71801/4.72742. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.71717/4.72843. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.71200/4.72386. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.71641/4.72056. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.71853/4.71765. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.71174/4.72098. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.71515/4.72743. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.71335/4.72664. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.71008/4.73402. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.70828/4.73331. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.71133/4.73569. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.71842/4.73605. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.71072/4.73533. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.71009/4.75612. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.71420/4.73317. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.70934/4.74905. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.70788/4.75010. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.71526/4.72938. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.70874/4.72753. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.71064/4.74051. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.71870/4.73184. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.71033/4.73775. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.71495/4.73137. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.70811/4.73808. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.71202/4.75827. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.70753/4.75345. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.71236/4.73850. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.71658/4.73682. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.71469/4.74322. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.71139/4.74206. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 4.71370/4.76175. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 4.70618/4.76560. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.70688/4.76403. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.71927/4.71903. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.72071/4.72071. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.71033/4.74001. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.70515/4.76746. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.70641/4.76049. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.71545/4.73704. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.70763/4.75480. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.70398/4.76282. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.70867/4.75507. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.70810/4.75112. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.07570682517832987\n",
      "Epoch 0, Loss(train/val) 4.94571/4.92627. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.91072/4.94336. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.91173/4.94792. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.91632/4.94368. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.91407/4.94878. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.91079/4.94992. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.90936/4.96108. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.91195/4.96439. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.91004/4.97124. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.91040/4.97322. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.90726/4.97445. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.90794/4.96777. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.90549/4.96195. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.90398/4.96705. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.90502/4.96105. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.90527/4.95118. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90246/4.93477. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.89941/4.94385. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.89979/4.93860. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.89530/4.94635. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.90224/4.93094. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.89459/4.92686. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.89488/4.92144. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.89605/4.93643. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.89631/4.92674. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.89423/4.93059. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.89278/4.92843. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.89324/4.92838. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.88468/4.93889. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.89357/4.94144. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.88687/4.92214. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.88921/4.91768. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.88605/4.93396. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.87947/4.94524. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.89051/4.93562. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.88635/4.92931. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.88446/4.92968. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.87933/4.94951. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.88217/4.93542. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90922/4.95522. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90319/4.92880. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.89523/4.94130. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89123/4.93201. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.88612/4.93966. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.88663/4.93669. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.88598/4.93198. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.88926/4.94310. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.88503/4.95147. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.88373/4.92577. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.88330/4.91311. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.88342/4.91888. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.87985/4.92431. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.88515/4.92174. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.88334/4.92215. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.87506/4.93893. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.88264/4.93791. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.87930/4.92864. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.88122/4.92372. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.87718/4.93011. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.87811/4.91644. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.88314/4.93420. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.88634/4.92777. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.87674/4.93275. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.87782/4.93150. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.87448/4.90653. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.87698/4.91558. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.87180/4.92579. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.87972/4.92503. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.87424/4.93264. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.88115/4.90790. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.87263/4.92265. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.87370/4.91480. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.87297/4.92257. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.87915/4.91542. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.87363/4.90392. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.87933/4.92880. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.86902/4.90398. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.86614/4.92119. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.87049/4.91634. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.86876/4.92911. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.86845/4.90298. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.86756/4.89929. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.87154/4.91839. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.87863/4.91851. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.87793/4.93665. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.86752/4.94393. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.86631/4.93102. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.88239/4.93356. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.87777/4.92556. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.86866/4.91472. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.87062/4.93190. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.87157/4.93260. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.86663/4.94604. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.87761/4.92839. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.88045/4.93356. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.87818/4.92542. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.87203/4.92789. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.87097/4.95551. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.87017/4.93234. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.87061/4.91489. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.16012815380508713\n",
      "Epoch 0, Loss(train/val) 4.96072/4.90665. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.91717/4.91862. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.91525/4.91697. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.91779/4.91483. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.91538/4.91767. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.91710/4.92241. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.91384/4.92912. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.91407/4.92348. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.91438/4.91230. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.91234/4.90612. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.90962/4.90715. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.91170/4.90226. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.91038/4.90161. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.90732/4.90309. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.90759/4.90570. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.90744/4.90948. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90513/4.90197. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.90311/4.90028. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.90473/4.90609. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.90114/4.90081. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.90356/4.89811. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.90063/4.89502. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.89801/4.89684. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.89994/4.90612. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.89922/4.90055. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.89390/4.91089. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.89193/4.90721. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.89159/4.91332. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.89287/4.89926. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.89130/4.90176. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.89282/4.88637. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.89392/4.89906. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.89000/4.89615. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.89101/4.90284. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.89054/4.91211. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.88983/4.92664. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.89337/4.90475. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.89258/4.91044. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.88790/4.89666. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.88766/4.93380. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.89217/4.89855. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.88185/4.91627. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.88926/4.89983. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.88707/4.90613. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.88711/4.91388. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.88272/4.91233. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.88664/4.90881. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.88203/4.92695. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89026/4.89601. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.88213/4.90725. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.88540/4.92219. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.88351/4.92938. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.88543/4.91397. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.88518/4.92089. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.88152/4.95935. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.90236/4.91581. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.89387/4.92623. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.89389/4.93337. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.89000/4.92590. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89213/4.93022. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.88810/4.94821. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.88830/4.94243. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.88732/4.94162. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.88924/4.93639. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.88274/4.94821. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.88687/4.93951. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.88175/4.95486. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.88174/4.93779. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.88029/4.93624. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.88319/4.93870. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.88232/4.93083. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.87954/4.93100. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.87297/4.94731. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88176/4.93486. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.88044/4.91963. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.87352/4.96537. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.87719/4.93119. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.87774/4.92148. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.87758/4.94203. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.87787/4.91745. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.87580/4.95152. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.87660/4.93642. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.87768/4.93890. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.86561/4.96528. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88001/4.91977. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.87305/4.93911. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.86594/4.95780. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.86893/4.96432. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.87721/4.94758. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.87798/4.93837. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.87035/4.93616. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.86557/4.94517. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.87306/4.94034. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.88347/4.95133. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.86345/4.98707. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.86694/4.96101. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.86434/4.97311. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.86463/4.97733. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.86923/4.94913. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.87105/4.96193. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.0657512425947884\n",
      "Epoch 0, Loss(train/val) 5.02512/4.94659. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.94906/4.94934. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.94874/4.94585. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.94866/4.95000. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.94641/4.95387. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.94713/4.95348. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.94778/4.95262. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.94475/4.95845. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.94722/4.95723. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.94306/4.95751. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.94542/4.96046. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.94322/4.96849. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.94120/4.96855. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.94167/4.97080. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.93709/4.97765. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.94494/4.96624. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.94198/4.96785. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.94148/4.97484. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.93924/4.97946. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.94137/4.97306. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.93997/4.98206. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 4.94221/4.96660. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.93771/4.96902. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.94122/4.97252. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.93700/4.96938. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.93390/4.97354. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.93579/4.97125. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.94048/4.97193. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 4.93609/4.96707. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.93467/4.98456. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.92996/4.99763. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.93478/4.97448. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.93141/4.98394. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.92929/4.98648. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.92878/4.98734. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.93117/4.97517. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.92931/4.99537. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.93060/4.97790. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.93056/4.97942. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.92993/4.98242. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.92734/4.98948. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.92612/4.99054. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.92807/4.97481. Took 0.18 sec\n",
      "Epoch 43, Loss(train/val) 4.92317/4.98618. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.92835/4.97900. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.92858/4.97350. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.92312/4.98939. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.92350/4.99273. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.92240/4.99279. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.92396/4.99411. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.92853/4.98423. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.92574/4.98170. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.91663/5.00034. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.92271/4.99607. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.91887/5.00866. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.92702/4.99998. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.92646/4.98636. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.92598/4.98813. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.92550/4.98958. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.92326/4.99738. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.92297/4.99354. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.92347/4.99541. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.91865/4.99608. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.91116/5.01097. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.92672/4.98351. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.91512/4.99646. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.92131/4.99234. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.91537/4.99524. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.91943/4.99358. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.92598/4.96926. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.92047/4.98352. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.92044/4.99989. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.92145/4.99502. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 4.92222/4.98211. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.91872/4.99304. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.91757/5.00081. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.92027/4.99737. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.91329/5.00478. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.91035/5.00947. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.91778/4.98480. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.91641/5.00918. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.91864/4.99839. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.91326/4.99373. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.91744/5.01367. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.91295/5.00648. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.92504/4.99449. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.92047/4.99967. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.91120/5.00916. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.91945/4.98381. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.93176/4.96050. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.93041/4.95940. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.92907/4.95239. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.92294/4.95802. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.92675/4.95954. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.92370/4.96954. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.91827/4.96603. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.92366/4.97087. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.92482/4.95546. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.92506/4.97913. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.92914/4.97344. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.08845235543978211\n",
      "Epoch 0, Loss(train/val) 5.00709/4.92725. Took 0.19 sec\n",
      "Epoch 1, Loss(train/val) 4.94095/4.97426. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.95207/5.01052. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.95101/5.05489. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.95698/5.06283. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.95677/5.01680. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.94901/4.99070. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.94298/4.98585. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.94002/4.99973. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.94249/4.99226. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.94010/4.98513. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.93973/4.98816. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.93953/4.99056. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.94020/4.99615. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.93925/4.99280. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.93810/4.99365. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.93695/4.99187. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.93645/4.99350. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.93654/4.99025. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.93479/4.99433. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.93314/5.00238. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.93803/4.99732. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.93512/5.00086. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.93335/5.00744. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.93015/5.01199. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.92916/5.00954. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.93064/5.01039. Took 0.18 sec\n",
      "Epoch 27, Loss(train/val) 4.92919/5.01044. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.92529/5.02539. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.93258/5.01392. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.92947/5.02358. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.93401/4.98797. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.92019/5.04442. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.92171/5.03783. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.92577/5.03118. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.92695/5.01186. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.92079/5.03698. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.92378/5.02441. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.92091/5.03669. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.92019/5.04050. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.92475/5.02579. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.92188/5.02971. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.92283/5.02852. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.92231/5.03173. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.91923/5.01794. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.91292/5.06268. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.92136/5.02388. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.91086/5.06709. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.92454/5.02481. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.91600/5.03024. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.91463/5.05223. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.91600/5.01302. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.92390/5.01752. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.91231/5.06051. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.91905/5.01652. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.91763/5.04705. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.91557/5.03091. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.91370/5.03375. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.91360/5.04873. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.91940/5.00139. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.91812/5.02292. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.91227/5.04230. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.91089/5.02410. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.91605/5.03123. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.91356/5.07231. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.91679/5.00053. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.91389/5.02902. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.90794/5.03183. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.91345/5.05894. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.91328/5.05490. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.90705/5.02048. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.91259/5.06792. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.90586/5.04809. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.90675/5.04242. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.90998/5.04417. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.91281/5.04699. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.90847/5.07930. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.90566/5.07491. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.90915/5.03174. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.90523/5.06422. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.90375/5.04097. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.90784/5.08240. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.90946/5.08269. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.90107/5.06328. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.91142/5.05615. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.90072/5.05070. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.90540/5.08314. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.90236/5.08245. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.91088/5.04215. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.90672/5.04043. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.90345/5.09594. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.89770/5.05290. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.90879/5.02980. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.90572/5.01900. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.89553/5.04893. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.89656/5.07745. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.90409/5.04033. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.90022/5.08612. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.90612/5.04667. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.90423/5.07666. Took 0.20 sec\n",
      "ACC: 0.515625, MCC: 0.013980301652228746\n",
      "Epoch 0, Loss(train/val) 4.86625/4.76908. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.77076/4.80797. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.77512/4.83231. Took 0.18 sec\n",
      "Epoch 3, Loss(train/val) 4.77708/4.83649. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.77846/4.85092. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.78038/4.86612. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.78302/4.85974. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.78320/4.82856. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.77634/4.81264. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.77236/4.81418. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.77538/4.81334. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.77515/4.80190. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.77081/4.81464. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.77148/4.81413. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.76916/4.81540. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.76601/4.82130. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.77104/4.82041. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.76746/4.82497. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.76808/4.81606. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.76889/4.82507. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.76931/4.81530. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.76536/4.82075. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.76590/4.83101. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.76486/4.81781. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.76140/4.83102. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.76493/4.82362. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.76466/4.83537. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.76738/4.81119. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.76505/4.81643. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.76261/4.83365. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.76313/4.82413. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.76430/4.80617. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.76242/4.82958. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.75502/4.83813. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.76569/4.82156. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.75559/4.84530. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.75292/4.85587. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.75663/4.83167. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.76180/4.83753. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.76060/4.84048. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.75766/4.84293. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.76006/4.82399. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.75707/4.84505. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.75562/4.83357. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.75612/4.84728. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.75440/4.85105. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.75701/4.83800. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.76083/4.83120. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.75139/4.82755. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.75645/4.83493. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.75339/4.83181. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.75565/4.84483. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.75769/4.84007. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.75039/4.84132. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.74853/4.85486. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.75540/4.82356. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.75225/4.84543. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.74831/4.85098. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.75498/4.84394. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 4.74964/4.83745. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.74990/4.82743. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.74782/4.84647. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.75063/4.83841. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.74468/4.85402. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.75025/4.84429. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.74882/4.86413. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.75020/4.85313. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 4.75062/4.83817. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.74421/4.84474. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.74531/4.83741. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.74804/4.86172. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.74937/4.83019. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.74618/4.86643. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.77267/4.83416. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.74990/4.84155. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.74879/4.83973. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.74559/4.84863. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.75257/4.82448. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.74682/4.85427. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.74882/4.83932. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.74487/4.84913. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.74764/4.87809. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.74791/4.85314. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.74610/4.83627. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.74660/4.85919. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 4.74135/4.81755. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.74500/4.84034. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.74634/4.85680. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 4.75030/4.81990. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 4.74303/4.84030. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.74344/4.85385. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.74866/4.81877. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.74306/4.82653. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.74731/4.81799. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.73855/4.82925. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.74334/4.84330. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.74413/4.81157. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.73908/4.82290. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.74298/4.82702. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.73652/4.85476. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.035451483867683334\n",
      "Epoch 0, Loss(train/val) 5.05162/5.02771. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.01236/5.04330. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.01718/5.01302. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.00712/5.00250. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.99394/5.00530. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.99654/5.00687. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.99902/5.00793. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.99638/5.00858. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.99806/5.00780. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.99859/5.00691. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.99564/5.01029. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.99253/5.01305. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.99471/5.01285. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.99603/5.01417. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.99190/5.01909. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.98928/5.02466. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.98676/5.02732. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.98992/5.02445. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.99027/5.03417. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.98516/5.03218. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.98476/5.03107. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.98512/5.02129. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.98367/5.03428. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.98082/5.03832. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.98046/5.03980. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.97942/5.03115. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.98144/5.04561. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.97408/5.04364. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.97736/5.04839. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.97921/5.03798. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.97596/5.04408. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.97453/5.04729. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.97298/5.04749. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 4.97665/5.04469. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.97822/5.05957. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.96679/5.07729. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.96817/5.05410. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.97583/5.04439. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.97808/5.06913. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.97090/5.06177. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.97123/5.05407. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.96963/5.09377. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.97410/5.04707. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.97674/5.06529. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.96668/5.08327. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.97224/5.06676. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.96960/5.06332. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.97096/5.07701. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.97260/5.06287. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.96163/5.08326. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.97087/5.04095. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.96845/5.07613. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 4.96765/5.07172. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.97143/5.05651. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.96879/5.07683. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.97308/5.07049. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.96594/5.08528. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.96661/5.09484. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.96544/5.08543. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.96245/5.09959. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.96974/5.06982. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.96540/5.08886. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.96405/5.08197. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.96276/5.09092. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.96845/5.10152. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.96179/5.08253. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.96422/5.08070. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.95840/5.13410. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.97061/5.06675. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.96760/5.07630. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.97064/5.05510. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.97040/5.05690. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.97357/5.06323. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.96943/5.04880. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.95858/5.09424. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.96738/5.06440. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.96656/5.07501. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.96563/5.08128. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.96390/5.07091. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.96499/5.09107. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.96029/5.09395. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.96484/5.07100. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.96120/5.09471. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.96432/5.07825. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.96158/5.08799. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.95860/5.10243. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.95880/5.10323. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.96001/5.09482. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.95948/5.08292. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.95932/5.09281. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.96065/5.08783. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.95958/5.05635. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 4.95569/5.09528. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.96054/5.08352. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.95476/5.10652. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.95010/5.13054. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.98757/5.02012. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.99481/5.02594. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.98774/5.02086. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.98131/5.04062. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.0905982365507463\n",
      "Epoch 0, Loss(train/val) 5.05329/4.95135. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.99125/4.98771. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.99281/5.00374. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.99322/5.00541. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.99320/5.01053. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.99661/5.01617. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.98958/5.03595. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.99013/5.04773. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.99387/5.05889. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.99340/5.06483. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.99111/5.05088. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.98873/5.04881. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.98666/5.05063. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.99006/5.05683. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.99122/5.06039. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.98937/5.04884. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.98879/5.05723. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.98559/5.06517. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.98824/5.05720. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.98604/5.05844. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.98684/5.06037. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.98744/5.06483. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.98920/5.03871. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.98481/5.05552. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.98684/5.05977. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.98295/5.07957. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.98637/5.08604. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.98751/5.06090. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.98409/5.06109. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.98401/5.05149. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.98320/5.07672. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.98204/5.07553. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.98080/5.09255. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.97949/5.08837. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.97872/5.08848. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.98111/5.07251. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.98237/5.07391. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.98161/5.05435. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.97884/5.08456. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.98182/5.06520. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.98461/5.05851. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.98491/5.03553. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.98241/5.04492. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.98371/5.06007. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.98024/5.05931. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.98023/5.06465. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.97670/5.07983. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.97953/5.09924. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.98223/5.08618. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.97823/5.07502. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.97747/5.07034. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 4.97765/5.07457. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.97903/5.06586. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.97416/5.07439. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.97456/5.08981. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.98061/5.07180. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.97752/5.07301. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.97538/5.09625. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.97320/5.08784. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.97054/5.10266. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.97627/5.08904. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.96870/5.10032. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.97437/5.10449. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.97225/5.09005. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.97000/5.09518. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.97164/5.09349. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.97744/5.05814. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.98077/5.05792. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.98150/5.07295. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.98101/5.06148. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.97472/5.07902. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.97902/5.05993. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.97547/5.05320. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.97139/5.07695. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.97534/5.07322. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.97009/5.08158. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.97240/5.09797. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.97385/5.09766. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.97037/5.08507. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.96796/5.10515. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.96955/5.07300. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.98989/5.08692. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.98995/5.02867. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.98711/5.03535. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.98424/5.04708. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.98307/5.04716. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.98299/5.05489. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.98243/5.04860. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.98124/5.06702. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.98280/5.06168. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.98432/5.06272. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.97698/5.06044. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.97818/5.06638. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.97575/5.05229. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.98468/5.06185. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.97958/5.06849. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.97776/5.06256. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.97641/5.08304. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.97677/5.09910. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.98142/5.07531. Took 0.19 sec\n",
      "ACC: 0.65625, MCC: 0.31814238148788887\n",
      "Epoch 0, Loss(train/val) 4.80536/4.76233. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.77641/4.74870. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.78266/4.75937. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.76713/4.75412. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.75447/4.74604. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.75376/4.74942. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.75377/4.75754. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.75339/4.75775. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.75094/4.75861. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.74972/4.76215. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.75042/4.75462. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.75183/4.76601. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.74707/4.75965. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.75151/4.76926. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.74736/4.76176. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.75058/4.76016. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.74796/4.77323. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.74575/4.76400. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.74606/4.76129. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.75579/4.74673. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.75247/4.75103. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.75082/4.75114. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.74974/4.74212. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.74813/4.75459. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.74602/4.75304. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.74721/4.76302. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.74966/4.75950. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.74271/4.76925. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.74284/4.76818. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.74793/4.76522. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.74499/4.77019. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.74522/4.77106. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.74617/4.77404. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.74063/4.78722. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.74408/4.76451. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.74532/4.77358. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.73770/4.78921. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.74377/4.77835. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.74477/4.77566. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.74582/4.76467. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.74076/4.76680. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.74332/4.77628. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.74114/4.76669. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.73975/4.77239. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.74162/4.77437. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.73722/4.80210. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.73647/4.79494. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.73922/4.77215. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.74097/4.77667. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.73756/4.78587. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.74140/4.76445. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.74286/4.78622. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.73420/4.78688. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.73671/4.78298. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.73848/4.77249. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.73606/4.79450. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.74071/4.77200. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.73347/4.79634. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.73248/4.79273. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.73509/4.78693. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.73610/4.79973. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.73562/4.78316. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.73569/4.78717. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.73412/4.79434. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.73083/4.78597. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.73628/4.77435. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.73836/4.78256. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.73660/4.78496. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.72988/4.80294. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.73750/4.76560. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.73170/4.80292. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.73282/4.77555. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.73035/4.79008. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.72756/4.81617. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.74417/4.77470. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.73599/4.78013. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.73051/4.79851. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.73185/4.77557. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.72920/4.81416. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.72786/4.76556. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.73254/4.78575. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.73337/4.78194. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.73141/4.79715. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.73167/4.77225. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.72786/4.80508. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.73432/4.76821. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.73872/4.78029. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.72959/4.79585. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.72322/4.81318. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.73447/4.77509. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.73188/4.79362. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.73286/4.78785. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.72802/4.78969. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.72984/4.78956. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.73092/4.77397. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.73013/4.79914. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.72761/4.79230. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.73158/4.77780. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.72980/4.79376. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.72886/4.79197. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.16834512458535864\n",
      "Epoch 0, Loss(train/val) 4.96133/5.01091. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.94913/4.93716. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.93334/4.93900. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.93583/4.93428. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.93380/4.93464. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.93814/4.93256. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.93696/4.93052. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.93274/4.92898. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.93669/4.92792. Took 0.22 sec\n",
      "Epoch 9, Loss(train/val) 4.93354/4.92957. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.93217/4.92704. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.93003/4.92622. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.92871/4.92685. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.93022/4.92611. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.92973/4.92608. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.93468/4.92617. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.92859/4.92399. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.93323/4.92676. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.93147/4.92749. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.93105/4.92536. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.92897/4.92414. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.92980/4.92301. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.93045/4.92208. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.92815/4.92367. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.93072/4.92334. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.92886/4.92191. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.92616/4.92221. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.92706/4.91705. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.92806/4.91984. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.92875/4.91891. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.92591/4.92218. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.92945/4.91844. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.92613/4.91456. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.92583/4.91416. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.92607/4.91378. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.92352/4.90562. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.92492/4.91564. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.92485/4.91645. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.92421/4.92231. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.92432/4.91125. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.92055/4.90534. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.92246/4.91372. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.92355/4.90973. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.92109/4.90961. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.92251/4.91249. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.92245/4.91115. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.91920/4.91233. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.92037/4.90802. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.91811/4.91105. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.92110/4.91048. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.92027/4.91016. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.92045/4.91624. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.91679/4.91107. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.91801/4.90934. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.91757/4.90626. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.91636/4.90813. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.91907/4.91098. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.91808/4.91581. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.92237/4.91968. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.91740/4.90890. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.91764/4.91293. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.91567/4.90839. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.91395/4.91236. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.92003/4.90924. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.91816/4.91197. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.91588/4.91004. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.91400/4.90596. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.91417/4.91299. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.91949/4.91368. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.91173/4.91068. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.91543/4.90322. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.91463/4.90543. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.91142/4.91016. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.91282/4.91512. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.91818/4.91040. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.91372/4.91056. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.91518/4.91168. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.91770/4.90794. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.90979/4.90757. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.91237/4.92521. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.91469/4.92409. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.91356/4.91658. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.91792/4.90891. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.91131/4.92113. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.91293/4.91534. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.90850/4.90990. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.90932/4.91145. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.91042/4.92177. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.91001/4.91145. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.91275/4.90854. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.91274/4.91542. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.91607/4.91612. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.91277/4.91235. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.90625/4.91500. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.91024/4.90844. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.90995/4.90671. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.90396/4.92393. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.90935/4.90484. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.91096/4.90919. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.91592/4.91061. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.07100716024967263\n",
      "Epoch 0, Loss(train/val) 4.81932/4.76929. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.75943/4.81168. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.76672/4.81969. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.76606/4.80210. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.77002/4.77523. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.76447/4.76483. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.75965/4.76546. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.75645/4.76505. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.75822/4.76469. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.75939/4.76429. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.75885/4.76564. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.76023/4.76285. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.75562/4.76174. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.75494/4.76431. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.75692/4.76162. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.75623/4.75906. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.75172/4.75897. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.75400/4.76060. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.75349/4.75927. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.75228/4.75229. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.75235/4.76072. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.75417/4.75580. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.74808/4.75754. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.75173/4.75933. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.75136/4.75839. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.75053/4.75637. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.74834/4.75334. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.74873/4.75581. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.74801/4.75430. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.74932/4.76728. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.74933/4.76530. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.74290/4.77589. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.74447/4.77640. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.75365/4.75538. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.74648/4.77083. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.74509/4.76377. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.74541/4.76528. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.74465/4.75888. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.74254/4.75217. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.74372/4.75993. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.74104/4.77629. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.74198/4.76528. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.74065/4.76110. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.74475/4.75028. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.73843/4.77828. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.75066/4.77527. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.75197/4.75927. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.74383/4.77315. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.74760/4.77060. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.74744/4.75811. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.74207/4.76904. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.74800/4.77296. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.74518/4.76231. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.74515/4.76161. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.74455/4.77173. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.74072/4.76483. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.74179/4.75477. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.73894/4.78352. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.73844/4.77306. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.74222/4.76017. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.74056/4.76108. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.73733/4.76911. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.73837/4.75912. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.73465/4.75275. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.73988/4.75969. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.73956/4.77445. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.73892/4.75620. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.73619/4.75830. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.73805/4.76089. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.73722/4.75285. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.74007/4.74652. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.73602/4.75952. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.73571/4.75321. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.73957/4.75477. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.73370/4.75043. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.73208/4.74816. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.73579/4.75099. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.73883/4.75395. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.73201/4.75968. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.73421/4.75579. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.73407/4.75283. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.72952/4.74911. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.72959/4.74987. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.73308/4.73542. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.72944/4.75401. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.73158/4.74852. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.72866/4.75653. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.73450/4.74855. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.73468/4.74961. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.72989/4.75662. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.72828/4.74823. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.72701/4.75143. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.72998/4.74888. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.72756/4.75135. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.72484/4.74811. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.73191/4.74070. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.72746/4.78829. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.73954/4.75789. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.72939/4.74382. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.72849/4.75244. Took 0.19 sec\n",
      "ACC: 0.5, MCC: -0.02404930351219409\n",
      "Epoch 0, Loss(train/val) 5.00504/4.90760. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.91695/4.89733. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.90368/4.89621. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.90313/4.89764. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.90502/4.89667. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.89999/4.89662. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.90091/4.89816. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.90696/4.90112. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.90416/4.89973. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.90199/4.89521. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.90135/4.89433. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.89844/4.89270. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.89953/4.89426. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.89894/4.89510. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.90057/4.89933. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.89915/4.89955. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.89492/4.88497. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.89840/4.87943. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 4.89995/4.90016. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.89373/4.89481. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.89568/4.89408. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.89441/4.89843. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.89797/4.87764. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.89890/4.87942. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.89319/4.87551. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.89548/4.87806. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.89190/4.87678. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.89399/4.87353. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.89478/4.93060. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.90159/4.90500. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.89298/4.89163. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.89328/4.86867. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.89177/4.90502. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.89267/4.89449. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.89153/4.89562. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.88897/4.90153. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.88969/4.89813. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.88636/4.90645. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89260/4.90197. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.89058/4.90992. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.88987/4.89249. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.88826/4.89993. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89128/4.91513. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.89153/4.90433. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.88988/4.90243. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.88533/4.90283. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.88271/4.90516. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89011/4.87094. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89517/4.86640. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.89851/4.87106. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89518/4.88565. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.88999/4.88807. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.88840/4.88679. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.88427/4.90236. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.88773/4.90523. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.88576/4.88876. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.88348/4.87987. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.88747/4.88828. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.88349/4.88453. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.88581/4.89356. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.88619/4.89478. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.88474/4.88914. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.88390/4.89485. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.87978/4.88710. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.88939/4.89820. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.88407/4.89423. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.88346/4.90087. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.88706/4.89798. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.88183/4.89589. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.88203/4.89661. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.88367/4.89065. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.88240/4.88576. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.88245/4.89429. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88488/4.90623. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.88281/4.89228. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.88415/4.90501. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.88110/4.89425. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.88140/4.91975. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.88340/4.89639. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.87671/4.89918. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88249/4.91233. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.88135/4.89167. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.88137/4.90149. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.88283/4.89575. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88038/4.90457. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.88144/4.91065. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.87901/4.90504. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.88025/4.91519. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.88114/4.90324. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.87718/4.90226. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.87800/4.90791. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.87658/4.90133. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 4.87452/4.90359. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.87816/4.91519. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88224/4.90529. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.87577/4.89065. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.87503/4.88621. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.87896/4.91181. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.88398/4.89891. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.88331/4.90488. Took 0.19 sec\n",
      "ACC: 0.625, MCC: 0.24932120796616944\n",
      "Epoch 0, Loss(train/val) 4.99155/4.99599. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.92342/4.92641. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.94038/4.90978. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.92935/4.90888. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.91789/4.90879. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.91577/4.90841. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.92057/4.90449. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.91977/4.90586. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.91709/4.90591. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.91470/4.90399. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.91322/4.89928. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.91328/4.89747. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 4.91381/4.90150. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.91323/4.90553. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.90951/4.90162. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.91137/4.90324. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90883/4.89799. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.90967/4.89719. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.91113/4.89746. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.91109/4.90489. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.90640/4.90177. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.90565/4.89508. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.90733/4.90275. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.91053/4.89350. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.90908/4.90256. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.90506/4.90017. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.90529/4.90189. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.90056/4.90312. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90374/4.90578. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.90282/4.90341. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.89869/4.90704. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.90024/4.91064. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.90114/4.90430. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.89673/4.91657. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.89588/4.91450. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.90271/4.91781. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.89927/4.91876. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.89740/4.92061. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89284/4.92725. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.89563/4.91562. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.89789/4.91816. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.89470/4.91841. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89727/4.92473. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.89241/4.93507. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.89462/4.92575. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.89534/4.94177. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.89871/4.92674. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.88791/4.93883. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89452/4.94373. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.89204/4.95100. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.88831/4.95100. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.88663/4.94805. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.88743/4.95391. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.89086/4.95549. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.88707/4.94856. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.89018/4.96055. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.89211/4.95761. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.88697/4.96532. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.88730/4.96352. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.88696/4.94166. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.88112/4.97133. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.88808/4.96059. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.87600/4.95608. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.87607/4.98250. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.88725/4.96665. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.89138/4.96690. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.87924/4.97722. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.87797/4.98066. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.88223/4.96454. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.87654/4.97217. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.87900/4.98640. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.88024/4.97890. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.87547/4.99727. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88021/4.97042. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.89331/4.96406. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.87639/4.99646. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.87486/4.98661. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 4.88194/4.95361. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.86806/4.98635. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.88152/4.94907. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88320/4.96838. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.87819/4.96952. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.87618/4.96829. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.87373/4.97000. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.87492/4.98071. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.87890/4.98149. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.87652/4.96177. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.86993/4.98803. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.87558/4.97568. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.87158/4.97957. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.87251/4.96673. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.87091/4.98100. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.87047/4.97284. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.87334/4.98843. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.87350/4.98317. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.86280/5.00870. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.87259/4.99474. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.86969/5.02254. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.86829/4.99356. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.86383/4.98389. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.1513518081969605\n",
      "Epoch 0, Loss(train/val) 4.93889/4.99363. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.90431/4.90202. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.90181/4.90031. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.90617/4.92035. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.90734/4.90802. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.89727/4.89751. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.89429/4.90868. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.89280/4.91629. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.89304/4.92787. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.89072/4.91181. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.88991/4.91358. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.88941/4.90968. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.89029/4.90791. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.88771/4.92390. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.88946/4.90888. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.88646/4.91762. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.88697/4.91703. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.88572/4.91635. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.88247/4.92682. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.88959/4.91814. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.88603/4.92705. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.88682/4.91966. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.88282/4.92564. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.88480/4.91114. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.88115/4.92955. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.88824/4.91078. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.88266/4.92335. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.88572/4.89562. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.89188/4.89309. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.89244/4.89643. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.88890/4.90208. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.89501/4.89748. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.89154/4.89552. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.89108/4.89781. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.89099/4.89803. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.89286/4.89884. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.89113/4.89991. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.89230/4.90330. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89212/4.90275. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.88759/4.90378. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.88767/4.90372. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.88731/4.90837. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.88887/4.90510. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.88876/4.90832. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.88531/4.92152. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.88348/4.90664. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 4.88317/4.90726. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.88138/4.92263. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.88330/4.90041. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.88934/4.89970. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.88580/4.90296. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.88560/4.90584. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.88393/4.90895. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.88525/4.91313. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.88443/4.91245. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.88098/4.91691. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.88434/4.90898. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.88323/4.90787. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.88353/4.91346. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.88349/4.92296. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.88076/4.91524. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.88407/4.90840. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.88385/4.90351. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.87955/4.91358. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.87983/4.90679. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.87595/4.90619. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.87805/4.90968. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.87825/4.90813. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.87711/4.91810. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.87650/4.91471. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.87689/4.91390. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.87583/4.90966. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.87553/4.91263. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.87624/4.91850. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.87561/4.90764. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.87809/4.90873. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.87923/4.91347. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.87137/4.90837. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.87580/4.91487. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.87118/4.91310. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.87258/4.91299. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.87089/4.92386. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.87481/4.90918. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.87245/4.91687. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.87498/4.91835. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.87087/4.92817. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.87388/4.92576. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.88221/4.91273. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.87427/4.91503. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.87296/4.91137. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.87426/4.91234. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.86766/4.91622. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.87272/4.90590. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.87263/4.92959. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.86879/4.94415. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.87121/4.92039. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.87094/4.91009. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.87352/4.91730. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.86823/4.91063. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.87242/4.90369. Took 0.19 sec\n",
      "ACC: 0.65625, MCC: 0.31697987101666647\n",
      "Epoch 0, Loss(train/val) 4.99578/4.89768. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.89433/4.90853. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.88884/4.91454. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.88922/4.91801. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.88613/4.91809. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.88608/4.91523. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.88219/4.91627. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.88111/4.91409. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.88320/4.91817. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.88325/4.92103. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.87553/4.92048. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.87662/4.92060. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.87798/4.92544. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.87281/4.92750. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87165/4.91832. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.87608/4.93387. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.87371/4.92020. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.87678/4.92542. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.87203/4.94532. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.87295/4.93771. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.86996/4.94361. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.87168/4.95220. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.86456/4.93730. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.87087/4.93564. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.87419/4.94077. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.86726/4.93548. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.86915/4.95007. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.86864/4.94146. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.86396/4.94444. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.86712/4.94078. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.86563/4.93767. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.86641/4.95628. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.86842/4.94431. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.86650/4.93920. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.86301/4.94657. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.86340/4.94989. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.86076/4.95005. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.86147/4.94888. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.86035/4.94370. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.85905/4.95006. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.86431/4.94728. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.86219/4.94733. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.86726/4.95501. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.85796/4.95157. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.86131/4.96346. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.86262/4.94804. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.85839/4.96325. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.85560/4.96439. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.85571/4.97567. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.86199/4.95554. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.85977/4.95911. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.85718/4.96202. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.86183/4.95561. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.86107/4.95644. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.85769/4.96659. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.85050/4.97427. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.85849/4.96139. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.85597/4.96463. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.85469/4.96234. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.85631/4.96454. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85454/4.96092. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.85693/4.96090. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85004/4.96197. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.86470/4.95094. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.86050/4.96145. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.86398/4.95645. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.86340/4.95090. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.86173/4.94907. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.85885/4.94671. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.86120/4.95108. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.85775/4.95594. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.85612/4.95624. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.85275/4.95686. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.86021/4.94482. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.86249/4.97746. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.86555/4.93448. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.85722/4.95264. Took 0.23 sec\n",
      "Epoch 77, Loss(train/val) 4.85974/4.95038. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.85244/4.96301. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.85378/4.95436. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.85880/4.95507. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.85598/4.95104. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.85965/4.94827. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.86128/4.93878. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.85539/4.94871. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.85889/4.94431. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.85522/4.95131. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.85064/4.95634. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.85369/4.96129. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.85953/4.94243. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.85501/4.94443. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85203/4.95099. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.86174/4.95235. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.84977/4.96656. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.85740/4.94754. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.85488/4.95814. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.85255/4.97165. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.84744/4.96507. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.84651/4.96556. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.85634/4.96170. Took 0.19 sec\n",
      "ACC: 0.625, MCC: 0.2135744251723958\n",
      "Epoch 0, Loss(train/val) 5.03734/4.95953. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.93822/4.94116. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.94715/4.93683. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.94563/4.94293. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.94122/4.94580. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.93910/4.94996. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.93930/4.95217. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.93839/4.95282. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.93921/4.95634. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.93523/4.94936. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.93910/4.93906. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.93590/4.94440. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.93347/4.94483. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.93957/4.94703. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.93456/4.95382. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.93527/4.95639. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.93496/4.95634. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.93338/4.95627. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.93130/4.96181. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.93266/4.96398. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.93293/4.95985. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.93059/4.95815. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.93072/4.95615. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.93008/4.95929. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.93079/4.95837. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.92861/4.96614. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.92765/4.96804. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.92813/4.95851. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.92758/4.97306. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.92794/4.96128. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.92252/4.95967. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.92729/4.92109. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.93394/4.94209. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.92884/4.94510. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.92523/4.95411. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.92560/4.96508. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.92375/4.95676. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.92299/4.96697. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.92381/4.96554. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.92391/4.96460. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.91761/4.96920. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 4.92188/4.98265. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.92138/4.95560. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.91875/4.98518. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.91673/4.97124. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.92082/4.97105. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.91899/4.96576. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.90999/4.98614. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.92016/4.95213. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.91243/4.97908. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.91956/4.96403. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.91623/4.98535. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.91434/4.97106. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.91122/4.98323. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.91441/4.98096. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.91098/4.98976. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.90759/4.98567. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.91109/4.98612. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.90801/4.98395. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.90913/4.98230. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.90447/4.98315. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.90526/4.99168. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.90569/5.00543. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.91366/4.98248. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.90942/4.98096. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.91077/4.97365. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.91100/4.99073. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.90624/4.98301. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.90566/4.98934. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.91073/4.99447. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.90762/4.99638. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.90321/5.00584. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.90022/4.99108. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.90601/4.99355. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.90972/4.99180. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.90567/4.98708. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.89647/5.01466. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.90552/5.00211. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.90425/4.99466. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.90199/5.00205. Took 0.21 sec\n",
      "Epoch 80, Loss(train/val) 4.89819/5.01455. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.89866/5.00967. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.90096/5.00007. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.90242/4.99448. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.89817/4.99910. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.90445/4.99817. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.89764/5.00573. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.89418/5.01715. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.89957/4.99437. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.90352/5.00181. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.89604/5.00257. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.90400/5.00039. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.89799/5.01302. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.90364/5.00891. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.90042/4.99775. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.90136/5.01749. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.90299/5.01476. Took 0.21 sec\n",
      "Epoch 97, Loss(train/val) 4.90245/4.98671. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.91981/4.98958. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.90702/4.99556. Took 0.20 sec\n",
      "ACC: 0.4375, MCC: -0.1341246360271594\n",
      "Epoch 0, Loss(train/val) 5.11623/5.05963. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.05046/5.05059. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.04926/5.05872. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.06149/5.05309. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.06037/5.04692. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.05678/5.04768. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.04964/5.04680. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 5.04680/5.04788. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.04677/5.04942. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.04736/5.05028. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 5.04654/5.05073. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.04595/5.04991. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.04513/5.05130. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.04669/5.05209. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.04263/5.05254. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.04209/5.05332. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.04127/5.05626. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.04072/5.05782. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.04217/5.05601. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 5.03573/5.06165. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.04152/5.06049. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.03921/5.06704. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 5.03946/5.05772. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 5.03795/5.06505. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.03785/5.07227. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.03336/5.07217. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.03541/5.07510. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 5.03624/5.07958. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.03291/5.08749. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.03586/5.07799. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.03289/5.08033. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 5.02869/5.09148. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.03311/5.08293. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 5.03259/5.08690. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 5.03161/5.09178. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.03316/5.08174. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.02663/5.09963. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.03325/5.05960. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 5.03076/5.07391. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 5.02728/5.08669. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 5.02642/5.11038. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.02916/5.07774. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.02951/5.08344. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.02747/5.07702. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 5.02318/5.10036. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.02610/5.09173. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.02603/5.07961. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.02866/5.08273. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.02131/5.09790. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 5.02093/5.09844. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.02385/5.08596. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 5.01713/5.10140. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.01933/5.09849. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 5.02616/5.07726. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.01515/5.10273. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 5.01855/5.10469. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 5.02229/5.08379. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.02695/5.08785. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 5.01969/5.07314. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.01881/5.08681. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.01356/5.11002. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.01865/5.11561. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.01943/5.06669. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.02103/5.07918. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.01915/5.10517. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.01681/5.10463. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.01887/5.08615. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.01661/5.11886. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 5.01743/5.08680. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.02161/5.06081. Took 0.21 sec\n",
      "Epoch 70, Loss(train/val) 5.00945/5.11482. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 5.02151/5.07458. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.01289/5.12031. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 5.01701/5.07299. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.01274/5.11148. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.01661/5.11046. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.01198/5.10772. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.00930/5.14046. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.00787/5.07914. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 5.03074/5.02726. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 5.03217/5.08214. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 5.01806/5.13071. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.00779/5.11078. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 5.01264/5.09048. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.00784/5.07184. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.00686/5.14500. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.01154/5.08354. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 5.00447/5.12605. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.00992/5.10957. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.00020/5.13605. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.00806/5.13931. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.00997/5.09035. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.00152/5.15934. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.00729/5.13473. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.00962/5.08107. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.00723/5.11936. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.00908/5.08043. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.00089/5.12568. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.00542/5.14342. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.00560/5.10003. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.1241446725317693\n",
      "Epoch 0, Loss(train/val) 4.84976/4.80975. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.81452/4.79987. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.80927/4.80331. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.80651/4.80506. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.80958/4.80653. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.80381/4.81085. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.80520/4.81223. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.80605/4.81289. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.80502/4.81329. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.80303/4.81464. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.80083/4.81412. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.80153/4.80796. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.80103/4.81625. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.80230/4.80990. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.80137/4.81576. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.79913/4.81572. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.79876/4.82105. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.79889/4.82499. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.79834/4.82839. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.79773/4.82030. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.79745/4.82532. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.79451/4.82963. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.79397/4.83371. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.79407/4.84338. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.78990/4.84941. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.79149/4.85026. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.78750/4.84716. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78632/4.85871. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78883/4.83066. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.79592/4.82618. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.78347/4.84436. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.78718/4.82234. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.78769/4.84196. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.78268/4.84148. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.78530/4.86848. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.78092/4.85506. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78508/4.83605. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.77953/4.86957. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.78494/4.85902. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.78335/4.85987. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.78045/4.88342. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.78185/4.84522. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.78099/4.86683. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.77612/4.88349. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77779/4.84739. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.78174/4.86247. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 4.77494/4.86346. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.77896/4.86235. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.77454/4.88568. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77720/4.86756. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 4.77190/4.89655. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.77723/4.88480. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.77994/4.86088. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.78088/4.86449. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.77031/4.90938. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.77284/4.86135. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.76698/4.89274. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.77698/4.86157. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.77090/4.88705. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.77038/4.89077. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.77558/4.85923. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.77018/4.89489. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.77007/4.90690. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.77564/4.87264. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76550/4.91611. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.76911/4.89372. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.77147/4.89186. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.77094/4.90091. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.76890/4.88505. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.76616/4.90873. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.77059/4.88431. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.76827/4.90336. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.76228/4.89495. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.76823/4.89845. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.76617/4.87175. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 4.76959/4.87770. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.76382/4.88426. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.76289/4.88189. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.76345/4.89812. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.77611/4.89894. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.77480/4.90275. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.76627/4.90968. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.76761/4.89718. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.76275/4.89543. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.76540/4.92582. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.77424/4.88821. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.75868/4.93078. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.77211/4.89281. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.76798/4.89163. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.76525/4.90258. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.77059/4.88714. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.76943/4.91031. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.77498/4.88773. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.76857/4.91574. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77216/4.90398. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.77248/4.91531. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.77059/4.90476. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.76110/4.92732. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.76181/4.92376. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.76282/4.92404. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.125\n",
      "Epoch 0, Loss(train/val) 4.85238/4.77008. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.76093/4.76863. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.75542/4.77373. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.75259/4.78295. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.75616/4.78787. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.75513/4.78869. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.75259/4.78816. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.75345/4.78967. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.75305/4.79083. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.75126/4.79691. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.75209/4.80154. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.75076/4.79944. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.74801/4.79947. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.75035/4.80139. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.74769/4.80656. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.74804/4.80920. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.74526/4.81087. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.74648/4.80823. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.74509/4.80394. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.74593/4.81125. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.74743/4.80524. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.74573/4.80724. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.74440/4.80990. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.74279/4.80433. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.74383/4.80775. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.74389/4.81054. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.74300/4.81046. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.74117/4.81631. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.74156/4.81724. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.74143/4.82168. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.74045/4.81328. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.74257/4.81266. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.73893/4.81594. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.73737/4.83092. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.73674/4.82844. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.74149/4.82523. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.74063/4.81883. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.73712/4.82517. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.73592/4.82178. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.73413/4.83893. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.73672/4.83034. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.73550/4.83015. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.73210/4.84721. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.73563/4.82931. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.73940/4.81633. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.73151/4.83782. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.73355/4.83435. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.73828/4.80797. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.73676/4.81082. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.73453/4.82818. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.72860/4.83785. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.73288/4.82791. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.72972/4.84476. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.73368/4.82333. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.73491/4.82244. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.73537/4.83124. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.74531/4.80781. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.73437/4.82252. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.73353/4.83744. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.72941/4.83012. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.72818/4.84293. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.73527/4.83556. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.73496/4.80295. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.73611/4.80394. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.73602/4.81785. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.73420/4.80572. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.73445/4.80883. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.73765/4.78848. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.73299/4.79340. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.73418/4.83340. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.73591/4.82910. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.72992/4.83274. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.72844/4.83948. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.72727/4.84621. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.72982/4.84106. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.73367/4.83270. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.73012/4.83271. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.72988/4.84782. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.73025/4.84992. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.73073/4.82909. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.72685/4.84679. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.72838/4.83396. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.72792/4.85240. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.73058/4.82979. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.72217/4.85868. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.72671/4.84924. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.72523/4.85738. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.73043/4.82295. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.72505/4.85314. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.72553/4.85909. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.72067/4.87137. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.72910/4.83741. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.73113/4.84073. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.72834/4.84731. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.72681/4.84683. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.72298/4.84659. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.72764/4.83622. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.72762/4.84531. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.72727/4.84200. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.72378/4.85302. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.040082172520323485\n",
      "Epoch 0, Loss(train/val) 4.49854/4.49495. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.49401/4.48859. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.48437/4.47566. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.48684/4.47423. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.48440/4.48605. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.47704/4.48272. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.47397/4.48125. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.47198/4.49000. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.47429/4.48602. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.47375/4.47969. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.46984/4.48112. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.47441/4.48353. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.47386/4.48345. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.46962/4.48045. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.47178/4.48258. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.47052/4.48540. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.46586/4.49367. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.46920/4.48112. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.47047/4.48389. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.47002/4.48704. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.46489/4.48711. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.46670/4.48607. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.46917/4.48210. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.46714/4.48636. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.46453/4.49181. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.46845/4.49237. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.47147/4.48174. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.46440/4.48090. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.46653/4.49334. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.46518/4.47619. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.46417/4.48398. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.46180/4.47967. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.46401/4.47728. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 4.46273/4.47438. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.46175/4.48440. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.46361/4.48558. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.46103/4.48080. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.46105/4.48066. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.45919/4.48057. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.46023/4.48081. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.46115/4.47042. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.45865/4.47901. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.45958/4.48130. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.45524/4.48746. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.45840/4.47453. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.45902/4.48190. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.45868/4.48997. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.45595/4.47557. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.45541/4.47552. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.45845/4.47806. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.45406/4.48699. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.45466/4.48956. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.45719/4.48862. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.45541/4.48294. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.45138/4.48044. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.45737/4.48265. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.45469/4.49187. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.45272/4.49032. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.44825/4.47809. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.45208/4.49363. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.45395/4.48203. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.45621/4.48913. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.45640/4.49135. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.45297/4.48668. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.45226/4.49188. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.44903/4.52116. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.45137/4.48837. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.45149/4.47813. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.44990/4.48065. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.45077/4.47178. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.45020/4.49629. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.44830/4.51593. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.44663/4.52788. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.45058/4.46996. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.44310/4.50821. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.44723/4.49664. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.45096/4.48695. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.45146/4.48269. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.45033/4.48061. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.44476/4.47529. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.44246/4.48830. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.45245/4.50126. Took 0.21 sec\n",
      "Epoch 82, Loss(train/val) 4.44895/4.48718. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.44651/4.48858. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.44760/4.47392. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.44227/4.50393. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.44715/4.50257. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.44452/4.48511. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.44760/4.42427. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.46292/4.45612. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.45813/4.44395. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.45333/4.46469. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.45703/4.47335. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.45447/4.46740. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.44982/4.47249. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.44950/4.47674. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.44787/4.46674. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.44510/4.46977. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.45006/4.48373. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.44889/4.47793. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.06859245370246428\n",
      "Epoch 0, Loss(train/val) 4.84725/4.83570. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.82358/4.81968. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.81730/4.81727. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.81715/4.81841. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.81735/4.81170. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.81472/4.79994. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.81269/4.79811. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.80825/4.79458. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.80391/4.80000. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.80813/4.80084. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.80989/4.80173. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.80882/4.79667. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.80432/4.79752. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.80002/4.80059. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.80391/4.80811. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.80681/4.80067. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.80378/4.80248. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.80041/4.80782. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.79784/4.80708. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.80299/4.81040. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.79980/4.80391. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.79950/4.81261. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.80172/4.81233. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.80288/4.78830. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.80304/4.79544. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.79991/4.79981. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.79369/4.80544. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.79626/4.80453. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.80878/4.79843. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.79699/4.79617. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.79332/4.80335. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.79428/4.80121. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.79588/4.80371. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.79537/4.80301. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.79464/4.80193. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.79505/4.81475. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78909/4.82147. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.79393/4.81494. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.79474/4.83175. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.80023/4.81516. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.79655/4.80864. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.79454/4.80251. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.79408/4.81004. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.79143/4.81000. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.78962/4.80412. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.79443/4.80750. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.79246/4.81571. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.78819/4.80828. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.78543/4.81202. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.79325/4.80602. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.79069/4.80505. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.78786/4.81322. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.79325/4.79183. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.78999/4.80778. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.79040/4.81307. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.79290/4.80061. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.78820/4.81746. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.79168/4.79593. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.79377/4.80290. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.78989/4.78739. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.79156/4.79893. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.79541/4.80000. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.78788/4.80601. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.78480/4.80780. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.78595/4.81371. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.78896/4.80168. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.79060/4.83153. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.79388/4.79638. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.78565/4.80580. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.78922/4.79854. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.79032/4.80397. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.78558/4.81623. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.78070/4.80455. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.78686/4.80692. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77886/4.81484. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.79056/4.81867. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.79839/4.81980. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.79277/4.82255. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.79369/4.81565. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.79275/4.80946. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.79706/4.81219. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.79062/4.81437. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.78425/4.81244. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.79455/4.80693. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.78771/4.80718. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.79195/4.81377. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.79327/4.80559. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.79180/4.80157. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.78681/4.80666. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.78630/4.82070. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.79361/4.80421. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.79110/4.81838. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.78877/4.81182. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.79238/4.81777. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.78258/4.82083. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79205/4.80130. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.79168/4.81389. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.78354/4.80913. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.78429/4.79857. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.78833/4.81344. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.08952126702661474\n",
      "Epoch 0, Loss(train/val) 5.00141/4.91521. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.93220/4.91891. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.93548/4.91851. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 4.94006/4.93222. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 4.94185/4.96089. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.93637/4.94974. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 4.93036/4.93603. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.92547/4.94001. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 4.92881/4.94632. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.92978/4.95074. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.92747/4.95264. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.92601/4.95341. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.92413/4.95373. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.92383/4.95742. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.92158/4.96468. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.92002/4.96507. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.91977/4.95777. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.91875/4.96022. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.91489/4.95149. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.91265/4.95824. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.91215/4.95349. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.91240/4.95618. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.90983/4.94950. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.91104/4.95916. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.90768/4.95566. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.90899/4.95277. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.90580/4.94955. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.90813/4.95466. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90499/4.95579. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.90186/4.94662. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90150/4.95363. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.90404/4.95184. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.90520/4.95869. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.90010/4.96052. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.90145/4.95972. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90406/4.95413. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.89644/4.95807. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.89595/4.96071. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89975/4.95416. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90088/4.96494. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90029/4.95557. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.89707/4.95846. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89242/4.96228. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.89754/4.96458. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.89972/4.94976. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.89375/4.95749. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.89065/4.97428. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.90034/4.95495. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89243/4.95958. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.89268/4.96388. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89433/4.96682. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.89536/4.95964. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89146/4.95636. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.89533/4.95592. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.89264/4.96632. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.89876/4.95551. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.89559/4.96120. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.89530/4.96371. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.88861/4.97560. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.88843/4.98290. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.89385/4.96187. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.89354/4.95793. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.88971/4.96436. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.88936/4.98075. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.88405/4.96960. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.89088/4.94785. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.89090/4.96668. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.88455/4.96179. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.89002/4.95484. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.88832/4.95935. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.89318/4.94405. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.88944/4.96851. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.88646/4.95909. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88870/4.96864. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.88714/4.95294. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.89311/4.95216. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.88395/4.96238. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.88821/4.96158. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.88995/4.96870. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.88867/4.95088. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88265/4.97675. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.88964/4.95872. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.88708/4.96534. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.88391/4.96657. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88576/4.97482. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.88607/4.98189. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.88495/4.94554. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.88891/4.98140. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.88635/4.95824. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.88889/4.96106. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.89418/4.96127. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.88862/4.97082. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.88477/4.96934. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.88087/4.96973. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88255/4.97946. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.88471/4.98342. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.88717/4.95926. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.88283/4.97966. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.88815/4.95658. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88271/4.97094. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.1158974358974359\n",
      "Epoch 0, Loss(train/val) 5.08203/4.96637. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.95227/4.93677. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.95415/4.93990. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.94737/4.94221. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.95164/4.94065. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.94586/4.93910. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.94982/4.93926. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.94760/4.94058. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.94535/4.93997. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.94353/4.94045. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.94408/4.94153. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.94296/4.94159. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 4.93929/4.93889. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.93959/4.94095. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.93946/4.93198. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.93762/4.92767. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.94294/4.92966. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.93563/4.92674. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.93899/4.92986. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.93481/4.93224. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.93609/4.92907. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.93321/4.93422. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.93306/4.92982. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.93356/4.93569. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.93320/4.93694. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.93356/4.94337. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.93413/4.94436. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.93184/4.94566. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.92788/4.94235. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.93139/4.94410. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.92873/4.93953. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.92873/4.94162. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.92814/4.94351. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.92904/4.94159. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.92835/4.94111. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.92488/4.93877. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.92971/4.94494. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.92557/4.94556. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.92884/4.93956. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.92783/4.94144. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.92457/4.94411. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.92650/4.94417. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.92532/4.94416. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.92592/4.94190. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.92784/4.94737. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.92190/4.93946. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.92349/4.94502. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.92650/4.94073. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.92354/4.94263. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.92378/4.94377. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.91761/4.94703. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.92171/4.94750. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.92507/4.94576. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.92185/4.94501. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.92087/4.94033. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.92004/4.94432. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.92107/4.94403. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.91487/4.94854. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.92241/4.94286. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.91839/4.94551. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.91730/4.94862. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.92224/4.94638. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.92045/4.94798. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.92294/4.94735. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.91570/4.95204. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.91775/4.94794. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.91286/4.95027. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.92262/4.94354. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.91643/4.94484. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.91738/4.94898. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.91767/4.94721. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.91674/4.94390. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.91560/4.93987. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.91527/4.94858. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.91176/4.95035. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.91390/4.93842. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.91379/4.94482. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.91557/4.95133. Took 0.21 sec\n",
      "Epoch 78, Loss(train/val) 4.91478/4.95613. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.91421/4.94650. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.91352/4.95258. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.91047/4.95194. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.91525/4.94817. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.91277/4.94786. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.91512/4.94068. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.91252/4.94210. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.91015/4.94578. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.90573/4.94147. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.91211/4.93763. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.91115/4.94596. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.90834/4.93848. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.91235/4.94519. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.91690/4.94104. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.90428/4.93830. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.90815/4.93344. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.90597/4.94355. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.91317/4.95503. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.91017/4.94902. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.91055/4.94060. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.91172/4.93989. Took 0.19 sec\n",
      "ACC: 0.4375, MCC: -0.12267022872162016\n",
      "Epoch 0, Loss(train/val) 5.04460/5.02327. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.98574/4.99283. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.97907/4.98060. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.97478/4.97503. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.97634/4.97554. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.97811/4.97613. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.97176/4.97648. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.97742/4.97599. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.97678/4.97425. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.97719/4.97740. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.97564/4.97569. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.97634/4.97947. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.97515/4.98224. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.97264/4.97982. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.97117/4.98622. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.97221/4.98593. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.96964/4.98784. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.97423/4.99694. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.97286/4.99291. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.97298/4.99307. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.97341/4.98784. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.96846/4.98549. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.97405/4.98621. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.97103/4.97937. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.97012/4.99465. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.97027/4.98076. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.96970/4.99942. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.96728/4.99670. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.97021/4.99468. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.96867/5.00344. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.96692/4.99884. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.96600/4.99649. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.96445/5.00409. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.96679/4.98844. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.96739/4.99992. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.96859/5.00363. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.96631/4.98570. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.96181/5.00887. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.96629/5.00001. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.96617/5.00528. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.96142/5.00783. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.96612/5.01331. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.96155/4.98772. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.95840/5.01655. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.96355/5.01704. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.96640/4.98912. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.96482/4.98918. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.96869/4.98373. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.96190/4.99723. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.96551/5.00135. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.96053/4.99494. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.95895/5.00001. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.95859/5.01369. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.95409/5.02368. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.95329/5.01838. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.96633/4.97978. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.97317/4.97956. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.96324/4.98829. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.95836/5.02025. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.96017/5.03086. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.95210/5.03154. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.95448/5.03444. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.95409/5.03524. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.95574/5.02222. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.95683/5.02291. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.94931/5.06167. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.95163/5.06030. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.95609/5.01010. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.95693/5.03012. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.94778/5.03992. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.95504/5.03015. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.95229/5.03752. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.95400/5.03337. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.95125/5.02884. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.95591/5.02237. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.95936/5.01751. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.95727/5.01338. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.95439/5.04928. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.95453/5.02806. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.95869/5.02240. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.95479/5.02779. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.95126/5.03644. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.95205/5.04951. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.95520/5.01893. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.95164/5.03789. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.94686/5.04133. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.94774/5.04559. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.95420/5.01952. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.95082/5.04410. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.94985/5.03319. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.94987/5.05845. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.94596/5.03539. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.94606/5.02720. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.95273/5.05089. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.94714/5.04261. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.95001/5.03828. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.95156/5.01941. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.94681/5.04709. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.95383/4.99965. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.94471/5.02680. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.043224813349211175\n",
      "Epoch 0, Loss(train/val) 4.63470/4.63680. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.61230/4.62560. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.60889/4.61723. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.60994/4.61527. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.61185/4.61254. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.61129/4.61051. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.60914/4.61079. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.60622/4.61437. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.60444/4.61699. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.60491/4.61686. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.60367/4.61569. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.60382/4.61516. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.60482/4.61655. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.60536/4.61850. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.60011/4.62130. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.60154/4.62610. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.59891/4.62961. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.60343/4.62688. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.60113/4.63211. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.59736/4.64203. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.60011/4.63598. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.59826/4.63707. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.60149/4.64079. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.59807/4.64171. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.59502/4.63427. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.59545/4.63625. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.59940/4.63594. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.59157/4.64218. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.59207/4.65384. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.59354/4.65196. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.59482/4.63077. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.59529/4.64265. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.59414/4.65280. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.58843/4.65505. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.59164/4.64977. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.58732/4.65846. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.58363/4.67171. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.58908/4.66439. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.58628/4.66385. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.58546/4.67855. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.58540/4.65216. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.59639/4.62670. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.59476/4.63048. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.59315/4.63945. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 4.59198/4.63991. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.59188/4.64502. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.59383/4.64283. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.59137/4.65091. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.58291/4.66872. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.58912/4.65340. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.58516/4.66828. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.58454/4.66659. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.58564/4.67382. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.58389/4.67198. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.58073/4.67032. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.58332/4.67247. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.58418/4.67072. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.58520/4.67506. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.58446/4.68695. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.57986/4.67006. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.58453/4.67736. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.59024/4.63703. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.59215/4.63983. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.58532/4.65046. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.59064/4.65316. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.58224/4.65458. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.58216/4.66159. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.58718/4.66170. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.58198/4.65792. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.58366/4.64895. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.58557/4.65905. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.58261/4.66179. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.58306/4.68178. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.58372/4.65199. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.58362/4.65982. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.58281/4.66006. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.58183/4.67123. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.58421/4.66477. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.57802/4.66577. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.58157/4.67538. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.57787/4.66712. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.58189/4.66672. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.57850/4.67676. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.57720/4.67507. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.58051/4.68094. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.58086/4.67431. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.57875/4.67179. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.57758/4.67061. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.57688/4.67453. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.58178/4.67051. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.58003/4.67823. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.58106/4.66844. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.57797/4.66762. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.57703/4.66758. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.57507/4.68668. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.57444/4.68357. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.57747/4.67107. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.57879/4.67205. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.57410/4.67872. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.57629/4.66624. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.12725695259515554\n",
      "Epoch 0, Loss(train/val) 4.89292/4.88014. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.86975/4.86137. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.86202/4.85717. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.86002/4.85816. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.85872/4.86389. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 4.85590/4.86625. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.85749/4.86759. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.85690/4.86888. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85282/4.86858. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.85478/4.87101. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.85007/4.87513. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.85194/4.87453. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.84925/4.87324. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.84753/4.88056. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.84765/4.87908. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.84683/4.87352. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.84991/4.87100. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.84814/4.87134. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.84480/4.87316. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.84606/4.87680. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84064/4.88475. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.84296/4.87797. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.84368/4.87217. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84016/4.87604. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.84108/4.87999. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.84240/4.87299. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.83948/4.87823. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.84387/4.86534. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.84403/4.86916. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.83987/4.88058. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.83827/4.87871. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.83402/4.89738. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.83646/4.87931. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84153/4.90313. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84571/4.88121. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.84408/4.87049. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.83431/4.88790. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.83896/4.89309. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.83072/4.88220. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.83668/4.88989. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.83142/4.90575. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.82814/4.90966. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83017/4.89571. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.83091/4.89656. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.82905/4.89567. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.82944/4.90614. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.82635/4.92398. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83130/4.90659. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.83056/4.90791. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.82836/4.92607. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.82432/4.91067. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.82717/4.91399. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.82513/4.92074. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.82875/4.91161. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.82494/4.92282. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.82052/4.94510. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.82994/4.91947. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.82404/4.91337. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.82684/4.91357. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.82772/4.91383. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.83746/4.90299. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.82755/4.91855. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.83167/4.91460. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83488/4.91107. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.83152/4.91979. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.82709/4.93600. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.82916/4.91825. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.82884/4.90954. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.82034/4.91523. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.82869/4.91276. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.82928/4.91368. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.82543/4.91686. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.82860/4.91060. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.82453/4.90177. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.82775/4.90335. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.82296/4.91376. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.82691/4.91892. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.82260/4.90721. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.82412/4.92662. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.82063/4.91492. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.82493/4.91559. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.82435/4.90998. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.81985/4.92196. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.81888/4.92778. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.82800/4.90956. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.82046/4.92378. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.82477/4.90935. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.81996/4.92713. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.81978/4.92522. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82233/4.91978. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.81770/4.93373. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82367/4.90615. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.81718/4.92642. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.81938/4.91903. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82147/4.91825. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.82083/4.93772. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82177/4.91329. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82143/4.91566. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.81524/4.91004. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.81856/4.92532. Took 0.20 sec\n",
      "ACC: 0.484375, MCC: -0.035451483867683334\n",
      "Epoch 0, Loss(train/val) 4.72196/4.72003. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.68013/4.68059. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.67469/4.68928. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.67882/4.69784. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.67772/4.69490. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.67663/4.68933. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.67564/4.68630. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.68049/4.67635. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.67847/4.67307. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.67677/4.67532. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.67513/4.67702. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 4.67418/4.68067. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.67204/4.68888. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.66724/4.70064. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.66652/4.70055. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.66810/4.70281. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.66860/4.69420. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.66314/4.70815. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.65880/4.72410. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.66297/4.71221. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.65852/4.71620. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.65916/4.72876. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.65743/4.73185. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.65565/4.72919. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.65967/4.71043. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.66068/4.74390. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.66117/4.70800. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.65410/4.74982. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.66757/4.68862. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.66043/4.72014. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.65643/4.73828. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.65275/4.74188. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.65286/4.73491. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.65414/4.72779. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.65031/4.77063. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.64799/4.74338. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.64964/4.75487. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.65551/4.75056. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.66175/4.70663. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.65566/4.74843. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.65651/4.72863. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.65111/4.76392. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.65009/4.76176. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.65072/4.74074. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.64358/4.79927. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.65168/4.74688. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.64745/4.76178. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.64878/4.74775. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.64227/4.78769. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.64694/4.75317. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.64544/4.77185. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.64727/4.77321. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.63853/4.79419. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.64496/4.76519. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.64785/4.77204. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.64553/4.77217. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.64606/4.76581. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.64361/4.76598. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.64636/4.78260. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.64687/4.75090. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.64173/4.77232. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.64032/4.78235. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.64145/4.79003. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.64527/4.75220. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.64555/4.78331. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.64056/4.78206. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.64985/4.74043. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.64265/4.79322. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.63758/4.80581. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.64432/4.76647. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.63750/4.80507. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.63896/4.78307. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.64369/4.77084. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.64138/4.77194. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.63837/4.80396. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.63882/4.77044. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.63994/4.80738. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.63963/4.79246. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.64672/4.79583. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.63687/4.76755. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.63622/4.80295. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.64187/4.75423. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.64398/4.78403. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.64098/4.76285. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.63924/4.79684. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.63702/4.78472. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.63661/4.81367. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.64696/4.75040. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.63409/4.80955. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.63659/4.81168. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.63548/4.81629. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.63555/4.79296. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.66379/4.69166. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.65593/4.71997. Took 0.22 sec\n",
      "Epoch 94, Loss(train/val) 4.65171/4.74832. Took 0.22 sec\n",
      "Epoch 95, Loss(train/val) 4.65067/4.73968. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.64996/4.74708. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.64646/4.75300. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.64897/4.74197. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.63825/4.80182. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.06158357771260997\n",
      "Epoch 0, Loss(train/val) 4.70817/4.64671. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.63682/4.63635. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.63060/4.63835. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.63462/4.63960. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.62929/4.64323. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.63023/4.64933. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.63002/4.65496. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.62622/4.65740. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.62380/4.65946. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.62561/4.64260. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.62489/4.64552. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.62656/4.65269. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.62754/4.65299. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.62776/4.65610. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.62554/4.66597. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.62456/4.66899. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.62856/4.66179. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.62399/4.66992. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.62606/4.67363. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.62238/4.67713. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.62293/4.67618. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.62353/4.67886. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.62081/4.68873. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.61862/4.68999. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.61914/4.68126. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.62020/4.68328. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.61980/4.67995. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.62139/4.68423. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.61736/4.69720. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.61812/4.69090. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.61695/4.69463. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.61610/4.68874. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.61670/4.68942. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.61199/4.68244. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.61610/4.68914. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.61759/4.69269. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.61541/4.70349. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.61619/4.70921. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.61582/4.68842. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.61107/4.70301. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.61230/4.70338. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.61493/4.70261. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.61570/4.71103. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.61255/4.72004. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.61300/4.72042. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.61054/4.72722. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.60686/4.72359. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.60697/4.72091. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.60726/4.72282. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.61054/4.72321. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.61201/4.71652. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.60721/4.72140. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.60373/4.74078. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.60394/4.72304. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.61025/4.73320. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.61134/4.72930. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.61010/4.72768. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.60906/4.73795. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.59929/4.74072. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.60910/4.72375. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.60640/4.73426. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.60265/4.73651. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.60165/4.74955. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 4.60220/4.72268. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.60686/4.73300. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.60280/4.75285. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.60607/4.73691. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.60263/4.73109. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.60004/4.72448. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.60168/4.75782. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.60426/4.74922. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.60254/4.73907. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.60562/4.73979. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.59963/4.75123. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.60530/4.75793. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.60220/4.75831. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.60667/4.74355. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.60388/4.71987. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.60030/4.76060. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.60431/4.73175. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.59987/4.73538. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.60330/4.74670. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.60368/4.71746. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.59982/4.78124. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.60513/4.74465. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.59851/4.74145. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.59692/4.75660. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.59851/4.76765. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.59642/4.76428. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.59640/4.75570. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.59537/4.73935. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.60029/4.75018. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.60046/4.77892. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.59749/4.74161. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.59537/4.78096. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.60513/4.71243. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.59896/4.77614. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.59741/4.75516. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.59553/4.77998. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.59373/4.75889. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.0629940788348712\n",
      "Epoch 0, Loss(train/val) 5.05148/5.00701. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.01916/5.00836. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.01947/5.00938. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.01622/5.00948. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.01968/5.00981. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.01663/5.01046. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.01531/5.01358. Took 0.21 sec\n",
      "Epoch 7, Loss(train/val) 5.01479/5.01312. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.01499/5.00878. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.01099/5.00689. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.01271/5.00803. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 5.00994/5.00563. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 5.00913/5.00416. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.00879/5.00387. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.01188/5.00488. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.01015/5.01027. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.00660/5.01065. Took 0.21 sec\n",
      "Epoch 17, Loss(train/val) 5.00899/5.01268. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.00451/5.00826. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 5.00507/5.00837. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.00564/5.00999. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.00687/5.00782. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 5.00377/5.00751. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.00435/5.01180. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.00480/5.01355. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.00416/5.02057. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.00279/5.01586. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 5.00409/5.02072. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.00177/5.03274. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.00372/5.04164. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.00262/5.03029. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.00154/5.03699. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.99722/5.03041. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.99860/5.04115. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.00104/5.02396. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.99521/5.04765. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.99664/5.03843. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.99652/5.04153. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.00584/5.00132. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.01208/4.99608. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 5.01245/5.00684. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 5.00980/5.00618. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.01015/4.99910. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 5.00454/5.00508. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.99695/5.01732. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.99588/5.03197. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.01307/4.97999. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.00699/4.98961. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.00698/4.99273. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.00346/5.00843. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.01146/5.00072. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 5.00319/5.01305. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.99772/5.02226. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.99236/5.02654. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.99496/5.02494. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.99984/5.00148. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.00683/4.99882. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.00532/5.00713. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.00478/5.00758. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.00366/5.00407. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.00116/4.99507. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 5.00358/5.00227. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.99981/5.00917. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.99465/5.01411. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.99232/5.03731. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.99900/5.01491. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.99322/5.03508. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.99562/5.02992. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.99265/5.03025. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.98882/5.05195. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.98685/5.03549. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.99506/5.04143. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.99867/5.04668. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.99603/5.03872. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.99224/5.04965. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.98933/5.04060. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.98784/5.05215. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.98943/5.05697. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.99235/5.05214. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.98556/5.06888. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.98424/5.05999. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.98877/5.06334. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.98709/5.08572. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.99186/5.06726. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.98608/5.04719. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.99631/5.04666. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.99991/5.04912. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.00054/5.03902. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.99520/5.05201. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.99391/5.04159. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.99758/5.05946. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.99300/5.05900. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.99293/5.05816. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.99210/5.06612. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.99075/5.06659. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.99649/5.04572. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.99302/5.06070. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.98859/5.05617. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.99388/5.05361. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.98906/5.06681. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.08952126702661474\n",
      "Epoch 0, Loss(train/val) 4.83933/4.77570. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.80220/4.76959. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.79204/4.77909. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79335/4.78136. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79372/4.78205. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.79130/4.78138. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.78853/4.77667. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.78712/4.77203. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.78640/4.77369. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.78589/4.77711. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.78379/4.77598. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78213/4.77665. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.78235/4.78614. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.77971/4.78064. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.77772/4.78296. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 4.77854/4.78848. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.77834/4.78238. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.77684/4.77969. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.77700/4.78355. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.77097/4.78626. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.77361/4.78443. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.77176/4.78202. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.77719/4.78434. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.77379/4.77675. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.77325/4.78135. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.77133/4.78981. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.77366/4.78407. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.77628/4.78480. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.77564/4.80060. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.77936/4.78830. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.78079/4.79215. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.77588/4.79377. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77837/4.78767. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.77195/4.78628. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.76911/4.79182. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.76976/4.79628. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77419/4.79004. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.76713/4.79810. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.76900/4.80093. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.76608/4.80558. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.76867/4.79701. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.76849/4.79373. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76666/4.79113. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.76319/4.80108. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77225/4.80464. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77409/4.80343. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77389/4.80648. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.77348/4.81296. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.77080/4.80072. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.76612/4.80383. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76631/4.81017. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.76982/4.80634. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.77029/4.79648. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.77070/4.79343. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76820/4.78946. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.76722/4.80415. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.77097/4.79626. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.77016/4.79750. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.76852/4.79160. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.76743/4.79307. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.76774/4.77947. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.76469/4.79056. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.76564/4.79579. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.76712/4.79597. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76514/4.79136. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.76611/4.78740. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.77068/4.78921. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.76790/4.79713. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.76689/4.80053. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.76835/4.80163. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.76401/4.79701. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.76346/4.80181. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.76696/4.80093. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.76293/4.80260. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.76289/4.80424. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.76312/4.79807. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.76525/4.79529. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.76536/4.80273. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.76148/4.79645. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.76252/4.79711. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.76439/4.78983. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.76563/4.79387. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.76308/4.79269. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.76381/4.80286. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.76407/4.79572. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.76268/4.79455. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.76427/4.79921. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.76026/4.80232. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.76173/4.79773. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.75953/4.80397. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.76105/4.79735. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.76300/4.80147. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.75983/4.79693. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.75836/4.79963. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.76357/4.79383. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75926/4.80310. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.76403/4.80068. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.76239/4.79810. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.75830/4.80321. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75751/4.80970. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.11559657831495045\n",
      "Epoch 0, Loss(train/val) 5.11004/5.07227. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.05297/5.08926. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.04677/5.08287. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.04560/5.07170. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.04399/5.06269. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.04283/5.05950. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.04085/5.05915. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.03873/5.06564. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.03831/5.07624. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.04000/5.08097. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.04137/5.07884. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.03813/5.06462. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.03527/5.05431. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.03496/5.04591. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.03031/5.04642. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.03003/5.05483. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.02985/5.09086. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.03924/5.07827. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.03414/5.06511. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.02970/5.07507. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.02661/5.08032. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.02707/5.08218. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.02614/5.07751. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.02777/5.09153. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.02832/5.07192. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.02396/5.07180. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.02548/5.07497. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.02250/5.06803. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.02409/5.06608. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.02791/5.05963. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.02053/5.05613. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.02026/5.06956. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.02242/5.07314. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.02351/5.04329. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.01629/5.07071. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.01909/5.06936. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 5.01686/5.06370. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 5.01847/5.05909. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 5.01821/5.07580. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 5.01683/5.06571. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.01957/5.05962. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.01551/5.07126. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.02124/5.07221. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.01787/5.06454. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 5.01391/5.08296. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.01618/5.05668. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 5.01784/5.06276. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.01216/5.08322. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.01909/5.05612. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 5.02002/5.06284. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.00954/5.07541. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.01120/5.06916. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.01005/5.04542. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.01689/5.05304. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.01296/5.06622. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.01463/5.06521. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.01276/5.06798. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.01372/5.06527. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.01244/5.05222. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 5.01523/5.07193. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.00700/5.09144. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 5.01106/5.06578. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.00870/5.06221. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.01049/5.05823. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.00523/5.08441. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.01342/5.06332. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.01481/5.05397. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.01087/5.06975. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 5.00856/5.05172. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.01031/5.07603. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 5.00942/5.05905. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.00772/5.04695. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.01391/5.07460. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.01665/5.09854. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.02992/5.04003. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.01626/5.03003. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.01339/5.04935. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 5.01921/5.04148. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.01291/5.02475. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.01443/5.02345. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 5.01723/5.04452. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.00949/5.06265. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.00680/5.05968. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.00393/5.04801. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.00640/5.07788. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.01405/5.03838. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.00847/5.06018. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.00769/5.03180. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.00552/5.05128. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.00580/5.06900. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.01747/5.03551. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.01401/5.01370. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.01244/5.05844. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.00705/5.05449. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 5.00269/5.04337. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.01554/5.02011. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.00468/5.01127. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.01267/5.04473. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.00421/5.01130. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.01016/5.03446. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.1308525335766877\n",
      "Epoch 0, Loss(train/val) 4.91453/4.92156. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.87106/4.85676. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.87160/4.84895. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.86820/4.85125. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.86775/4.84676. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.87124/4.84421. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.87093/4.84430. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.86866/4.85291. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 4.86602/4.86429. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.86243/4.86458. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.86126/4.85817. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.85667/4.85960. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.85910/4.85718. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85658/4.86211. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85217/4.85724. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85322/4.84929. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85219/4.85072. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.84905/4.86025. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85258/4.85791. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.85053/4.85050. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.84790/4.85561. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.84559/4.84970. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85363/4.84149. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85574/4.85084. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.85436/4.84833. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.84934/4.84661. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84599/4.84224. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84801/4.84453. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.84623/4.83865. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84808/4.85212. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84540/4.85744. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84488/4.85466. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.84153/4.85807. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.83927/4.86961. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84471/4.86713. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84704/4.86101. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.84574/4.84924. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84063/4.85488. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.83894/4.85658. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84307/4.85845. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.84297/4.85251. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84046/4.85034. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.85062/4.85924. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.84872/4.84703. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84389/4.85341. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84179/4.84047. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84474/4.85484. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83888/4.86155. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.84298/4.85906. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.82922/4.84772. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.84747/4.86123. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.84121/4.84388. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.84378/4.87306. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.83614/4.82555. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.84010/4.86253. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.83954/4.84829. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.83701/4.86000. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.83999/4.84016. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 4.84020/4.86150. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.83457/4.86823. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.83751/4.85308. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.83384/4.85559. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.83768/4.86156. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83768/4.86794. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.83446/4.85680. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.83462/4.85984. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.83907/4.85849. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83310/4.86480. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.83655/4.85791. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.82847/4.85360. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.83190/4.86178. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83359/4.85824. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83017/4.87605. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83453/4.87251. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.83056/4.84238. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.83145/4.86450. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83186/4.84225. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.82728/4.86097. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.82618/4.87517. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.82837/4.84715. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.83188/4.85354. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83032/4.85489. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.83055/4.83837. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.82621/4.87996. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.82738/4.88403. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83447/4.84874. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.82890/4.86746. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83165/4.85620. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.82939/4.83156. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82656/4.84447. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 4.82657/4.87129. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82448/4.85519. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82267/4.84550. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.82750/4.86135. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82124/4.87355. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82524/4.86622. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82213/4.86333. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82253/4.87205. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.81938/4.85542. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.82247/4.88908. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.03202563076101743\n",
      "Epoch 0, Loss(train/val) 4.96504/4.98457. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.94211/4.96227. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.94031/4.98598. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.93921/4.99721. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.93834/4.97348. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.93616/4.94915. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.92722/4.94572. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.92673/4.94518. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.92585/4.95001. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.92451/4.95180. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.92461/4.94982. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.92118/4.96100. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.92546/4.95700. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.92919/4.94654. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.92262/4.94849. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.92146/4.95379. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.92023/4.95921. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.92286/4.96010. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.92135/4.95644. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.92030/4.96312. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 4.91761/4.96376. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.91859/4.96394. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.91846/4.95989. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.91795/4.95871. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.91844/4.95079. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.91732/4.95239. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.91467/4.95691. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.91590/4.96605. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.91437/4.95651. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 4.91449/4.94934. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.91447/4.94753. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.91210/4.95855. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.91045/4.96283. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.91035/4.96321. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.91255/4.96321. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.91128/4.96452. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.91349/4.95519. Took 0.22 sec\n",
      "Epoch 37, Loss(train/val) 4.91044/4.95406. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.90520/4.96937. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.91733/4.95113. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.91311/4.95556. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.90946/4.96868. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.91135/4.96422. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.90816/4.96685. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.91253/4.95185. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.90943/4.94650. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.91217/4.95895. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.91080/4.94592. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.90772/4.95483. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.91158/4.95011. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.90912/4.95444. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.92035/4.94808. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.91282/4.96130. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.90894/4.95339. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.91110/4.92868. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.91448/4.91463. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.90972/4.93533. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.90898/4.94619. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.90956/4.94120. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.90107/4.95861. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.90730/4.96630. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.91645/4.93965. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.90875/4.94315. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.90651/4.94972. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.90778/4.96165. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.90500/4.95497. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.90728/4.95782. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.90332/4.95065. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.90248/4.96729. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.90485/4.95188. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.89932/4.96040. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.90437/4.96294. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.90721/4.95490. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.90414/4.95926. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.90468/4.96515. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.90516/4.95080. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.90590/4.95559. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.90724/4.96446. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.90133/4.96014. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.90490/4.94794. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.90165/4.95408. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.89785/4.95260. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.89965/4.95118. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.90322/4.96008. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.89750/4.95047. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.89818/4.94551. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.90165/4.94583. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.90489/4.95306. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.89971/4.96695. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.90074/4.95053. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.89837/4.95708. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.89692/4.95154. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.89948/4.93819. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.89511/4.96381. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.89742/4.95364. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.90266/4.94616. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.89536/4.96027. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.90062/4.95415. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.89749/4.96220. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.89765/4.95101. Took 0.21 sec\n",
      "ACC: 0.5, MCC: -0.002944085489318884\n",
      "Epoch 0, Loss(train/val) 4.86051/4.76841. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.77724/4.81356. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.78155/4.84248. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 4.78720/4.86892. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.78960/4.84814. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.78828/4.81554. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.78137/4.80217. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.77587/4.80947. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.77717/4.82260. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.78098/4.82411. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.77954/4.81289. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.77460/4.82363. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.77445/4.82357. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.77373/4.81927. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.77008/4.82037. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.77119/4.82733. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.77351/4.82591. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.76895/4.82396. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.76364/4.83731. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.77337/4.81690. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.76255/4.83371. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.76187/4.86283. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.77312/4.81750. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.76209/4.84966. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.76151/4.83214. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.75894/4.85194. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.76151/4.84531. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 4.75935/4.84841. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.76036/4.84195. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.75120/4.85548. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.76262/4.85429. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.76300/4.82491. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.75894/4.84743. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 4.75507/4.84555. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.75695/4.83117. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.75573/4.84893. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.75564/4.81462. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.75415/4.83916. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.75829/4.82749. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.75679/4.83772. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.75212/4.83781. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.75957/4.82687. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 4.75741/4.82424. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.75566/4.81958. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.75587/4.82784. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.75201/4.83847. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.75094/4.83155. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.75963/4.81153. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.74910/4.84786. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.76041/4.83526. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.75750/4.83180. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.75586/4.81586. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.74981/4.83463. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.75351/4.83755. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.75204/4.81951. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.74781/4.83728. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.75049/4.82603. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.75001/4.83723. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.75082/4.82763. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.74520/4.84746. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.75102/4.83178. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.75382/4.82175. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.74949/4.81821. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.75198/4.83657. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.74406/4.83301. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.74726/4.84916. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75423/4.83360. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.75112/4.82066. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.74202/4.84159. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.74533/4.86767. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.74870/4.83709. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.74595/4.83976. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.74598/4.83643. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.75259/4.82289. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.75007/4.83712. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.74908/4.83605. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.74638/4.82854. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.74390/4.82898. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.74608/4.83099. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.75100/4.84709. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.74861/4.84083. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.74912/4.83206. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.74602/4.82979. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.74243/4.85026. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.74472/4.83913. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.74404/4.84795. Took 0.21 sec\n",
      "Epoch 86, Loss(train/val) 4.74579/4.83572. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.74080/4.84480. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.74983/4.84323. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.74388/4.82788. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.74314/4.84105. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.74766/4.84472. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.74560/4.84255. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.74484/4.86280. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.74586/4.83463. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.74119/4.85363. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.74465/4.85526. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.73619/4.83970. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.74007/4.83693. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 4.73943/4.85086. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.0657512425947884\n",
      "Epoch 0, Loss(train/val) 4.65979/4.61166. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.60505/4.61089. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.59984/4.61547. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.60072/4.61876. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.59879/4.62630. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.60049/4.62883. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.60210/4.63149. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.59919/4.63202. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.59701/4.63375. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.59620/4.63588. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.59797/4.64278. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.59526/4.64591. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.59217/4.64242. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.59248/4.64809. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.59507/4.62429. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.59262/4.64773. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.59446/4.64219. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.59761/4.64026. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.59263/4.63780. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.58901/4.65359. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.59683/4.63754. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.59052/4.65082. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.59058/4.65226. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.59143/4.64961. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.59275/4.64227. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.59404/4.64314. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.59156/4.67526. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.59230/4.64836. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.58685/4.68076. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.58760/4.66166. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.58822/4.68275. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.58558/4.66943. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.58665/4.67218. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.59037/4.66667. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.58588/4.67847. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.58373/4.69668. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.58622/4.67041. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.58462/4.66743. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.58752/4.69870. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.58742/4.67748. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.59008/4.68535. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.58335/4.67880. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.58213/4.71413. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.58780/4.69047. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.58423/4.69409. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.58316/4.72293. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.59110/4.65821. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.58871/4.67885. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.58516/4.70326. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.58725/4.71062. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.58953/4.68640. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.58635/4.70637. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.58640/4.69643. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.58271/4.69304. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.58261/4.72593. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.58812/4.68635. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.58228/4.70857. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.58034/4.69375. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.58262/4.69121. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.58118/4.70508. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.58249/4.68368. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.58307/4.71004. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.58107/4.68807. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.58136/4.69044. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.58403/4.71459. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 4.58189/4.70706. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.57774/4.72422. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.57991/4.70910. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.57641/4.73798. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.58436/4.71990. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.57980/4.71091. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.59203/4.63973. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.58774/4.66929. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.58511/4.69163. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.58581/4.69379. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.58487/4.66929. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.58317/4.67652. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.57716/4.69823. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.58143/4.70389. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.58972/4.65487. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.58261/4.68195. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.57913/4.69028. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.58016/4.67512. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.57761/4.68249. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.58028/4.70795. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.57404/4.69709. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.57845/4.69966. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.58013/4.69166. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.57564/4.70149. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.57909/4.71355. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.57598/4.68811. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.57297/4.71985. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.57831/4.68163. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.57284/4.74160. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.57060/4.71353. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.57476/4.73801. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.57920/4.70043. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.58049/4.70972. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.56913/4.74613. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.57436/4.72217. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.08952126702661474\n",
      "Epoch 0, Loss(train/val) 5.07821/5.07029. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.06077/5.03991. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.05947/5.03616. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.06259/5.04154. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.06203/5.07078. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.05538/5.06514. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 5.05202/5.05710. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.05012/5.06133. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.05133/5.06249. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.05189/5.06281. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.05000/5.06768. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.05010/5.07285. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.05150/5.07335. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.04925/5.07227. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 5.04833/5.07503. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.04815/5.07963. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.04881/5.07410. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.04644/5.07667. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 5.04564/5.08173. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.04510/5.07864. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.04300/5.07887. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 5.04997/5.07007. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.05212/5.07814. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.04932/5.07242. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.04591/5.07921. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.04266/5.08907. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.04418/5.08755. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.04637/5.09257. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 5.04342/5.08974. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.03913/5.09768. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.04414/5.08576. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.04259/5.10080. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 5.03826/5.10720. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.03898/5.11755. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.04295/5.07347. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 5.04898/5.07405. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.04064/5.06841. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.04185/5.08165. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.03632/5.08451. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.04261/5.07104. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.04139/5.06811. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.04002/5.07574. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 5.04081/5.07709. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.03596/5.08919. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.03614/5.08488. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.03833/5.07503. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.03404/5.08346. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.03958/5.07841. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.03656/5.08560. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 5.03312/5.08065. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.03798/5.07652. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.03175/5.09077. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 5.03341/5.08133. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.03437/5.09036. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.03532/5.07255. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.03909/5.07727. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.03296/5.06834. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.03565/5.07209. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.03026/5.08092. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.02822/5.07707. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.03012/5.08338. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.02871/5.09375. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.02720/5.08899. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 5.03504/5.08539. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.02524/5.09265. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.03141/5.10717. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.03073/5.08918. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.02482/5.09636. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.03096/5.09011. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.02570/5.10463. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.03408/5.08957. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.02733/5.09296. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.02599/5.09156. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.03027/5.08633. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.02435/5.09043. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.02988/5.08516. Took 0.21 sec\n",
      "Epoch 76, Loss(train/val) 5.02647/5.08923. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.04757/5.08468. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.03218/5.08390. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.02696/5.09303. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.03520/5.09323. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.03302/5.09504. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.03177/5.08823. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.02026/5.09931. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.03335/5.09464. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.02518/5.10349. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.01889/5.11295. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.03148/5.10257. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 5.02657/5.09397. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.02419/5.09879. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 5.02278/5.11165. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.01616/5.13287. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.02792/5.09877. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.01768/5.10751. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.01873/5.13716. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.02059/5.11328. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.01771/5.12050. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.02885/5.09628. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.02143/5.11890. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.02027/5.11027. Took 0.20 sec\n",
      "ACC: 0.53125, MCC: 0.040082172520323485\n",
      "Epoch 0, Loss(train/val) 4.95531/4.88515. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.89958/4.88266. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.89631/4.88462. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.89361/4.88836. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.89309/4.89039. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.89137/4.89775. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.89035/4.90492. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.88841/4.92174. Took 0.18 sec\n",
      "Epoch 8, Loss(train/val) 4.88681/4.92899. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.88498/4.92726. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.88263/4.93296. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.88171/4.92817. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.88557/4.91048. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.88058/4.92257. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87901/4.93477. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.88314/4.93461. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.87883/4.94232. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.88052/4.95068. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.87704/4.94949. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.87493/4.96363. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.87652/4.96083. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.87294/4.98064. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.87405/4.96225. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.87279/4.96459. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.87169/4.95891. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.87072/4.97511. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.87482/4.94958. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.87142/4.97252. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.87218/4.96557. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.86829/4.97161. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 4.87245/4.97513. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.87043/4.95914. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.87041/4.97704. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.87078/4.96821. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.86719/4.98865. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.86832/4.96684. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.86559/4.99058. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.86871/4.97192. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.86694/4.98823. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.86819/4.98452. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.86154/5.00151. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.86754/4.95469. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.86556/5.00681. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.86753/4.96043. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.86123/5.00959. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.86107/5.00355. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.86580/4.96951. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.86336/4.98242. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.86252/4.99038. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.86373/5.00255. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.86335/4.96773. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.86240/5.00547. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.86119/4.97551. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.86293/5.00917. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.86154/5.00133. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.86262/5.01212. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.86361/4.98990. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.86062/4.99723. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.86176/5.00020. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.85999/5.00389. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.86100/5.00921. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.85853/4.97970. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85651/5.02713. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.85773/4.99959. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.86020/5.00365. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.85523/5.04522. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.85819/5.00882. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.86342/5.00770. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.85682/5.01570. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.85225/5.03157. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.85459/5.02423. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.85381/5.02304. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.85828/4.99688. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.85236/5.01654. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.85661/5.01635. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.85911/4.98769. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.85444/5.01984. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.84798/4.97333. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.86222/5.02176. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.85580/5.03114. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.85180/5.03722. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.85362/5.03162. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.85774/4.97823. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.85547/5.03418. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.85339/5.00481. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.85100/5.02759. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.85779/4.97825. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.85007/5.04839. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.85877/4.99308. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.85618/5.00685. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.85317/5.04527. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85136/5.02157. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.85201/5.02021. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.84920/5.02051. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.84794/5.04074. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.85657/5.01358. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.85175/5.01522. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83944/5.04262. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.85096/5.03829. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.85059/5.01795. Took 0.20 sec\n",
      "ACC: 0.421875, MCC: -0.15467466504810498\n",
      "Epoch 0, Loss(train/val) 4.85814/4.82317. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.83235/4.83129. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.83876/4.83528. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.84371/4.85740. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.83765/4.83766. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.82513/4.83699. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.82391/4.84154. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.82643/4.84570. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.82577/4.84964. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.82591/4.85208. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.82406/4.86352. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.82095/4.85506. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.82229/4.86920. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.82037/4.86317. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.81849/4.87967. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.81917/4.85882. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.82310/4.86428. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.82476/4.85984. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81996/4.85976. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.82182/4.86569. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.81857/4.86838. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.81849/4.87385. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.82005/4.85693. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.81970/4.87443. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.81677/4.87315. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.81537/4.87217. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82010/4.87075. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.81768/4.86925. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.81922/4.86288. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.81261/4.87694. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.81832/4.87394. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.81397/4.89040. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.81592/4.85194. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.82513/4.83767. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.82275/4.83469. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.81943/4.83738. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.82107/4.83703. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.81954/4.83658. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 4.82134/4.83925. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.82020/4.83982. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.81875/4.84310. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.82018/4.84208. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.82038/4.84005. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.81821/4.84088. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.81856/4.84003. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.81780/4.84327. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.81711/4.84713. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.81735/4.84588. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.81687/4.84480. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.81449/4.85446. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.81589/4.85291. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.80985/4.85860. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81480/4.85123. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.82159/4.85211. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81316/4.84617. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.81255/4.85115. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.80990/4.85777. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.80907/4.85505. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81394/4.85551. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.80893/4.86227. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.81366/4.85375. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.81085/4.85571. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.80746/4.86629. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.80806/4.86383. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.80954/4.86685. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.80521/4.87213. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.80766/4.88957. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.80537/4.86994. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.80260/4.87509. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.80857/4.86931. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.80757/4.86201. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.80675/4.88497. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.80864/4.86620. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.80839/4.86321. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.80530/4.85983. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.80506/4.86385. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.80820/4.86886. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.79986/4.88177. Took 0.21 sec\n",
      "Epoch 78, Loss(train/val) 4.80388/4.88742. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.80101/4.87705. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.80077/4.88579. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.80414/4.88378. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.80722/4.88275. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.80132/4.89306. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.80055/4.89774. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.79525/4.89703. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.80517/4.90127. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.79504/4.89803. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.80301/4.89379. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.79971/4.89089. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.79783/4.88929. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.79855/4.88540. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.79433/4.91991. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.79959/4.91156. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.79177/4.90651. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.80722/4.90023. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.79118/4.92275. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.79787/4.90774. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.79981/4.90090. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.79929/4.91205. Took 0.19 sec\n",
      "ACC: 0.4375, MCC: -0.12725695259515554\n",
      "Epoch 0, Loss(train/val) 4.85131/4.78267. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.80593/4.78245. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.80524/4.78450. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.80419/4.78394. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.80413/4.78458. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.80327/4.78708. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.80191/4.78716. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.80018/4.78883. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.80021/4.79091. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79737/4.79404. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79855/4.79420. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.79962/4.79065. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.80057/4.78968. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79884/4.79609. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.80155/4.79653. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.80088/4.78839. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.79826/4.79713. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.79568/4.79928. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.79766/4.79971. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.79515/4.80580. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.79732/4.80574. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.79477/4.81053. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.79291/4.81539. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.79596/4.79528. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.79663/4.79887. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.79351/4.80467. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78837/4.81221. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.79289/4.81033. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.79264/4.81055. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.79294/4.80885. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.79255/4.81473. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.79417/4.81010. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.79371/4.80673. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.78953/4.81188. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.79037/4.81061. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.79242/4.80521. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.79008/4.81404. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.79031/4.81639. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.78670/4.82037. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.79086/4.81649. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.78633/4.81454. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.78877/4.82076. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.78808/4.82585. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.78787/4.82697. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.78473/4.82520. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.78855/4.82872. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78551/4.81984. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.78811/4.80778. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.79371/4.80550. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.78572/4.82788. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.78882/4.81502. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.78754/4.83008. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.78566/4.83183. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.78580/4.81893. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.78303/4.82697. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.79723/4.80499. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.79188/4.81311. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.79092/4.82770. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.78842/4.83195. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.79053/4.82677. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.78644/4.83196. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.78535/4.82688. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.78418/4.84219. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.78429/4.83176. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.78163/4.84203. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.78271/4.84004. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.78267/4.84628. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.78324/4.85271. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.78214/4.84660. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.78656/4.84549. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.78195/4.84266. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.78460/4.88469. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.78694/4.83047. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.78306/4.85354. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.78728/4.83465. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.78328/4.84067. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.78492/4.84268. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.78380/4.83302. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.78306/4.83778. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.78150/4.84444. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.78480/4.84374. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.78707/4.85597. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.78371/4.84933. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.78009/4.85958. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.78254/4.85627. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.78442/4.84046. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.77805/4.85467. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.78044/4.83695. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.78326/4.83384. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.78280/4.84728. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.78227/4.82905. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.77770/4.86228. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.78331/4.84972. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.78176/4.86338. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77873/4.86341. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.78345/4.83945. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.77972/4.87044. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.77984/4.84685. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.77722/4.87492. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.79487/4.81872. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.0778902520624173\n",
      "Epoch 0, Loss(train/val) 5.05360/5.03570. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.03657/5.03023. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.02848/5.03199. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.02509/5.03267. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 5.02685/5.03086. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.02506/5.03042. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.02097/5.02762. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.02840/5.03078. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.02370/5.02768. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.02367/5.02392. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.02507/5.02292. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.02108/5.02057. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.02102/5.02066. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.02428/5.01863. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.02093/5.01557. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.02228/5.01759. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.01949/5.01789. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.01927/5.01646. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.02010/5.01465. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.01936/5.01560. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.01678/5.01453. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.01937/5.01293. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.01409/5.01486. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.01757/5.01287. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.01516/5.00954. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 5.01372/5.00827. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.01396/5.00968. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.01343/5.00671. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.01814/5.01361. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.01556/5.01541. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.01541/5.01637. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.01578/5.01510. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.01245/5.01506. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.01233/5.01472. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.01357/5.02468. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.00868/5.02984. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 5.01247/5.02145. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.01131/5.01066. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.01334/5.01371. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 5.01088/5.01487. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 5.01244/5.01286. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.00711/5.01572. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.01202/5.00941. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.00729/5.00193. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.01310/5.01044. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.00763/5.01216. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.00719/5.00987. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.01079/5.01249. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 5.00980/5.00666. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.00996/5.00329. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.01218/5.00840. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.00637/5.00090. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.00676/4.99805. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.00750/4.99914. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 5.00535/5.00268. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.01183/4.99701. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.00825/4.99752. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.00355/4.99648. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 5.00366/5.00071. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.00299/5.00755. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 5.00785/5.00936. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 5.00393/5.00874. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 5.00793/5.00270. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.00707/5.01066. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 5.00694/5.00734. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 5.00252/4.99474. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.00640/5.00686. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.00695/5.03163. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 5.01404/5.02116. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.00706/5.02323. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.00481/5.01821. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 5.00508/5.01981. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 5.00684/5.01638. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.00258/5.02027. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.00433/5.01387. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 5.00415/5.01403. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.00232/5.00951. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.00295/5.00695. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.00283/5.00833. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.00349/5.01248. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.00507/5.01729. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.99915/5.02417. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.00785/5.02085. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.00511/5.01034. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.00584/5.01065. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.99874/5.01381. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.00281/5.00627. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.00086/5.00672. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.00206/5.00950. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.99864/5.01107. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.00327/5.00094. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.00207/5.00965. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.00023/5.00560. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.99842/5.00865. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 5.00710/5.01670. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.00149/5.00663. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.00344/5.00555. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 5.00052/5.00522. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.00314/5.00761. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 5.00004/5.01368. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.65537/4.62217. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.62842/4.62588. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.62892/4.63431. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.63549/4.64546. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.64441/4.62335. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.64094/4.62604. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.62339/4.62094. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.62721/4.62189. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.62790/4.62185. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.62634/4.62214. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.62541/4.62178. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.62531/4.62124. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.62582/4.62262. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.62635/4.62299. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.62232/4.62829. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.62353/4.63103. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.62692/4.62465. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.61813/4.63032. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.61435/4.63426. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.62114/4.62715. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.61958/4.62930. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.61649/4.62949. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.61199/4.63871. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.61133/4.63011. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 4.61079/4.63094. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.61132/4.62571. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.61258/4.63616. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.61106/4.62742. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.60955/4.64070. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.60719/4.63799. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.60490/4.62673. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.60394/4.63799. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.61034/4.62764. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.60835/4.64059. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 4.60193/4.63192. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.60124/4.63291. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.60433/4.63837. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.60430/4.62175. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.60482/4.64502. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.60259/4.63741. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.60638/4.64770. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.60154/4.63991. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.59567/4.65976. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.60656/4.63126. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.59923/4.64565. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.60322/4.65173. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.60764/4.63723. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.59900/4.64943. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.60257/4.65241. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.60169/4.64687. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.59914/4.65246. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.60340/4.65582. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.60433/4.64569. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.59578/4.65552. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.59562/4.64361. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.59549/4.65175. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.60380/4.63436. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.60186/4.65253. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.59704/4.65810. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.59391/4.65483. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.59987/4.64180. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.60215/4.63457. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.58964/4.66831. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.59756/4.63639. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.59227/4.64244. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.59532/4.66713. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.59348/4.65137. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.58903/4.65785. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.58747/4.63730. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.59469/4.66562. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.59402/4.64067. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.59092/4.63139. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.59665/4.64694. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.59468/4.65298. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.59051/4.65936. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.58241/4.65847. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.59112/4.63439. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.58955/4.63810. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.58888/4.65834. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.58956/4.64370. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.59159/4.65580. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.58645/4.64269. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.58698/4.65536. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.59205/4.65540. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.59205/4.63902. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.58354/4.64099. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.59012/4.65480. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.58715/4.65549. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.59120/4.66154. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.59019/4.63971. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.58776/4.66229. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.58964/4.66535. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.58776/4.66378. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.58455/4.65645. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.59028/4.65477. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.58344/4.66452. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.57933/4.67660. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.58789/4.69178. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.58897/4.65791. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.58292/4.68584. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.025861699363244256\n",
      "Epoch 0, Loss(train/val) 4.76850/4.72021. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.73149/4.72704. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.72293/4.72913. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.72261/4.72662. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.72362/4.72644. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.72423/4.72312. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.72240/4.72100. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.72196/4.72689. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.71816/4.72248. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 4.72110/4.72083. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.71705/4.71527. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.71970/4.71426. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.71687/4.71011. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.71382/4.71152. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.71326/4.71161. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.71141/4.71049. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.71534/4.71320. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.71578/4.71006. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.71489/4.70959. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.71449/4.70643. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.71294/4.70974. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.70849/4.70959. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.70885/4.71072. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.70839/4.70669. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 4.70465/4.70099. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.70460/4.70376. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.70377/4.70431. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.70500/4.71156. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.69846/4.70750. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.70253/4.70358. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.70051/4.71381. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.69908/4.70365. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.69794/4.70844. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.69302/4.69985. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.70181/4.71796. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.69513/4.71284. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.69088/4.70595. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.69556/4.70599. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.69048/4.70231. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.69129/4.70080. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.69176/4.70071. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.68965/4.72716. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.68807/4.71885. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.68545/4.72283. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.68823/4.71185. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.68413/4.73221. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.68667/4.70019. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.68654/4.69725. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.68267/4.70181. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.68686/4.72601. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.68710/4.71565. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.69396/4.70949. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.68466/4.69651. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.68180/4.69949. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.68579/4.71120. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.68152/4.72989. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.68687/4.70395. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.68482/4.70447. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.68537/4.71436. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.68067/4.70978. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.68362/4.71972. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.67871/4.72484. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.68586/4.70014. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.68082/4.71155. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.68616/4.70755. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.67828/4.71942. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.67953/4.71739. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 4.67696/4.70157. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.68273/4.70017. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.67255/4.70602. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.67264/4.69702. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.67539/4.69808. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.67961/4.70777. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.68235/4.69956. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.67469/4.69449. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.67490/4.70548. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.67649/4.68361. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.67550/4.70003. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.67063/4.70700. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.67330/4.71800. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.67244/4.72810. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.67265/4.72945. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.66752/4.72827. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.67522/4.70367. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.67378/4.71023. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.68331/4.70750. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.66416/4.72973. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.67674/4.68756. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.66111/4.72351. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.67589/4.70971. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.67129/4.72199. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.67708/4.71798. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.67175/4.73726. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.70052/4.73017. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.69499/4.72725. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.69428/4.72263. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.69240/4.72413. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.68716/4.73230. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.68445/4.71629. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.67819/4.71435. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.02938413738897565\n",
      "Epoch 0, Loss(train/val) 4.88301/4.82871. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.82115/4.81182. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.81401/4.81815. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.81197/4.82536. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.80886/4.83631. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.80674/4.84080. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.80648/4.83811. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.80242/4.84054. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.80644/4.83611. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.80210/4.84026. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.80094/4.83458. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.80205/4.83641. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.80301/4.83310. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.80210/4.83154. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.80127/4.83263. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.80067/4.83057. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.80243/4.82236. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.79863/4.82422. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.79439/4.82521. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.79390/4.83635. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.79603/4.82535. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.79872/4.83222. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.79744/4.82405. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.79438/4.82689. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.79587/4.83590. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.79638/4.83256. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78981/4.84099. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.79830/4.82495. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78992/4.84529. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.79554/4.83538. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.79417/4.82696. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.78975/4.83336. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.79027/4.81108. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.79241/4.81987. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.79021/4.81480. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.78479/4.82415. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78816/4.81375. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.78979/4.82534. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.78821/4.81585. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.78781/4.82794. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.78559/4.83202. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.78468/4.82674. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.78494/4.83727. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.79175/4.83252. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.78734/4.81617. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.78498/4.82647. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.79014/4.82657. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.78764/4.82592. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.78485/4.82581. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.78608/4.83720. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.78904/4.83292. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.78364/4.82558. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.78550/4.83879. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.78303/4.82963. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.78326/4.83270. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.78847/4.82485. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.78246/4.84269. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.78497/4.82643. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.78205/4.84288. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.78560/4.83766. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.78201/4.82750. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 4.78285/4.84672. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.78200/4.83586. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.78402/4.82906. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.77937/4.83759. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.77780/4.84340. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.77939/4.84663. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.78111/4.82875. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.77739/4.85403. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77905/4.85631. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.78550/4.84865. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.77931/4.84028. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.77535/4.84881. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.78118/4.82980. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.78130/4.83116. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.78196/4.82647. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.77583/4.82726. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77496/4.83473. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.77547/4.83441. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.77507/4.84668. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.78113/4.84556. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.77338/4.85092. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.77773/4.84922. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.77980/4.82779. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.77521/4.82885. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.77478/4.83418. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.77651/4.86389. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.78183/4.81302. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.79046/4.81006. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.78517/4.83918. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.78257/4.84114. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.78185/4.85308. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.78199/4.83653. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77498/4.85586. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77931/4.83793. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.77181/4.84984. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.77253/4.83317. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.77514/4.83700. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.77507/4.83011. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.77332/4.86227. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.1513518081969605\n",
      "Epoch 0, Loss(train/val) 4.84298/4.79082. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.79573/4.79049. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.79743/4.79783. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79173/4.80899. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.79169/4.80721. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 4.79200/4.80570. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.78799/4.80375. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79442/4.79724. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.78899/4.79015. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.78954/4.79069. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79144/4.78467. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78743/4.79620. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.78547/4.79087. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.78402/4.78992. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78622/4.78377. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.78094/4.79418. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78247/4.79383. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.77821/4.80177. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.77770/4.80224. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.77065/4.81557. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.77554/4.81370. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.77944/4.80154. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.77650/4.81281. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.77463/4.81902. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.76727/4.82070. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.77710/4.81608. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.77286/4.82187. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.77306/4.82161. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.76705/4.82797. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.76988/4.83041. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.76783/4.82404. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77690/4.83611. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.77509/4.82796. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.77312/4.82430. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.77489/4.83003. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77484/4.81933. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.76862/4.82288. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.76661/4.84103. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.76678/4.83099. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.76538/4.83716. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.76295/4.83997. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.76721/4.83975. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76751/4.82709. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.75970/4.85396. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.76170/4.83943. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77023/4.84175. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.75826/4.85344. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.76846/4.85207. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.76701/4.86601. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.76857/4.84393. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76270/4.85503. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.76308/4.86375. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.76332/4.84686. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76528/4.81944. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76842/4.81901. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.75740/4.84542. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.76511/4.84362. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76212/4.85935. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.76223/4.85214. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.75459/4.86611. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.76185/4.85011. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 4.75594/4.86159. Took 0.22 sec\n",
      "Epoch 62, Loss(train/val) 4.76233/4.84960. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.75868/4.85869. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.75618/4.84998. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.75686/4.86374. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75853/4.86067. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.75933/4.85936. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.75519/4.85674. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.75861/4.83786. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.76568/4.87668. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.76703/4.86590. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.76278/4.85209. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.75984/4.81225. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.76065/4.85965. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.75753/4.84268. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.76132/4.84268. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.76532/4.83523. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.76681/4.84796. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.75991/4.84261. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.76156/4.84070. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.76322/4.83744. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.75635/4.84634. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.76069/4.83520. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.76285/4.84715. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.75705/4.84334. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.75434/4.85679. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.75177/4.87139. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.75350/4.88174. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.75810/4.87042. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.75787/4.84149. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.75845/4.83919. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.75313/4.85316. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.75433/4.85760. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.75356/4.85481. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75324/4.86201. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.75605/4.85526. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.74892/4.85604. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.75269/4.87848. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.73997/4.88939. Took 0.20 sec\n",
      "ACC: 0.5625, MCC: 0.12267022872162016\n",
      "Epoch 0, Loss(train/val) 4.77720/4.78842. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.74835/4.76652. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.74306/4.74913. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.74141/4.74585. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.73962/4.74865. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.73606/4.75565. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.73772/4.75741. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.73256/4.75497. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.73192/4.76018. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.73150/4.76534. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.72979/4.76426. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.73081/4.76144. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.72869/4.77009. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.72906/4.77122. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.72445/4.77446. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.72449/4.77830. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.72503/4.77440. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.71960/4.78916. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.72027/4.78645. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.72687/4.76833. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.72516/4.79507. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.72806/4.76970. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.72631/4.77884. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.72441/4.77769. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.72534/4.78429. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.72089/4.77949. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.72027/4.79194. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.71508/4.78296. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.71658/4.78696. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.71901/4.77655. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.71755/4.78497. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.71475/4.80109. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.71402/4.79625. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.71638/4.77138. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.71344/4.80360. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.71750/4.77187. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.71708/4.76843. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.70747/4.81280. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.71704/4.78234. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.71628/4.77367. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.71382/4.77322. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.70843/4.79299. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.71249/4.77737. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.70880/4.79702. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.70808/4.76651. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.70828/4.79514. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.71201/4.79711. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.70788/4.79442. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.71095/4.77783. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.70582/4.81476. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.70999/4.78810. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.70868/4.78821. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.70850/4.81118. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.70695/4.79746. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.71061/4.80336. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.70668/4.82389. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.70159/4.82704. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.71044/4.77748. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.70889/4.79769. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.70093/4.82675. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.70855/4.80111. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.70085/4.83559. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.70880/4.78973. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.70830/4.80983. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.70066/4.82323. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.69877/4.80937. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.70890/4.79883. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.72511/4.77671. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.73234/4.77048. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.72978/4.77606. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.72871/4.78420. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.72349/4.79620. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.72632/4.77034. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.72758/4.78265. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.72393/4.77949. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.72371/4.77024. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.72027/4.78419. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.72011/4.77724. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.72523/4.78626. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.71673/4.80393. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.72257/4.77643. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.72070/4.79606. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.72028/4.79619. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 4.71875/4.79125. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.72430/4.76790. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.72321/4.76781. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.72132/4.77251. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.71992/4.77699. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.71843/4.79555. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.71773/4.81206. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.72300/4.79652. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.72131/4.78398. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.71740/4.78780. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.72280/4.78957. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.72137/4.79009. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.71720/4.78637. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.72099/4.78780. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.72150/4.77918. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.71336/4.77169. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.71692/4.79234. Took 0.20 sec\n",
      "ACC: 0.46875, MCC: -0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 5.11957/5.11414. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.09899/5.07517. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 5.08395/5.07112. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.08484/5.06898. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.08396/5.06758. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.08439/5.06552. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 5.08739/5.06975. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.08254/5.07024. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.08135/5.07200. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.08166/5.07285. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.07923/5.07160. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.07918/5.06825. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 5.07987/5.06655. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.08303/5.06323. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.07924/5.06233. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 5.08073/5.06515. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 5.07673/5.06890. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.07526/5.06436. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.07561/5.06240. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 5.07599/5.06793. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.07936/5.07084. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.07475/5.06921. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.07068/5.06343. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.06976/5.07125. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.07239/5.06572. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 5.06960/5.07778. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 5.06918/5.06799. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.07435/5.06537. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.06613/5.07828. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.07222/5.07223. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.06907/5.08143. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.08025/5.06447. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 5.07444/5.07168. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 5.07020/5.07652. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.07183/5.06662. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 5.06716/5.07580. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.06835/5.07958. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.06837/5.07840. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 5.06419/5.07706. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.06895/5.08166. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.06572/5.08550. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 5.06112/5.08487. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 5.06397/5.09269. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.06079/5.09874. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 5.07025/5.08122. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.07032/5.07955. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 5.05969/5.08118. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.06210/5.09000. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 5.06591/5.09248. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.06213/5.10794. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.06121/5.10367. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.06162/5.09418. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.05629/5.09740. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.05882/5.10163. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.05756/5.11484. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.05700/5.10823. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.05936/5.11821. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.05318/5.12137. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.05391/5.13779. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 5.06300/5.11545. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.05223/5.12940. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.05637/5.13009. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.05646/5.10697. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 5.05745/5.12567. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.05200/5.11850. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.05945/5.11343. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 5.05655/5.12089. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 5.05286/5.13366. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 5.05462/5.14864. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.05039/5.12253. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.05291/5.13188. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.05331/5.11354. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 5.04638/5.14735. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 5.05854/5.11327. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.05174/5.13864. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.05998/5.11053. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 5.04790/5.13995. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.05058/5.13509. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.05134/5.12479. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.04443/5.14066. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 5.06319/5.12920. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 5.05076/5.12527. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.05030/5.13538. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.04619/5.13353. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.04707/5.15420. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.04790/5.14802. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 5.04267/5.16593. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 5.04091/5.16955. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 5.04369/5.14927. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 5.04494/5.14837. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 5.04677/5.14063. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.04827/5.16077. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 5.04606/5.16736. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 5.04618/5.15829. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.04663/5.12849. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 5.05352/5.14364. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 5.03761/5.16731. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.04573/5.14451. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.04411/5.14664. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.03835/5.17635. Took 0.20 sec\n",
      "ACC: 0.4375, MCC: -0.11834526708278773\n",
      "Epoch 0, Loss(train/val) 4.88940/4.87516. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.86376/4.92308. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.86585/4.96348. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.86888/4.88900. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.85726/4.86884. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.85418/4.88690. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.85469/4.89198. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.85453/4.89848. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85220/4.89440. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.85389/4.90668. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.85272/4.89681. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.85147/4.90717. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85055/4.90834. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.84964/4.91429. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.84931/4.90115. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.85361/4.90471. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.85013/4.90870. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85058/4.91339. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.84899/4.92016. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.84745/4.92880. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.84798/4.91788. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.85014/4.91091. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.84932/4.92303. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84647/4.91541. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.84503/4.92368. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.84740/4.91590. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84280/4.92693. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84519/4.92422. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.84453/4.93168. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84476/4.90929. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84325/4.93821. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.84596/4.91189. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.84031/4.92665. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.84256/4.93090. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.84073/4.93559. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84181/4.93815. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84064/4.94641. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84034/4.94857. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.83646/4.94294. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.84048/4.95308. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.83431/4.97196. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.83677/4.95536. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.83611/4.95637. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.83506/4.97548. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.83409/4.98127. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.84128/4.92554. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.83378/4.99120. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.85385/4.89717. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.84637/4.90867. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.84363/4.92489. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.83895/4.93317. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.84521/4.92403. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.84100/4.93510. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.83938/4.94688. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.83872/4.95515. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.83668/4.95025. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.83539/4.97205. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.83465/4.94865. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.83658/4.95023. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.83544/4.96192. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.83405/4.96059. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.83026/4.96141. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.83538/4.94565. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.84190/4.94802. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.83734/4.95347. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.83068/4.96228. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.82904/4.98789. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83626/4.95694. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.83138/4.97847. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.83102/5.01022. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.82746/4.98786. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.83597/4.96470. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83050/4.98101. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83198/4.97284. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.82897/4.98953. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.83334/4.94530. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83112/4.97939. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.83075/4.97050. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.82722/4.99818. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.82664/4.97946. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.82837/4.98135. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.82743/4.98651. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.83119/4.95706. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.82650/5.01428. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.82608/4.97952. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.82443/4.98377. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.82929/4.99930. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.83073/4.95123. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.82649/4.96910. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82657/4.99210. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82564/4.98792. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82903/4.99061. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82627/4.97935. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82392/4.98100. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.82734/5.02722. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82234/4.98617. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.82167/5.02842. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82542/4.98157. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.82425/5.01919. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.82246/5.00842. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.09379580992210836\n",
      "Epoch 0, Loss(train/val) 5.00190/4.94536. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.94735/4.96817. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.94537/4.96309. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.94520/4.96009. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.94534/4.96295. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.94346/4.96185. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.94311/4.95785. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.94247/4.95962. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.94210/4.96197. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.94219/4.96066. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.93865/4.95949. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.94121/4.95791. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.93994/4.96105. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.94030/4.96114. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.93712/4.96208. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.93635/4.96046. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.93712/4.96180. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.93911/4.96080. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.93798/4.96676. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.93533/4.97197. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.93551/4.97474. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.93658/4.97302. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.93425/4.98047. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.93355/4.97971. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.93089/4.99727. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.92995/4.99549. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.93211/5.01436. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.93109/4.97996. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.93786/5.00126. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.92792/5.02723. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.93158/5.00143. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.92885/5.00994. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.92886/5.01761. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.93079/4.99699. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.92906/4.99451. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.93320/5.01585. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.92460/5.01150. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.93481/5.00525. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.93002/5.00130. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.93052/5.00844. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.92954/5.01876. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.92936/5.01872. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.92582/5.03848. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.92904/5.02569. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.92551/5.03106. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.92551/5.03647. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.92457/5.05154. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.92488/5.03903. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.92468/5.03350. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.92210/5.05002. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.92490/5.05258. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.92029/5.05827. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.92556/5.04633. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.92202/5.04843. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.92258/5.07209. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.92254/5.06299. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.92315/5.03966. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.91946/5.04793. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.92169/5.06337. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.92303/5.03554. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.92273/5.05025. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.92096/5.05809. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.92212/5.05054. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.92194/5.05897. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.92530/5.05137. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.92044/5.05589. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.92075/5.06589. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.91996/5.05196. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.92141/5.06581. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.92039/5.07511. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.92195/5.04160. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.91890/5.07519. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.92004/5.05964. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.91665/5.08600. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.92660/5.01896. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.91975/5.05616. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.91771/5.09865. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.92232/5.04794. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.91805/5.07645. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.91752/5.04327. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.92521/5.03424. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.92128/5.05164. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.91869/5.04415. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.91960/5.05863. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.92054/5.04860. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.91671/5.07629. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.91802/5.05566. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.91494/5.09586. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.91983/5.03883. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 4.91926/5.07266. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.91742/5.07115. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.91533/5.05743. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.91303/5.07556. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.91582/5.06737. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.91329/5.09218. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.91954/5.05771. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.91552/5.07565. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.91480/5.06983. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.91335/5.08067. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.91311/5.08294. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.0629940788348712\n",
      "Epoch 0, Loss(train/val) 4.95543/4.84969. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86096/4.85483. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.85703/4.84874. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.85516/4.84444. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.85528/4.84142. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.85375/4.83996. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.85616/4.83925. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.85271/4.83867. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.85651/4.83806. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.85522/4.83889. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.85451/4.83830. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.85327/4.84453. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85593/4.84082. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85575/4.84283. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85361/4.84595. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.84984/4.84826. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85121/4.85138. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.84998/4.84957. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85109/4.85096. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.84815/4.85277. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84655/4.84824. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85066/4.85083. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85035/4.85272. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84604/4.85490. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.84163/4.85515. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.84465/4.86036. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84530/4.85696. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84203/4.86443. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.84058/4.86530. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84578/4.86544. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84485/4.86497. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.83819/4.87424. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.84538/4.87164. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84140/4.86241. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85228/4.85259. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84740/4.85874. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84440/4.86120. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.84340/4.86338. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84967/4.87338. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84378/4.85780. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84447/4.86477. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84319/4.86502. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.84403/4.86561. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.84203/4.85920. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.84222/4.87489. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84233/4.87803. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84442/4.87468. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84033/4.87874. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.84350/4.87383. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.84050/4.87222. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.83994/4.87104. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.83699/4.88585. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.83657/4.88426. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.84344/4.88884. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.84187/4.87559. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.84092/4.88417. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84475/4.87161. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.83551/4.87942. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.84068/4.86965. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.84145/4.87322. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.83476/4.88214. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84109/4.88393. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.83986/4.87460. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.84048/4.87555. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.83669/4.88817. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.83995/4.88202. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.83756/4.88935. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83414/4.88503. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.83756/4.88069. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.85032/4.84584. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.84406/4.87894. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83963/4.87267. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83428/4.88905. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.83551/4.87989. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.83561/4.90146. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.83651/4.87659. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83990/4.87704. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.83503/4.88615. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.83243/4.89046. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.83125/4.89769. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.83803/4.87342. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83801/4.88786. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.84040/4.89553. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.83062/4.88620. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83637/4.89880. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83557/4.86906. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.83478/4.89198. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83443/4.88966. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83553/4.90175. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83171/4.89337. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.83108/4.90657. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.84634/4.85761. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.83910/4.87442. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83794/4.87118. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.85160/4.86624. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.84354/4.85272. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.84524/4.85470. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83789/4.85423. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.84174/4.86347. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.83848/4.86604. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.025861699363244256\n",
      "Epoch 0, Loss(train/val) 5.05906/4.96193. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.96231/4.96881. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.96394/4.96903. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.96518/4.96883. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.96673/4.97991. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.96493/4.99320. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.96469/4.99654. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.95942/4.99733. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.95853/4.99893. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.95723/4.99644. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.95638/4.99521. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.95326/4.99825. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.95608/4.99966. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.95386/4.99293. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.95445/4.97618. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.95379/4.97763. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.95768/4.98414. Took 0.22 sec\n",
      "Epoch 17, Loss(train/val) 4.95321/4.98402. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.95311/4.98352. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 4.95326/4.97517. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 4.95053/4.97935. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.94984/4.98794. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.95243/4.98849. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.94992/4.99036. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.95066/4.97956. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.95042/4.97795. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.94905/4.97766. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.95114/4.98347. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.94937/4.98156. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.94775/4.98334. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.94902/4.97325. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.94843/4.97208. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.94972/4.98894. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.95018/4.98846. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.94943/4.97939. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.94438/4.99234. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.94925/4.98907. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.94613/4.99255. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.94278/5.00044. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.94607/4.99468. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.94670/4.97906. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.93904/4.98553. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.94152/4.97597. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.94373/4.99209. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.93898/4.98422. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.94562/4.98303. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.94103/4.98397. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.94308/4.98260. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.94059/4.99906. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.93963/4.99153. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.94131/4.98325. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.94133/4.98949. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.93552/4.98541. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.93843/4.98644. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.93344/4.99555. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.94036/4.99239. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.93493/5.00058. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.94227/4.98197. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.93927/4.98749. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.94017/4.97858. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.93220/4.98825. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.93779/4.98080. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.94143/4.98520. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.93732/4.97698. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.93529/4.98259. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.93852/4.99003. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.93739/4.99192. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.93668/4.98361. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.93178/4.98474. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.93468/4.99983. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.93390/4.99103. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.93482/4.99243. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.93094/4.99452. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.93110/4.97816. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.92959/4.99183. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.93538/4.99062. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.93023/4.99029. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 4.93315/4.97645. Took 0.21 sec\n",
      "Epoch 78, Loss(train/val) 4.93258/4.97559. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.93682/4.98906. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.93605/4.98269. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.93313/4.99401. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.93189/4.98785. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.92380/4.97926. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.93639/4.97083. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.93778/4.99011. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.94295/4.96953. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.93956/4.96406. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.93750/4.99007. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.94133/4.98056. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.93345/5.00279. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.93487/5.00803. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.93571/5.00254. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.93717/4.97584. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.93893/4.96970. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.94206/4.96652. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.93722/4.98024. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.93286/4.97855. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.93123/4.99838. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.92736/4.98934. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.04350001325672093\n",
      "Epoch 0, Loss(train/val) 5.06425/5.03449. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.00339/4.98802. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.00028/4.98436. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.00152/4.99098. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.00263/4.99306. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 5.00168/4.99418. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.99628/4.98696. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.99353/4.98771. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.99597/4.98667. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.99383/4.98892. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.99512/4.98600. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.99701/4.98998. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.99327/4.98530. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.99251/4.98316. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.99017/4.99305. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.98966/4.98742. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.98778/4.98352. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.98577/4.98484. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.98676/4.98151. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.98904/4.98083. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.98481/4.97439. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.98211/4.98292. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.98384/4.97582. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.98351/4.97557. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.98395/4.97266. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.97968/4.96812. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.98225/4.96305. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.97600/4.97392. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.97959/4.96996. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.97653/4.97199. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.97572/4.96353. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.97735/4.96574. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.97966/4.97184. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.97626/4.97031. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.97440/4.96663. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.97381/4.97430. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.97537/4.96980. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.97604/4.96763. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.97137/4.96490. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.96974/4.97314. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.96634/4.96136. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.97334/4.96926. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.98561/4.97972. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.97955/4.97013. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.98796/5.00946. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.99358/4.98718. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.98761/4.97806. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.98595/4.98073. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.97847/4.97572. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.97680/4.97755. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.98163/4.98707. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.97920/4.98620. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.97592/4.96766. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.97431/4.97870. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.97254/4.96755. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.97698/4.99305. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.97247/4.97693. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.97322/4.96409. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.97622/4.97603. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.97344/4.96713. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.97572/4.97397. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.97288/4.96286. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.97214/4.96659. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.97370/4.96973. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.97291/4.96795. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.97300/4.96631. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.96648/4.95465. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.97075/4.96308. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.96971/4.96340. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.96991/4.95792. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.96654/4.97171. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.97323/4.97017. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.96672/4.95899. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.96883/4.96747. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.96613/4.97076. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.96872/4.97868. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.97007/4.97485. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.96605/4.96915. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.96993/4.96262. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.96550/4.96553. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.96382/4.97385. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.96974/4.96936. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.97104/4.97034. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.96574/4.96957. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.96920/4.97824. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.96229/4.96287. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.96897/4.96582. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.96591/4.97536. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.95898/4.97239. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.96453/4.96736. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.96863/4.98049. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.96681/4.97113. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.95897/4.97310. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.95637/4.97089. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.96462/4.97933. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.96396/4.97754. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.95751/4.97746. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.96719/4.99358. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.98320/4.98552. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.97379/4.97577. Took 0.20 sec\n",
      "ACC: 0.546875, MCC: 0.0778902520624173\n",
      "Epoch 0, Loss(train/val) 4.84092/4.77398. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.79619/4.78857. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.79888/4.77683. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79153/4.76946. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.78031/4.76850. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.77901/4.76990. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.78260/4.77048. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.78077/4.77374. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.78140/4.77427. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.77848/4.77465. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.77527/4.77426. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.77621/4.77337. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.77245/4.77537. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.77258/4.77293. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.76929/4.77469. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.77578/4.77555. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.76511/4.77867. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.76351/4.77994. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.76879/4.78549. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.76678/4.79134. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.76497/4.77485. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.76582/4.77694. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.76248/4.77719. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.76060/4.77744. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.76595/4.78216. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.76437/4.77627. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.75972/4.77275. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.76047/4.77718. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.75538/4.79334. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.76035/4.77076. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.76429/4.78068. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.75459/4.78012. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.75975/4.77968. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.76104/4.77352. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.75434/4.78293. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.75979/4.76701. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.75658/4.77519. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.75261/4.77320. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.75363/4.77791. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.75704/4.77039. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.76000/4.77875. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.75324/4.77591. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.75595/4.77062. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.75556/4.77634. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.75919/4.77583. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.75568/4.77494. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.75955/4.77474. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.75715/4.76460. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.74905/4.76679. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.75612/4.76686. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.75279/4.78602. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.75641/4.75969. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.75529/4.77606. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.74733/4.77555. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.75867/4.76913. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.75851/4.78336. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.75658/4.78321. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.75781/4.77293. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.75663/4.77094. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.75527/4.77235. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.75312/4.78112. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.75412/4.76243. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.75582/4.77089. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.74863/4.77129. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.75744/4.76767. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.75396/4.76950. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75103/4.76936. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.74837/4.77585. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.76414/4.77000. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.76031/4.76614. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.76018/4.76533. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.75667/4.76226. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.75433/4.76717. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.75672/4.77490. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.75754/4.78040. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 4.76224/4.79382. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.76142/4.74644. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.76259/4.74490. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.75548/4.76029. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.75706/4.75118. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.75623/4.77542. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.75420/4.77183. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.75243/4.77924. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.75593/4.78395. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.75381/4.78132. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.75437/4.77205. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.75309/4.75299. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.75442/4.76296. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.75526/4.76044. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.74895/4.75941. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.75520/4.78255. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.75286/4.76168. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.74865/4.78733. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.75011/4.79867. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.75640/4.77762. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75119/4.77672. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.75309/4.78512. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.74885/4.79355. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.75063/4.77648. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.74659/4.77547. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.08222643447147887\n",
      "Epoch 0, Loss(train/val) 4.85491/4.81480. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.81925/4.82141. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.81389/4.82072. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.81022/4.82324. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.81239/4.82577. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.81139/4.83152. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.80927/4.83081. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.81414/4.82564. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.81086/4.82141. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.80938/4.82060. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.80972/4.81681. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.80735/4.81175. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.80490/4.81132. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.80312/4.81662. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.80634/4.80778. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.80724/4.81295. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.80439/4.81022. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.80428/4.80685. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.80327/4.82012. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.79777/4.81127. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.79990/4.79950. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.79656/4.79073. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.79343/4.82530. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.79620/4.82059. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.79675/4.82516. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.79162/4.84010. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.79147/4.83826. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.79547/4.82510. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.79076/4.82702. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.79128/4.83563. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.79088/4.82385. Took 0.21 sec\n",
      "Epoch 31, Loss(train/val) 4.80060/4.82408. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 4.79843/4.81970. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.79209/4.83096. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.78937/4.84484. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.79207/4.83274. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.78747/4.82667. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.78404/4.82710. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.78687/4.83070. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.78991/4.80177. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.78944/4.81914. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.79050/4.80980. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 4.78613/4.80924. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.78509/4.80458. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.78524/4.81823. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.78598/4.82717. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78656/4.81782. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.77825/4.82549. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.78156/4.82271. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.78313/4.82911. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.77673/4.82133. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.78418/4.81031. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.78351/4.82340. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.78370/4.81749. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.79636/4.80849. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.79062/4.81037. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.79114/4.81527. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.77890/4.82505. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.77533/4.82490. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.78110/4.82123. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.78314/4.81578. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.77915/4.81728. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.78364/4.81656. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 4.77704/4.81479. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.78046/4.80958. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.77644/4.80716. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.77546/4.83200. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.77519/4.78879. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.78336/4.82783. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77330/4.82342. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.77427/4.80292. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.77625/4.81253. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.77477/4.84322. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.77454/4.81022. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77010/4.81901. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.77137/4.82407. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.77718/4.84609. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77896/4.81178. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.77374/4.82109. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.77611/4.82625. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.77482/4.81844. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.77427/4.80943. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.77554/4.79598. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.77397/4.80425. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.77493/4.80707. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.76734/4.82088. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.77538/4.79850. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.76562/4.81409. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.77398/4.81374. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.77384/4.80686. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.77425/4.80578. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.76717/4.80260. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.77516/4.83761. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77178/4.80608. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77587/4.82929. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.76980/4.81242. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.76519/4.82360. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.76475/4.82804. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.77200/4.80786. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.77057/4.79063. Took 0.22 sec\n",
      "ACC: 0.515625, MCC: 0.025552902682603722\n",
      "Epoch 0, Loss(train/val) 4.66952/4.68835. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.62347/4.63160. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.61896/4.61140. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.60722/4.61631. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.60568/4.62404. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.60963/4.62653. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.61042/4.62106. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.61016/4.61695. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.60770/4.61899. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.60951/4.61733. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.60612/4.61669. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.60580/4.61684. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.60593/4.62633. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.60783/4.61769. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.60655/4.61564. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.60749/4.61731. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 4.60679/4.61583. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.60711/4.60998. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.60280/4.60966. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.60006/4.61198. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.60242/4.60363. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.59768/4.60565. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.59944/4.60525. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 4.59801/4.60484. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.59641/4.60417. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.59761/4.60886. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.59840/4.60756. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.59475/4.61470. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.59371/4.60725. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.59412/4.60666. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.59011/4.60815. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.59125/4.61273. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.58626/4.60619. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.58942/4.61687. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.58888/4.60871. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.59344/4.60110. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.58008/4.63179. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.58905/4.61458. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.57900/4.61333. Took 0.22 sec\n",
      "Epoch 39, Loss(train/val) 4.58354/4.62072. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.58718/4.62159. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.58293/4.62779. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.58230/4.62549. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.58418/4.61415. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.58790/4.62044. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.58411/4.62693. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.58328/4.62991. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.57952/4.62496. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.57761/4.63681. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.57534/4.63040. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.57206/4.63189. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.58149/4.63203. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.57990/4.62974. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.57602/4.62288. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.57466/4.63525. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.59515/4.61417. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.58854/4.60309. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.58614/4.60158. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.58456/4.61018. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.58571/4.60961. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.58402/4.60126. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.57617/4.63499. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.58055/4.62030. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.57926/4.62320. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.58268/4.63259. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.57185/4.63886. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.57345/4.63754. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.57280/4.63424. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.57486/4.63312. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.57777/4.64602. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.56716/4.63180. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.57484/4.64540. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.57940/4.61391. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.57301/4.64615. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.57661/4.63927. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.57433/4.62728. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.56687/4.62587. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.56828/4.64390. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.57394/4.62658. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.56982/4.63701. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.57646/4.61729. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.60318/4.62782. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.59521/4.61318. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.58963/4.60594. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.58527/4.61990. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.58294/4.63006. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.58323/4.63093. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.57798/4.61754. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.57821/4.61981. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.57635/4.63347. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.57750/4.63920. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.57565/4.63216. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.57472/4.62973. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.57604/4.63771. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.57629/4.62016. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.57249/4.63956. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.57447/4.63007. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.57037/4.63091. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.57845/4.62796. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.57487/4.64107. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.03138824102871723\n",
      "Epoch 0, Loss(train/val) 4.95109/4.90458. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.94855/4.93949. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.95384/5.05416. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.95685/4.98885. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.93481/4.94459. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.92925/4.97003. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.93161/4.96906. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.92806/4.97730. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.92798/4.98973. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.92961/4.98518. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.92710/4.98771. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.92686/4.98993. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.92207/5.01137. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.92618/5.00086. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.92218/5.01125. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.92086/5.01446. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.92017/4.99429. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.92341/4.99505. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.91902/5.00946. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.91787/5.00734. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.92073/5.00678. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.91902/5.00913. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.91547/5.00732. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.91725/5.00636. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.91629/5.02164. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.91825/5.01021. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.91310/5.02859. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.91268/5.02073. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.91687/5.01304. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.91271/5.02107. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90857/5.03231. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.91617/5.02543. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.90884/5.03089. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.91110/5.02593. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.91611/5.02787. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90886/5.03509. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.91150/5.02052. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.91022/5.03635. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.90545/5.06287. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90805/5.02374. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.91162/5.04307. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.91114/5.04048. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.90273/5.07772. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.90956/5.08485. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.91050/5.04238. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.91097/5.02220. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.90664/5.03322. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.90838/5.04271. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.90633/5.03493. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.90292/5.05627. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.90432/5.03816. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.91027/5.02537. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.90852/5.02044. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.90927/4.98559. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.90710/5.03668. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.90900/5.00173. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.90449/5.04502. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.90299/5.02356. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.89722/5.07172. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.90824/5.02346. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.90325/5.04344. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.90719/5.02577. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.90092/5.03328. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.90818/5.04540. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.90189/5.03421. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.90194/5.04517. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.89966/5.01971. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.91141/5.01313. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.89659/5.04601. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.89882/5.02837. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.90191/5.04477. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.89840/5.05406. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.90194/5.01001. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.90308/5.02646. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.89628/5.02945. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.90277/5.03253. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.89896/5.05315. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.89647/5.02963. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.89589/5.03631. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.90025/5.04438. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.89443/5.05783. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.89386/5.07584. Took 0.22 sec\n",
      "Epoch 82, Loss(train/val) 4.89877/5.01867. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 4.89127/5.02055. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 4.89294/5.05821. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 4.89763/5.05117. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.89219/5.04389. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.89285/5.06676. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.88712/5.07057. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.89183/5.03333. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.89349/5.09393. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.89015/5.04649. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.89043/5.06282. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.89384/5.06112. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.89010/5.05584. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.89001/5.07400. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.89110/5.04435. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.89767/5.00567. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.89576/5.02149. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.90340/5.00603. Took 0.19 sec\n",
      "ACC: 0.4375, MCC: -0.10421364855284106\n",
      "Epoch 0, Loss(train/val) 5.05189/4.99979. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.01758/4.99986. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.02514/4.99918. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.02998/5.00702. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.02163/5.02066. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.01265/5.00653. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.00920/5.00401. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.01303/5.00899. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 5.01302/5.00789. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.01168/5.00632. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 5.00942/5.00648. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 5.01187/5.00541. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.00995/5.00297. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.00970/5.00326. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.00832/5.00853. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 5.00824/5.00914. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 5.00810/5.00661. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.01013/5.00656. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.00769/5.00585. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.00644/5.00418. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.00798/5.00491. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 5.00898/5.00575. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.00478/5.00561. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.00419/5.00596. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.00490/5.00349. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.00510/5.00768. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.00272/5.00477. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.01060/5.00942. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.00779/5.00846. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.00732/5.00473. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.00503/5.00359. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 5.00420/5.00499. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.00312/5.00512. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.00465/5.00608. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.00348/5.00543. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.00357/5.00781. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.00272/5.00877. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.00364/5.00878. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 5.00107/5.00882. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.00033/5.00999. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.00466/5.01126. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 5.00111/5.00969. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.00113/5.01219. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.00087/5.01379. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 5.00232/5.01622. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 5.00068/5.02247. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.00179/5.01175. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.99789/5.02167. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.99936/5.01946. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.99767/5.01737. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.99910/5.01688. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.99848/5.02820. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.99773/5.01170. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.99834/5.02043. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.99947/5.02672. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.99822/5.02890. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.99984/5.02587. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.00307/5.01778. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.00405/5.02036. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.00414/5.01529. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.00002/5.02015. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.99927/5.01603. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.99640/5.02112. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.99869/5.03155. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.99403/5.03623. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.99900/5.01925. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.00346/5.01917. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.00099/5.02216. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.99738/5.04971. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.99379/5.03715. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.99663/5.04627. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.99409/5.04443. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.98957/5.05911. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.99388/5.05254. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.99101/5.04517. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 4.99095/5.05380. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.99089/5.07284. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.99104/5.06269. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.98762/4.98280. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.01103/5.01283. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.00325/5.00321. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.99675/5.02107. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.99632/5.01491. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.99903/5.02391. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.98983/5.03965. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.99155/5.04605. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.99528/5.04378. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.99298/5.04404. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.99301/5.04890. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.98734/5.06268. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.99540/5.03616. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.98577/5.05344. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.99438/5.00551. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.99590/5.02291. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.99666/5.02237. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.99422/5.02692. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.99191/5.03038. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.99384/5.03210. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.98644/5.03941. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.99049/5.03166. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.10677275981195139\n",
      "Epoch 0, Loss(train/val) 4.86704/4.88145. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.84732/4.85478. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.84214/4.87174. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.83886/4.87770. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.83774/4.88372. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.83915/4.88702. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.83885/4.87809. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.84008/4.89781. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.83867/4.88542. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.83438/4.87984. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.83740/4.87806. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.83612/4.87409. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.83519/4.86989. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.83582/4.86867. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.83256/4.86693. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.82828/4.87510. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.82942/4.88201. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.82875/4.88055. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.82812/4.87994. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 4.82127/4.90048. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.82539/4.87858. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.82494/4.88833. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.82290/4.88087. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.82339/4.88766. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82534/4.89254. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82091/4.89248. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.81892/4.90131. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.82409/4.89241. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.81825/4.91708. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.81894/4.89982. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.82184/4.89652. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.82687/4.88209. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.82349/4.89971. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.82405/4.87560. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.81923/4.90640. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82264/4.89966. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.81784/4.91187. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.81785/4.91278. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.81864/4.91321. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.81596/4.91057. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.81950/4.91974. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.81514/4.92452. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.81417/4.92532. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.81235/4.91384. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.81628/4.93004. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.81299/4.94037. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.81487/4.91051. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.81609/4.92778. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.80754/4.94584. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.80735/4.93957. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.80827/4.93683. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.80502/4.95733. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.80705/4.97443. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.80732/4.96221. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.80532/4.95645. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.80795/4.94419. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.80885/4.93579. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.80817/4.93718. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.80124/4.95904. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.81252/4.92094. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.80457/4.95944. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 4.80463/4.95768. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.80619/4.97068. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.80399/4.95511. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.80449/4.95283. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.79897/4.94079. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.80233/4.95812. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.80695/4.94753. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.80597/4.95438. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.80192/4.96302. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.79868/4.95263. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.80699/4.94446. Took 0.21 sec\n",
      "Epoch 72, Loss(train/val) 4.79920/4.97843. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.80615/4.94031. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.79998/4.95736. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.80323/4.97338. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.79901/4.98214. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.79452/4.95167. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.80154/4.96218. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.80129/4.98562. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.79904/4.98653. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.79277/4.99508. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.80003/4.95375. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.79995/4.97034. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.79853/4.98894. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.79454/5.00012. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.79293/5.00506. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.79459/4.96913. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.79752/4.94548. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82232/4.90764. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.83334/4.88656. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.82843/4.90944. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.82889/4.91233. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82719/4.91448. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82766/4.91574. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.82360/4.92191. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82590/4.92434. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.82109/4.93306. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.81567/4.93392. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.81961/4.92043. Took 0.20 sec\n",
      "ACC: 0.5, MCC: -0.009163235455864739\n",
      "Epoch 0, Loss(train/val) 4.81627/4.73957. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.75754/4.79510. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.76542/4.79696. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.76387/4.75134. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.76228/4.73200. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.75497/4.73892. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.75236/4.75021. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.75340/4.75317. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.75297/4.74748. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.75553/4.74951. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.75395/4.74520. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 4.75244/4.74992. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.74964/4.75630. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 4.75167/4.75429. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.74946/4.76554. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.75111/4.76212. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.75201/4.74754. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.74831/4.75546. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.74803/4.75593. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.74908/4.75940. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.74476/4.77926. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.75339/4.76441. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.75332/4.75307. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.74768/4.76061. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.74761/4.77965. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.74655/4.79016. Took 0.22 sec\n",
      "Epoch 26, Loss(train/val) 4.74359/4.78085. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.74409/4.79654. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.74670/4.77912. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.73942/4.79491. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.73899/4.80241. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.73987/4.79663. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.73995/4.79222. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.74183/4.77716. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.73837/4.81478. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.73704/4.80834. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.73472/4.83113. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.73474/4.80043. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.73463/4.79976. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.73413/4.80792. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.74059/4.81207. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.73325/4.81487. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.73196/4.84141. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.72718/4.81360. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.73280/4.84095. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.72614/4.84564. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.72726/4.83659. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.73544/4.78844. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.73161/4.83842. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.73286/4.82801. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.72495/4.88723. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.72714/4.84447. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.73145/4.83610. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.72959/4.83452. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.73141/4.85193. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.72251/4.87047. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.72503/4.84245. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 4.72342/4.88284. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.72766/4.85301. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.72977/4.83680. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.72186/4.85789. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.71505/4.88649. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.72741/4.84037. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.72146/4.84453. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.72415/4.83774. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.72268/4.80173. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.74928/4.78384. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.73775/4.82129. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.73255/4.84651. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.73360/4.81964. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.73596/4.81945. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.73202/4.83229. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.73072/4.84005. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.72845/4.85355. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.73244/4.85175. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.73257/4.83340. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.72882/4.85825. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.71778/4.85200. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.73243/4.81055. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.73700/4.80014. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.73321/4.81360. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.73400/4.77587. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.74634/4.76360. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.73680/4.79572. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.73305/4.81506. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.73295/4.82066. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.73349/4.81226. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.72778/4.84065. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.73206/4.83908. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.73012/4.83216. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.72536/4.84130. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.73089/4.80379. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.72716/4.85238. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.73345/4.81916. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.73006/4.81813. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.72829/4.84524. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.72436/4.86370. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.72522/4.82298. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.72343/4.84161. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.72696/4.84795. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.15694120514358612\n",
      "Epoch 0, Loss(train/val) 5.15692/5.11752. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.14043/5.14788. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.12958/5.13599. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.12697/5.13153. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 5.13071/5.13622. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 5.12982/5.13521. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 5.13045/5.13077. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 5.13132/5.12859. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.12928/5.12910. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.13003/5.13112. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.13092/5.13533. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.12512/5.13982. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 5.12710/5.13781. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.12580/5.13765. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.12528/5.13989. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 5.12556/5.14713. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 5.12673/5.14020. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 5.12694/5.14579. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 5.12443/5.14256. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.12268/5.15552. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.11957/5.14631. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 5.12313/5.13394. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.12125/5.13413. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 5.12061/5.14395. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.11950/5.14338. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.11752/5.15069. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.11754/5.14715. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.11618/5.15379. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 5.11625/5.16299. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.11452/5.15789. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.11934/5.14577. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.11922/5.16046. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 5.11292/5.17775. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 5.11313/5.15454. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 5.11416/5.17080. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.11089/5.16662. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.11380/5.17785. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 5.11174/5.16749. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 5.11156/5.17179. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 5.11437/5.16313. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.11475/5.13730. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 5.12023/5.13840. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 5.11517/5.16329. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 5.11393/5.15807. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 5.11679/5.13213. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.11857/5.12829. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.11653/5.14713. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 5.11512/5.13828. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 5.11318/5.15319. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 5.11278/5.14943. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.11382/5.14993. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.11097/5.14777. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 5.11055/5.14417. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 5.11679/5.15811. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.11161/5.16183. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.10993/5.16800. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.10612/5.16997. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 5.10726/5.16378. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 5.11124/5.17680. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 5.11331/5.16760. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.10359/5.18959. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.10737/5.17492. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.11202/5.16488. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.10928/5.17659. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 5.10486/5.16085. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.10367/5.19425. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.10233/5.16943. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.10828/5.18423. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.10613/5.19449. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 5.09964/5.20059. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.11327/5.17017. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.10170/5.19047. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.10949/5.20349. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.10448/5.19937. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 5.09946/5.20445. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.09992/5.19099. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.10498/5.19081. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 5.10799/5.18525. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.10136/5.22196. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 5.10254/5.17896. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.10548/5.18932. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.10268/5.20432. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.09639/5.22702. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.09484/5.20941. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.09899/5.21401. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.09974/5.21491. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.10400/5.20243. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.09648/5.21392. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.09653/5.21922. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.09986/5.23247. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.10202/5.19895. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.09737/5.20988. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.09830/5.21529. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.09293/5.22833. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 5.09623/5.22410. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.09418/5.22579. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.09856/5.22963. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.09917/5.21883. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 5.10154/5.24073. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 5.09711/5.20974. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 4.98930/4.92231. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.93028/4.90221. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.93000/4.90247. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.93003/4.90949. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.92654/4.92145. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.92173/4.92701. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.91747/4.91627. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.92342/4.91533. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.92006/4.92197. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.92138/4.93048. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.91933/4.92415. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.91537/4.92608. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.91657/4.92252. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.91723/4.92303. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.91321/4.92587. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.91425/4.92158. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.91372/4.91728. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.91271/4.92833. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.91223/4.92073. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.91058/4.92288. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.90922/4.91730. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.90954/4.92559. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.91365/4.92378. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.91097/4.92328. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.90951/4.92993. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.91028/4.91764. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.91460/4.91712. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.90698/4.93076. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90941/4.91347. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.91244/4.91333. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90990/4.92502. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.90445/4.92112. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.90789/4.90973. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.90675/4.90937. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.90863/4.90101. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90549/4.90738. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.90879/4.90000. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.90501/4.91644. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.90425/4.90542. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90047/4.91156. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90005/4.88907. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.90106/4.91530. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.90214/4.90005. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.89913/4.90599. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.89761/4.89818. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.90155/4.90485. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.90225/4.92026. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89768/4.91623. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89315/4.90940. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.89300/4.91150. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89537/4.91061. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.89278/4.90412. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89324/4.90020. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.89859/4.91909. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.90060/4.90524. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.89990/4.91340. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.90355/4.91100. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.89894/4.91460. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.89838/4.90080. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89922/4.90525. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.89223/4.91114. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.89363/4.89705. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.89212/4.90632. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.89830/4.90557. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.89668/4.90674. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.89438/4.88505. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.90435/4.91211. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.89336/4.89011. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.89298/4.89837. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.89730/4.89572. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.89175/4.90223. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.88806/4.89994. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.89510/4.91262. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.89524/4.89412. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.89143/4.89954. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.88811/4.89848. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.90202/4.89085. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.89711/4.90167. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.89340/4.89865. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.89124/4.89485. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88779/4.88899. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.88871/4.90295. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.89422/4.90957. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.88675/4.91782. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88955/4.91544. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.89023/4.93081. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.88794/4.91596. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.88760/4.90880. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.89268/4.91718. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.88873/4.92569. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.88942/4.90437. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.89074/4.90202. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.88810/4.90895. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.89106/4.90628. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.89175/4.93272. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.89097/4.92164. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.88940/4.90468. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.88742/4.91190. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.88532/4.91291. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.88469/4.92133. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.011958266722236254\n",
      "Epoch 0, Loss(train/val) 4.92572/4.83761. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.86398/4.87146. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 4.85281/4.88938. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.85168/4.89413. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.84700/4.90313. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.85067/4.90701. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.84689/4.90472. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.84472/4.89654. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.84570/4.89520. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.84805/4.89447. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84400/4.89350. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.84032/4.89629. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.84151/4.89359. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.84103/4.89294. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.83990/4.88626. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.83956/4.89303. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.84195/4.89101. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.84129/4.89076. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83946/4.88912. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.83722/4.88842. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.83832/4.89099. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.83888/4.89197. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.83827/4.88994. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.83539/4.88264. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.83527/4.89364. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.83591/4.89073. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.83741/4.88885. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.83288/4.88640. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.83230/4.87916. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.83252/4.87575. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.82858/4.88693. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.83268/4.88692. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.83550/4.88086. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.82827/4.87964. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.83081/4.89379. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.83203/4.88004. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.82812/4.87726. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.83142/4.88445. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.83125/4.88592. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.82391/4.88646. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.82523/4.88357. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.82453/4.90196. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 4.82742/4.90505. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.83004/4.88686. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.83012/4.88575. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.83834/4.87593. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.83646/4.86849. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.82977/4.87311. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.83198/4.87588. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.82687/4.88608. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.83093/4.88611. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.82567/4.88057. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.82607/4.87864. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.82619/4.88779. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.82706/4.86405. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.82525/4.87835. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.81726/4.88265. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.82007/4.88181. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.82587/4.89213. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.82425/4.88858. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.82647/4.90063. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.82510/4.89916. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.82616/4.90303. Took 0.18 sec\n",
      "Epoch 63, Loss(train/val) 4.82291/4.90045. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.82189/4.89647. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.82040/4.89662. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.82083/4.90300. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.82123/4.89060. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.81594/4.90023. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.81898/4.89403. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.82228/4.88682. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81770/4.88807. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.81955/4.90433. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.81937/4.88190. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81762/4.88698. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.82159/4.88644. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.81669/4.88318. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.81514/4.88725. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.82013/4.89568. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.81513/4.88796. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.80947/4.87282. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.81120/4.87268. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.80965/4.90383. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.81749/4.89407. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.81320/4.89365. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.81200/4.88840. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.80868/4.88433. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.81811/4.90601. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.81711/4.90689. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.81284/4.88656. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.80460/4.89165. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.81675/4.86806. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.81523/4.88235. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82452/4.88178. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.82990/4.88667. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82389/4.90772. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82721/4.89283. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82387/4.88052. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82396/4.88572. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.81883/4.88658. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.12682630177432805\n",
      "Epoch 0, Loss(train/val) 4.87488/4.81664. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.80175/4.80628. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.81991/4.83771. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.81616/4.86112. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.80223/4.82259. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.78993/4.81373. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.78780/4.82211. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79259/4.82622. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79070/4.81987. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.78712/4.81524. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.78684/4.81618. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78307/4.81673. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.78702/4.82261. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.78531/4.81668. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.77859/4.81686. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78281/4.82964. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78462/4.82335. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.78013/4.81600. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78060/4.82283. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78402/4.81256. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.77658/4.81222. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.77504/4.81580. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.77785/4.82153. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.77536/4.81645. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.77045/4.81600. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.77650/4.81387. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78517/4.82531. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78546/4.83728. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78732/4.82294. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.77901/4.82484. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.77591/4.80629. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77223/4.82099. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77550/4.80805. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.76744/4.81840. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.76948/4.81834. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77025/4.80918. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77243/4.78529. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.77349/4.81660. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77513/4.81183. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.78341/4.79381. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.78437/4.80756. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.79995/4.82608. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.78733/4.81896. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.78467/4.82748. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.78801/4.83606. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.78530/4.84251. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78624/4.83414. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.77519/4.86463. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.77735/4.85762. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.78468/4.83460. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.77910/4.83782. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.78264/4.83105. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.78136/4.82673. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.77835/4.83221. Took 0.21 sec\n",
      "Epoch 54, Loss(train/val) 4.77876/4.84640. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.77329/4.83911. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.77877/4.83094. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.77816/4.83172. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.77427/4.86017. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.77329/4.84800. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.77825/4.80963. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.77723/4.81864. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.78012/4.83389. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.77626/4.83395. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.77467/4.82979. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.77351/4.83114. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.77565/4.82318. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.77115/4.83211. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77624/4.83137. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77416/4.81631. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.76878/4.83495. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.76877/4.83373. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.76511/4.82283. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.77216/4.81308. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77269/4.82579. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.77339/4.83264. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.77125/4.82762. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77663/4.80738. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.77795/4.77990. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.77096/4.79145. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.77791/4.81438. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.77961/4.79741. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.76993/4.79124. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.76745/4.78927. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.77587/4.80028. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.77248/4.78555. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.76934/4.80803. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.76457/4.78972. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.76051/4.81157. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.77196/4.78135. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.77189/4.78704. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.76579/4.78337. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.76929/4.79694. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77031/4.79119. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.76812/4.80773. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.76290/4.80025. Took 0.21 sec\n",
      "Epoch 96, Loss(train/val) 4.76303/4.79120. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.76606/4.78504. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.76120/4.78921. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.76027/4.82447. Took 0.20 sec\n",
      "ACC: 0.484375, MCC: -0.06052275326688024\n",
      "Epoch 0, Loss(train/val) 4.78050/4.78178. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.76287/4.76312. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.76118/4.80679. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.76354/4.77779. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.74888/4.75722. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.74292/4.76973. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.74565/4.76131. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.74139/4.77421. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.74538/4.75857. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.74404/4.75357. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.74153/4.75434. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.73903/4.79473. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.74557/4.75140. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.74141/4.75491. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.74007/4.75344. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.73926/4.75776. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.74106/4.75854. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.74045/4.75837. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.73794/4.76953. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.74416/4.76256. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.74252/4.76390. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.73835/4.75835. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.73462/4.76721. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.73568/4.75047. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.73757/4.75743. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.73399/4.76118. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.73349/4.76504. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.73333/4.76854. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.73731/4.75137. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.73695/4.75094. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.73571/4.75547. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.73431/4.74569. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.73239/4.75644. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.73243/4.75928. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.73290/4.75745. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.73724/4.74678. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.73077/4.76953. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.74087/4.74301. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.73613/4.74723. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.73547/4.73585. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.72578/4.76389. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.73624/4.73743. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.72924/4.74504. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.72892/4.74755. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 4.73372/4.73715. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.72993/4.74567. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.72793/4.74364. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.72834/4.73580. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.72993/4.73720. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.72654/4.73911. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.71966/4.76833. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.73510/4.73671. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.72754/4.74542. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.72649/4.74519. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.72943/4.74489. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.72918/4.73598. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.72533/4.73597. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 4.72977/4.74171. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.72499/4.74177. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.72323/4.73963. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.72399/4.73639. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.72084/4.73895. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.72368/4.73425. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.72327/4.74825. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.72480/4.73463. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.72084/4.73000. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.72023/4.74849. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.72192/4.75411. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.72116/4.75109. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.72150/4.74415. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.72088/4.74983. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.72062/4.74411. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.72059/4.74622. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.71420/4.75757. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.71996/4.74944. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.71871/4.73986. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.71700/4.75130. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.71254/4.75726. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.72121/4.75216. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.71484/4.75400. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.72255/4.75113. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.72027/4.73656. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.71700/4.74575. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.71746/4.74426. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.71801/4.74384. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.71346/4.75036. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.72065/4.74708. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.71408/4.74300. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.71880/4.74775. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.71598/4.74167. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.71229/4.75645. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.71434/4.75622. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.71362/4.75849. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.71864/4.75027. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.71867/4.74918. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.71927/4.74440. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.71481/4.74594. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.71341/4.74923. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.71562/4.75580. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.72024/4.74879. Took 0.19 sec\n",
      "ACC: 0.4375, MCC: -0.12267022872162016\n",
      "Epoch 0, Loss(train/val) 4.96069/5.00220. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.91750/4.99750. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.91701/5.02009. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.91570/4.98944. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.92050/4.93837. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.91661/4.91123. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.91059/4.91219. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.90725/4.91648. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.90602/4.92354. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.90511/4.92179. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.90852/4.91087. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.90834/4.91183. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.90833/4.91486. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.90346/4.93008. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.90079/4.96087. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.90461/4.93552. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.89990/4.92254. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.89761/4.95680. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.90873/4.92877. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.90133/4.92073. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.90079/4.93800. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.89586/4.94464. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.90201/4.94069. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.90097/4.93716. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.89399/4.94453. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.89395/4.95251. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.89692/4.96108. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.89428/4.96052. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.89367/4.96083. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.89171/4.95686. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.89027/4.96232. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.89115/4.95952. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.89661/4.94609. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.88723/4.97286. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.89307/4.93838. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.89790/4.93173. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.89769/4.95106. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.89801/4.95239. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89452/4.95456. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.89381/4.96956. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.89515/4.98390. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.89266/4.97290. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89380/4.95748. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.88816/4.97384. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.89036/4.97470. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.88970/4.97130. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.88778/4.97783. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.88981/4.97488. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89178/4.97594. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.88933/4.97288. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.88792/4.96975. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.89369/4.93320. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.90763/4.93584. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.89573/4.92348. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.89321/4.94070. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.88841/4.95773. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.88981/4.95196. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.88874/4.95515. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.88956/4.96322. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89019/4.95473. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.88997/4.96831. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.89080/4.95585. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.88890/4.95814. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.89073/4.95606. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.88996/4.96212. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.88843/4.95999. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.88805/4.96349. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.88852/4.95622. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.88987/4.96228. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.88560/4.95080. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.88724/4.97682. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.88691/4.96560. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.88605/4.96346. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88720/4.98107. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.88578/4.96315. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.89152/4.92372. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.89580/4.95504. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.89239/4.95421. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.89089/4.94643. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.88619/4.95036. Took 0.21 sec\n",
      "Epoch 80, Loss(train/val) 4.88401/4.96692. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 4.88648/4.94681. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.88576/4.96526. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.88559/4.96265. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88473/4.94531. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.88459/4.94456. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.88438/4.95926. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.88577/4.96256. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.88953/4.94922. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.88329/4.95498. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.88429/4.96359. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.88153/4.95560. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.88114/4.96989. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.88702/4.96495. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88483/4.93876. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.88565/4.95942. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.88112/4.96659. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.88164/4.96032. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.88393/4.96377. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88249/4.94326. Took 0.20 sec\n",
      "ACC: 0.515625, MCC: 0.03138824102871723\n",
      "Epoch 0, Loss(train/val) 5.00677/4.98697. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.96409/5.00898. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.96610/4.99130. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.96546/4.97535. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.96629/4.96362. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.96607/4.95429. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.96319/4.95387. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.95820/4.95599. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.95934/4.96103. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.96152/4.95859. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.95762/4.95591. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.95862/4.95790. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.95735/4.95802. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.95358/4.95694. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.96122/4.95260. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.95733/4.95429. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.95894/4.96290. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.95404/4.96249. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.95358/4.96843. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.95878/4.96887. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.95220/4.96850. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.95169/4.97397. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.95448/4.96176. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.95407/4.96411. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.95486/4.96468. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.95141/4.96582. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.94767/4.97317. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.95129/4.96628. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.94887/4.96309. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.95145/4.96158. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.95006/4.97542. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.95157/4.97418. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.95110/4.97978. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.95117/4.97701. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.95263/4.97216. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.94984/4.97421. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.94887/4.97527. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.94401/4.97947. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.94758/4.97582. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.94885/4.96673. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.94948/4.97340. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.94856/4.97664. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.94638/4.98145. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.94573/4.98359. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.94324/4.98337. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.94328/4.98248. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.94502/4.98390. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.93902/4.99111. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.93453/4.99055. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.94218/4.98792. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.93488/4.99659. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.94225/4.97811. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.93994/4.98890. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.94061/4.98836. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.93448/4.99905. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.93842/4.98891. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.93640/5.00982. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.92993/5.01556. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.94028/5.01398. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.93454/5.00105. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.93572/5.00183. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.92981/5.02199. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.93714/5.00838. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.92971/5.00456. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.93660/5.01351. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.93589/5.01071. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.92903/5.02323. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.92912/5.03990. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.92782/5.00907. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.93323/5.02889. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.92949/5.03485. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.93106/5.00767. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.92619/5.04280. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.93026/5.01954. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.92228/5.05977. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.93424/5.00847. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.93390/5.03045. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.93680/5.00310. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.92205/5.03562. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.92911/5.03728. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.92885/5.04088. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.92444/5.04579. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.93407/5.02800. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.92581/5.03024. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.92177/5.05636. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.92251/5.03597. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.91822/5.07511. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.92196/5.04906. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.92102/5.04420. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.92051/5.06540. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.92520/5.04602. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.91738/5.05292. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.92190/5.07100. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.92169/5.05185. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.91972/5.07117. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.92048/5.07246. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.92150/5.03797. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.92608/5.06823. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.91679/5.03646. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.91666/5.08020. Took 0.19 sec\n",
      "ACC: 0.40625, MCC: -0.17930563858025494\n",
      "Epoch 0, Loss(train/val) 4.76456/4.77236. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.72931/4.70512. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.72435/4.70333. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.71743/4.71125. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.72025/4.71523. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.71753/4.71904. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.71819/4.73629. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.71758/4.75008. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.71731/4.74412. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.71174/4.73141. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.71025/4.74308. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.71157/4.73554. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.71020/4.73332. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.70867/4.74081. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.70804/4.74714. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.70688/4.74178. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.71737/4.72125. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.71427/4.70835. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.71238/4.71467. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.71133/4.71429. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.70489/4.71567. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.70980/4.72410. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.71204/4.72641. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.70821/4.72813. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.70846/4.73375. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.70916/4.73719. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.70441/4.74498. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.70393/4.73295. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.70704/4.73047. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.70482/4.74159. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.70411/4.73681. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.70634/4.73717. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.70720/4.72888. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.70394/4.74025. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.70730/4.73899. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.70502/4.73554. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.70172/4.74215. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.69990/4.74395. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.70219/4.74640. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.69965/4.74781. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.69813/4.75738. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.69674/4.76289. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.69913/4.75516. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.69601/4.73739. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.70130/4.74111. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.70568/4.73517. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.70315/4.72807. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.70101/4.74056. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.70168/4.74413. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.69994/4.74011. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.70150/4.73603. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.69795/4.74075. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.69721/4.74391. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.69562/4.74848. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.69866/4.73381. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.69043/4.74350. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.69424/4.74026. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.70265/4.74095. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.69849/4.73794. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.69531/4.73482. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.69341/4.74706. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.69640/4.74458. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.69797/4.74991. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.69284/4.73802. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.69084/4.74943. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.69200/4.74588. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.69290/4.74278. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.68859/4.74091. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.68503/4.75308. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.68708/4.74772. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.68988/4.74991. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.69532/4.73954. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.68802/4.74778. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.68514/4.74671. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.69090/4.73805. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.68764/4.75340. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.68159/4.76085. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.68896/4.73972. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.68421/4.74779. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.69820/4.74684. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.69306/4.74322. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.68123/4.74487. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.68691/4.75358. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.68046/4.75326. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.68463/4.74608. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.68519/4.74689. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.67901/4.74382. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.68525/4.73117. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.67988/4.76325. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.67849/4.77303. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.68210/4.73553. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.68814/4.72057. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.68319/4.73799. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.68613/4.74001. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.68344/4.73424. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.68280/4.74873. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.68168/4.75758. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.68476/4.74341. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.67126/4.76012. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.68047/4.75235. Took 0.19 sec\n",
      "ACC: 0.5, MCC: -0.02404930351219409\n",
      "Epoch 0, Loss(train/val) 4.81439/4.78754. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.78274/4.78634. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.78569/4.77958. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.78932/4.78080. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.78688/4.78948. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.78996/4.78369. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.78667/4.78076. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.77938/4.78004. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.78189/4.78332. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.78338/4.77896. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.78075/4.78428. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.77911/4.78679. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.77997/4.78692. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.77947/4.78697. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.77869/4.78828. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.77614/4.79326. Took 0.18 sec\n",
      "Epoch 16, Loss(train/val) 4.77618/4.78820. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.77702/4.79621. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.77657/4.78294. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.77587/4.78594. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.77172/4.79818. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.77007/4.79305. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.77220/4.79387. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.77160/4.80725. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.77227/4.79218. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.77155/4.80825. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.76874/4.80108. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.76504/4.82173. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 4.76886/4.80651. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.76720/4.80984. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.76318/4.81505. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.76972/4.81197. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.76099/4.83114. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.76578/4.81516. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.76618/4.82255. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.76021/4.81318. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.76399/4.82412. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.76292/4.81397. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.76389/4.81296. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.76028/4.82776. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.76433/4.81111. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.76056/4.83133. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76126/4.81964. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.76328/4.82086. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.75702/4.83380. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.75902/4.83167. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.76030/4.83742. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.75511/4.83224. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.75693/4.83780. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.75306/4.82927. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.75374/4.83654. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.75811/4.83670. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.75759/4.82460. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.75544/4.84464. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.75567/4.82881. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.75138/4.85828. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.74949/4.86239. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.75428/4.84935. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.75178/4.84302. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.75154/4.83086. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.75063/4.84988. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.75213/4.84584. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.75206/4.83368. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.74464/4.86947. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.75840/4.80936. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.76714/4.80220. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75256/4.85947. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.75182/4.82069. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.75611/4.81806. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.75367/4.81758. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.74412/4.84037. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.74856/4.86352. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.75236/4.83900. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.74694/4.83041. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.75252/4.85798. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.75261/4.82663. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.74955/4.84999. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.75366/4.85371. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.74446/4.83942. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.74582/4.87359. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.74673/4.84738. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.76551/4.82894. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.76063/4.83219. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.74751/4.87335. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.74353/4.85933. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.74507/4.81821. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.75800/4.78555. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.76452/4.81809. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.75421/4.82078. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.75096/4.85265. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.74261/4.83990. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.74925/4.83700. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.74357/4.84978. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.74517/4.87646. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.74363/4.85236. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.74375/4.88059. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.74311/4.87298. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.74775/4.80451. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.73461/4.85856. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.74726/4.85341. Took 0.19 sec\n",
      "ACC: 0.5, MCC: -0.002944085489318884\n",
      "Epoch 0, Loss(train/val) 4.77265/4.78107. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.76802/4.77633. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 4.76134/4.78392. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.76002/4.77851. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.76116/4.77746. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.75961/4.77953. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.75956/4.77766. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.75633/4.78201. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.75367/4.78625. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.75504/4.78297. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.75592/4.79322. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.75313/4.79005. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.75234/4.78476. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.75355/4.79275. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.75209/4.79685. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.75174/4.79714. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.75347/4.79564. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.75268/4.80114. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.74983/4.79725. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.75383/4.80774. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.75096/4.80501. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.75051/4.80925. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.74803/4.81260. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.74801/4.81155. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.74987/4.80412. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.75055/4.81646. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.75111/4.80944. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.75158/4.80046. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.74574/4.82173. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.75114/4.80631. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.74597/4.81432. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.75189/4.81050. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.74484/4.82261. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.75097/4.80916. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.74417/4.83147. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.74750/4.81012. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.74599/4.80863. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.74505/4.81679. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.74382/4.82289. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.74352/4.81893. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.74581/4.82696. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.74678/4.82222. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.74253/4.82021. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.74358/4.82163. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.74584/4.81464. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.74308/4.81897. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.73946/4.82923. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.73816/4.84460. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.74860/4.81151. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.74249/4.82841. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.73671/4.84326. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.74300/4.80552. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.74324/4.81978. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.73946/4.83025. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.73993/4.82277. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.74156/4.82058. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.73610/4.84970. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.74240/4.81450. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.73596/4.86686. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.73656/4.84415. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.73640/4.84030. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.74344/4.83688. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.73381/4.84685. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.73103/4.84699. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.73385/4.84586. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.73610/4.88331. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.73618/4.83311. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.73414/4.86611. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.73566/4.83836. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.74022/4.81810. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.73588/4.85533. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.73042/4.86538. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.73693/4.83216. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.73140/4.87233. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.73260/4.86159. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.73527/4.85631. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.73132/4.88472. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.73344/4.86682. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.72872/4.88254. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.73731/4.86957. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.73332/4.87768. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.73322/4.86036. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.72820/4.87038. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.72901/4.86268. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.72839/4.89498. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.72802/4.88723. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.73004/4.85686. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.73286/4.89640. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.73053/4.86270. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.73426/4.87081. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.72447/4.93461. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.72770/4.86064. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.72586/4.91382. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.72932/4.86050. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.72733/4.88517. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.72790/4.89858. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.72958/4.87470. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.72308/4.90073. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.72920/4.85934. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.72985/4.87008. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.15694120514358612\n",
      "Epoch 0, Loss(train/val) 4.92756/4.88730. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86637/4.92115. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.86407/4.92294. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.86417/4.91508. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.85974/4.91038. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.85763/4.91204. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.85632/4.91091. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.85563/4.90355. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85707/4.92533. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.85709/4.90766. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.85669/4.90829. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.85778/4.91231. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85549/4.91546. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.86020/4.91208. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.86257/4.90424. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85894/4.89913. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.86439/4.89911. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.86382/4.87605. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85926/4.88098. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.85792/4.88946. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.85843/4.88999. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85427/4.89346. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85519/4.90085. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85857/4.89065. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.86053/4.89244. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.85839/4.88361. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.85553/4.87705. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.85441/4.88866. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.85490/4.89377. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.85538/4.90205. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.85560/4.89695. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.85194/4.90079. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.85474/4.90207. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.85478/4.88864. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.85185/4.90050. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.85101/4.90493. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.85235/4.91265. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.85187/4.90626. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84994/4.89954. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.85494/4.90402. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.85125/4.90084. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.85383/4.90744. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.85432/4.89705. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.84662/4.90375. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.85322/4.90651. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.85400/4.88657. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.85271/4.90186. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84611/4.89573. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.85075/4.90271. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.84787/4.89438. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.84962/4.89271. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.84669/4.90286. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.83976/4.91694. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.85372/4.89875. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.84740/4.88681. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.84803/4.89824. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84269/4.89953. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.84806/4.89702. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 4.84631/4.87538. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.84992/4.89447. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.84422/4.90630. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84234/4.89449. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.84751/4.89571. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83911/4.90458. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.84302/4.91701. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.84220/4.90203. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.84337/4.92598. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.84328/4.90769. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.84346/4.90003. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84077/4.88661. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.83875/4.91323. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.84420/4.90520. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83709/4.92487. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84025/4.89777. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.83501/4.90731. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.83097/4.91425. Took 0.18 sec\n",
      "Epoch 76, Loss(train/val) 4.84787/4.92092. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.84114/4.89285. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.84518/4.90717. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.83486/4.90012. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.83646/4.91101. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83576/4.89063. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.84272/4.89194. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.83683/4.91364. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83822/4.91008. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83748/4.91464. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.83396/4.90318. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83150/4.92131. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.84128/4.90300. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83183/4.93465. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.83762/4.90530. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.83310/4.90711. Took 0.18 sec\n",
      "Epoch 92, Loss(train/val) 4.83480/4.91382. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83351/4.90551. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83767/4.91718. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.83398/4.91867. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82790/4.90591. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83131/4.92843. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.82705/4.91985. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.82674/4.92262. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 5.10837/5.03782. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.04774/5.03449. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.04612/5.03688. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.04142/5.03896. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.03922/5.03790. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.04069/5.03586. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.03613/5.03588. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.03780/5.03592. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.03542/5.03697. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.03328/5.04189. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.03415/5.04274. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 5.03277/5.04826. Took 0.18 sec\n",
      "Epoch 12, Loss(train/val) 5.03237/5.04255. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.03347/5.04620. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.03253/5.04221. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.03260/5.04182. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.03222/5.04804. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.03244/5.04989. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.03469/5.06170. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.03136/5.05518. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.03025/5.04864. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.02899/5.05692. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.03013/5.04734. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.02941/5.04453. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.03020/5.05196. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.02860/5.05568. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.02792/5.04371. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.03041/5.05000. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.03182/5.04594. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.02905/5.04874. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.02809/5.04545. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.02925/5.05741. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.03060/5.05395. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.02842/5.06154. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.02928/5.05260. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.02625/5.05638. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.02437/5.05695. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 5.02770/5.05823. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.02650/5.04244. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.02369/5.04939. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.02643/5.05576. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.02571/5.05315. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.02324/5.05772. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.02280/5.04328. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 5.03093/5.05446. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.03057/5.03379. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 5.02559/5.03379. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.02791/5.05095. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.02502/5.04558. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.02813/5.03437. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.02516/5.02999. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.02504/5.04973. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.02377/5.03326. Took 0.18 sec\n",
      "Epoch 53, Loss(train/val) 5.02396/5.04964. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.02755/5.02895. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.02623/5.03141. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.02302/5.02776. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.02355/5.02195. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.02109/5.02592. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.02121/5.01632. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.02584/5.03243. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.02649/5.02079. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.01741/5.02174. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.01952/5.01992. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.01998/5.02587. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.01678/5.03317. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.02223/5.03000. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.01678/5.03641. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.01094/5.03400. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.01819/5.02328. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.01681/5.03564. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.01440/5.02681. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 5.01450/5.02954. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.01505/5.02923. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.01286/5.03557. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.01590/5.03442. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.01479/5.03762. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.01393/5.03529. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.01666/5.02786. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.01376/5.03995. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.01209/5.04672. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.01482/5.03790. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.01315/5.04313. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.01200/5.05073. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.01193/5.04133. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.01037/5.06455. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.01105/5.04971. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.01097/5.03839. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.00878/5.03260. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.00857/5.01924. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.00786/5.03038. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.01129/5.02733. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.00816/5.04254. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.01307/5.02456. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.01474/5.04762. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.00827/5.04361. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.01125/5.02535. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.00576/5.02988. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.01003/5.02382. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.00718/5.04308. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.09847634407689815\n",
      "Epoch 0, Loss(train/val) 4.86724/4.78402. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.80738/4.79288. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.79780/4.79311. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79922/4.79289. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.79871/4.79784. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79822/4.79998. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.79642/4.80577. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79545/4.80521. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79733/4.80356. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79097/4.80495. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79347/4.80809. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 4.79546/4.80816. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79405/4.80508. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79218/4.80405. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.79483/4.80960. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.79641/4.80574. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.79678/4.80481. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.79177/4.80643. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.79555/4.80732. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.79254/4.80997. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.79040/4.81305. Took 0.18 sec\n",
      "Epoch 21, Loss(train/val) 4.78702/4.81571. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78821/4.81590. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78582/4.81534. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.79029/4.81111. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.78298/4.82319. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78194/4.81869. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78692/4.81750. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78432/4.81729. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78291/4.81496. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.79322/4.80027. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.79756/4.80639. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.79330/4.81089. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.79236/4.80915. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.79289/4.80516. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.78765/4.80318. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78913/4.80200. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.78979/4.81056. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.78609/4.81044. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.79054/4.81565. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.79109/4.81268. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.79072/4.81091. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.78894/4.81451. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.79038/4.82596. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.78487/4.82415. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.78732/4.83261. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78704/4.83420. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.78449/4.81772. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.78526/4.82150. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.78529/4.82006. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.77835/4.83808. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.78035/4.83498. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.77947/4.84596. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.78364/4.81917. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.78095/4.82146. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.78208/4.81961. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.77879/4.82418. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.78623/4.82569. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.78518/4.82692. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.78519/4.81838. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.78185/4.82376. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.78199/4.81991. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.77926/4.81697. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.77780/4.81838. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.78483/4.81292. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.77735/4.81508. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.78143/4.81777. Took 0.18 sec\n",
      "Epoch 67, Loss(train/val) 4.77587/4.82387. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77798/4.81570. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77534/4.81661. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.78180/4.82015. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.77354/4.82917. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.77945/4.82199. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.77854/4.80661. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77842/4.81704. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.77212/4.82307. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.77568/4.82641. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77428/4.81280. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.77266/4.81466. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.77459/4.81318. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.77088/4.82825. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.77385/4.82189. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.76918/4.83453. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.77096/4.80824. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.78718/4.80993. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.78220/4.80788. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.77555/4.81819. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 4.77019/4.82819. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 4.77607/4.80481. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.76976/4.81718. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.79740/4.82742. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.79241/4.80388. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.79541/4.81298. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.78989/4.82122. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.78855/4.83718. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79062/4.82859. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.78744/4.83384. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.78756/4.83904. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.78872/4.82610. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.78767/4.83447. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.16511700362686718\n",
      "Epoch 0, Loss(train/val) 5.03920/4.98220. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.00609/4.96727. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.00837/4.96364. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.01997/4.96993. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 5.01371/4.99966. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.99954/4.99158. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.00049/4.98381. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.00321/4.98597. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.00322/4.98879. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.99906/4.98432. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.99702/4.97806. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.99879/4.97701. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.99497/4.97732. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.99357/4.97085. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.99550/4.98021. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.99024/4.98180. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.99327/4.98073. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.99330/4.98193. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.99065/4.98673. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.99169/4.98045. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.99355/4.98093. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.98944/4.99064. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.98937/4.98702. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.99043/4.98400. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.98778/4.99134. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.98995/4.98514. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.98788/4.99283. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.98611/4.99115. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.98697/4.99779. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.99239/4.97392. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.98994/4.97423. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.98841/5.00278. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.99112/4.99492. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.98464/5.00476. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.98425/4.99796. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.98380/5.00969. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.98068/5.00804. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.98589/5.01521. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.98416/5.00749. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.97957/5.01759. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.98645/4.99914. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.98518/5.00894. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.98149/5.02339. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.97548/5.01635. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.98013/5.03817. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.97902/5.03618. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.97748/5.04007. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.97396/5.03287. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.97401/5.03401. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.96901/5.05446. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.97355/5.03730. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.97445/5.06706. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.97280/5.08275. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.97135/5.08908. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.97859/5.02159. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.97601/5.04963. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.96682/5.04624. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.96641/5.07426. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.97203/5.08387. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.97003/5.00338. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.98658/4.98689. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.98378/4.98515. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.97714/5.04766. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.97313/5.04837. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.97275/5.06702. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.96890/5.04137. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.97553/5.02298. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.97291/5.03703. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.97357/5.06041. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.97490/5.02534. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.97312/5.01389. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.96996/5.03591. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.96780/5.07273. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.97040/5.06153. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.96132/5.02110. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.96436/5.03519. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.96491/5.04206. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.96908/5.04237. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.96391/5.05657. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.96336/5.05215. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.96080/5.09314. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.97024/5.05985. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.96481/5.06182. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.96598/5.08688. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.96217/5.04593. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.96220/5.05500. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.96004/5.06886. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 4.95662/5.06558. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 4.96091/5.05503. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.96085/5.09335. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.96284/5.07777. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.95631/5.09022. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.96458/5.06820. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.96420/5.04751. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.95517/5.06539. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.95927/5.08285. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.95905/5.05661. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.95535/5.10108. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.96225/5.05977. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.95331/5.06352. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.16515911704838038\n",
      "Epoch 0, Loss(train/val) 5.01656/4.98059. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.98231/4.98631. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.98125/5.02777. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.98509/5.04969. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.98321/5.00831. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.97218/4.98257. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.97099/4.99398. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.96664/5.00790. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.96831/5.01168. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.96694/5.00589. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.96715/5.00558. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.96571/5.01216. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.96373/4.99333. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.95962/5.00288. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.96327/5.00579. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.96158/4.99602. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.95902/4.99641. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.95979/5.00493. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.95909/5.00667. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.95967/4.99397. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.96067/4.98737. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.96071/4.99875. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.95909/5.00042. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.95838/5.00599. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.96105/5.00479. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.95649/5.00411. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.95385/5.00857. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 4.95742/4.99842. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.95065/5.01141. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.95780/4.98612. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.95056/5.01673. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.95764/4.98866. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.95340/5.01327. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.95396/4.99751. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.95199/5.00604. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.95223/4.99255. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.95045/5.00010. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.94958/4.99527. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.95093/4.99974. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.95140/5.00036. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.95093/4.99681. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.94573/5.00930. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.95405/4.99426. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.94882/4.99890. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.94647/5.00134. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.95179/4.99650. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.94946/4.99891. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.94726/5.00704. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.94313/5.01032. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.95345/4.99104. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.95301/5.00341. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.94604/5.00210. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.94558/4.99550. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.94707/5.00847. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.94784/4.98772. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.94453/5.00947. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.94706/5.00152. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.94724/5.02096. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.94705/4.99777. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.94303/5.00946. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.94912/5.00685. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.93958/5.01680. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.95161/4.99618. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.94677/5.00522. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.94304/5.00317. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.94093/5.01625. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.94030/5.01706. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.94124/5.00279. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.94241/5.03031. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.94680/4.99623. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.94109/5.01216. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.94123/5.00391. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.94948/4.96911. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.95744/4.97796. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.94707/4.99523. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.94557/5.01567. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.94227/5.00775. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.94822/5.00102. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.94215/5.00731. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.94259/5.01181. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.95066/4.98985. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.93897/5.01035. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.93590/5.00957. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.94250/5.02487. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.94150/5.00792. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.93607/5.02698. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.94239/5.00734. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.93904/5.01243. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.93883/5.00534. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.93887/5.02652. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.94178/4.99576. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.93669/5.03096. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.93979/5.01254. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.93815/5.01617. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.94087/5.01090. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.93212/5.01561. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.93670/5.02505. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.93218/5.00563. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.93102/5.02666. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.93895/5.02399. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.11383959916814836\n",
      "Epoch 0, Loss(train/val) 4.94500/4.96607. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.90198/4.90314. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.89413/4.88308. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.89019/4.87883. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.89217/4.88183. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.88629/4.88570. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.88605/4.88515. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.88725/4.88853. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.88408/4.89450. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.88212/4.89308. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.88084/4.89305. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.88837/4.89611. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.89040/4.89188. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.88296/4.89932. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.88378/4.90062. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.88229/4.90608. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.87620/4.90775. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.87918/4.91744. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.88036/4.91836. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.87686/4.91952. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.88079/4.90931. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.87670/4.91553. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.87409/4.91069. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.87568/4.91172. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.87610/4.91029. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.87343/4.91425. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.86906/4.91676. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.87444/4.91006. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.86986/4.91415. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.88564/4.91069. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.87906/4.90671. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.87484/4.91872. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.87954/4.91636. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.87527/4.92189. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.87045/4.92578. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 4.87034/4.93526. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.87405/4.92823. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.86327/4.93089. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.87334/4.92197. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.86958/4.92986. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 4.87288/4.92151. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.86561/4.93575. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.87578/4.90262. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.86931/4.92698. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.86600/4.94689. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.86041/4.95701. Took 0.18 sec\n",
      "Epoch 46, Loss(train/val) 4.86942/4.93671. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.86634/4.93189. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.86154/4.93280. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.86623/4.92267. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.86526/4.92486. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.86730/4.93169. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.86094/4.93180. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 4.86114/4.94083. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.86476/4.94886. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 4.85409/4.94857. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.85793/4.96563. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.86246/4.93464. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.85998/4.93003. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.85650/4.93619. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85649/4.94624. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.85247/4.96675. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 4.86396/4.96255. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.86285/4.93385. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.85687/4.95678. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.85142/4.97160. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.85919/4.96271. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.85783/4.96354. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.86072/4.94941. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.85499/4.96673. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.85803/4.95760. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.85111/4.97739. Took 0.22 sec\n",
      "Epoch 72, Loss(train/val) 4.85015/4.99478. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.85644/4.97271. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.85520/4.96427. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 4.84783/4.99025. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.85530/4.97438. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.84980/4.99425. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.85401/4.97082. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.86707/4.97499. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.85819/4.93638. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.85887/4.95981. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.85315/4.97235. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.85342/4.95954. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.85680/4.94680. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.85651/4.95334. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.85707/4.95236. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.86332/4.94052. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.85698/4.94951. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.85678/4.95346. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.85866/4.95082. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85337/4.95214. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.85469/4.94845. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.84825/4.98480. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.85540/4.96301. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.84767/4.95630. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.85640/4.98494. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.86022/4.94235. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.86787/4.92843. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.86101/4.94112. Took 0.20 sec\n",
      "ACC: 0.578125, MCC: 0.15694120514358612\n",
      "Epoch 0, Loss(train/val) 4.89712/4.91608. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.87680/4.90333. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.87108/4.90599. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87094/4.90778. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.87546/4.90438. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.87270/4.90611. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.87130/4.92323. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.87202/4.92336. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.87114/4.92381. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87308/4.92378. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.87684/4.89576. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.87181/4.89304. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.86952/4.90956. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.86699/4.90111. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.86386/4.90280. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.86317/4.90103. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.86381/4.91053. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.86184/4.90673. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.86338/4.90284. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.85747/4.91187. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.85786/4.90770. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85789/4.89808. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85848/4.90498. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85570/4.91263. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85768/4.90051. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.85543/4.91107. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.85104/4.91495. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.85605/4.91273. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.85320/4.90135. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.85005/4.91216. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84710/4.93587. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86380/4.89021. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.85609/4.91205. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.85427/4.90554. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85330/4.92501. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.85437/4.91556. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.85157/4.92981. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.85577/4.89993. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.85128/4.92943. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.85401/4.92040. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.85402/4.92783. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.85184/4.92579. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.84791/4.94084. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.84808/4.94433. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.85437/4.92279. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.84578/4.95339. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84741/4.90435. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.85684/4.91487. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.85247/4.90130. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.86466/4.90155. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.85559/4.91939. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.85340/4.91194. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.85427/4.92539. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.85351/4.92962. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.85249/4.92053. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.85399/4.93266. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84862/4.93522. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.84656/4.93696. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.84734/4.91242. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.84747/4.94214. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.84700/4.93664. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84416/4.97660. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.84130/4.95423. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.84441/4.95744. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.84479/4.95717. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.84538/4.95599. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.84150/4.95187. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.84735/4.95792. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.84004/4.94350. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84608/4.90733. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.84480/4.96209. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.84166/4.92186. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83642/4.96394. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84432/4.91282. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.84163/4.93067. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.84529/4.94289. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.83954/4.93600. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.84346/4.91140. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.84447/4.94942. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.84069/4.94427. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.84060/4.94982. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.84273/4.95756. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.84084/4.92717. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.83685/4.94636. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83959/4.93045. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.84257/4.95725. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.83529/4.95832. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.84032/4.94547. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.84172/4.95544. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83675/4.95889. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.84152/4.92302. Took 0.22 sec\n",
      "Epoch 91, Loss(train/val) 4.84107/4.96789. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.83876/4.94201. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83178/4.96946. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83696/4.94460. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.83649/4.96965. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83894/4.93159. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83941/4.97030. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.83768/4.95591. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.83980/4.94807. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.013980301652228746\n",
      "Epoch 0, Loss(train/val) 4.85434/4.74966. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.76410/4.75870. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 4.75717/4.75499. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.75149/4.75174. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.74718/4.75588. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.74973/4.76206. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.75228/4.76159. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.75018/4.76268. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.74957/4.76632. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.74688/4.76690. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.74494/4.76719. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.74348/4.76644. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.74541/4.76674. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.74500/4.77588. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.74344/4.77921. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.74534/4.77136. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.74498/4.77322. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.74448/4.77437. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.73818/4.78428. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.74114/4.78693. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.73834/4.78612. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.73782/4.78344. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.74007/4.78470. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.73937/4.78596. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.73732/4.78570. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.73932/4.78133. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.73954/4.79238. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.73915/4.79222. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.73829/4.79944. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.73788/4.79034. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.73474/4.79928. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.73476/4.79443. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.73492/4.79598. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.73360/4.79684. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.73485/4.79441. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.73479/4.79375. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.73581/4.78845. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 4.73030/4.80599. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.73405/4.80059. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.73169/4.80458. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.73214/4.80256. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.72730/4.80423. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.73029/4.79527. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.73242/4.80350. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.73430/4.79335. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.73020/4.79797. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.72745/4.81310. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.73239/4.80509. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.72951/4.80943. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.72670/4.81829. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.72591/4.81782. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.73058/4.77019. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.73854/4.81209. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.73259/4.80166. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.72596/4.83104. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.72282/4.80450. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.73748/4.78896. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.72954/4.80679. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.72187/4.82428. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.72909/4.82005. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.72598/4.80237. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.72208/4.82410. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.72219/4.84810. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.72435/4.83514. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.72749/4.80367. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.72363/4.81899. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.72114/4.80985. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.72563/4.81177. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.72040/4.82807. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.72398/4.80137. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.72124/4.80521. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.71550/4.82265. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.72159/4.82067. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.71746/4.83189. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.72710/4.81847. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.72910/4.78677. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.72743/4.80322. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.72008/4.82943. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.72064/4.81773. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.71850/4.83292. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.71584/4.82326. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.71581/4.82092. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.71684/4.83594. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.72033/4.81383. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.71812/4.82762. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.71517/4.83173. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.71928/4.81146. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.71124/4.82856. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.71371/4.83160. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.70571/4.83881. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.71169/4.85317. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.70690/4.84665. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.71412/4.87230. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.71012/4.83879. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.71349/4.86666. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.71300/4.83236. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.70106/4.88966. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.70744/4.83219. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.71197/4.82603. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.70717/4.84258. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.11834526708278773\n",
      "Epoch 0, Loss(train/val) 4.83451/4.82234. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.79665/4.80421. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.79483/4.81787. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.79050/4.80959. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79172/4.81031. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79245/4.81426. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.79414/4.81566. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79137/4.81821. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79230/4.80829. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79441/4.81824. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79031/4.83053. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78922/4.83821. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.78913/4.83990. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79263/4.82959. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78794/4.84640. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78571/4.84999. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78756/4.85319. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.79060/4.83755. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78189/4.85750. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78741/4.84338. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.78177/4.84634. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78678/4.82801. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78115/4.83266. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78475/4.83583. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.77823/4.84786. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78413/4.85274. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78179/4.84347. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78038/4.85852. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78193/4.84162. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78137/4.84795. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.77620/4.85081. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77902/4.84882. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77722/4.86486. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.77171/4.86038. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.77517/4.85933. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77621/4.86123. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77438/4.85971. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.77431/4.86176. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.77412/4.86202. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.76926/4.86423. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77562/4.85142. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.77469/4.85714. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76707/4.87846. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.77203/4.82702. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77754/4.81700. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77627/4.82355. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77612/4.82228. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.76871/4.83889. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.76195/4.83228. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77376/4.81745. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76233/4.83696. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.77233/4.82583. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.76723/4.83944. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76271/4.84376. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.77019/4.83409. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.76541/4.85168. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.76742/4.85642. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76246/4.88201. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.76086/4.83843. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.76440/4.85721. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.77529/4.80012. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.77770/4.84029. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.76747/4.86382. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.76927/4.84455. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76583/4.87252. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.76200/4.87383. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.76285/4.87083. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.76672/4.86794. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.76422/4.87769. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.78779/4.82618. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.77861/4.83632. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.77568/4.84697. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.77286/4.87768. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.76524/4.83918. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.76684/4.85779. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.76744/4.85759. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.76457/4.85496. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.76407/4.85909. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.76891/4.85158. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.76683/4.86070. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.76271/4.88339. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.75893/4.89784. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.77575/4.83009. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.76445/4.88975. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.76482/4.87722. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.76325/4.90572. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.77019/4.87112. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.75760/4.89312. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.76087/4.87823. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.76498/4.89193. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.75399/4.89285. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.76815/4.86246. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.75786/4.87265. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.74797/4.93696. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.76634/4.88071. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75888/4.89893. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.75675/4.90519. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.76061/4.87212. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.76085/4.88276. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75357/4.90902. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.09379580992210836\n",
      "Epoch 0, Loss(train/val) 4.76088/4.69014. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.68967/4.68225. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.69135/4.67915. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.69070/4.67702. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.69205/4.67882. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.69178/4.67977. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.69210/4.67983. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.69149/4.68051. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.69253/4.68019. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.69210/4.68163. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.68956/4.68585. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.69007/4.68723. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.68636/4.68773. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.68736/4.68580. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.68832/4.68639. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.68681/4.68587. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.68586/4.68962. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.68562/4.69148. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.68673/4.69021. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.68527/4.69249. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.68637/4.69418. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.68364/4.69520. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.68389/4.69637. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.68145/4.69374. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.68145/4.70198. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.68238/4.70100. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.68151/4.70370. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.67888/4.70505. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.68171/4.70585. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.68136/4.70834. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.67997/4.69880. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.67865/4.71364. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.67982/4.71346. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.67566/4.71466. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.67632/4.71376. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.67724/4.71739. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.67267/4.72111. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.67809/4.72562. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.67815/4.72266. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.67291/4.72813. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.67979/4.74099. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.67592/4.72675. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.67643/4.72362. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.67479/4.73453. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.67503/4.74415. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.66987/4.77727. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.67498/4.75979. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.67379/4.75287. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.66649/4.76525. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.66512/4.77587. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.66750/4.77288. Took 0.22 sec\n",
      "Epoch 51, Loss(train/val) 4.67327/4.74226. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.67087/4.77199. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.66843/4.77352. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.66840/4.75990. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.66533/4.77198. Took 0.21 sec\n",
      "Epoch 56, Loss(train/val) 4.66526/4.76685. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.66380/4.78831. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.66370/4.79498. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.66460/4.78478. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.66574/4.78268. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.66573/4.79385. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.66638/4.79303. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.65950/4.79172. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.66728/4.76064. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.66450/4.78869. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.66192/4.78751. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.66432/4.77417. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.65729/4.77268. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.66213/4.82873. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.66448/4.78053. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.66277/4.80652. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.65956/4.78467. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.66394/4.76945. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.65622/4.81539. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.66318/4.79155. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.65733/4.82149. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.66417/4.77869. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.65616/4.81734. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.66058/4.79024. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.65684/4.82300. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.65723/4.78576. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.65895/4.82116. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.65794/4.77796. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.65997/4.81020. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.65902/4.81390. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.66112/4.78561. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.65582/4.81914. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.65535/4.83210. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.65916/4.82003. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.65119/4.80870. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.65404/4.81804. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.65732/4.81106. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.65296/4.82666. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.64695/4.84977. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.64997/4.78186. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.65740/4.80770. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.65478/4.83752. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.65246/4.81337. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.64936/4.86945. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.86715/4.83827. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.83958/4.83755. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 4.84068/4.83556. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.83796/4.83792. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.83612/4.83639. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.84096/4.83499. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.83341/4.83538. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.83814/4.83490. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.83808/4.83568. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.83606/4.83560. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.83870/4.83544. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.83753/4.83513. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.83216/4.83523. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.83957/4.83599. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.83544/4.83766. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.83541/4.84079. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.83560/4.84252. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.83563/4.84326. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83317/4.83748. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.83091/4.84050. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.83644/4.84018. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.83573/4.84332. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.82927/4.84881. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.83044/4.84768. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.83190/4.85181. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.83395/4.84979. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82981/4.85574. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.83318/4.84839. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.82617/4.84996. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.82493/4.84926. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.82939/4.85416. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.83205/4.84940. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.82843/4.85845. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.82977/4.84890. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.82624/4.85015. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82434/4.85928. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.82437/4.86208. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.82002/4.86787. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.82396/4.85740. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.82641/4.83020. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.83365/4.82639. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.83414/4.82655. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83142/4.82454. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.82728/4.82676. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.83066/4.82828. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.82480/4.82818. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.82436/4.83572. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.82535/4.84638. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.82830/4.83703. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.82599/4.83655. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 4.82323/4.84370. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.82598/4.83749. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.82426/4.84391. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.81730/4.85177. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.82637/4.84343. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.82147/4.84694. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.82275/4.83695. Took 0.22 sec\n",
      "Epoch 57, Loss(train/val) 4.82335/4.84939. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.81989/4.86094. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.81748/4.85666. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 4.81942/4.86666. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81529/4.86264. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.81800/4.84996. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.81978/4.84957. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81918/4.85314. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81645/4.86329. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.82001/4.85906. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.81408/4.86197. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.81894/4.85670. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.81675/4.86226. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.81691/4.84969. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81572/4.85896. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.81028/4.86615. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.81513/4.86744. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81290/4.88065. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.81539/4.84841. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.81358/4.85745. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.81420/4.88286. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.81764/4.85921. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.81061/4.87144. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.81172/4.85441. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.81154/4.88653. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.81573/4.86133. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.81108/4.86168. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.81001/4.87584. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.81234/4.87869. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.81359/4.86710. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.80588/4.88328. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.80977/4.87260. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.80471/4.88163. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.81228/4.87749. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.81191/4.87425. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.80776/4.87715. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.81129/4.87238. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.81758/4.85576. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82978/4.85297. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82394/4.85302. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.82581/4.84897. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82468/4.89109. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.82602/4.86479. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.06362847629757777\n",
      "Epoch 0, Loss(train/val) 4.97976/4.89666. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.92774/4.89519. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.94436/4.90313. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.93827/4.94029. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.92173/4.92153. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.92030/4.91405. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.92493/4.91675. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.92361/4.91807. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.92153/4.91815. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.92064/4.91061. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.92174/4.92027. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.91759/4.91701. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.91996/4.91504. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.91799/4.91656. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.91780/4.91291. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.91583/4.90437. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.91938/4.91932. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.91561/4.91445. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.91589/4.91029. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.91705/4.90905. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.91912/4.92563. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.91613/4.91828. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.91155/4.91628. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.91562/4.91320. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.91510/4.90756. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.91697/4.91512. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.91566/4.91402. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.91656/4.91588. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.91486/4.91567. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.91612/4.91787. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.91345/4.91274. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.91263/4.91567. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.91525/4.91472. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.91660/4.91993. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.91198/4.90883. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.91363/4.91903. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.91129/4.90723. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.91242/4.91175. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.91177/4.91276. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.91650/4.92707. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.91179/4.91406. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.91067/4.91632. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.90989/4.91220. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.91390/4.91540. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.91401/4.93057. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.91299/4.91758. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.90898/4.91095. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.91301/4.93294. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.90971/4.92169. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.91051/4.91942. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.90963/4.92599. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.91194/4.92805. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.91085/4.92259. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.91306/4.92842. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.90484/4.91899. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.90914/4.92640. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.90915/4.92624. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.90930/4.92165. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.91181/4.92307. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.90744/4.92998. Took 0.21 sec\n",
      "Epoch 60, Loss(train/val) 4.90676/4.91975. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.90802/4.93071. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.90366/4.92444. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.90850/4.92022. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.91016/4.93102. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.90643/4.92329. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.90629/4.92224. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.90797/4.93397. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.90644/4.93057. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.90630/4.92969. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.90773/4.93824. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.90894/4.92197. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.91209/4.94102. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.91157/4.94222. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.90858/4.92074. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.91016/4.94293. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.90728/4.93832. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.90728/4.93469. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.90743/4.93908. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.90344/4.93125. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.90975/4.93883. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.90699/4.93230. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.90537/4.93573. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.90268/4.94157. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.90821/4.92483. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.91391/4.93763. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.90705/4.94249. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.90988/4.91958. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.90805/4.92592. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.90155/4.93011. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.90841/4.92951. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.90512/4.92225. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.90967/4.92479. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.90532/4.92767. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.90935/4.93077. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.90633/4.92805. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.90675/4.91847. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.90319/4.92133. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.90402/4.92198. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.90264/4.92379. Took 0.20 sec\n",
      "ACC: 0.5, MCC: 0.02404930351219409\n",
      "Epoch 0, Loss(train/val) 4.94379/4.89074. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.87230/4.88908. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.87150/4.89447. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87076/4.90339. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.87173/4.90628. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.87043/4.90807. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.87368/4.90679. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.86888/4.90744. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.87043/4.90826. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.86723/4.91004. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.87007/4.90650. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.86875/4.90902. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.86758/4.91101. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.86725/4.91396. Took 0.22 sec\n",
      "Epoch 14, Loss(train/val) 4.86837/4.91234. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.86720/4.90371. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.86759/4.90318. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.86835/4.90542. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.86944/4.90674. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.86541/4.90964. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.85941/4.91719. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.86559/4.90673. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.86235/4.90861. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.86344/4.91373. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.86332/4.90839. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.86571/4.89975. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.86033/4.91487. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.86115/4.91081. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.85596/4.90898. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.85983/4.90599. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.85770/4.91449. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.85839/4.90289. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.85948/4.91166. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.85485/4.91028. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85135/4.92491. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.85344/4.90369. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.85315/4.89847. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.85259/4.91142. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.85232/4.90963. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.85135/4.92664. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.84836/4.91820. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.85333/4.90807. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.84940/4.89894. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.84609/4.91834. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84632/4.91252. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.84899/4.91389. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84365/4.92636. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84837/4.89386. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.84847/4.91061. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.85154/4.88479. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.84801/4.88980. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.84203/4.89329. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.85251/4.87373. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.85668/4.91443. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.86810/4.88460. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.86478/4.89184. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.85887/4.89579. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.86092/4.89875. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.85838/4.89364. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.86044/4.89242. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85761/4.89186. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.85698/4.90263. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.84956/4.90677. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.85994/4.87905. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.84721/4.91511. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.85842/4.87493. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.85643/4.88223. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.85106/4.90307. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.85022/4.89717. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84742/4.87104. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.84547/4.90161. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.84867/4.88932. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.84561/4.88023. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.84162/4.88873. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.84161/4.90446. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.84149/4.88645. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.84440/4.88160. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.84120/4.89112. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.83651/4.88707. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.85201/4.88261. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.85622/4.87465. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.84880/4.87686. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.84698/4.89011. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84254/4.87508. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.84216/4.90722. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.84650/4.86571. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.84228/4.88597. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83879/4.87549. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83532/4.89727. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83941/4.88919. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.83348/4.89465. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.84636/4.88513. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.83997/4.88140. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83850/4.89178. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83698/4.89309. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.83259/4.87317. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.84204/4.87786. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83931/4.88688. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.83793/4.90385. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.84176/4.87848. Took 0.20 sec\n",
      "ACC: 0.453125, MCC: -0.06643282473893375\n",
      "Epoch 0, Loss(train/val) 5.04311/5.04381. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.96236/5.01346. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.97300/5.01750. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.97989/4.98410. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.98996/4.96950. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.98400/4.97106. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.97318/4.97114. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.97047/4.97300. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.96907/4.97215. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.96786/4.97317. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.96713/4.97350. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.96639/4.97365. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.96315/4.97438. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.96301/4.97511. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.96400/4.97301. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.96110/4.97729. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.95884/4.97705. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.95901/4.97656. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.95904/4.97434. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.95763/4.97754. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 4.95868/4.97190. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.95634/4.98215. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.95877/4.97558. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.95965/4.97403. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.95544/4.97441. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.95262/4.97250. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.95305/4.96949. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.95480/4.98166. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.95769/4.96286. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.95506/4.97476. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.94855/4.97125. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.94637/4.98184. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.94828/4.98498. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.94786/4.98092. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.95036/4.98690. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.94381/4.99603. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.94925/4.98948. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.95110/4.99253. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.94653/4.99852. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.94738/4.98766. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.94732/4.98399. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.94687/4.99052. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.94746/4.98362. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.94411/4.98301. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.94074/4.98209. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.94337/4.98133. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.93947/4.99658. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.94722/4.97631. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.94946/4.96836. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.94321/4.98509. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.94195/4.97916. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.94086/4.99612. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.93937/4.97890. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.94461/4.98137. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.94175/4.98596. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.93969/4.98954. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.94273/4.98345. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.93689/4.99116. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.93814/4.97380. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.93800/4.98593. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.94049/4.98388. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.93604/5.00208. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.93926/4.99875. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.93810/4.99622. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.93535/4.97329. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.93586/4.99217. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.93968/4.98233. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.95059/4.96911. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.94144/4.96818. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.94105/4.97708. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.93487/4.98337. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.93603/4.97568. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.93855/4.97085. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.93753/4.99052. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.93650/4.97565. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.93659/4.97450. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.93162/4.99719. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.94121/4.99234. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.92704/4.97441. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.93107/4.99335. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.93336/4.97871. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.92561/4.98778. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.93568/4.98023. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.93163/5.00355. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.92698/4.97480. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.93067/4.97625. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.92565/4.98764. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.93540/4.98521. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.93463/4.99499. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.93124/4.97803. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.92590/5.00932. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.93075/5.00861. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.92823/4.99762. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.93030/4.99623. Took 0.22 sec\n",
      "Epoch 94, Loss(train/val) 4.93192/5.01123. Took 0.21 sec\n",
      "Epoch 95, Loss(train/val) 4.93318/5.01793. Took 0.21 sec\n",
      "Epoch 96, Loss(train/val) 4.93903/4.99874. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.93334/4.98351. Took 0.23 sec\n",
      "Epoch 98, Loss(train/val) 4.93296/4.98187. Took 0.21 sec\n",
      "Epoch 99, Loss(train/val) 4.93215/4.99333. Took 0.20 sec\n",
      "ACC: 0.5, MCC: -0.021109792565893827\n",
      "Epoch 0, Loss(train/val) 4.92554/4.85097. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.85744/4.87051. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 4.85941/4.88511. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 4.86530/4.88736. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 4.86437/4.87750. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.86681/4.85765. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.85961/4.85131. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.85631/4.85215. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.85424/4.85169. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.85346/4.85299. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.85417/4.85132. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 4.85473/4.85052. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.85132/4.85022. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.85038/4.84968. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.85403/4.85121. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.85142/4.85455. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 4.84859/4.85769. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.84659/4.85839. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.85049/4.85903. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.84757/4.85753. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.84570/4.87053. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 4.84555/4.86493. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.84698/4.86539. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.84509/4.86837. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.84213/4.88802. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.84385/4.86040. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.84416/4.89435. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.84215/4.88289. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.84312/4.86589. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.84334/4.87973. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.84103/4.86501. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.84043/4.88025. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.84692/4.86362. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.83971/4.86639. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.83967/4.86293. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.84098/4.86915. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.83494/4.87381. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.83526/4.86879. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.83643/4.89021. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.84265/4.86022. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.84093/4.87756. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.83760/4.86261. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.83715/4.87361. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.83262/4.88110. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.83661/4.87714. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.82726/4.89006. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.82990/4.87658. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.83779/4.86206. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.82963/4.87600. Took 0.23 sec\n",
      "Epoch 49, Loss(train/val) 4.82917/4.86009. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 4.82923/4.86872. Took 0.22 sec\n",
      "Epoch 51, Loss(train/val) 4.82704/4.87623. Took 0.22 sec\n",
      "Epoch 52, Loss(train/val) 4.83167/4.86894. Took 0.22 sec\n",
      "Epoch 53, Loss(train/val) 4.82321/4.87301. Took 0.23 sec\n",
      "Epoch 54, Loss(train/val) 4.82687/4.87677. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.82620/4.87298. Took 0.21 sec\n",
      "Epoch 56, Loss(train/val) 4.82503/4.88558. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 4.83017/4.88330. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.82306/4.85340. Took 0.21 sec\n",
      "Epoch 59, Loss(train/val) 4.82300/4.88540. Took 0.22 sec\n",
      "Epoch 60, Loss(train/val) 4.82548/4.86215. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.81584/4.88137. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.82205/4.87049. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.82321/4.86428. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81899/4.88014. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.82487/4.85983. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.81662/4.87718. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.81490/4.85943. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 4.81984/4.85721. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.81582/4.87374. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.81132/4.86877. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81474/4.85162. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.81624/4.86002. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.81880/4.86061. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81460/4.88281. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.81571/4.87227. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.80254/4.88338. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.81855/4.87560. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.81344/4.86679. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.81313/4.86223. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.81014/4.86642. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.80952/4.87072. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.80696/4.87648. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80335/4.89911. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.81163/4.87264. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.80803/4.88214. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.80637/4.88643. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.81228/4.87796. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.80662/4.86383. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.81222/4.86411. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.80706/4.89015. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.84043/4.85806. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82708/4.86743. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82185/4.87261. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82566/4.84395. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82094/4.85346. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.81762/4.86541. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82633/4.84562. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82723/4.86287. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.82006/4.86634. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.08304547985373996\n",
      "Epoch 0, Loss(train/val) 4.87643/4.80110. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.81573/4.77480. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.80127/4.78098. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.79732/4.79044. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 4.79626/4.79043. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.79907/4.78980. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.79470/4.79374. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.80113/4.79156. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79452/4.79001. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79139/4.79791. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79245/4.80609. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.79214/4.80389. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79322/4.80014. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79318/4.79720. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78885/4.80594. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78997/4.80592. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.79116/4.80380. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78956/4.80334. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78463/4.81409. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78643/4.81116. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.78486/4.80964. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78247/4.81560. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78864/4.79233. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78482/4.80211. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.77891/4.81323. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78092/4.80666. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.77791/4.81177. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.78153/4.80646. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.77925/4.79736. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.77778/4.80688. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.77119/4.82864. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77785/4.80793. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77478/4.80155. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.77609/4.80424. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.77518/4.79847. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77014/4.82230. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.77159/4.80921. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.76910/4.81424. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77132/4.80280. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77371/4.80704. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.76798/4.81385. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.76599/4.80613. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.77043/4.80293. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.77209/4.79153. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.76491/4.80890. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.76717/4.81779. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.77287/4.79472. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.76525/4.81517. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.76619/4.81035. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 4.76443/4.81861. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.76705/4.79498. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.75725/4.82299. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.76548/4.80848. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76514/4.80269. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76677/4.81076. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.76095/4.81374. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.76730/4.81472. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76613/4.80977. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.76172/4.81032. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.75480/4.83418. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.76474/4.79760. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.76363/4.80121. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.76118/4.80812. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.76554/4.80250. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.75952/4.83279. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.76446/4.79544. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.76041/4.79887. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.75674/4.81108. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.76280/4.80326. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.76224/4.82243. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.75569/4.82010. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.76291/4.80691. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.76235/4.80511. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.75152/4.83380. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.76124/4.80367. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.75541/4.81565. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.75508/4.82933. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.75221/4.81957. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.76296/4.80040. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.75262/4.84791. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.75381/4.81596. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.76843/4.83508. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.78247/4.80605. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.76965/4.82405. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.77180/4.80559. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.75903/4.81385. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.75572/4.82273. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.75058/4.81241. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.74862/4.84444. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.75613/4.81368. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.75373/4.82065. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.75308/4.81998. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.75122/4.82829. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.74589/4.82912. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.76174/4.81318. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75744/4.85356. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.77455/4.81757. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.77474/4.83662. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.77380/4.81314. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.76468/4.83637. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.14201043973789815\n",
      "Epoch 0, Loss(train/val) 4.92217/4.90920. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.89645/4.90132. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.89154/4.89839. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.89013/4.89823. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.88940/4.90145. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.89314/4.89911. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.88905/4.89654. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.88837/4.89685. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.88772/4.89640. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.88845/4.89280. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.88312/4.89659. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.88425/4.89456. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.88718/4.89168. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.88767/4.89325. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.88180/4.89513. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.88468/4.89425. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.88296/4.89373. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.88095/4.89772. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.87822/4.89838. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.88106/4.90357. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.88115/4.90670. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.87974/4.90387. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.87472/4.91780. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.87479/4.91354. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.88002/4.91498. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.87655/4.92373. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.87765/4.91630. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.87682/4.92463. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.87230/4.92907. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.87215/4.93840. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.87569/4.93653. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.87136/4.92549. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.87517/4.92717. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.87222/4.94272. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.87218/4.93080. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.87053/4.93904. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.86943/4.95584. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.87286/4.93607. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.87092/4.94230. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.86942/4.94870. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.87297/4.94037. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.86472/4.95464. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.87218/4.94517. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.86842/4.94946. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.86221/4.95292. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.86590/4.93852. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.86584/4.94674. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.86442/4.94535. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.86787/4.94105. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.86465/4.95623. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.86225/4.95957. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.86303/4.95232. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.86361/4.95329. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.87195/4.94140. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.86866/4.94426. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.86579/4.95720. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.86632/4.93524. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.86379/4.96866. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.86506/4.95241. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.86580/4.96178. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.86607/4.95225. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.86276/4.96377. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.86415/4.93726. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.86697/4.94821. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.85922/4.97518. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.86030/4.95045. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.86178/4.95688. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.86450/4.94721. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.86163/4.95745. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.85896/4.96777. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.85964/4.98008. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.86777/4.95103. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.86448/4.95483. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.86024/4.97294. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.87034/4.95638. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.86160/4.98433. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.86199/4.95531. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.85802/4.96092. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.85679/4.97771. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.85748/4.97552. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.85765/4.97368. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.85279/4.97381. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.85408/4.96559. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.85757/4.98570. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.85335/4.97774. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.85795/4.99108. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.85690/5.00633. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.84855/4.97961. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.85975/4.94880. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.85512/4.97816. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.85124/5.00036. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85956/4.95867. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.88082/4.92166. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.86451/4.96594. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.85579/4.94837. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.85705/4.97731. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.85257/4.95841. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.85952/4.98746. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.86146/4.93878. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.85856/4.99281. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.15318083468998522\n",
      "Epoch 0, Loss(train/val) 4.92689/4.91474. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.91524/4.89497. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 4.90032/4.89875. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.90375/4.89789. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.90568/4.89875. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.90314/4.90094. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.90534/4.90224. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.90442/4.90561. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.90121/4.90520. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.89821/4.91229. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.90379/4.90783. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.90497/4.90458. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.90339/4.90495. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.90334/4.90969. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.90073/4.91551. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.90054/4.92063. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.89812/4.92146. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.90066/4.91203. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.89996/4.92288. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.89753/4.92851. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.89573/4.93325. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.89556/4.93923. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.89244/4.93956. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.89910/4.92895. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.89391/4.94384. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.89038/4.94360. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.89143/4.94221. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.89302/4.93950. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.88988/4.95684. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.89077/4.94663. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.88710/4.94647. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.88556/4.95156. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.88763/4.95319. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.89031/4.93831. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.88230/4.97597. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.89120/4.94219. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.88537/4.96281. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.88201/4.96108. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.88424/4.97712. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.88430/4.97361. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.88308/4.94777. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.88090/4.95822. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.88605/4.96028. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.89047/4.94038. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.89271/4.95634. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.88557/4.97031. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.88326/4.96234. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.88383/4.95959. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.87885/4.96588. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.89804/4.91932. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89191/4.91961. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.88938/4.92640. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.89168/4.94889. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.89276/4.96031. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.88901/4.96010. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.88453/4.94213. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.89743/4.92388. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.90747/4.91332. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.90296/4.91044. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.90141/4.91623. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.90036/4.92136. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.89671/4.93687. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.89848/4.92405. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.89376/4.92794. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.89290/4.95089. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.89380/4.95443. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 4.89030/4.96302. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.89020/4.97809. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.89432/4.97317. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.89598/4.92391. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.89070/4.96130. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.89329/4.92838. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.89740/4.94986. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.89185/4.94091. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.89462/4.94455. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.89319/4.93653. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.89275/4.96552. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.89311/4.94532. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.89317/4.93558. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.89077/4.95156. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.89075/4.96070. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.88983/4.96544. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.88521/4.94258. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.88398/4.96433. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88857/4.96240. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.87944/4.95989. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.88845/4.95364. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.88633/4.97556. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.87688/4.97777. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.88087/4.96807. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.88061/4.98574. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.88463/4.97886. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.88385/4.96709. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.88057/4.97160. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88251/4.98449. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.88102/4.98021. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.87645/5.00035. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.88009/4.96323. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.87721/4.98625. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.88259/4.96525. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.1889822365046136\n",
      "Epoch 0, Loss(train/val) 4.93877/4.92812. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.93051/4.90924. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.92208/4.91685. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.91985/4.91788. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.91637/4.92297. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.91457/4.92174. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.91688/4.92440. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.91506/4.93122. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 4.91297/4.92925. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.91657/4.93800. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.91423/4.94059. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.91452/4.94282. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.91005/4.94000. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.91413/4.94594. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.91145/4.95157. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.91220/4.95465. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.91062/4.95126. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.90978/4.95727. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.91173/4.96070. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.90702/4.96537. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.90662/4.96455. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.90749/4.95691. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.90426/4.97036. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.90556/4.97373. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.90344/4.95580. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.89812/5.00251. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.90309/4.98950. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.89807/4.99551. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90152/4.97363. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.89882/4.99155. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.89783/5.01325. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.89940/4.99385. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.90222/5.00469. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.90782/4.95707. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.90417/4.96311. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90278/4.98542. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.90325/4.99112. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.90531/4.98082. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.90326/4.98361. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.89932/4.98338. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.89724/4.98173. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.89502/4.98559. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89869/4.97010. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.89742/4.98405. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.89660/4.99167. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.89282/4.98206. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.89462/4.99369. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89319/4.97128. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.89561/4.97487. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.89276/4.98616. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.88973/4.99421. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.88685/4.98995. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.89174/4.99071. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.88946/4.98754. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.89212/5.00401. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.89302/4.99816. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.88462/4.99366. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.88629/4.99453. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.88614/4.98825. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.88786/4.99151. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.88591/5.00217. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.88366/4.97922. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 4.88625/5.00128. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.87833/4.98195. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.89025/4.98868. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.88686/4.99643. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.87644/4.99510. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.89049/4.99546. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.87915/5.02509. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.88531/4.98495. Took 0.21 sec\n",
      "Epoch 70, Loss(train/val) 4.89208/4.98016. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.88762/4.99822. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.88362/4.99063. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.87585/5.02476. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.87856/5.00188. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.88551/4.98535. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.87748/5.01783. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.87935/5.01821. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.87795/4.98007. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.88161/5.00691. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88176/4.98294. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.88263/5.00188. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.87029/5.05893. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.88229/4.97954. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.88361/5.03203. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.87944/4.99348. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.87022/5.06044. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.87967/5.00671. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.87904/4.98900. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.87744/5.02927. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.87894/5.03095. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.87014/5.06678. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.86710/5.00042. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.87435/5.04536. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.87435/5.01182. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.87064/5.02085. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.87694/5.02749. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.86772/5.05970. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.87620/4.99458. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.86863/5.04745. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.1563263498701806\n",
      "Epoch 0, Loss(train/val) 5.15175/5.11383. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.07720/5.04497. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.05868/5.04328. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.04557/5.04450. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.04808/5.04827. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.04842/5.05242. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.04985/5.05021. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.05099/5.04939. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 5.05208/5.04709. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.04997/5.04753. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 5.04711/5.04842. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.04541/5.05307. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.04301/5.05808. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.04217/5.07109. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.04052/5.08505. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 5.04385/5.07137. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 5.04264/5.06760. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.04256/5.06271. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 5.04662/5.05384. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.04185/5.06027. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.03827/5.07436. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.03198/5.10490. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.03371/5.10998. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 5.03585/5.09206. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.03392/5.11754. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.03530/5.08529. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.03163/5.09799. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.03339/5.08949. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.02787/5.11130. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 5.03273/5.09493. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.03274/5.11299. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.02842/5.09658. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.02605/5.10923. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 5.02919/5.11266. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 5.03046/5.14441. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.02567/5.11267. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.02795/5.10900. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 5.02964/5.11282. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.02369/5.10393. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.02379/5.12570. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 5.02688/5.12169. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.02517/5.12111. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.03192/5.06279. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 5.04180/5.05902. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.03091/5.07309. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.02815/5.08556. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.03356/5.08490. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.02860/5.10408. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.02260/5.12073. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.02204/5.14990. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.03170/5.09618. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.02509/5.10958. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 5.02364/5.11713. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.02187/5.14383. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.02375/5.12348. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.02081/5.14235. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.02144/5.13454. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.02456/5.12068. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.02004/5.14187. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.02546/5.11894. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.02048/5.14242. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 5.02677/5.10291. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 5.02248/5.12671. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.02202/5.13053. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.02135/5.11840. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 5.02081/5.13933. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.02407/5.13641. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.01597/5.13569. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 5.02392/5.12551. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.02041/5.13768. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.01446/5.14899. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.02511/5.12155. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 5.01752/5.14985. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.01940/5.12799. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.01797/5.15210. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.01749/5.14151. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.01737/5.14309. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.01750/5.12081. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.01707/5.12308. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.01240/5.13793. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.02290/5.11546. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.01301/5.14080. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.01253/5.16527. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.01508/5.13709. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.01301/5.12941. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 5.01435/5.13842. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.01432/5.12669. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.01912/5.13432. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.01926/5.11059. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.01990/5.13436. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.01279/5.14081. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.01303/5.13985. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.01429/5.14386. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.01131/5.16717. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.01155/5.17262. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.00533/5.16355. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.01340/5.15212. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.01069/5.18573. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.01010/5.17021. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.02054/5.13741. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.15819299929208316\n",
      "Epoch 0, Loss(train/val) 4.84061/4.82712. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.79646/4.80704. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.79236/4.79806. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79167/4.79430. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79155/4.78809. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.78938/4.78951. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.78815/4.78901. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.78942/4.78795. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.78741/4.78912. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.78755/4.78903. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.78272/4.78711. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78647/4.78765. Took 0.21 sec\n",
      "Epoch 12, Loss(train/val) 4.78224/4.78790. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.78550/4.78337. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78580/4.78450. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78397/4.79081. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78382/4.79642. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78338/4.79399. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.78152/4.79608. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.77791/4.80150. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.77802/4.80269. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78254/4.80377. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78093/4.80781. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.77785/4.81127. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.77838/4.80826. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.77766/4.81337. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.77814/4.81267. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.77762/4.81811. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.77921/4.81208. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.77558/4.81610. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.77565/4.81790. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77927/4.81241. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.77519/4.82971. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.77538/4.82177. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.77062/4.82651. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77417/4.83227. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77209/4.83679. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.77387/4.81858. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77227/4.84378. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.76951/4.83406. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77380/4.83510. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.77868/4.81574. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.78215/4.79461. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.77668/4.80409. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77441/4.81173. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77590/4.82120. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77482/4.82898. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.77364/4.83960. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.77537/4.83029. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77409/4.82320. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76980/4.83724. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.77054/4.84211. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.76923/4.83767. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 4.76811/4.85030. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.77301/4.83701. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.76864/4.83384. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.76915/4.83602. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76789/4.82987. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.76931/4.84113. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.76399/4.86282. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.76988/4.83398. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.76647/4.85522. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.76418/4.85566. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.76840/4.84179. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76543/4.82675. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.78046/4.83528. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.78509/4.80161. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.77672/4.81412. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77362/4.82885. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.76925/4.82689. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.76958/4.83661. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.77047/4.83767. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.76460/4.85089. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.76677/4.85635. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.76917/4.85059. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.76524/4.84561. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.76548/4.84942. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.76745/4.85693. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.76379/4.85069. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.76589/4.88028. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.76180/4.85877. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.76761/4.85622. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.76560/4.84652. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.76334/4.86122. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.76406/4.86450. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.76206/4.86106. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.76329/4.87435. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.76511/4.87450. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.76017/4.88245. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.77026/4.82393. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.77555/4.81342. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.77383/4.82401. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.77243/4.82543. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.76405/4.84835. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.76802/4.85320. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.76698/4.85066. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.76593/4.85477. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.76079/4.85267. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.76641/4.84965. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.76639/4.85805. Took 0.20 sec\n",
      "ACC: 0.46875, MCC: -0.02834008097165992\n",
      "Epoch 0, Loss(train/val) 4.97086/4.96825. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.95881/4.94983. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.95947/4.96142. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.95865/4.95737. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.97454/4.94744. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.96907/4.96125. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.95293/4.94540. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.95562/4.94552. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.95488/4.94439. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.95555/4.94668. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.95497/4.94515. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.95426/4.94236. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.95744/4.94791. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.95322/4.94632. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.95395/4.94253. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.95240/4.93486. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 4.95129/4.93083. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.95021/4.93149. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.95084/4.93719. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.95425/4.93641. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.95132/4.93204. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.94965/4.92992. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.94993/4.92910. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.95103/4.93607. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.94823/4.92228. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.94699/4.92928. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.94867/4.93387. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.94615/4.92586. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.94556/4.92659. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.94500/4.92956. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.94781/4.93539. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.94444/4.92536. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.94402/4.93902. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.94896/4.92957. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.94560/4.93533. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.94269/4.92239. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.94262/4.93041. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.94137/4.92299. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.94446/4.92931. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.94087/4.92170. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.93633/4.91544. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.94174/4.92823. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.93867/4.92129. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.93220/4.91918. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.93841/4.93431. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.94049/4.93161. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.93503/4.92581. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.93511/4.92613. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.93015/4.92600. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.93167/4.93009. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.93243/4.93548. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.93595/4.93141. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.93109/4.94408. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.93338/4.92116. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.93179/4.93528. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.94595/4.93732. Took 0.21 sec\n",
      "Epoch 56, Loss(train/val) 4.94124/4.92643. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.93720/4.92593. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.93100/4.93181. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.93608/4.93194. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.93543/4.93649. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.93867/4.93872. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.92870/4.93261. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.93416/4.93900. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.93154/4.93974. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.93376/4.93867. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.92799/4.93449. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.92891/4.93102. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.93268/4.93454. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.93175/4.93293. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.92827/4.94389. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.92761/4.93999. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.92870/4.94060. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.92545/4.94120. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.92522/4.93760. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.93083/4.93696. Took 0.21 sec\n",
      "Epoch 76, Loss(train/val) 4.92820/4.94379. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 4.92842/4.94413. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.92505/4.94308. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.92278/4.94812. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.92897/4.93652. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.92022/4.94217. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.92018/4.94792. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.92344/4.94418. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.92486/4.94148. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.92111/4.95397. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.92238/4.95001. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.92280/4.95773. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.92601/4.94373. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.92203/4.94236. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.92315/4.95285. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.91723/4.94940. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.92229/4.95703. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.92199/4.93652. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.91941/4.94739. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.92174/4.94950. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.91666/4.95724. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.92224/4.95363. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.92244/4.95855. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 4.91957/4.95324. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.10677275981195139\n",
      "Epoch 0, Loss(train/val) 4.89039/4.81652. Took 0.70 sec\n",
      "Epoch 1, Loss(train/val) 4.85893/4.80983. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.84095/4.81335. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.84348/4.81468. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.83727/4.81898. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.83908/4.82202. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.83726/4.82144. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.83960/4.82126. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.83690/4.82411. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.83943/4.82235. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.83586/4.81407. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.83639/4.81335. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.83705/4.81511. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.83241/4.80919. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.83552/4.81239. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.83284/4.81136. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.83338/4.81023. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.83222/4.81334. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83433/4.80924. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.82878/4.80428. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.83063/4.80013. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.83074/4.80483. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.83141/4.80653. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.83043/4.80604. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82408/4.79722. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82540/4.79625. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82401/4.79477. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.82487/4.79660. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.82038/4.79515. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.82296/4.79271. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.81939/4.78662. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.82770/4.79203. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.82380/4.80049. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.82114/4.80817. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.82362/4.80952. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.81708/4.81257. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.81703/4.82995. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.81739/4.82766. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.81823/4.81912. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.81697/4.82244. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.83088/4.80454. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.82582/4.81254. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.82215/4.80427. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.82774/4.82144. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.82425/4.82426. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.82265/4.83547. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.81684/4.82561. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.81681/4.82934. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.82511/4.82817. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.81864/4.83269. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.81768/4.82363. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.81873/4.82340. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81816/4.82318. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.81806/4.82543. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.82009/4.80777. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.81003/4.81877. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.82313/4.80557. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.81294/4.80573. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81531/4.79245. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.81623/4.81092. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.81504/4.81473. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81400/4.81953. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.81371/4.81582. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.81328/4.81771. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81202/4.81776. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81283/4.80348. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.81217/4.80974. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.80674/4.80985. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.81258/4.81133. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.81135/4.80463. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.81142/4.82976. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81156/4.81138. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.80508/4.81419. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.80580/4.80562. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81501/4.80972. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.80827/4.81591. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.80631/4.80145. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80472/4.80933. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.81113/4.81268. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.80823/4.81564. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.80195/4.81368. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.80609/4.81331. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.80789/4.81050. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80614/4.81405. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 4.80361/4.82343. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.80529/4.80199. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.80568/4.80924. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.80204/4.81387. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.80280/4.83762. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.80668/4.82257. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.79764/4.81666. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.80862/4.83334. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.79928/4.80642. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.80405/4.82373. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.80580/4.82400. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.80307/4.83429. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.80804/4.81361. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.80430/4.82088. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.80595/4.82851. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.80143/4.81370. Took 0.19 sec\n",
      "ACC: 0.5, MCC: -0.011953709238683663\n",
      "Epoch 0, Loss(train/val) 4.64284/4.58003. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.59434/4.58241. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.59162/4.58108. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.59205/4.58510. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.59156/4.58425. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.58832/4.58590. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.58390/4.59006. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.58228/4.59153. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.58260/4.59002. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.58380/4.61262. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.59314/4.60075. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.58628/4.59706. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.58511/4.59693. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.58633/4.58957. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.58753/4.60750. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.59545/4.58674. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.59088/4.58291. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.58518/4.58568. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.58385/4.59018. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.58806/4.60484. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.58819/4.59579. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.58981/4.59189. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.58668/4.59264. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.58515/4.59249. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.58331/4.59518. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.58280/4.59392. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.58106/4.59337. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.58455/4.59109. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.58558/4.58816. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.58037/4.58557. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.58317/4.59081. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.57823/4.58117. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.58379/4.58700. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.58465/4.59335. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.58065/4.60064. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.58409/4.59901. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.57929/4.60059. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.58097/4.62528. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.58299/4.60356. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.57883/4.59359. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.58160/4.59438. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.57756/4.58831. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.58232/4.58048. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.57826/4.59514. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.57428/4.58938. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.57666/4.58622. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.57570/4.58893. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.57428/4.58998. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.57226/4.58794. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.56805/4.59158. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.57673/4.57953. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.56135/4.55952. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.58925/4.60165. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.57921/4.60247. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.57131/4.58648. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.57240/4.57821. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.56535/4.57980. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.56861/4.57886. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.57065/4.58036. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.56331/4.58434. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.57263/4.57349. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.57243/4.68722. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.59556/4.59385. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.58780/4.59278. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.58147/4.59131. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.58123/4.59472. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.58044/4.58267. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.58341/4.58517. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.57860/4.60782. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.57804/4.59169. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.58148/4.57427. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.57244/4.58660. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.57628/4.58347. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.58447/4.58050. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.58155/4.57341. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.57606/4.59320. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.57369/4.57907. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.57146/4.57054. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.57077/4.59126. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.56775/4.58929. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.57137/4.58166. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.57271/4.55383. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.58725/4.55677. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.57700/4.55004. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.57539/4.54750. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.56939/4.54583. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.56924/4.54662. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.57521/4.55607. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.57052/4.54410. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.57348/4.55372. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.56846/4.54693. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.56056/4.54977. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.56417/4.56015. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.56490/4.55039. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.56039/4.55135. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.56569/4.55185. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.56943/4.55060. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.56346/4.53572. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.56795/4.55381. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.56219/4.54381. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.20581195086702989\n",
      "Epoch 0, Loss(train/val) 4.89754/4.84795. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.83990/4.83240. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.84141/4.83216. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.83992/4.83153. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 4.83604/4.83166. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.84001/4.83281. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.83587/4.83009. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.83686/4.83123. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.83439/4.83486. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.83625/4.83749. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.83343/4.84021. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.83300/4.83953. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.82898/4.83789. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.82944/4.83697. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.83025/4.83456. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.82929/4.83503. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.82376/4.83120. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.82557/4.82729. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81888/4.83323. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.82384/4.83699. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.82360/4.84406. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.81970/4.84304. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.82112/4.84362. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.81762/4.83690. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.81999/4.85123. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82217/4.84594. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82578/4.86183. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.82744/4.85065. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.82938/4.84814. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.82440/4.85160. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.82457/4.85074. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.81826/4.83844. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.82026/4.83363. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.81588/4.83305. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.82457/4.84239. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.81616/4.85473. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.81596/4.86535. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.81981/4.85626. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.81897/4.84420. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.81244/4.85132. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.81322/4.86212. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.81148/4.85509. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.81514/4.85846. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.81267/4.84627. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.80637/4.85687. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.80993/4.85541. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.81104/4.86053. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.80974/4.84840. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.80854/4.84789. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.81207/4.84467. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.80625/4.85575. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.80864/4.85347. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81037/4.84210. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.80798/4.85163. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.80941/4.85180. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.80813/4.85332. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.80810/4.85346. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.80293/4.85079. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.80384/4.84134. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.80138/4.84086. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.82111/4.84926. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81363/4.85652. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.81451/4.85127. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.81370/4.85311. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.80944/4.85466. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.80848/4.85833. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.80982/4.85133. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.81016/4.85706. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.80805/4.84674. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.80981/4.84984. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.80807/4.85396. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.80687/4.84808. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.80607/4.85588. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.80593/4.85023. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.80739/4.85795. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.80795/4.85019. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.81046/4.84741. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80615/4.85492. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.80570/4.85206. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.80733/4.86291. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.80623/4.85502. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 4.80480/4.86126. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.80575/4.85258. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80127/4.84824. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 4.80086/4.84386. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.80780/4.83833. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.79411/4.85696. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.80839/4.84935. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.80449/4.84912. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.80157/4.85347. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.80622/4.83939. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.80323/4.86036. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.80390/4.84799. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.80163/4.85528. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.80402/4.84335. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.80172/4.85373. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.80144/4.84318. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.80341/4.85591. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.79902/4.84245. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.79729/4.84690. Took 0.18 sec\n",
      "ACC: 0.5625, MCC: 0.14201043973789815\n",
      "Epoch 0, Loss(train/val) 4.89239/4.85596. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.83969/4.82847. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.83248/4.82302. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.83255/4.82604. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.83213/4.82666. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.82642/4.82582. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.83093/4.82583. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.82834/4.82515. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.82544/4.83200. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.82576/4.83512. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.83159/4.83325. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.82113/4.83245. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.82421/4.83916. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.82482/4.84209. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.82274/4.84286. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.82525/4.83859. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.81979/4.84085. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.82253/4.84715. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.82510/4.84454. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.82006/4.84915. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.81988/4.84812. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.81883/4.84838. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.82131/4.85095. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.81914/4.85307. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82019/4.84771. Took 0.18 sec\n",
      "Epoch 25, Loss(train/val) 4.81795/4.84994. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.81828/4.85219. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.81675/4.86722. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.81184/4.87222. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.81406/4.87576. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.81926/4.86602. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.81328/4.86232. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.81851/4.84712. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.81430/4.86914. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.82951/4.82664. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82263/4.83203. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.81664/4.84170. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.81381/4.86097. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.81723/4.85226. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.81539/4.86964. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.81069/4.86139. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.81153/4.87751. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.82307/4.82580. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.82067/4.82969. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.82017/4.83685. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.81638/4.82847. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.82075/4.82598. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.81265/4.83907. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.81639/4.83020. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.81733/4.83769. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.81530/4.83923. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.81901/4.82950. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81386/4.83797. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.82297/4.82139. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81279/4.84559. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.81375/4.83166. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.81124/4.83628. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.81889/4.82998. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81084/4.83789. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.80789/4.84755. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.80720/4.84472. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.80851/4.84742. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.81370/4.84725. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.80653/4.86599. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.80761/4.84649. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81060/4.83894. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.80631/4.86014. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.81162/4.85035. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.80586/4.85531. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.80774/4.84179. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.80741/4.86247. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.80449/4.86504. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.80758/4.86035. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.80221/4.87056. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81216/4.85815. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.80711/4.86699. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.80176/4.89097. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80605/4.85916. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.80862/4.86863. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.81445/4.85837. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.81277/4.84804. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.80730/4.88734. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.80378/4.88166. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80558/4.86958. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.80360/4.87044. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.80199/4.89100. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.80474/4.85806. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.79993/4.87185. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.79938/4.87044. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.80596/4.86946. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.79268/4.86220. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.79590/4.82823. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.81445/4.84194. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.80589/4.83631. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.81129/4.83983. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.81082/4.84728. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.81098/4.84667. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81053/4.84816. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.80892/4.85001. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.80836/4.84805. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.10677275981195139\n",
      "Epoch 0, Loss(train/val) 5.07546/5.02289. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.99253/5.03173. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.98840/5.04684. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.99271/5.05683. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.98579/5.05905. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.98975/5.05550. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.98827/5.05605. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.98403/5.05070. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.98405/5.04629. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.98050/5.05432. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.98375/5.05108. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.98487/5.04306. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.98567/5.04064. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.98374/5.04015. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.98736/5.03351. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.98212/5.04266. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.97976/5.03949. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.98119/5.03841. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.97900/5.03378. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.97712/5.04214. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.97913/5.02561. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.97655/5.04108. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.97705/5.01402. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.97644/5.00629. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.97570/5.02946. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.97983/5.02434. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.97734/5.03271. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.97529/5.03008. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.97474/5.02154. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.97385/5.03310. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.97414/5.00927. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.98013/5.02307. Took 0.22 sec\n",
      "Epoch 32, Loss(train/val) 4.97422/5.03264. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.97517/5.03119. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.97282/5.03306. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.97338/5.02946. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.97150/5.03773. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.96822/5.03984. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.97243/5.03057. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.96857/5.03105. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.96559/5.04426. Took 0.23 sec\n",
      "Epoch 41, Loss(train/val) 4.96942/5.04673. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.96927/5.03554. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.96788/5.04983. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.96946/5.03958. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.96621/5.04515. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.96772/5.03292. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.96822/5.03484. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.96830/5.05000. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.96529/5.04936. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.96996/5.02712. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.96705/5.00597. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.97236/5.02020. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.97275/5.04099. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.97138/5.04163. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.96816/5.05064. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.96609/5.04963. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.97026/5.02314. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.96519/5.07122. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.96218/5.05139. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.97228/5.02956. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.96351/5.04836. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.96611/5.04858. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.95875/5.04854. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.96520/5.05813. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.97039/5.01500. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.96283/5.05171. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.96438/5.05183. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.96156/5.03748. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.97159/5.03192. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.96300/5.05069. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.96681/5.06807. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.96475/5.03329. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.95950/5.07331. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.96456/5.04143. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.96718/5.07731. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.95767/5.05306. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.96155/5.02680. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.95958/5.05662. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.95802/5.07962. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.96723/5.01650. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.95756/5.05339. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.95996/5.07100. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.96132/5.04013. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.95904/5.07631. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.95930/5.05147. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.95510/5.06656. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.96224/5.04170. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.95997/5.03174. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.95539/5.06922. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.95168/5.07771. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.95511/5.05721. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.96121/5.02910. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.95470/5.04930. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.95297/5.04672. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.95651/5.04622. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.95471/5.03917. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.95914/5.07845. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.95745/5.04176. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.95287/5.07459. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.06362847629757777\n",
      "Epoch 0, Loss(train/val) 4.70197/4.61943. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.62599/4.63530. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.62318/4.64058. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.62485/4.64088. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.62596/4.64332. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.62624/4.64189. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.62374/4.64157. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.61998/4.64710. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.61885/4.65725. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.61723/4.66374. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.61767/4.65622. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.61885/4.66320. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.61874/4.65753. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.61378/4.67121. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.61498/4.66680. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.61835/4.66673. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.61635/4.66952. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.61353/4.66104. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.60961/4.66551. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.61252/4.67258. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.61278/4.67567. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.61923/4.65649. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.60942/4.67186. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.61130/4.66762. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.60888/4.67957. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.60805/4.66467. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.61144/4.66414. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.60975/4.67388. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.60227/4.67704. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.60903/4.67389. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.60707/4.67831. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.61000/4.67499. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.60715/4.67143. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.60344/4.68094. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.60394/4.67816. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.60486/4.67743. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.60408/4.67563. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.60174/4.69133. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.60756/4.67057. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.60430/4.68109. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.60536/4.67550. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.60405/4.68005. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.60242/4.69088. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.60603/4.67912. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.60507/4.66776. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.59844/4.68301. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.60208/4.67499. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.60325/4.67728. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.59688/4.68304. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.60315/4.66521. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.60082/4.67623. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.60431/4.66364. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.61148/4.65299. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.60579/4.67673. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.59955/4.68601. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.60073/4.68353. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.60211/4.67262. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.60003/4.67647. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.59414/4.67873. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.59366/4.69128. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.59675/4.68299. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.60055/4.67942. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.60057/4.67702. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.60322/4.67709. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.59709/4.67992. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.59771/4.68484. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.59531/4.68333. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.59776/4.66932. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.59396/4.66537. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.59838/4.68015. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.59838/4.66909. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.59924/4.67002. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.59242/4.67901. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.60304/4.67159. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.59073/4.66224. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.59234/4.67112. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.59708/4.66706. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.59933/4.67560. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.59517/4.66909. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.59942/4.66807. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.59401/4.68875. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.58989/4.67096. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.59526/4.69048. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.59532/4.66921. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.59536/4.66827. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.59313/4.68499. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.58858/4.69677. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.58786/4.66765. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.59499/4.69263. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.59271/4.67621. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.59311/4.68634. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.58907/4.69437. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.59707/4.68778. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.58254/4.70640. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.59140/4.70379. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.59241/4.65251. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.58979/4.68505. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.59292/4.66650. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.59318/4.68077. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.58757/4.67416. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.06859245370246428\n",
      "Epoch 0, Loss(train/val) 4.91399/4.84114. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.83485/4.84415. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 4.83297/4.84793. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.83398/4.84600. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.83596/4.84526. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.83146/4.84494. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.83473/4.84527. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.83225/4.84380. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.83314/4.84572. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.83243/4.84611. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.83249/4.84621. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.83572/4.84949. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.83355/4.84900. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.83460/4.84842. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.83395/4.84513. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.82866/4.84564. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.82950/4.86345. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.82723/4.85890. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.82605/4.86773. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.82941/4.86666. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.82945/4.86165. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.82788/4.85918. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.82466/4.86806. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.82440/4.87712. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82210/4.88485. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82204/4.87027. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82095/4.87952. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.82355/4.88202. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.82047/4.88933. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.82259/4.87851. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.81877/4.88218. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.81746/4.88673. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.82023/4.87553. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 4.81873/4.88446. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.82043/4.87469. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82070/4.89267. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.81881/4.88055. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.81693/4.89203. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 4.82045/4.87648. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.82024/4.88208. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.81817/4.89433. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.81946/4.88119. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.81648/4.88159. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.81674/4.88143. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.81594/4.89271. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.81914/4.87639. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.81204/4.90137. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.81824/4.89708. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.82066/4.88141. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.81384/4.88256. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.81465/4.88948. Took 0.22 sec\n",
      "Epoch 51, Loss(train/val) 4.81722/4.88323. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81482/4.90873. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.81661/4.88562. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81953/4.89048. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.81330/4.89799. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.81623/4.88797. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.81448/4.88410. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81506/4.89376. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.81946/4.88245. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.81562/4.88408. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81764/4.87476. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.81278/4.88548. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.81709/4.89173. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81162/4.89757. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81556/4.87666. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.81488/4.88852. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.81159/4.88549. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.81534/4.89737. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.81480/4.90487. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.81725/4.86634. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81516/4.90566. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.81304/4.88251. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.81196/4.90193. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81185/4.90314. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.80533/4.89883. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.81081/4.87393. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.81061/4.88818. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.82078/4.88440. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.81401/4.89387. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.81531/4.89128. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.81243/4.90441. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.81371/4.89916. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80947/4.89276. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.80429/4.89946. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.81457/4.90438. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.81645/4.90094. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.81032/4.90516. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.81654/4.89737. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.81253/4.89872. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.80818/4.90433. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.80647/4.90623. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.80681/4.90676. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.80653/4.90419. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.81219/4.89911. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.80786/4.92389. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.80887/4.91029. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81152/4.87898. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.80624/4.90574. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.80667/4.93045. Took 0.20 sec\n",
      "ACC: 0.53125, MCC: 0.0657512425947884\n",
      "Epoch 0, Loss(train/val) 4.87839/4.76603. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.78680/4.79326. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 4.77941/4.78569. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.77448/4.78079. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.77236/4.78285. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.77313/4.78482. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.77194/4.78673. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.77143/4.79303. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.76992/4.79424. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.76937/4.79906. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.77028/4.80171. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.76804/4.80486. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.76562/4.81068. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.76455/4.80705. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.76493/4.81002. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.76312/4.81662. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.76450/4.81744. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.76230/4.81930. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.76155/4.81754. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.76649/4.80696. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.76219/4.81282. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.76321/4.81179. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.76078/4.82235. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.76155/4.82130. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.75907/4.81969. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.76115/4.82487. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.76119/4.81834. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.76173/4.81489. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.75931/4.81277. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.75761/4.82443. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.75752/4.82357. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.76389/4.80596. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.75598/4.82209. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.75646/4.82714. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.75752/4.82264. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.75792/4.82751. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.76114/4.81857. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.75682/4.81737. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.75695/4.83214. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.75471/4.83643. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.75673/4.83405. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.75443/4.83170. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.74936/4.84776. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.75285/4.84531. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.75155/4.85268. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.75683/4.83575. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.75194/4.83243. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.75204/4.83933. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.75500/4.85556. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.75610/4.83402. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.74984/4.83923. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.75477/4.82606. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.75400/4.84486. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.74907/4.85692. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.75448/4.84665. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.75165/4.85453. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.75233/4.84886. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.74758/4.85572. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.74783/4.86479. Took 0.22 sec\n",
      "Epoch 59, Loss(train/val) 4.74864/4.86907. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.75097/4.85974. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.74723/4.85467. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.74485/4.87201. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.74673/4.85958. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.74334/4.87219. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.74961/4.83260. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75108/4.85390. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.74345/4.87071. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.74523/4.86675. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.74855/4.85880. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.74561/4.85141. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.74649/4.86379. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.73989/4.88390. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.74397/4.86341. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.74426/4.85472. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.74456/4.87235. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.74174/4.86822. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.74104/4.88238. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.74578/4.87499. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.74087/4.88044. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.75534/4.85404. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.74933/4.86593. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.74076/4.88326. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.74372/4.87628. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.74095/4.88533. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.74360/4.88283. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.74456/4.87446. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.73398/4.87848. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.74194/4.87259. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.73766/4.89553. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.74259/4.88482. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.74485/4.87134. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.74015/4.88354. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.73861/4.87411. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.74572/4.86902. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.74277/4.86413. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.73418/4.88991. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.73875/4.87699. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.73657/4.89768. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.73756/4.88580. Took 0.20 sec\n",
      "ACC: 0.46875, MCC: -0.06262242910851495\n",
      "Epoch 0, Loss(train/val) 5.16343/5.06730. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.06222/5.10613. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.06703/5.12535. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.06689/5.13231. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.07149/5.12879. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.07436/5.10560. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.07572/5.08358. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.07081/5.07745. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.06486/5.07716. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.06019/5.08360. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.05897/5.08697. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 5.05710/5.09411. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.06369/5.08832. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.06071/5.08459. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.05783/5.08765. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.05748/5.08267. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.05906/5.08070. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.05577/5.08550. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.05755/5.08564. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.05524/5.08583. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.05343/5.08522. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.05369/5.08547. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.05388/5.09017. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.05156/5.08868. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.05502/5.09443. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.05554/5.09423. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.05316/5.09081. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.05200/5.09110. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.05018/5.09027. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.04719/5.08873. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.05017/5.09185. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.05241/5.08784. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.04783/5.09794. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.05228/5.08590. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.05105/5.09041. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.04881/5.09447. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.04778/5.09700. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.05217/5.08858. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.04530/5.10182. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.04916/5.09580. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.04755/5.10458. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.04892/5.10343. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.05161/5.10919. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.04625/5.09913. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.04795/5.10315. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.04423/5.10083. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.04850/5.10762. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.04399/5.10457. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 5.04535/5.09057. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.04762/5.10095. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.04927/5.10016. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.04270/5.10941. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.04221/5.12128. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.04628/5.10600. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.04260/5.10747. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.04504/5.11304. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 5.04012/5.11766. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 5.04780/5.10793. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.05013/5.09056. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.05078/5.09182. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.04859/5.10510. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.04932/5.09764. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.04117/5.10257. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.04463/5.11519. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.04410/5.11132. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.04423/5.11510. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.04111/5.11107. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.04078/5.12594. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.04421/5.11699. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.04278/5.13260. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.04287/5.11537. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.04383/5.11627. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.04074/5.13057. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 5.04047/5.13172. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.04395/5.12217. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.03610/5.12896. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.03688/5.15043. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.04302/5.11501. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 5.03995/5.13778. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 5.04143/5.12493. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.03512/5.14236. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.03714/5.14961. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 5.04127/5.13644. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.03886/5.14082. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.04015/5.13802. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.03527/5.13467. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.03763/5.14875. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.04021/5.11104. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.03619/5.13479. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.03658/5.12981. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.04721/5.11248. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.04513/5.11057. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.04109/5.14252. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.03861/5.10104. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.03893/5.14057. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.03632/5.13774. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.03165/5.15521. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.03274/5.15306. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.03032/5.16383. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.03340/5.15336. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 4.91569/4.86469. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.84164/4.83998. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.83815/4.83912. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 4.83756/4.84010. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.83682/4.83699. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.83708/4.83767. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.83537/4.83766. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.83814/4.84129. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.83660/4.84244. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.83354/4.84499. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.83293/4.84637. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.83179/4.84954. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.83116/4.84997. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.83018/4.85222. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.83046/4.85117. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.82750/4.84923. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.83590/4.84755. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.82688/4.84742. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83168/4.84539. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.82524/4.84690. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.82446/4.84755. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.82797/4.84419. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.82838/4.84509. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.82143/4.84907. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.81803/4.85186. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82672/4.84152. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82538/4.85028. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.82080/4.84997. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.81983/4.84664. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.82119/4.85012. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.81567/4.84946. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.81878/4.84370. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.82386/4.84010. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.81354/4.85178. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.81722/4.83842. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.81386/4.84538. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.81318/4.84455. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.81950/4.83664. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.81325/4.83763. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.81056/4.83137. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.81387/4.83134. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.82198/4.80840. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.81190/4.82331. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.81867/4.80930. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.81322/4.81140. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.80912/4.82017. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.81242/4.82495. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.81798/4.81373. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.80806/4.82322. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.80751/4.82359. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.80977/4.82497. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.80520/4.82040. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.80658/4.82065. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.81239/4.82966. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81205/4.81171. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.80762/4.80539. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.82211/4.88619. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.82151/4.83689. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.80650/4.84192. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.80652/4.82317. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.79999/4.81511. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.80493/4.81624. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.80147/4.81672. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.80033/4.82100. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.80030/4.82475. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.79771/4.81220. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.80283/4.81017. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.79459/4.81286. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.80548/4.81877. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.79979/4.81811. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.79775/4.81769. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.80156/4.81547. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.79543/4.84720. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 4.81179/4.80255. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81049/4.81186. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.80167/4.80355. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.80362/4.80920. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80410/4.79348. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.80027/4.81017. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.79710/4.82466. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.80625/4.82072. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.80360/4.80745. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.80598/4.83079. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80915/4.81291. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.79324/4.82523. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.79898/4.81034. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.79960/4.83406. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.79852/4.84471. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83746/4.81110. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83205/4.81474. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.83624/4.83414. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.83345/4.83867. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82839/4.83814. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82712/4.83473. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.83003/4.83798. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.82534/4.83488. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82751/4.83356. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82426/4.83207. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82454/4.83765. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.82180/4.83189. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.08818077954471167\n",
      "Epoch 0, Loss(train/val) 4.93838/4.88386. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.88282/4.87867. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.88217/4.87742. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.88167/4.88164. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.88487/4.88438. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.88071/4.88548. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.88074/4.88591. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.87680/4.88910. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.87606/4.89203. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87393/4.88488. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.87769/4.88610. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.87542/4.88834. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.87587/4.88949. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.87268/4.88937. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87326/4.89531. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.87389/4.89471. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.87132/4.89707. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.86854/4.90021. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.86897/4.90933. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.87007/4.90042. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.87038/4.89141. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.86621/4.89941. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.86828/4.90259. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.86639/4.89807. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.86646/4.90353. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.86460/4.91053. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.86552/4.89559. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.86683/4.90428. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.86668/4.89970. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.86816/4.90121. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.86704/4.89847. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86052/4.90751. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.86526/4.90832. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.86073/4.91597. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.86288/4.91469. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.86426/4.91096. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.85919/4.91993. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.86033/4.91710. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.85957/4.91558. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.85840/4.92039. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.86572/4.89795. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.85396/4.92745. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.86229/4.90081. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.85751/4.91298. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.85187/4.91756. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.86015/4.90022. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.86101/4.90497. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.85573/4.92562. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.85646/4.91605. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.86243/4.91533. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.85421/4.90407. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.85338/4.91348. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.85804/4.91216. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.85280/4.90748. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.85201/4.89440. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.85843/4.91027. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.85632/4.91026. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.85670/4.91492. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.85626/4.91808. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.85254/4.91505. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85186/4.93534. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.85207/4.91521. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85273/4.92559. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.85255/4.93016. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.85444/4.91552. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.85124/4.91788. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.85774/4.91068. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.85450/4.92603. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.85231/4.90618. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.84565/4.93816. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.84905/4.92379. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.85177/4.91998. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.85401/4.90928. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84814/4.91280. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.84703/4.93878. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.84738/4.94231. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.84541/4.92270. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.85719/4.89893. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.85194/4.91446. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.85514/4.91355. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 4.85206/4.90963. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.84417/4.95845. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.84782/4.90526. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84514/4.93958. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.84568/4.90889. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.85116/4.92385. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.85235/4.90946. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.85665/4.91538. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.84873/4.91298. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.84924/4.91996. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.84667/4.92056. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.84506/4.91711. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.85178/4.90429. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.84597/4.95888. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.84811/4.90780. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.84762/4.91458. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.84509/4.91285. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.84054/4.93373. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.84674/4.90855. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.84314/4.92995. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.03643724696356275\n",
      "Epoch 0, Loss(train/val) 4.90753/4.82356. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.80993/4.83799. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.81335/4.82791. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.81410/4.82497. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.81249/4.82552. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.80993/4.82954. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.80818/4.83296. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.81088/4.83609. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.80764/4.83936. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.81015/4.84118. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.81009/4.82539. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.81191/4.83050. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.80976/4.83395. Took 0.22 sec\n",
      "Epoch 13, Loss(train/val) 4.80544/4.83686. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.80472/4.83852. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.80374/4.83440. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.80124/4.83287. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.80469/4.83594. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.80489/4.83528. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.80532/4.83958. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.80151/4.83670. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.80134/4.83330. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.80249/4.82985. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.80053/4.83524. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.80033/4.83647. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.80455/4.83803. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.80575/4.83875. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.80567/4.86113. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.80658/4.88895. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.80467/4.88322. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.80328/4.86796. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.80145/4.85770. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.79906/4.85962. Took 0.18 sec\n",
      "Epoch 33, Loss(train/val) 4.79812/4.86623. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.79584/4.86651. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.79984/4.86552. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.79516/4.86201. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.79484/4.87027. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.79740/4.85837. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.79300/4.86431. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.79687/4.86856. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.79527/4.86069. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.79730/4.85786. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.79579/4.84931. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.79652/4.85273. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.79675/4.85171. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.79335/4.85658. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.78909/4.85809. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.79167/4.85588. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.79058/4.84759. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.79522/4.86031. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.78851/4.85048. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.78947/4.84426. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.79507/4.84681. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.79373/4.85453. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.78847/4.85448. Took 0.22 sec\n",
      "Epoch 56, Loss(train/val) 4.78738/4.86340. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.78356/4.86046. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.78829/4.85301. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.79114/4.86260. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.78723/4.85631. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.78988/4.86668. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.78801/4.86001. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.79122/4.86182. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.78757/4.85305. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.78536/4.85012. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.78500/4.85028. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.78583/4.85007. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.78680/4.85090. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.78474/4.84024. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.78925/4.84848. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.78019/4.84327. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.78356/4.84342. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.78651/4.83550. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.78145/4.84941. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.78359/4.85959. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.78956/4.84539. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.78350/4.84036. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.78416/4.84834. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.78345/4.85752. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.78142/4.86194. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.78292/4.84584. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.78430/4.84323. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.78262/4.84825. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.78253/4.84043. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.78256/4.84516. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.78287/4.83177. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.78492/4.87348. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.78015/4.84427. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.78792/4.83714. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.77745/4.86096. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.77782/4.83048. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.77665/4.84324. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.78236/4.84469. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.77868/4.83908. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.77956/4.83026. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.78623/4.85116. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.78282/4.84268. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.78256/4.84725. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.77711/4.83728. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.004034850217792016\n",
      "Epoch 0, Loss(train/val) 4.87664/4.80503. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.82425/4.82528. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.82272/4.83167. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.82750/4.82872. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.82988/4.82387. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.82584/4.81770. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.82114/4.81850. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.81911/4.82093. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.82091/4.82209. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.82244/4.82122. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.81727/4.82100. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.82145/4.82084. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.81780/4.82161. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.81984/4.82410. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.81769/4.82298. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.81668/4.82955. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.81569/4.82942. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.81595/4.83197. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81512/4.82968. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.81235/4.83287. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.81050/4.83509. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.80955/4.84055. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.81010/4.83814. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.81001/4.83881. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.80927/4.83793. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.80859/4.84191. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.80599/4.83372. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.81421/4.81899. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.81037/4.82370. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.81022/4.82741. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.80775/4.82791. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.80645/4.82349. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.80925/4.82389. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.80807/4.82602. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.80848/4.83041. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.80719/4.82963. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.80668/4.83206. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.80684/4.83395. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.80657/4.83281. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.80569/4.83529. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.80658/4.83667. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.80525/4.83288. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.80357/4.83657. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.80432/4.83915. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 4.80523/4.83695. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.80192/4.83671. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.80358/4.83553. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.80329/4.83090. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.80484/4.83484. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.80265/4.83324. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.80339/4.83145. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.80299/4.83337. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.80622/4.83184. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.80406/4.83784. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.80431/4.82879. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.80442/4.83080. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.80084/4.83560. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.80167/4.83578. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 4.80686/4.83572. Took 0.22 sec\n",
      "Epoch 59, Loss(train/val) 4.79926/4.84959. Took 0.21 sec\n",
      "Epoch 60, Loss(train/val) 4.80192/4.84346. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.80131/4.84018. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.80048/4.84130. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.79867/4.85699. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.80829/4.83128. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.80156/4.84506. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.79947/4.83978. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.80034/4.83758. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.80371/4.83451. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.79711/4.84274. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.79877/4.84547. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.79850/4.83356. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.80012/4.83242. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.79953/4.83851. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.79597/4.83950. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.79844/4.83780. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.79392/4.84560. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.79993/4.83830. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.79425/4.84092. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.79750/4.83387. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.79500/4.81438. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.80587/4.82034. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.80034/4.82363. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.79589/4.82526. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.79928/4.82013. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.79706/4.82421. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.80299/4.82475. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.79655/4.82784. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.79898/4.82625. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.79135/4.82749. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.79676/4.82891. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.79202/4.83080. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.79741/4.81226. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.79627/4.83600. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.79343/4.82886. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79207/4.83822. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.79383/4.83693. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.79211/4.83619. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.78980/4.84759. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.79131/4.83105. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.054187192118226604\n",
      "Epoch 0, Loss(train/val) 4.80980/4.79276. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.75940/4.74488. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.74378/4.73024. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.73205/4.73352. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.73528/4.74257. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.73742/4.74590. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.73986/4.74167. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.73837/4.74097. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.73379/4.73752. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.73399/4.74435. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.73180/4.74614. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.73386/4.74497. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.73028/4.74455. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.73116/4.74416. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.73223/4.73947. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.73080/4.75272. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.72654/4.74825. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.72709/4.75621. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.72504/4.75994. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.72773/4.75325. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.72701/4.75083. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.72507/4.75644. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.72295/4.75460. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.72528/4.76588. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.72371/4.75105. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.72308/4.76375. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.72119/4.75841. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.71797/4.77526. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.72204/4.76585. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.71868/4.76380. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.72462/4.76561. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.72025/4.76459. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.71851/4.77139. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.71617/4.77131. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.71540/4.78042. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.71883/4.77255. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.71807/4.76598. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.71515/4.79310. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.71739/4.76646. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.71321/4.78767. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.71789/4.78948. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.71522/4.78942. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.71299/4.78584. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.71668/4.78465. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.71904/4.78528. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.71011/4.77689. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.71531/4.77875. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.71517/4.78324. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.71634/4.78674. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.71289/4.78392. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.71425/4.77866. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.71555/4.77585. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.71378/4.79111. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.70772/4.79145. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.70776/4.81439. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.71219/4.78194. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.70842/4.79247. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.70558/4.82701. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.71476/4.80206. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.71112/4.77395. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.70765/4.78420. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.70797/4.78719. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.70419/4.80814. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.71209/4.78831. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.71029/4.79073. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.71089/4.78494. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.70650/4.77723. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.70811/4.79714. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.71133/4.81300. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.71506/4.75529. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.71188/4.77397. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.70835/4.81483. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.70418/4.80366. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.71237/4.79025. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.70513/4.80780. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.70683/4.80318. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.70445/4.80232. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.70327/4.80591. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.69950/4.81596. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.70219/4.79911. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.70806/4.81360. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.70243/4.81978. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.70671/4.77962. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.70972/4.78032. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.70086/4.78747. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.70226/4.79448. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.70980/4.81380. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.70760/4.79505. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.70662/4.78434. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.71033/4.78310. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.71214/4.79714. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.70569/4.79884. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.70951/4.79896. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.70251/4.80499. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.70349/4.79030. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.70067/4.79446. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.70326/4.80175. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.70869/4.80023. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.70533/4.79290. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.70060/4.79063. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.04866091991018888\n",
      "Epoch 0, Loss(train/val) 4.84192/4.80905. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.79086/4.74173. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.79099/4.74208. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.78929/4.74410. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.78669/4.74697. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.78701/4.74677. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.78622/4.74805. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.78454/4.74766. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.78844/4.75024. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.78681/4.75367. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.78680/4.76480. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78662/4.78294. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.78575/4.79949. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.78315/4.80465. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78514/4.80822. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.77892/4.82324. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.77754/4.80727. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.77833/4.80300. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78076/4.80875. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.77733/4.81606. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.77651/4.80569. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.77418/4.82490. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.77859/4.80704. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.77501/4.83024. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.77646/4.80373. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.77154/4.81605. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.77332/4.81459. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.77452/4.80775. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.77274/4.78367. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78419/4.79216. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.77850/4.78981. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.77795/4.79337. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77653/4.79726. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.77712/4.77710. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 4.77395/4.79671. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.77356/4.78428. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77787/4.79756. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.76808/4.81086. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77331/4.80097. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77097/4.79109. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77596/4.80439. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.77081/4.82753. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76844/4.81997. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.76779/4.81950. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.75740/4.81187. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.76495/4.80634. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.76819/4.80648. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.76489/4.83865. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.76741/4.81310. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.76270/4.83258. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76502/4.81658. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.77564/4.80800. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.77513/4.81488. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76778/4.83415. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76809/4.83493. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.75903/4.83845. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.75609/4.84832. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76487/4.81429. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.76558/4.81911. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.75372/4.85966. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 4.76358/4.83544. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.75780/4.82312. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.75956/4.82666. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.74826/4.82922. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76266/4.83273. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.75579/4.84510. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75461/4.84514. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.75157/4.87249. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.75657/4.81739. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.75810/4.85007. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.74966/4.86698. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.75454/4.87945. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.75083/4.86297. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.75692/4.82974. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.74833/4.84866. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.74533/4.92763. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.74743/4.83554. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.74574/4.86335. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.75340/4.84833. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.74939/4.85583. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.75515/4.84456. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.74819/4.84598. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.75070/4.85828. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.74807/4.87012. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.74804/4.83653. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.74980/4.87337. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.74587/4.86629. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.74865/4.86498. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.74395/4.85368. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 4.74805/4.87860. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.73977/4.86594. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.74807/4.87791. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.74399/4.87414. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.74782/4.84960. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.74727/4.89171. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.74696/4.83280. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.74213/4.86975. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.74522/4.83695. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.74646/4.87599. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.74466/4.85321. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.040082172520323485\n",
      "Epoch 0, Loss(train/val) 4.93317/4.86473. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.87004/4.89565. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.87765/4.91616. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87372/4.90806. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.87646/4.89734. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.88086/4.87853. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.87610/4.87534. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.86943/4.87539. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.86721/4.87807. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.86604/4.87821. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.86683/4.88258. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.86537/4.88879. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.86533/4.89867. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.86298/4.90773. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.86071/4.91178. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85984/4.91373. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85784/4.91639. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85695/4.91458. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85566/4.91728. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.85698/4.91336. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.85295/4.91379. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.85364/4.91152. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85605/4.90913. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85433/4.91432. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85115/4.91880. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.85028/4.92624. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84887/4.92470. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.84883/4.92767. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.85075/4.93117. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84762/4.92841. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.84348/4.93404. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84609/4.93138. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.84779/4.93691. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.84699/4.94136. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84556/4.94124. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.84203/4.95310. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84446/4.92818. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.84578/4.94134. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84192/4.93277. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84750/4.94738. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84465/4.92864. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84427/4.93067. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.85304/4.90688. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.86282/4.90725. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.85298/4.90973. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84437/4.94936. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84327/4.96428. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84409/4.94613. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.84536/4.93599. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.85160/4.92797. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.84634/4.92587. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.84583/4.93657. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.84749/4.94088. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.84271/4.94373. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.85680/4.93590. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.85756/4.92939. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84892/4.93253. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.85639/4.92874. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.85578/4.91598. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.85181/4.91463. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85669/4.91148. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.85492/4.92426. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85380/4.91732. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.84936/4.91928. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.85531/4.91617. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.84762/4.92519. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 4.84753/4.92898. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.84973/4.93249. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.85184/4.93139. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84712/4.92230. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.85067/4.92429. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.84809/4.93094. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.84587/4.93571. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84810/4.93704. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.85181/4.93215. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.84911/4.92344. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.84696/4.93734. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.84686/4.93541. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.84731/4.91626. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.84492/4.93311. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.84436/4.92480. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.84897/4.93220. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.84197/4.94690. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.84672/4.93790. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.84416/4.94611. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.84401/4.94012. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.84599/4.93969. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.84127/4.94153. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.84130/4.95799. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.84583/4.94695. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.84420/4.95126. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.84458/4.94608. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.83807/4.96016. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.84713/4.94059. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.84389/4.93468. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.84695/4.92647. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83809/4.94587. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.84239/4.95075. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.84518/4.93944. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.83887/4.95137. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.16150120428611295\n",
      "Epoch 0, Loss(train/val) 5.00020/4.96824. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.95804/4.98519. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.95335/4.98557. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.95575/4.97676. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.95455/4.97531. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.95489/4.97874. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.95753/4.97864. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.95415/4.99179. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.95088/5.00019. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.95083/5.00361. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.94689/4.99516. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.94716/4.99491. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.94512/4.99721. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.94557/5.00011. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.94596/4.99754. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 4.94310/5.00180. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.94203/5.00081. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.94646/5.00129. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.94220/5.00580. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.94078/5.01287. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.94162/5.01330. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.94072/5.01068. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.94410/5.00907. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.94209/5.00924. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.94263/5.01006. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.94169/5.00231. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.93729/5.02009. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.94026/5.01353. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.94129/5.00956. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.93962/5.01814. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.94156/5.01540. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.93392/5.02524. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.93866/5.01247. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.93908/5.01224. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.93731/5.02191. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.93752/5.02720. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.94030/5.02804. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.93409/5.02815. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.93673/5.03408. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.93395/5.04903. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.93190/5.04459. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.93688/5.04510. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.93031/5.06320. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.93098/5.04455. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.93239/5.06219. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.93030/5.03928. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.93036/5.03518. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.93331/5.03061. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.93052/5.03954. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.92956/5.04526. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.93069/5.03390. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.92993/5.03090. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.92853/5.05350. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.93009/5.05119. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.92755/5.05666. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.92646/5.05200. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.92599/5.05699. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.91987/5.04641. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.92995/5.07069. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.92653/5.07505. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.92839/5.05836. Took 0.22 sec\n",
      "Epoch 61, Loss(train/val) 4.92287/5.07753. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.92499/5.05115. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.91996/5.04976. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.92634/5.03627. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.92285/5.06557. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.92271/5.06917. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.92340/5.07572. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.91998/5.08958. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.92377/5.04721. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.91328/5.07449. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.91864/5.04081. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.92411/5.03804. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.91831/5.05248. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.91803/5.06122. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.91242/5.09077. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.92363/5.05858. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.91524/5.05345. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.92000/5.04970. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.91587/5.06657. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.91319/5.08723. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.92293/5.05640. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.91504/5.06383. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.92062/5.05228. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.91709/5.05822. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.91658/5.07268. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.91438/5.07022. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.91442/5.07712. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.91089/5.05582. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.91396/5.11491. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.91374/5.09053. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.91202/5.08433. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.91180/5.07362. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.91051/5.08419. Took 0.21 sec\n",
      "Epoch 94, Loss(train/val) 4.90854/5.06313. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.90185/5.07368. Took 0.21 sec\n",
      "Epoch 96, Loss(train/val) 4.90381/5.09679. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.91167/5.04779. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.91621/5.07393. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.91723/5.08412. Took 0.19 sec\n",
      "ACC: 0.5, MCC: -0.0009775171065493646\n",
      "Epoch 0, Loss(train/val) 4.85299/4.83467. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.79259/4.78889. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.79665/4.78578. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79838/4.78431. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79953/4.78413. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.80203/4.78652. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.79961/4.79053. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79578/4.79131. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.79097/4.78865. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.79323/4.78846. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79288/4.78728. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.79225/4.78727. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79056/4.79016. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.78866/4.79209. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.79023/4.79311. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.78964/4.79528. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78929/4.79835. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78713/4.80068. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.78607/4.80176. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78581/4.81221. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.78016/4.82847. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78115/4.82860. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78130/4.84690. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78854/4.81795. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78225/4.82553. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78220/4.83062. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.77655/4.83881. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.77443/4.85244. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.78050/4.84100. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78139/4.83728. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.77689/4.84778. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77032/4.85400. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.77398/4.84442. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.77892/4.84700. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.76794/4.86393. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.77635/4.85029. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77926/4.83584. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.77595/4.83892. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.76794/4.83876. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77334/4.84384. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77182/4.84894. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 4.77258/4.84922. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76933/4.85238. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.77511/4.84736. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.76506/4.86681. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77343/4.84884. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.77173/4.84981. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.76844/4.85366. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.76987/4.85318. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.76900/4.85056. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76486/4.85090. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.76682/4.86993. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.77145/4.83769. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.77184/4.83548. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.76635/4.86675. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.77097/4.84541. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.76161/4.85583. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76280/4.86041. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.76328/4.85171. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.76834/4.85721. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.76743/4.84469. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.76183/4.86614. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.76929/4.85087. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.76814/4.85370. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76310/4.86042. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.75921/4.86508. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.76476/4.85129. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.76698/4.85110. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77011/4.86357. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.75954/4.85871. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.76902/4.85626. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.75828/4.88257. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.79790/4.80920. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.78727/4.80058. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.78224/4.81193. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.78115/4.81835. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.77806/4.82468. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77654/4.82996. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.78146/4.82248. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.78720/4.80677. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.78663/4.80785. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.78262/4.79814. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.78322/4.80235. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.78195/4.80977. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.77858/4.80416. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.78255/4.81240. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.77318/4.80766. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.78770/4.80510. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.77505/4.81303. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.77522/4.81349. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.77968/4.82733. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.77739/4.82659. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.77712/4.83744. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77706/4.83356. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.78107/4.83481. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.77119/4.85510. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.77302/4.84596. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.77708/4.83134. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.77181/4.83990. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.77379/4.84863. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.09416472308615167\n",
      "Epoch 0, Loss(train/val) 4.84970/4.78179. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.77818/4.77774. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.77322/4.77957. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.77099/4.78297. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.77509/4.77923. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.77106/4.77761. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.77071/4.77603. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.77082/4.77681. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.76924/4.78134. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.76932/4.78244. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.76785/4.78101. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.76921/4.78421. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.76519/4.78292. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.76754/4.77644. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.77091/4.77478. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.76854/4.77258. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.76551/4.78026. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.76584/4.77694. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.76452/4.77168. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.76202/4.77287. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.76128/4.77703. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.76275/4.77806. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.76121/4.77442. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.76039/4.77992. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.76359/4.77676. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.76217/4.77811. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.76620/4.79479. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.76123/4.79847. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.76361/4.79610. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.76264/4.78222. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.76008/4.78335. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.75936/4.78569. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.75877/4.78821. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.76087/4.78818. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.75750/4.78652. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.75873/4.78492. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.76185/4.78370. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.75623/4.79011. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.75932/4.78848. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.76135/4.78857. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.75888/4.78981. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.75941/4.78004. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.75307/4.78664. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.75530/4.79416. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.75704/4.78982. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.75512/4.78416. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.75335/4.78589. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.75324/4.78898. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.75869/4.78584. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.75749/4.78502. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.75703/4.78628. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.75347/4.78365. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.75305/4.78444. Took 0.22 sec\n",
      "Epoch 53, Loss(train/val) 4.75207/4.79615. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.75344/4.79869. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.75738/4.78876. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.75348/4.79012. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.75412/4.79118. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.75974/4.78269. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.75244/4.79889. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.75392/4.79518. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.75385/4.79200. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.75365/4.77700. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.75289/4.79194. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.75179/4.79523. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.75107/4.80596. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75246/4.79215. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.74536/4.79699. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.74853/4.80726. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.75451/4.80812. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.75687/4.78721. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.74872/4.79450. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.75160/4.79892. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.75211/4.79603. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.74689/4.79307. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.74626/4.80125. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.75599/4.79471. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.75297/4.79487. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.74256/4.80022. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.74559/4.81100. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.74917/4.81371. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.74911/4.79403. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.74691/4.80136. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.74819/4.79273. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.74393/4.79248. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.74076/4.80166. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.74925/4.78850. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.74850/4.79891. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.74331/4.80456. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.74345/4.81541. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.74823/4.80819. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.74271/4.81074. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.74369/4.80899. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.73627/4.83219. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.74745/4.81814. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.74608/4.79995. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.74756/4.80408. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.76532/4.80477. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.75947/4.79866. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75749/4.80653. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 4.93172/4.87012. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.82484/4.81760. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.81472/4.78946. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79629/4.79339. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.79498/4.79681. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79750/4.79588. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.80046/4.79399. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79713/4.79322. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79841/4.79641. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79760/4.79823. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79596/4.80162. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.79352/4.79424. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79443/4.79458. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79012/4.79442. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.79064/4.79429. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78968/4.79084. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78674/4.79033. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78693/4.78751. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78664/4.78631. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.78805/4.78563. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.78696/4.79244. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 4.78848/4.77318. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.78965/4.78046. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.78568/4.78474. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.79331/4.76893. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.79560/4.78187. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.79354/4.78274. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.79282/4.77981. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.79016/4.77552. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.78859/4.77220. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.78871/4.76917. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.78453/4.76492. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.78646/4.77179. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.78391/4.76844. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.78263/4.77324. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.78502/4.77645. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78302/4.78520. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.78093/4.78362. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.78053/4.77954. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77969/4.77806. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77808/4.78172. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.77936/4.78098. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.78154/4.78500. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.78095/4.77969. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77914/4.79824. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.77850/4.78629. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77803/4.78629. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.77553/4.78746. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.77833/4.78197. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77620/4.78513. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.77178/4.79075. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.77266/4.79037. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.77341/4.79189. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.77068/4.79097. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.77454/4.78179. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.77812/4.78448. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.77306/4.78561. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.77114/4.77736. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.76848/4.79876. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.76891/4.80905. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.76961/4.78830. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.77287/4.79987. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.76481/4.80328. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.76541/4.80262. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.77050/4.78514. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.76386/4.80654. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.77039/4.80288. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.76356/4.79151. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77317/4.78351. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.76495/4.79496. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.76288/4.80994. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.76413/4.80034. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.75603/4.80802. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.76079/4.81044. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.76771/4.80416. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.76571/4.78412. Took 0.21 sec\n",
      "Epoch 76, Loss(train/val) 4.76255/4.79063. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.76200/4.81101. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.76501/4.79276. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 4.75830/4.82009. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.76345/4.79831. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.76109/4.78502. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.77734/4.79649. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.77621/4.77521. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.77107/4.79056. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.76739/4.79629. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.76131/4.79077. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.76559/4.80093. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.76575/4.79539. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.76448/4.79178. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.75818/4.79877. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.75964/4.79569. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.76285/4.79963. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.75395/4.80195. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.76498/4.79281. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75866/4.80685. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.75535/4.80879. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.75979/4.80862. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.75526/4.80280. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75582/4.81144. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.035451483867683334\n",
      "Epoch 0, Loss(train/val) 4.99937/5.00061. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.98865/4.97538. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.97791/4.98017. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.97729/4.98324. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.97962/4.98273. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.97926/4.99111. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.98242/4.99575. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.98131/4.98657. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.98360/4.97087. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.97833/4.96750. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.97867/4.96867. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.97625/4.96972. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.97886/4.96887. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.97663/4.96847. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.97692/4.96701. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.97633/4.96686. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.97690/4.96734. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.97776/4.96940. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.97677/4.96761. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.97646/4.96646. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.97419/4.96793. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.97338/4.96606. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.97366/4.97231. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.97598/4.97295. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.97635/4.96919. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.97382/4.96676. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.97502/4.96808. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.97116/4.97177. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.97453/4.96848. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.97149/4.96869. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.97122/4.96862. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.97290/4.96709. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.96978/4.97565. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 4.96658/4.97574. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.97182/4.97313. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.97002/4.96358. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.97023/4.96264. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.96486/4.96668. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.96849/4.97519. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.96778/4.97364. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.96046/4.95969. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.96427/4.96617. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.96470/4.96562. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.96650/4.97073. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.96293/4.95338. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.95700/4.95726. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.96575/4.97838. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.96653/4.95829. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.96228/4.95841. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.96079/4.95660. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.95514/4.97397. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.95642/4.97429. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.96057/4.96930. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.96156/4.95901. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.95593/4.95627. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.95606/4.96138. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.95144/4.96729. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.95659/4.96074. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.95747/4.96330. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.95707/4.96587. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.96034/4.96349. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.95860/4.95734. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.95070/4.96262. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.95699/4.95633. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.95576/4.96588. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.95624/4.96363. Took 0.22 sec\n",
      "Epoch 66, Loss(train/val) 4.95705/4.95760. Took 0.22 sec\n",
      "Epoch 67, Loss(train/val) 4.95716/4.96059. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.95201/4.95535. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.95310/4.96403. Took 0.22 sec\n",
      "Epoch 70, Loss(train/val) 4.95403/4.95833. Took 0.21 sec\n",
      "Epoch 71, Loss(train/val) 4.95313/4.94928. Took 0.21 sec\n",
      "Epoch 72, Loss(train/val) 4.94930/4.96915. Took 0.22 sec\n",
      "Epoch 73, Loss(train/val) 4.95144/4.96141. Took 0.21 sec\n",
      "Epoch 74, Loss(train/val) 4.95661/4.95322. Took 0.22 sec\n",
      "Epoch 75, Loss(train/val) 4.95666/4.96101. Took 0.22 sec\n",
      "Epoch 76, Loss(train/val) 4.94822/4.96266. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 4.95711/4.97154. Took 0.22 sec\n",
      "Epoch 78, Loss(train/val) 4.95661/4.95589. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 4.95168/4.96950. Took 0.21 sec\n",
      "Epoch 80, Loss(train/val) 4.94990/4.96024. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 4.95363/4.96615. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.95166/4.96680. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.94968/4.96831. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.95202/4.95714. Took 0.22 sec\n",
      "Epoch 85, Loss(train/val) 4.95450/4.95969. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.94402/4.96344. Took 0.22 sec\n",
      "Epoch 87, Loss(train/val) 4.95037/4.98492. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 4.95236/4.96357. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 4.94957/4.95655. Took 0.22 sec\n",
      "Epoch 90, Loss(train/val) 4.94815/4.96451. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 4.95298/4.97579. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.94715/4.97394. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 4.94610/4.97688. Took 0.22 sec\n",
      "Epoch 94, Loss(train/val) 4.95216/4.96921. Took 0.21 sec\n",
      "Epoch 95, Loss(train/val) 4.94766/4.96172. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.94890/4.98097. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.95426/4.95826. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.94641/4.97334. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.94708/4.97476. Took 0.21 sec\n",
      "ACC: 0.515625, MCC: 0.06240659293657412\n",
      "Epoch 0, Loss(train/val) 4.90425/4.88343. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 4.86742/4.85757. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 4.86448/4.86259. Took 0.22 sec\n",
      "Epoch 3, Loss(train/val) 4.86685/4.86461. Took 0.21 sec\n",
      "Epoch 4, Loss(train/val) 4.86333/4.86751. Took 0.23 sec\n",
      "Epoch 5, Loss(train/val) 4.86582/4.86921. Took 0.21 sec\n",
      "Epoch 6, Loss(train/val) 4.86390/4.86898. Took 0.22 sec\n",
      "Epoch 7, Loss(train/val) 4.86085/4.86508. Took 0.21 sec\n",
      "Epoch 8, Loss(train/val) 4.85864/4.87282. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 4.86067/4.86260. Took 0.22 sec\n",
      "Epoch 10, Loss(train/val) 4.86184/4.87082. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 4.85742/4.87040. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.85634/4.87122. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.85987/4.87082. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 4.85001/4.88575. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.85398/4.87674. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.85685/4.87318. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.85555/4.87652. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 4.85237/4.88211. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.85220/4.87809. Took 0.22 sec\n",
      "Epoch 20, Loss(train/val) 4.85182/4.87387. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.85311/4.88063. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 4.84813/4.88139. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 4.84957/4.88169. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.84346/4.87098. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.84650/4.86938. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.84663/4.86579. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.84523/4.86109. Took 0.21 sec\n",
      "Epoch 28, Loss(train/val) 4.84795/4.86136. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 4.84574/4.86530. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 4.84662/4.86232. Took 0.22 sec\n",
      "Epoch 31, Loss(train/val) 4.84758/4.86679. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.84381/4.86860. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.84220/4.86841. Took 0.22 sec\n",
      "Epoch 34, Loss(train/val) 4.84483/4.87016. Took 0.21 sec\n",
      "Epoch 35, Loss(train/val) 4.84082/4.86458. Took 0.22 sec\n",
      "Epoch 36, Loss(train/val) 4.84475/4.86161. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.84288/4.86256. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 4.83897/4.86759. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.84175/4.86250. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.84056/4.86791. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 4.83648/4.86126. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.85045/4.85837. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.84033/4.87009. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 4.84589/4.85887. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.83112/4.87493. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.83673/4.86820. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.83108/4.86566. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.83371/4.87259. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.83847/4.85524. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 4.83914/4.86134. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 4.84001/4.86438. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 4.83324/4.86570. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 4.83957/4.87016. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.83744/4.87011. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 4.82905/4.85928. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.83849/4.85861. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 4.83476/4.85930. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.83794/4.86402. Took 0.22 sec\n",
      "Epoch 59, Loss(train/val) 4.84095/4.86737. Took 0.21 sec\n",
      "Epoch 60, Loss(train/val) 4.82942/4.87349. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 4.84019/4.86259. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.83109/4.87126. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 4.83880/4.85760. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 4.83137/4.86101. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.83333/4.86935. Took 0.21 sec\n",
      "Epoch 66, Loss(train/val) 4.83521/4.87072. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.83484/4.87499. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 4.83350/4.86995. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.83371/4.86508. Took 0.22 sec\n",
      "Epoch 70, Loss(train/val) 4.83032/4.87016. Took 0.21 sec\n",
      "Epoch 71, Loss(train/val) 4.82695/4.87076. Took 0.21 sec\n",
      "Epoch 72, Loss(train/val) 4.82936/4.86341. Took 0.21 sec\n",
      "Epoch 73, Loss(train/val) 4.82661/4.87021. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.82933/4.86282. Took 0.21 sec\n",
      "Epoch 75, Loss(train/val) 4.83780/4.87120. Took 0.21 sec\n",
      "Epoch 76, Loss(train/val) 4.83445/4.86880. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 4.83836/4.86390. Took 0.21 sec\n",
      "Epoch 78, Loss(train/val) 4.83113/4.86462. Took 0.22 sec\n",
      "Epoch 79, Loss(train/val) 4.83014/4.85631. Took 0.21 sec\n",
      "Epoch 80, Loss(train/val) 4.82611/4.87386. Took 0.22 sec\n",
      "Epoch 81, Loss(train/val) 4.82877/4.86567. Took 0.21 sec\n",
      "Epoch 82, Loss(train/val) 4.82082/4.86093. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 4.83571/4.86391. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 4.83217/4.85512. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.82971/4.85695. Took 0.21 sec\n",
      "Epoch 86, Loss(train/val) 4.83081/4.86556. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 4.82682/4.86611. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 4.82700/4.86244. Took 0.21 sec\n",
      "Epoch 89, Loss(train/val) 4.83192/4.87394. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 4.82362/4.87952. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 4.82833/4.86839. Took 0.22 sec\n",
      "Epoch 92, Loss(train/val) 4.82809/4.88153. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 4.83748/4.86207. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.82811/4.86592. Took 0.21 sec\n",
      "Epoch 95, Loss(train/val) 4.82729/4.88331. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.83428/4.86757. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.83072/4.87522. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.81903/4.85516. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.82772/4.87507. Took 0.20 sec\n",
      "ACC: 0.546875, MCC: 0.11030375355930092\n",
      "Epoch 0, Loss(train/val) 4.96640/4.91983. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.93313/4.91716. Took 0.21 sec\n",
      "Epoch 2, Loss(train/val) 4.92811/4.92041. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.92761/4.92267. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.92432/4.92425. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.92500/4.92462. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.92399/4.92558. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.92117/4.92721. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.92348/4.92733. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.92173/4.93062. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.91726/4.93681. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.92092/4.94192. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.91942/4.94903. Took 0.21 sec\n",
      "Epoch 13, Loss(train/val) 4.91619/4.95055. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.91494/4.94642. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 4.91449/4.94444. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.91660/4.94965. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.91402/4.95812. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.90962/4.96292. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.90901/4.95439. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.91089/4.96402. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.90973/4.96793. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.90863/4.96198. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.90762/4.96820. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.90548/4.97376. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 4.90257/4.97517. Took 0.21 sec\n",
      "Epoch 26, Loss(train/val) 4.90634/4.96374. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.89986/4.98197. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.90623/4.96998. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 4.91196/4.96727. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.90874/4.96441. Took 0.21 sec\n",
      "Epoch 31, Loss(train/val) 4.90595/4.98831. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 4.90754/4.98734. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.89921/4.98895. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.90285/4.97554. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90150/4.98884. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.90089/4.98156. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.90034/4.98401. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.90278/4.98588. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 4.89860/5.00041. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.89549/5.00350. Took 0.22 sec\n",
      "Epoch 41, Loss(train/val) 4.89021/5.03132. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.89154/4.99820. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.88727/4.99269. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.88716/5.02374. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.89229/5.03507. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.88607/5.02605. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.89154/5.03806. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.88782/5.02715. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.88886/5.01113. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.88414/4.98814. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.89182/4.97847. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.88396/5.01840. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.88448/4.99037. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.88570/5.06689. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.88349/5.00745. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.87918/5.01790. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.89355/5.01042. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.87956/5.00814. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.88704/4.99426. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.87985/5.00315. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.87517/5.05198. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.88141/5.05807. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.88414/5.03713. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.87526/5.01827. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.87248/5.05843. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.88016/5.03511. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.88172/4.98332. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.87571/5.03306. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.87014/5.09079. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.88577/5.00635. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.87151/5.05673. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.87585/5.05849. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.86539/5.02822. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.86940/5.05370. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.88330/5.03587. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.87165/5.06212. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.86663/5.04003. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.86213/5.04370. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.87363/5.04795. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.86899/5.04210. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.88301/5.02064. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.87181/5.04585. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.86945/5.07060. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.87125/5.01539. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.87182/5.00834. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.86232/5.03613. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.87911/5.00728. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.86146/5.05824. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.86839/5.00860. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.86488/5.06152. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.86078/5.04120. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.87636/5.01963. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.87372/5.04087. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.86769/5.03720. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.85553/5.07701. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.86798/5.07925. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.86395/5.02897. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.89352/4.97850. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.90650/5.00259. Took 0.20 sec\n",
      "ACC: 0.5, MCC: 0.008866995073891626\n",
      "Epoch 0, Loss(train/val) 5.12902/5.06555. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 5.07906/5.04814. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.06522/5.05063. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.05266/5.05059. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 5.05436/5.05179. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 5.05736/5.05061. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 5.05546/5.04944. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 5.05366/5.04841. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.05338/5.04921. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.05159/5.05009. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 5.05193/5.04736. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 5.04894/5.04356. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 5.04881/5.04317. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.05078/5.04233. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 5.04726/5.04129. Took 0.21 sec\n",
      "Epoch 15, Loss(train/val) 5.04537/5.04029. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 5.04549/5.03849. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 5.04499/5.04345. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 5.04562/5.04074. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.04244/5.04085. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 5.04344/5.03819. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.04490/5.04010. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 5.04236/5.03806. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 5.04258/5.03984. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.04159/5.04699. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 5.04114/5.04327. Took 0.22 sec\n",
      "Epoch 26, Loss(train/val) 5.03731/5.04996. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 5.04350/5.04789. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 5.04218/5.05504. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 5.03767/5.04901. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.04117/5.04588. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 5.04094/5.05541. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 5.03433/5.05024. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 5.03428/5.04744. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.03479/5.05361. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 5.03632/5.04507. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 5.03499/5.04661. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.03574/5.04944. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.03688/5.05120. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.03361/5.04861. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.03464/5.04908. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.03517/5.04909. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.03016/5.04922. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 5.03429/5.04652. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 5.03363/5.04670. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.02815/5.04808. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.02828/5.04232. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 5.03027/5.04180. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.02624/5.05569. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 5.03305/5.04295. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 5.03023/5.05151. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 5.02890/5.05338. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 5.03180/5.04758. Took 0.22 sec\n",
      "Epoch 53, Loss(train/val) 5.03005/5.04399. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 5.02494/5.04733. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 5.02807/5.05435. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 5.03185/5.04441. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.02856/5.03978. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 5.03103/5.04078. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 5.02792/5.05252. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 5.02575/5.05098. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 5.02954/5.04523. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 5.02326/5.04754. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 5.02432/5.04243. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 5.02508/5.04574. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 5.02408/5.05232. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 5.02276/5.05387. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 5.02278/5.05561. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.02243/5.05437. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 5.02201/5.05453. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.02326/5.04892. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 5.02478/5.05281. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 5.02351/5.05367. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.01931/5.04934. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.01997/5.03860. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 5.03742/5.03659. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 5.03006/5.03644. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 5.03711/5.03844. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.03042/5.04922. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 5.03018/5.04898. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 5.02327/5.04254. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 5.02332/5.04815. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 5.02701/5.04069. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.02621/5.06069. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.02689/5.05153. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.02265/5.05452. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 5.01587/5.04683. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 5.01998/5.04699. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 5.01839/5.06022. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 5.02269/5.03873. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.02053/5.04478. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.01969/5.04626. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 5.01926/5.05876. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.01641/5.05971. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 5.01635/5.05485. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.01907/5.05131. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.01237/5.06369. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 5.02138/5.06228. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 5.01715/5.05436. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 5.01785/5.05385. Took 0.20 sec\n",
      "ACC: 0.453125, MCC: -0.10024600283175306\n",
      "Epoch 0, Loss(train/val) 4.86412/4.79825. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.78810/4.77849. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.78964/4.77589. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.79498/4.78017. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.79271/4.78340. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.78376/4.77914. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.78258/4.77662. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.78210/4.77521. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.78178/4.77492. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.78168/4.77258. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.77960/4.77425. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.77765/4.77718. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.77599/4.77592. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.77578/4.77889. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 4.77698/4.77788. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.77596/4.76739. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.77638/4.77334. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.77224/4.77341. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.77580/4.77246. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.77635/4.78506. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.77392/4.77933. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.77212/4.78667. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.77283/4.78744. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.77330/4.78594. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.76980/4.78500. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.77249/4.79436. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.77178/4.79296. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.76762/4.79501. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.76933/4.78966. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.77190/4.79694. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.76681/4.79239. Took 0.22 sec\n",
      "Epoch 31, Loss(train/val) 4.77316/4.78421. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.76976/4.77510. Took 0.22 sec\n",
      "Epoch 33, Loss(train/val) 4.76841/4.78895. Took 0.23 sec\n",
      "Epoch 34, Loss(train/val) 4.77041/4.78479. Took 0.22 sec\n",
      "Epoch 35, Loss(train/val) 4.76581/4.78327. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 4.76708/4.80206. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 4.76685/4.79223. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.76506/4.80853. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.76410/4.82428. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 4.76526/4.82043. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.76443/4.81912. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.76292/4.80752. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 4.76124/4.80477. Took 0.21 sec\n",
      "Epoch 44, Loss(train/val) 4.76097/4.80887. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 4.75733/4.82525. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 4.75742/4.80058. Took 0.22 sec\n",
      "Epoch 47, Loss(train/val) 4.75235/4.83451. Took 0.21 sec\n",
      "Epoch 48, Loss(train/val) 4.75987/4.81632. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 4.76344/4.80969. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.75706/4.79461. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 4.75179/4.84093. Took 0.22 sec\n",
      "Epoch 52, Loss(train/val) 4.76218/4.82318. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.75461/4.80342. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.75583/4.81027. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 4.75386/4.81469. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.75279/4.83117. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.75932/4.83694. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 4.75420/4.81601. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.75151/4.82701. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.75668/4.83510. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 4.75329/4.83944. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 4.75320/4.81640. Took 0.21 sec\n",
      "Epoch 63, Loss(train/val) 4.75107/4.82097. Took 0.21 sec\n",
      "Epoch 64, Loss(train/val) 4.75314/4.83938. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 4.75293/4.82268. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.75332/4.84553. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.75336/4.84713. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.75174/4.84765. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.75264/4.80525. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.75135/4.83670. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.75887/4.80896. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.74894/4.83424. Took 0.22 sec\n",
      "Epoch 73, Loss(train/val) 4.74899/4.86006. Took 0.21 sec\n",
      "Epoch 74, Loss(train/val) 4.74657/4.84528. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.74908/4.84815. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.74600/4.85577. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.75289/4.81335. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.75752/4.81489. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.75110/4.82477. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.74834/4.83586. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.74533/4.85227. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.74905/4.85278. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.74656/4.85002. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.75144/4.82521. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.74029/4.85856. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.75226/4.85427. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.74474/4.84373. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.74846/4.86070. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.74724/4.85276. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 4.74714/4.85844. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.75191/4.86094. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.75459/4.82048. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.75227/4.84361. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.74560/4.85368. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.74989/4.84085. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.74564/4.83481. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.74128/4.85552. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.75109/4.84401. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.75280/4.84497. Took 0.20 sec\n",
      "ACC: 0.4375, MCC: -0.07116958850708793\n",
      "Epoch 0, Loss(train/val) 4.96647/4.90693. Took 0.23 sec\n",
      "Epoch 1, Loss(train/val) 4.91494/4.92366. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.91223/4.93370. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.91873/4.93225. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.91418/4.93888. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.91489/4.94295. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.91796/4.93765. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.91767/4.92066. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.91597/4.90903. Took 0.21 sec\n",
      "Epoch 9, Loss(train/val) 4.90991/4.90853. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.90962/4.91283. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.91010/4.91453. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.91025/4.91337. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.91018/4.91316. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.90791/4.91525. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.90983/4.91565. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90618/4.91607. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.90933/4.91490. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.90774/4.91493. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.90431/4.91387. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.90424/4.91524. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.90217/4.91895. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.90682/4.91956. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.90277/4.91798. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.90137/4.91959. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.90067/4.92391. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.90311/4.92205. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.90127/4.92113. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.90254/4.91674. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.89818/4.91780. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.89962/4.92144. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.89972/4.91742. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.89729/4.92255. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.89910/4.91611. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.89644/4.91601. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.89962/4.90967. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.89580/4.91666. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.89630/4.91267. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.90008/4.91672. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.89416/4.92119. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.90041/4.91415. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 4.89756/4.92503. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.90052/4.93319. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 4.89487/4.94106. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.89117/4.94606. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.89910/4.91990. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.89304/4.93680. Took 0.21 sec\n",
      "Epoch 47, Loss(train/val) 4.89515/4.93449. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.89303/4.93305. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.89246/4.94512. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.88751/4.95873. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.89720/4.93203. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.89486/4.92647. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.88965/4.95603. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.89039/4.95462. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.89198/4.93121. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.88991/4.94256. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.89706/4.93198. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.89755/4.92280. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89432/4.94933. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.89617/4.93365. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.89227/4.95977. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.89060/4.95541. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.88516/4.97247. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.89000/4.93179. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.89055/4.92737. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.88939/4.94262. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.89179/4.94641. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.88592/4.96529. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.89035/4.95393. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.89226/4.94214. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.88633/4.95716. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.89041/4.93229. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.88575/4.97640. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.89015/4.95002. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.88415/4.96124. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.88482/4.96126. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.88191/4.96755. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.89017/4.96280. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.88661/4.96084. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88520/4.95677. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.87772/4.96618. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.88401/4.96143. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.88062/4.97306. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.88133/4.99091. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.88461/4.96360. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.88072/4.95226. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.87964/4.99267. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.87813/5.00215. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.91176/4.90329. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.89786/4.91419. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.89138/4.92383. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.89018/4.92685. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.88846/4.92820. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.88306/4.95014. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.88496/4.94019. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.88589/4.95892. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.87900/4.96198. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.88912/4.92260. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.87576/4.94725. Took 0.21 sec\n",
      "ACC: 0.453125, MCC: -0.1250803567515525\n",
      "Epoch 0, Loss(train/val) 4.77863/4.78389. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.75738/4.76499. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.74505/4.76301. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.74479/4.76278. Took 0.22 sec\n",
      "Epoch 4, Loss(train/val) 4.74500/4.76192. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.74131/4.75980. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.74008/4.76634. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.74085/4.76926. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.73706/4.77236. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.73653/4.78508. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.74355/4.77972. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.73885/4.77619. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.73513/4.78880. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.73475/4.78304. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.73469/4.79333. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.73132/4.79549. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.72960/4.80206. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.73201/4.79755. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.73346/4.80077. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 4.73188/4.79760. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.73089/4.80602. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.72453/4.80651. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.72963/4.80756. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.72985/4.81150. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.72093/4.81721. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.72597/4.81640. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.72358/4.81855. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.72344/4.82824. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.72643/4.82618. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.72450/4.82174. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.71915/4.82304. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.72335/4.84154. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.72214/4.81729. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.71385/4.86306. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.72457/4.82167. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.72214/4.82431. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 4.71867/4.82280. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.71699/4.84286. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.71959/4.83698. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.72156/4.82891. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.71468/4.85946. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.71684/4.84510. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 4.71358/4.84685. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.71526/4.84037. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.71652/4.86323. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.71414/4.85625. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.71907/4.83300. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.70937/4.86715. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.71586/4.87172. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 4.71044/4.85254. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 4.71126/4.86259. Took 0.21 sec\n",
      "Epoch 51, Loss(train/val) 4.71589/4.86225. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.71536/4.83443. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.70962/4.86795. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.70699/4.88079. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.70597/4.87688. Took 0.21 sec\n",
      "Epoch 56, Loss(train/val) 4.71924/4.85001. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.71361/4.86858. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.71784/4.85176. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.71153/4.85261. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.71173/4.85968. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.71377/4.87166. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.70871/4.86912. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.71340/4.85852. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.71277/4.89102. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.71266/4.85695. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.71059/4.86064. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.71317/4.85915. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.71337/4.87935. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.70940/4.86557. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.70832/4.88625. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.70438/4.88524. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.71419/4.85772. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.71441/4.88462. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.70834/4.87605. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.70363/4.88176. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.72197/4.82298. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.73287/4.82262. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.72732/4.82904. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.72122/4.82625. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.71595/4.85781. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.73105/4.79923. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.72145/4.82730. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 4.71767/4.84311. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.71771/4.85338. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.71195/4.84200. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.72119/4.84741. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.71429/4.86013. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.72118/4.83253. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.71538/4.85912. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 4.71736/4.86021. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 4.71171/4.86309. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.71172/4.85696. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.71531/4.85741. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.71162/4.84262. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.71451/4.89159. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.70656/4.84364. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.70671/4.85912. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.71167/4.85766. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.71546/4.83826. Took 0.20 sec\n",
      "ACC: 0.5, MCC: -0.002944085489318884\n",
      "Epoch 0, Loss(train/val) 4.91792/4.86544. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.90052/4.89816. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.90050/4.84896. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.90130/4.85100. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.88935/4.87792. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.87500/4.86597. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.88241/4.85893. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.88297/4.86373. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.87762/4.86822. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.87609/4.85356. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.88380/4.86182. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.87759/4.87451. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.87349/4.86979. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.87766/4.87441. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.88182/4.88136. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.87499/4.88534. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.87666/4.88245. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.87446/4.88200. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.87277/4.88250. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.87174/4.88302. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.87452/4.88205. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.87019/4.88695. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.86990/4.87958. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.87542/4.88119. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 4.86787/4.89212. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 4.86927/4.87942. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.86776/4.88359. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.86962/4.86911. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.87238/4.87149. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 4.87056/4.88419. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.86714/4.88206. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.86713/4.88155. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.86575/4.87856. Took 0.22 sec\n",
      "Epoch 33, Loss(train/val) 4.86620/4.88297. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.86723/4.88894. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.86888/4.89357. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.86473/4.88339. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.86546/4.88471. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.86700/4.89261. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.86438/4.89478. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.86482/4.91232. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.87780/4.88795. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.87605/4.88371. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.87466/4.88121. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.87154/4.88645. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.87265/4.88594. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.87271/4.89158. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.87275/4.89197. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.87189/4.89730. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.87211/4.90353. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.87325/4.88564. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.87373/4.89876. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.86978/4.90064. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.87377/4.89208. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.87460/4.90012. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.86923/4.89550. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.87284/4.90441. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 4.86968/4.90499. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.86798/4.90909. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.87396/4.90155. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.87106/4.90211. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.86920/4.90579. Took 0.21 sec\n",
      "Epoch 62, Loss(train/val) 4.86484/4.90655. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.86664/4.91071. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.86812/4.90776. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.86934/4.91089. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.86565/4.90797. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.86991/4.90001. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.86398/4.92252. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.86561/4.92911. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.86627/4.91722. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.87065/4.92334. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.86580/4.91214. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.86264/4.92087. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.86305/4.93430. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.86688/4.91111. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.87296/4.88746. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.87523/4.89023. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.87138/4.88963. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.86944/4.89458. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.86791/4.90181. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.86832/4.90886. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.86474/4.92228. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.86767/4.93330. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.86376/4.93476. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.86425/4.94232. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.86371/4.93747. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.86343/4.95041. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.86460/4.93208. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.85981/4.97096. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.86105/4.94649. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.86088/4.94572. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.86385/4.96197. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.85558/4.95642. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.86010/4.94564. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.85848/4.95459. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.85453/4.97783. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.86463/4.94057. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.85778/4.97412. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.85216/4.98188. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.03126526997403612\n",
      "Epoch 0, Loss(train/val) 5.11937/5.06497. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 5.06421/5.07359. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.06360/5.07791. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.06660/5.07405. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 5.06571/5.07043. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 5.06560/5.06844. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 5.06414/5.06999. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 5.06184/5.06914. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 5.06531/5.07051. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 5.06027/5.06950. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 5.05950/5.06894. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 5.06057/5.07311. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 5.05843/5.07550. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.05969/5.07244. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 5.05541/5.07649. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 5.05880/5.08516. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 5.05601/5.09140. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 5.05479/5.09429. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 5.05205/5.09722. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.05225/5.09112. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 5.05136/5.09535. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 5.04960/5.09689. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 5.05016/5.10063. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 5.05147/5.08535. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 5.05227/5.09612. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 5.04969/5.09878. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 5.05244/5.10017. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 5.05233/5.09323. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 5.04864/5.09867. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 5.04873/5.09562. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 5.04391/5.10181. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 5.04821/5.09750. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 5.04760/5.11600. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 5.05126/5.09254. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 5.05564/5.07926. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.05540/5.08519. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 5.05208/5.09133. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 5.05081/5.09012. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 5.05043/5.08905. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 5.04991/5.09427. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 5.05084/5.09632. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 5.04839/5.10394. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 5.04896/5.10461. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 5.04714/5.09957. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 5.04705/5.09772. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 5.04959/5.09283. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.04650/5.09317. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 5.04851/5.09929. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 5.04752/5.10040. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 5.04858/5.10687. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 5.04843/5.09812. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 5.04688/5.09796. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 5.04830/5.08838. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 5.04808/5.09759. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 5.04384/5.11467. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 5.04447/5.08439. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 5.05184/5.09712. Took 0.21 sec\n",
      "Epoch 57, Loss(train/val) 5.04797/5.08890. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 5.04518/5.08254. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 5.04656/5.08916. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 5.04646/5.09883. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 5.04704/5.10176. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.04427/5.11832. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 5.04898/5.09098. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 5.04544/5.10669. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 5.04550/5.09815. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 5.04531/5.10774. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 5.04960/5.08490. Took 0.21 sec\n",
      "Epoch 68, Loss(train/val) 5.04219/5.09168. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.04640/5.10518. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 5.04450/5.09963. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 5.04013/5.10022. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.04488/5.09951. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 5.04137/5.09417. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 5.04227/5.08345. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 5.04108/5.09205. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 5.04471/5.08746. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 5.04203/5.08597. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 5.04291/5.10012. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 5.04194/5.08454. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 5.03899/5.07996. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 5.04120/5.09145. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 5.03954/5.08479. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 5.04233/5.07784. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 5.04491/5.10286. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.04210/5.08568. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 5.03928/5.11885. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 5.04234/5.10703. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 5.03554/5.08143. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 5.03699/5.08834. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 5.03462/5.10066. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 5.03512/5.09355. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.03873/5.11041. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 5.04036/5.10328. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 5.03579/5.10978. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.03601/5.10952. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 5.03628/5.10644. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 5.03675/5.10083. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 5.03181/5.11251. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 5.03666/5.08807. Took 0.20 sec\n",
      "ACC: 0.578125, MCC: 0.14180020247260586\n",
      "Epoch 0, Loss(train/val) 4.86617/4.84535. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.84777/4.85203. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.84892/4.86090. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.84298/4.87174. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.84951/4.85885. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.84935/4.84019. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.84163/4.83671. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.83839/4.83712. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.84002/4.83695. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.84062/4.84115. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.83972/4.84600. Took 0.21 sec\n",
      "Epoch 11, Loss(train/val) 4.83611/4.85813. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.83924/4.84803. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.84023/4.84661. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.83748/4.84899. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.83350/4.85218. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.83223/4.85312. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.83614/4.84417. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83082/4.85285. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.83359/4.85861. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.83047/4.85675. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.83138/4.84641. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.83199/4.84638. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.83325/4.84130. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82984/4.84212. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.83421/4.84348. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82958/4.84110. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.83782/4.86219. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.83286/4.84185. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.83196/4.84426. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.82584/4.84622. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.82601/4.84183. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.82757/4.85095. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.82800/4.84922. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.82451/4.84552. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82770/4.84475. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.82645/4.84033. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.82912/4.83827. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.82882/4.84290. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.82742/4.84837. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.82772/4.84848. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.82980/4.84843. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.82619/4.84027. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.82786/4.84412. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.82927/4.84566. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.82605/4.84588. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.82946/4.84296. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.82318/4.84660. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.82546/4.84316. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.82385/4.84154. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.82402/4.84236. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.82430/4.84830. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.82390/4.84074. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.82325/4.84487. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.82185/4.84236. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.82463/4.84570. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.82288/4.85185. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.82731/4.84362. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.83125/4.84698. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.83081/4.84513. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.82745/4.83815. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.82871/4.84112. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.82256/4.84361. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.82240/4.85079. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.82601/4.84615. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.82410/4.84749. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.82302/4.84693. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.82298/4.84609. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.82464/4.84733. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.82242/4.84738. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.82306/4.84489. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.82092/4.84228. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.82190/4.84121. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.81872/4.84093. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.82507/4.85903. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.81950/4.84499. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.82023/4.85102. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.81834/4.85558. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.81947/4.84828. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.81885/4.84192. Took 0.21 sec\n",
      "Epoch 80, Loss(train/val) 4.82382/4.84020. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.82083/4.85280. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.81900/4.84780. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.81502/4.84310. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.81817/4.84783. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.81823/4.84265. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.81682/4.84401. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.83143/4.83689. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83744/4.82256. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82916/4.82840. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82644/4.83577. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.82360/4.83124. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.82078/4.83598. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.82351/4.83159. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.81755/4.83213. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.82095/4.82956. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.81979/4.82984. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.82368/4.83317. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.81391/4.83974. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.81735/4.82931. Took 0.20 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.86832/4.81345. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.82840/4.80245. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.81954/4.80586. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.81498/4.80612. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.81847/4.80082. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.81530/4.79967. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.81741/4.79729. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.81736/4.79305. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.81687/4.81153. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.81788/4.81170. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.81193/4.80576. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.81753/4.80750. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.80884/4.80263. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.81103/4.80398. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.81127/4.80208. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.80930/4.80258. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 4.81094/4.80424. Took 0.23 sec\n",
      "Epoch 17, Loss(train/val) 4.81111/4.80706. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 4.81057/4.80320. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 4.80807/4.79364. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 4.81014/4.78958. Took 0.22 sec\n",
      "Epoch 21, Loss(train/val) 4.80851/4.78228. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 4.80877/4.78095. Took 0.21 sec\n",
      "Epoch 23, Loss(train/val) 4.80617/4.78301. Took 0.21 sec\n",
      "Epoch 24, Loss(train/val) 4.80812/4.78475. Took 0.21 sec\n",
      "Epoch 25, Loss(train/val) 4.80829/4.79480. Took 0.21 sec\n",
      "Epoch 26, Loss(train/val) 4.80832/4.79328. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.80736/4.78763. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.80227/4.78865. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 4.79984/4.79099. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 4.80740/4.78989. Took 0.21 sec\n",
      "Epoch 31, Loss(train/val) 4.80839/4.78980. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 4.80538/4.78828. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.80507/4.78375. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 4.80230/4.78750. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.80313/4.78722. Took 0.21 sec\n",
      "Epoch 36, Loss(train/val) 4.80142/4.78965. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.80710/4.79274. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.80189/4.78534. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.80233/4.78775. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.79942/4.78694. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.79927/4.78968. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 4.80471/4.78773. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.80449/4.78342. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.79574/4.78680. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.79815/4.79171. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.79764/4.78850. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.80662/4.79205. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.80171/4.78848. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 4.79989/4.78991. Took 0.21 sec\n",
      "Epoch 50, Loss(train/val) 4.79950/4.78276. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.79904/4.78918. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.79817/4.79239. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.79999/4.79038. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.79911/4.79038. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.80142/4.78741. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.79902/4.79663. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.79871/4.78322. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.79322/4.79376. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.79628/4.79022. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.79637/4.78786. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 4.79488/4.79702. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.79952/4.79014. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.79267/4.80477. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.80154/4.79055. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.79535/4.81166. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.79180/4.80490. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.78800/4.81449. Took 0.25 sec\n",
      "Epoch 68, Loss(train/val) 4.79900/4.81137. Took 0.24 sec\n",
      "Epoch 69, Loss(train/val) 4.79230/4.79775. Took 0.24 sec\n",
      "Epoch 70, Loss(train/val) 4.79373/4.79625. Took 0.24 sec\n",
      "Epoch 71, Loss(train/val) 4.79580/4.81564. Took 0.25 sec\n",
      "Epoch 72, Loss(train/val) 4.78517/4.82407. Took 0.25 sec\n",
      "Epoch 73, Loss(train/val) 4.79132/4.81370. Took 0.23 sec\n",
      "Epoch 74, Loss(train/val) 4.79029/4.80019. Took 0.22 sec\n",
      "Epoch 75, Loss(train/val) 4.79596/4.82169. Took 0.22 sec\n",
      "Epoch 76, Loss(train/val) 4.79393/4.81988. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 4.79764/4.79529. Took 0.22 sec\n",
      "Epoch 78, Loss(train/val) 4.79294/4.81067. Took 0.21 sec\n",
      "Epoch 79, Loss(train/val) 4.79229/4.79957. Took 0.21 sec\n",
      "Epoch 80, Loss(train/val) 4.78992/4.80544. Took 0.21 sec\n",
      "Epoch 81, Loss(train/val) 4.79428/4.80611. Took 0.21 sec\n",
      "Epoch 82, Loss(train/val) 4.79404/4.81024. Took 0.21 sec\n",
      "Epoch 83, Loss(train/val) 4.78697/4.80769. Took 0.21 sec\n",
      "Epoch 84, Loss(train/val) 4.78990/4.80902. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 4.79094/4.81632. Took 0.21 sec\n",
      "Epoch 86, Loss(train/val) 4.79641/4.81394. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 4.78843/4.82650. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 4.79330/4.83160. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.79388/4.79919. Took 0.22 sec\n",
      "Epoch 90, Loss(train/val) 4.81363/4.81255. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 4.81094/4.80991. Took 0.23 sec\n",
      "Epoch 92, Loss(train/val) 4.80183/4.81056. Took 0.24 sec\n",
      "Epoch 93, Loss(train/val) 4.80394/4.81257. Took 0.24 sec\n",
      "Epoch 94, Loss(train/val) 4.79991/4.81507. Took 0.24 sec\n",
      "Epoch 95, Loss(train/val) 4.80086/4.81028. Took 0.24 sec\n",
      "Epoch 96, Loss(train/val) 4.79801/4.80787. Took 0.24 sec\n",
      "Epoch 97, Loss(train/val) 4.79693/4.81848. Took 0.24 sec\n",
      "Epoch 98, Loss(train/val) 4.79515/4.81265. Took 0.24 sec\n",
      "Epoch 99, Loss(train/val) 4.79267/4.81669. Took 0.24 sec\n",
      "ACC: 0.546875, MCC: 0.0778902520624173\n",
      "Epoch 0, Loss(train/val) 4.84562/4.86077. Took 0.24 sec\n",
      "Epoch 1, Loss(train/val) 4.84400/4.87044. Took 0.24 sec\n",
      "Epoch 2, Loss(train/val) 4.83383/4.88182. Took 0.24 sec\n",
      "Epoch 3, Loss(train/val) 4.83629/4.87731. Took 0.24 sec\n",
      "Epoch 4, Loss(train/val) 4.83639/4.86374. Took 0.24 sec\n",
      "Epoch 5, Loss(train/val) 4.83667/4.85353. Took 0.23 sec\n",
      "Epoch 6, Loss(train/val) 4.83268/4.85180. Took 0.24 sec\n",
      "Epoch 7, Loss(train/val) 4.82723/4.85272. Took 0.24 sec\n",
      "Epoch 8, Loss(train/val) 4.82460/4.84744. Took 0.24 sec\n",
      "Epoch 9, Loss(train/val) 4.82634/4.84585. Took 0.24 sec\n",
      "Epoch 10, Loss(train/val) 4.82361/4.84576. Took 0.25 sec\n",
      "Epoch 11, Loss(train/val) 4.82551/4.84172. Took 0.25 sec\n",
      "Epoch 12, Loss(train/val) 4.82419/4.84180. Took 0.24 sec\n",
      "Epoch 13, Loss(train/val) 4.82724/4.83950. Took 0.24 sec\n",
      "Epoch 14, Loss(train/val) 4.81953/4.83889. Took 0.24 sec\n",
      "Epoch 15, Loss(train/val) 4.82350/4.83529. Took 0.25 sec\n",
      "Epoch 16, Loss(train/val) 4.82310/4.83713. Took 0.23 sec\n",
      "Epoch 17, Loss(train/val) 4.81879/4.84102. Took 0.22 sec\n",
      "Epoch 18, Loss(train/val) 4.81882/4.84358. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 4.81856/4.82404. Took 0.22 sec\n",
      "Epoch 20, Loss(train/val) 4.81774/4.83079. Took 0.21 sec\n",
      "Epoch 21, Loss(train/val) 4.82090/4.83849. Took 0.21 sec\n",
      "Epoch 22, Loss(train/val) 4.81726/4.84867. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.81545/4.83956. Took 0.22 sec\n",
      "Epoch 24, Loss(train/val) 4.81508/4.84856. Took 0.23 sec\n",
      "Epoch 25, Loss(train/val) 4.81522/4.84722. Took 0.21 sec\n",
      "Epoch 26, Loss(train/val) 4.81728/4.83361. Took 0.23 sec\n",
      "Epoch 27, Loss(train/val) 4.81475/4.84608. Took 0.22 sec\n",
      "Epoch 28, Loss(train/val) 4.81297/4.83999. Took 0.21 sec\n",
      "Epoch 29, Loss(train/val) 4.81124/4.84619. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 4.80988/4.85370. Took 0.22 sec\n",
      "Epoch 31, Loss(train/val) 4.81776/4.84581. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 4.81355/4.84528. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.81317/4.83847. Took 0.21 sec\n",
      "Epoch 34, Loss(train/val) 4.81035/4.85134. Took 0.22 sec\n",
      "Epoch 35, Loss(train/val) 4.80895/4.85280. Took 0.22 sec\n",
      "Epoch 36, Loss(train/val) 4.80928/4.83440. Took 0.21 sec\n",
      "Epoch 37, Loss(train/val) 4.81861/4.83920. Took 0.21 sec\n",
      "Epoch 38, Loss(train/val) 4.80130/4.86352. Took 0.21 sec\n",
      "Epoch 39, Loss(train/val) 4.81341/4.83025. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.80694/4.84321. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.80955/4.84449. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.80202/4.83411. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.81148/4.84755. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.81282/4.85065. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.81094/4.85253. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.80717/4.85027. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.80880/4.84371. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.81077/4.83618. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.80297/4.84761. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.81197/4.86466. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.81658/4.86735. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.80692/4.87325. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.81034/4.85884. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.80686/4.83751. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 4.80956/4.84577. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.80597/4.83890. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.80582/4.83762. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.80513/4.85531. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.80043/4.83522. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.80309/4.83127. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.80516/4.84143. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.80244/4.85554. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.79935/4.87313. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.80432/4.84510. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.80545/4.84661. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.80696/4.84446. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.80008/4.83546. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.80457/4.85348. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.80524/4.86433. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.80639/4.84982. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.80687/4.83850. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.80305/4.85442. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.80409/4.85475. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.79877/4.85858. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.80539/4.83887. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.79962/4.84063. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80122/4.84639. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.80490/4.84965. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.80017/4.85160. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.80418/4.83529. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.79813/4.84186. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.80005/4.85201. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.80166/4.85109. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.79917/4.84519. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.80121/4.84865. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.79918/4.84264. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.79538/4.85389. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.79528/4.87085. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.80347/4.84753. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.79865/4.83299. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.80662/4.85362. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.79386/4.84824. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.79026/4.86535. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.79326/4.85196. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.79132/4.87344. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.79554/4.85289. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.79737/4.85081. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.79893/4.83522. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.79554/4.86268. Took 0.20 sec\n",
      "ACC: 0.5, MCC: -0.03546361409014303\n",
      "Epoch 0, Loss(train/val) 5.00100/4.93893. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.91684/4.90904. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.91035/4.90378. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.90900/4.90850. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.90938/4.91045. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.90865/4.91417. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.90755/4.91443. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.90775/4.91484. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.90882/4.91402. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.90528/4.91633. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.90777/4.91228. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.90397/4.90379. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.90822/4.90089. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.90342/4.89977. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.90206/4.89680. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.90241/4.89667. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.90322/4.89465. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.89784/4.89601. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.90001/4.88946. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.89693/4.88922. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.89804/4.88630. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.89626/4.88908. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.89764/4.88428. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.89420/4.88304. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.89573/4.87395. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.89179/4.88040. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.89298/4.88104. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.89304/4.89544. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.89625/4.87717. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.89114/4.87124. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.89473/4.87491. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.88583/4.87599. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.88802/4.86953. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.88519/4.88994. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.88814/4.88438. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.88606/4.88188. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.88836/4.88510. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.88297/4.87255. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.87936/4.87472. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.88621/4.87970. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.88405/4.87794. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.88503/4.89142. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.88232/4.88692. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.88189/4.87042. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.88137/4.88113. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.88636/4.87196. Took 0.21 sec\n",
      "Epoch 46, Loss(train/val) 4.88245/4.87271. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.88125/4.89075. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.88291/4.86296. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 4.88926/4.87219. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.90148/4.88501. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.89126/4.88823. Took 0.21 sec\n",
      "Epoch 52, Loss(train/val) 4.89125/4.88940. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.88700/4.89407. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.88408/4.90454. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.88490/4.88735. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.88691/4.87449. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.88012/4.87625. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.89033/4.92049. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.88437/4.89082. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.88198/4.89419. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.88384/4.86746. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.88245/4.85262. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.88082/4.87354. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.87540/4.86861. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.88038/4.86795. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.87118/4.86718. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.87790/4.87666. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.87376/4.88626. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.87596/4.87974. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.87212/4.88145. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.87314/4.88596. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.87271/4.88847. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.86945/4.88344. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.87092/4.88164. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.87068/4.87058. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.87453/4.87366. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.87630/4.88373. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.87984/4.87654. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.87727/4.86302. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88599/4.85183. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.88147/4.84245. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.87955/4.84848. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.87916/4.86119. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88182/4.88308. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.87909/4.88206. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.88104/4.87674. Took 0.18 sec\n",
      "Epoch 87, Loss(train/val) 4.87616/4.87728. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.88056/4.88079. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.88281/4.87890. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.87669/4.88041. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.87700/4.88779. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.88386/4.86042. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.87890/4.88788. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.87373/4.88721. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.87765/4.87735. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.87976/4.87601. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.87254/4.88135. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.87407/4.87688. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.87244/4.88778. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.03138824102871723\n",
      "Epoch 0, Loss(train/val) 5.02439/5.00554. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.99153/4.98214. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.98146/4.99021. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.98501/4.98721. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.98145/4.98597. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.98279/4.98537. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.98657/4.98396. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.98391/4.98361. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.98437/4.98861. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.98021/4.99413. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.98021/4.99348. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.98026/4.99320. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.97747/4.99572. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.97941/4.99090. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.97885/4.99827. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.97695/4.99902. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.98059/5.01500. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.97966/5.01373. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.98147/4.99888. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.97977/5.00507. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.97612/5.01719. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.97428/5.01714. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.97377/4.99997. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.97690/4.99728. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.97499/5.00916. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.97408/4.99982. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.97123/5.02372. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.97366/5.02125. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.97636/5.00699. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.97681/5.00766. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.97383/5.01928. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.97291/5.00721. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.97311/5.01620. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.97248/5.01374. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.97125/5.02084. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.97740/4.99696. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.97000/5.01322. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.97537/5.02316. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.96537/5.03400. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.97001/5.02109. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.96763/5.01662. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.97042/5.01618. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.96662/5.02635. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.96636/5.02528. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.96972/5.03563. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.96653/5.00590. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.97444/5.00758. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.97166/5.02646. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.97089/5.00778. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.97207/5.01046. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.97310/5.00995. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.97208/5.02011. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.97389/5.00421. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.96676/5.02955. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.96228/5.00995. Took 0.21 sec\n",
      "Epoch 55, Loss(train/val) 4.96069/5.02112. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.97367/4.99217. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.97210/4.98766. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.96411/5.00546. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.96239/5.01637. Took 0.18 sec\n",
      "Epoch 60, Loss(train/val) 4.96889/5.00181. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.96642/4.99921. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.95846/5.04140. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.97709/4.98712. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.97410/4.99615. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.96924/4.99640. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.96801/5.01149. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.96454/5.00957. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.96324/5.00994. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.96084/5.02458. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.96553/4.99495. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.96480/5.02237. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.97100/4.99110. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.96891/4.99868. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.96284/5.02570. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 4.95831/5.01925. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.96108/5.04958. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.97749/4.97976. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.96772/4.98907. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.96060/5.02659. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.95874/5.01146. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.96235/5.00510. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.96165/5.02240. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.95327/5.01909. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.95604/5.01654. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.95268/5.00042. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.97991/4.98895. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.96412/5.00608. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.96415/5.00291. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.95304/5.04290. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.95869/4.99631. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.95090/5.00203. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.96108/4.99803. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.97547/5.00267. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.97183/5.00563. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.96666/5.01641. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.96461/5.02875. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.95875/5.03341. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.96033/5.02725. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.95577/5.03084. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.12267022872162016\n",
      "Epoch 0, Loss(train/val) 4.87151/4.81765. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.82943/4.81020. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.81816/4.83892. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.80643/4.81746. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.80086/4.81872. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.80237/4.81990. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.80235/4.82630. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.80181/4.82422. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.80381/4.81943. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.80075/4.82184. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79711/4.82143. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.79762/4.81927. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79930/4.82883. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79917/4.82233. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.79497/4.83002. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.79635/4.83245. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.79443/4.83223. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.79265/4.82847. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78727/4.84333. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.79556/4.83003. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.79137/4.83246. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78946/4.84049. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.79365/4.82796. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78943/4.83992. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78742/4.84430. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78765/4.83896. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.79011/4.83876. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78841/4.85004. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78552/4.83598. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78956/4.84860. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.78465/4.84363. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.78801/4.83060. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.78850/4.85762. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.78396/4.83961. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.78054/4.86528. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.78070/4.85001. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.78216/4.81935. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.78620/4.83974. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.78366/4.84442. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.78007/4.84156. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77832/4.86572. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.79296/4.82550. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.78474/4.83571. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.78298/4.84398. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.78018/4.85294. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77837/4.85126. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.78321/4.84878. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.78022/4.85498. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.77706/4.86184. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.78006/4.85631. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.77758/4.85757. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.78046/4.85736. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.77850/4.87366. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.77430/4.87556. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.78115/4.86656. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.76989/4.88163. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.76990/4.89533. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.78081/4.86792. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.76886/4.89930. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.77682/4.86068. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.77820/4.86814. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.77285/4.88625. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.77649/4.87878. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.77708/4.86258. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.77051/4.87240. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.77123/4.88161. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.76784/4.88119. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.78020/4.86196. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77139/4.87603. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77250/4.88438. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.77283/4.87429. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.77137/4.87088. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.77166/4.86767. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.76900/4.89350. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.77355/4.86771. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.76856/4.87506. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.77092/4.87016. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77045/4.87054. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.77078/4.86658. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.76939/4.88224. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.76861/4.89619. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.76849/4.90093. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.77683/4.88733. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.76766/4.87693. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.77032/4.87872. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.76857/4.89326. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.77089/4.87739. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.76553/4.89271. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.75945/4.92756. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.76484/4.90073. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.76016/4.91951. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.75463/4.93926. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.76191/4.90188. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.76052/4.90243. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.76240/4.90310. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.76311/4.90782. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.76075/4.89699. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.76488/4.88898. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.76286/4.92023. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.76415/4.91010. Took 0.20 sec\n",
      "ACC: 0.484375, MCC: -0.013980301652228746\n",
      "Epoch 0, Loss(train/val) 4.81030/4.71172. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.76650/4.71045. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.76356/4.71705. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.76204/4.72563. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.75798/4.72994. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.75714/4.72918. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.75474/4.72889. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.75349/4.73295. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.75196/4.73282. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.75070/4.73446. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.75042/4.73752. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.75075/4.74106. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.75114/4.74736. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.74761/4.74847. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.74758/4.74805. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.74441/4.75046. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.74219/4.75427. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.74390/4.75049. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.74312/4.76470. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.74103/4.75652. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.74032/4.76993. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.74110/4.76141. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.74014/4.77184. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.73789/4.77188. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.73743/4.79015. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.73689/4.77964. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.73574/4.77370. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.73834/4.77177. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.73534/4.77895. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.73445/4.78427. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.73733/4.76748. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.73569/4.79273. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.73591/4.77076. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.73121/4.78951. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.73330/4.77699. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.73493/4.77084. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.72435/4.80712. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.73020/4.77646. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.72704/4.78050. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.72998/4.77561. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.73033/4.77725. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.73221/4.76393. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.72650/4.79421. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.72490/4.78973. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.72709/4.77592. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.72317/4.79741. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.72623/4.80274. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.72730/4.77531. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.72621/4.78535. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.72788/4.79408. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.72413/4.81152. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.72266/4.79289. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.72609/4.78636. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.72903/4.77528. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.72920/4.79219. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.72363/4.80050. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.72130/4.77481. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.72133/4.76813. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.72177/4.76858. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.73389/4.75893. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.72595/4.82672. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.71981/4.81708. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.72395/4.78742. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.71454/4.80286. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.72085/4.77919. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.72048/4.78985. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.72121/4.80461. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.72712/4.79495. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.72086/4.80311. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.71151/4.81160. Took 0.21 sec\n",
      "Epoch 70, Loss(train/val) 4.72384/4.78280. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.71294/4.81754. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.71587/4.80041. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.72145/4.78541. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.71539/4.81060. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.72327/4.79390. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.72141/4.80340. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.71588/4.81586. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.71627/4.79185. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.72166/4.81127. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.72061/4.81340. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.71297/4.81790. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.71313/4.81160. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.71138/4.82824. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.71227/4.81398. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.71483/4.81591. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.71040/4.83170. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.71244/4.80007. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.71275/4.81223. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.71395/4.82163. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.71006/4.81304. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.70681/4.85514. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.70932/4.80786. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.70966/4.81574. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.70685/4.83942. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.70924/4.82448. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.71646/4.83673. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.70618/4.84257. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.70525/4.83358. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.70266/4.84034. Took 0.19 sec\n",
      "ACC: 0.4375, MCC: -0.1259881576697424\n",
      "Epoch 0, Loss(train/val) 4.94322/4.93378. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.87192/4.91748. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.88026/4.89257. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87640/4.88101. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.87099/4.88158. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.86847/4.88831. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.86714/4.89501. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.86924/4.89036. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.86859/4.88475. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.86467/4.88299. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.86442/4.88348. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.86414/4.87889. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85733/4.88550. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85760/4.88544. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85907/4.88838. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85992/4.88228. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85895/4.86401. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85774/4.87406. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85454/4.86851. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.85779/4.85559. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.85708/4.86431. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85144/4.86122. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.84968/4.86706. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84613/4.87467. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.84987/4.87528. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.84645/4.88051. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.85122/4.88428. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.85101/4.88153. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.84491/4.88460. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84364/4.87897. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84546/4.87986. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84638/4.87576. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.84461/4.88803. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84930/4.88901. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84537/4.88757. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84518/4.89527. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84219/4.90089. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84356/4.90151. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.83963/4.89554. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84319/4.89925. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84197/4.88453. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84099/4.90470. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.85425/4.89513. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.84314/4.89305. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.83820/4.91056. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.84452/4.90558. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84800/4.89163. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84812/4.89861. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.84674/4.90740. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.84388/4.90679. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.83573/4.91084. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.84321/4.92232. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.83562/4.91067. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.84421/4.91099. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.84410/4.91465. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.84682/4.89930. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.84725/4.93576. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.84317/4.93548. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.84365/4.91166. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.83482/4.92849. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.84435/4.91275. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84229/4.91287. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.84045/4.92732. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83695/4.92415. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.83750/4.93261. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.83380/4.93516. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.83183/4.92919. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83627/4.92225. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.84077/4.91834. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84201/4.93954. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.85246/4.91222. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.84966/4.90036. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.84810/4.90303. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84538/4.90068. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.84064/4.91832. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.84265/4.90092. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.84410/4.90320. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.84062/4.89430. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.83403/4.92618. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.84185/4.93960. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.83733/4.94878. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.83692/4.95121. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.84079/4.93059. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.83395/4.92888. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83936/4.90989. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.83406/4.97773. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.83813/4.92055. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83558/4.91777. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83447/4.92661. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83580/4.93219. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82726/4.94730. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.83392/4.93800. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.83453/4.95949. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83618/4.90989. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82958/4.95605. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.83105/4.95850. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82888/4.93493. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.82618/4.94001. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82593/4.96527. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.83271/4.94018. Took 0.19 sec\n",
      "ACC: 0.609375, MCC: 0.21885688981825285\n",
      "Epoch 0, Loss(train/val) 4.94721/4.91949. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.89790/4.88727. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.88813/4.89169. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.88879/4.89306. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.89143/4.89121. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.88946/4.89240. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.88639/4.89656. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.88849/4.89470. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.88804/4.89455. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.88693/4.89469. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.88419/4.89692. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.88442/4.90327. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.88671/4.90571. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.88307/4.91007. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.88085/4.92115. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.88185/4.91856. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.87977/4.92579. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.87885/4.93537. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.87697/4.93964. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.87666/4.93634. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.87901/4.93580. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.87489/4.92614. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.87976/4.94566. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.87061/4.96814. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.87662/4.93739. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.87265/4.95181. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.87057/4.95714. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.87289/4.95143. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.87020/4.95636. Took 0.22 sec\n",
      "Epoch 29, Loss(train/val) 4.87167/4.94776. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.87191/4.95815. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86863/4.96659. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.87030/4.96103. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.88134/4.90248. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.87010/4.94555. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.86855/4.95753. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.86661/4.95928. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.87082/4.97221. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.86612/4.96034. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.87467/4.94848. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.87870/4.93634. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.87273/4.96085. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.87237/4.96056. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.87395/4.94956. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.86905/4.95660. Took 0.21 sec\n",
      "Epoch 45, Loss(train/val) 4.86189/4.99245. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.86686/4.96998. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.87093/4.98647. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.86888/4.96962. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.86681/4.95083. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.86755/4.97488. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.86645/4.95930. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.87499/4.94616. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.87314/4.96453. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.86457/4.99050. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.86339/4.98190. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.86580/4.97420. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.86496/4.98186. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.86221/4.97206. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.86453/4.98109. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.86283/4.97086. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.86162/4.97972. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.86327/4.95396. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.86509/4.95840. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.86246/4.97060. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 4.85847/4.97435. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.86244/4.97556. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.85703/5.00320. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.86244/4.97582. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.86099/4.96907. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.86151/4.98538. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.85895/4.97558. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.86208/4.99014. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.86174/4.97616. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.85694/4.99014. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.85716/4.98257. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.86037/4.97097. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.86142/4.98201. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.85299/5.00206. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.85913/4.98055. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.85525/4.98843. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.85826/4.99130. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.86103/4.97187. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.85922/4.98199. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.85838/4.99286. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.85537/4.98776. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.85444/4.99358. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.85619/4.98450. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.85526/5.00013. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.86025/4.97947. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.85580/4.98873. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85225/5.01876. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.85967/4.99487. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.85354/5.00765. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.85649/4.98852. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.85615/5.00219. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.84693/5.02340. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.85482/5.00306. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.85052/5.01010. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.86029/4.95475. Took 0.19 sec\n",
      "ACC: 0.65625, MCC: 0.314970394174356\n",
      "Epoch 0, Loss(train/val) 4.97016/4.84033. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.82931/4.84691. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.82842/4.83071. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.82975/4.82403. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.82370/4.82399. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.82653/4.82467. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.82541/4.82502. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.82470/4.82739. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.82271/4.82988. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.82732/4.82702. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.82378/4.82471. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.82227/4.82286. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.82214/4.82309. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.82218/4.82372. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.82241/4.82530. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.82146/4.82612. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.82161/4.82782. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.82183/4.82674. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81826/4.82896. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.81832/4.83392. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.82032/4.83106. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.82239/4.81981. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.81928/4.82193. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.81739/4.82164. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82020/4.82152. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.81966/4.82037. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.81782/4.82022. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.81909/4.82071. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.81873/4.81820. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.81660/4.82164. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.81440/4.82075. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.81605/4.82165. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.81945/4.82321. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.81809/4.82972. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.81643/4.83570. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.81472/4.83357. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.81684/4.82630. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.81421/4.83157. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.81504/4.83202. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.81573/4.83200. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.81308/4.83740. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.81333/4.83370. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.81510/4.83909. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.81164/4.83708. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.81269/4.84696. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.81331/4.84158. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.81061/4.84289. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.81373/4.84154. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.81514/4.83969. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.81530/4.84015. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.81228/4.84891. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.81327/4.84497. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81067/4.85503. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.80952/4.85894. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81294/4.84516. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.81205/4.85634. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.81096/4.84628. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.80969/4.84920. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.80938/4.86215. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.80781/4.86952. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.80676/4.86337. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.80456/4.86076. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.80760/4.87751. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.80656/4.87404. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81258/4.85331. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.80631/4.85897. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.80345/4.86866. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.80589/4.87310. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.80512/4.86891. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.80546/4.85625. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.80975/4.84025. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.80868/4.86443. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.81113/4.85449. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.81014/4.87166. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.80281/4.86753. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.80900/4.85955. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.81452/4.87680. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80737/4.86380. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.80139/4.88997. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.80669/4.88308. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.80753/4.87762. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.80706/4.88457. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.80149/4.88628. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.79846/4.90881. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.80101/4.89097. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.80561/4.88388. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.80589/4.86674. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.80288/4.87854. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.79238/4.91587. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.79981/4.88449. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.79895/4.90497. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.81455/4.81237. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.81631/4.81313. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.81570/4.81841. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.81369/4.81994. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.81299/4.82574. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.81180/4.82249. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81178/4.83215. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.81124/4.82819. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.81201/4.83263. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.11383959916814836\n",
      "Epoch 0, Loss(train/val) 5.00585/4.95666. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.97784/4.96666. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.97101/4.96316. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.97094/4.95545. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.97591/4.95013. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.97292/4.95551. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.96473/4.95470. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.96583/4.95172. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.96471/4.95038. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.96168/4.94827. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.95956/4.94942. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.96188/4.94997. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.96333/4.95189. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.96716/4.95445. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.96253/4.95286. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.95695/4.95000. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.96245/4.95689. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.96150/4.95894. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.95659/4.96087. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.95871/4.95874. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.95447/4.96077. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.95731/4.96230. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.95699/4.96341. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.95853/4.95892. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.95632/4.96057. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.95289/4.96306. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.96011/4.95966. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.95561/4.96091. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.95500/4.95660. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.95722/4.95003. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.96591/4.95614. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.96035/4.95239. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.95940/4.95168. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.95564/4.95687. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.95530/4.95333. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.95416/4.95716. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.95864/4.95887. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.95348/4.96176. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.95285/4.96001. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.95224/4.96278. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.95396/4.95668. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.95252/4.96604. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.95147/4.96419. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.94911/4.97107. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.95113/4.95371. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.95049/4.95661. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.94987/4.95638. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.94561/4.96514. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.94850/4.96391. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.94546/4.96629. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.94378/4.95392. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.94934/4.95826. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.94974/4.95841. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.94319/4.95699. Took 0.21 sec\n",
      "Epoch 54, Loss(train/val) 4.94452/4.98543. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.95112/4.95923. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.95135/4.96312. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.94757/4.95892. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.94438/4.96739. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.94628/4.96472. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.94219/4.97712. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.94380/4.94880. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.95567/4.94889. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.95218/4.94790. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.95062/4.95539. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.94903/4.95424. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.94856/4.95514. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 4.94554/4.95640. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.94340/4.95950. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.94433/4.95907. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.94398/4.96098. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.94216/4.96911. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.94435/4.95200. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.94370/4.96568. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.94062/4.96217. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.94159/4.96680. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.94050/4.96836. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.93828/4.96247. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.94174/4.97042. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.94304/4.94908. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.93902/4.96558. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.93898/4.96428. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.93798/4.97803. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.94787/4.96774. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.95491/4.94952. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.95089/4.95006. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.95056/4.94423. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.94844/4.94758. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.94484/4.94469. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.94744/4.95650. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.94316/4.95400. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.94533/4.96009. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.94290/4.96920. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.94304/4.96713. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.94389/4.97421. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.93760/4.96176. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.93738/4.98275. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.94063/4.97214. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.93888/4.97203. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.93941/4.97903. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.16150120428611295\n",
      "Epoch 0, Loss(train/val) 4.97380/4.93008. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.91608/4.91435. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.90753/4.91686. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.91023/4.91437. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 4.91010/4.91035. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.90945/4.90972. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.90632/4.92125. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.91113/4.91884. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.90756/4.91928. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.90721/4.91883. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.90615/4.91891. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.91000/4.92251. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.90717/4.92071. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.90688/4.92263. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.90897/4.92315. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.90598/4.92379. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90607/4.92457. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.90526/4.92595. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 4.90657/4.92593. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.90339/4.92624. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.90233/4.92857. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.90408/4.92342. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.90083/4.92713. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.90560/4.92575. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.90206/4.92358. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.90224/4.92341. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.90060/4.91983. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.89847/4.92197. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90056/4.92456. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.89924/4.92727. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90108/4.92639. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.89870/4.92046. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.89998/4.92030. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.89604/4.91657. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.89924/4.91827. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.89847/4.91345. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.89972/4.91215. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.89680/4.91456. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89226/4.91745. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.89768/4.92150. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.89229/4.91944. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.89240/4.92054. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89782/4.92054. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.89758/4.90525. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.89218/4.90290. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.89385/4.90721. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.89116/4.91174. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89015/4.90661. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.90826/4.89907. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.90189/4.90617. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.90212/4.90169. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.90218/4.90177. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89840/4.90736. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.89672/4.88156. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.90214/4.90198. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.89736/4.90948. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.89776/4.91653. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.90148/4.90392. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.89291/4.90764. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89786/4.90045. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.89560/4.89496. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.89272/4.89723. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.89089/4.90653. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.89044/4.90748. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.88902/4.89679. Took 0.21 sec\n",
      "Epoch 65, Loss(train/val) 4.89558/4.89136. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.88994/4.90129. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.88689/4.90518. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.90182/4.90945. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.90536/4.91205. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.89810/4.91197. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.89845/4.91053. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.89439/4.91924. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.89367/4.90745. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.89669/4.90595. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.89292/4.90386. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.89049/4.90942. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.89090/4.90202. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.88599/4.91996. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.89912/4.90390. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.89885/4.91659. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.89894/4.91079. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.89473/4.91528. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.89191/4.91263. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.88708/4.90519. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.88527/4.89698. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.89075/4.89771. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.89256/4.89501. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.88559/4.90373. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.89090/4.90037. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.88010/4.90264. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.88703/4.89800. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.88076/4.90422. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.88476/4.92004. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88817/4.89921. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.88228/4.89877. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.87632/4.90876. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.88534/4.88822. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.88182/4.90432. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88416/4.91122. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.20321305705762227\n",
      "Epoch 0, Loss(train/val) 4.60462/4.57611. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.56389/4.58197. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.55612/4.57028. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.55778/4.56657. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.55605/4.56459. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 4.55027/4.56413. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.55045/4.56255. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.54579/4.55968. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.55054/4.56025. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.54589/4.55317. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.54261/4.56672. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.54546/4.55654. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.53644/4.55955. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.54552/4.55377. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.53773/4.55568. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.53301/4.56758. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.53536/4.55291. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.53635/4.54705. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.53814/4.56238. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.53941/4.54922. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.53485/4.54903. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.53742/4.57315. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.53303/4.57679. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.52961/4.57294. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.52812/4.56885. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.53524/4.58115. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.54019/4.58943. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.53489/4.57001. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.53196/4.57414. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.52756/4.57484. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.53370/4.56980. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.53015/4.58570. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.53564/4.57407. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.52678/4.56420. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.52907/4.56270. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.53215/4.57815. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.52906/4.58227. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.52771/4.57695. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.52844/4.55973. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.52692/4.57163. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.52823/4.56400. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.52990/4.57153. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.52635/4.55214. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.53308/4.56438. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.52966/4.56749. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.52247/4.56146. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.53411/4.58156. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.53430/4.58663. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.53028/4.56837. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.53283/4.57585. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.52343/4.57519. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.52168/4.58001. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.52754/4.57737. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.52702/4.56700. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.52423/4.58535. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.53006/4.57467. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.52557/4.56844. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.52259/4.56974. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.52441/4.57352. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.52528/4.58775. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.52508/4.57594. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.53222/4.56216. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.52548/4.57536. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.52607/4.57997. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.51992/4.58411. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.54571/4.59219. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.54126/4.58702. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.54133/4.59310. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.53678/4.59814. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.53541/4.58506. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.53584/4.59580. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.53372/4.59685. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.53887/4.58018. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.52949/4.58976. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.53161/4.60091. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.53291/4.59995. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.53134/4.57784. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.53409/4.60184. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.52940/4.58918. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.52978/4.57964. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.52697/4.59525. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.52656/4.57314. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.52971/4.58994. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.53087/4.57460. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.53143/4.58410. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.52723/4.56958. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.53171/4.59662. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.53342/4.58514. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.53135/4.57848. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.52978/4.57959. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.52843/4.59505. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.52981/4.57828. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.53154/4.58152. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.52978/4.56790. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.52865/4.58919. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 4.52210/4.58081. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.52419/4.58021. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.52458/4.59299. Took 0.18 sec\n",
      "Epoch 98, Loss(train/val) 4.52140/4.58490. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.52437/4.55139. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.11030375355930092\n",
      "Epoch 0, Loss(train/val) 4.90967/4.89265. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.90309/4.87576. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.89586/4.88970. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.89163/4.89227. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.88450/4.88279. Took 0.18 sec\n",
      "Epoch 5, Loss(train/val) 4.88290/4.88274. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.88221/4.88637. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.87960/4.88748. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.88028/4.89005. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.88017/4.89125. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.87963/4.89443. Took 0.18 sec\n",
      "Epoch 11, Loss(train/val) 4.87724/4.89562. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.87874/4.89462. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.87950/4.89950. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87566/4.89620. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.87504/4.89648. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.87501/4.90934. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.87451/4.89941. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.87611/4.89370. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.87461/4.89199. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.87121/4.89711. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.87252/4.89198. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.86868/4.90272. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.87224/4.89483. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.87129/4.89704. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.87045/4.89388. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.86529/4.89731. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 4.86496/4.90282. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.87041/4.89929. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.86806/4.89315. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.86491/4.89521. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86438/4.88629. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.86780/4.86963. Took 0.21 sec\n",
      "Epoch 33, Loss(train/val) 4.86504/4.88944. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.86770/4.88370. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.86249/4.88501. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.86463/4.88838. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.86310/4.88865. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.86170/4.91021. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.86441/4.89707. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.86543/4.90006. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.86394/4.88645. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.85992/4.89910. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.85713/4.90395. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.85863/4.89379. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.85922/4.88041. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.85703/4.89711. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.85165/4.90418. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.85276/4.87983. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.88994/4.87320. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.87959/4.88843. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.87690/4.88489. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.87473/4.88148. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.87458/4.87502. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.87476/4.87421. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.87137/4.87543. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.87263/4.87384. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.86962/4.87965. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.87267/4.88972. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.86024/4.88430. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85948/4.88721. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.86049/4.88337. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85370/4.89365. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.84839/4.89363. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.85070/4.90217. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.85814/4.88894. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.84788/4.89008. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.85189/4.89677. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.84832/4.89215. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84865/4.89196. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.84987/4.89293. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.84776/4.88563. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.84447/4.89169. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.85124/4.89457. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.84669/4.89275. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.84592/4.91307. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.84902/4.88718. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.84474/4.86324. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.84265/4.87162. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.84469/4.90634. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.84184/4.89082. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.84485/4.86885. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.84104/4.89262. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84507/4.86177. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83920/4.88863. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.84003/4.89269. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.83719/4.89131. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83377/4.90936. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.83437/4.87926. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.84028/4.87694. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.84282/4.92849. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.84951/4.89497. Took 0.21 sec\n",
      "Epoch 92, Loss(train/val) 4.84235/4.87484. Took 0.21 sec\n",
      "Epoch 93, Loss(train/val) 4.84983/4.87454. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.84224/4.88151. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.84834/4.87576. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.84001/4.88483. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83949/4.89885. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.84290/4.89026. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.84003/4.91641. Took 0.20 sec\n",
      "ACC: 0.625, MCC: 0.2393322795582759\n",
      "Epoch 0, Loss(train/val) 4.99262/4.98320. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.97345/4.95787. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.97684/4.95807. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.96445/4.95375. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.95163/4.94926. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.95087/4.95323. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.95470/4.95375. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.95572/4.95199. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.95486/4.95203. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.95224/4.95160. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.95016/4.95329. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.95028/4.95560. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.94882/4.95864. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.95123/4.96036. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.94830/4.95933. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.94804/4.96249. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.95023/4.95581. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.95161/4.95939. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.95141/4.96094. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.94923/4.95972. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.94694/4.96396. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.95140/4.96835. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.94805/4.96911. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.94671/4.97473. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.94880/4.96054. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.94688/4.97195. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.94802/4.97987. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.94485/4.97156. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.94718/4.97459. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.94185/4.99488. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.94402/4.97388. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.94138/4.99206. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.94341/4.97390. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.94383/4.98276. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.94064/4.98326. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.94098/4.99197. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.93985/4.99993. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.94090/4.98636. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.93737/4.99171. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.93995/5.00699. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.94009/4.99342. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.94303/5.00433. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.93764/5.00302. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.93584/5.01685. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.93740/5.01437. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.93845/4.98042. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.94284/5.00459. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.93698/4.99877. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.93846/4.99791. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.93595/5.00035. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.93759/4.99808. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.93456/4.98732. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.93483/5.01698. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.93525/5.00711. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.93277/5.01064. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.93320/5.00721. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.93312/5.02146. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.93467/4.99550. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.93313/4.98912. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.93818/5.02060. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.94548/4.97563. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.94143/4.99145. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.93454/4.99968. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.93233/4.99587. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.93377/4.99674. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.93131/5.00457. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.93336/4.98761. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.92627/5.00466. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.93489/4.98779. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.93232/5.00020. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.92566/5.02077. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.93400/5.00629. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.92850/5.01370. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.93007/5.01927. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.93076/5.00530. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.92826/5.01899. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.92591/5.00907. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.92830/4.99346. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.92776/5.02104. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.93074/4.99073. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.92338/5.03892. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.93681/4.99436. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.93683/4.98287. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.93449/4.98895. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.93230/4.99578. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.92625/5.00526. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.92707/5.01296. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.93213/4.99128. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.92434/5.00283. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.92713/4.99428. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.92336/5.02678. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.92916/5.02645. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.92742/4.99896. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.92863/5.00632. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.92615/5.00399. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.92548/5.02334. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.92743/4.97934. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.93661/4.96842. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.93353/4.96880. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.94184/4.97133. Took 0.20 sec\n",
      "ACC: 0.515625, MCC: 0.025552902682603722\n",
      "Epoch 0, Loss(train/val) 5.01247/4.94632. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.92725/4.91875. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.92047/4.91663. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.92504/4.92246. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.92421/4.92513. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.92356/4.92712. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.92351/4.93060. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.92046/4.93114. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.91862/4.92995. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.91895/4.93440. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.91569/4.93524. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.91494/4.93702. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.91535/4.93778. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.91587/4.93924. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.91358/4.94014. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.91144/4.94340. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.91160/4.94105. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.90962/4.94940. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.91261/4.93974. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.91055/4.94505. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.91033/4.94253. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.90896/4.94848. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.90824/4.94154. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.91230/4.94606. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.90865/4.95176. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.90958/4.94120. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.90468/4.96248. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.90800/4.95572. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90766/4.96493. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.90490/4.96229. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90551/4.95908. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.90387/4.97244. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.89919/4.97851. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.90485/4.97276. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.90113/4.97700. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90143/4.98003. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.89811/4.98620. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.90426/4.95702. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89920/4.98917. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90092/4.99515. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.89713/4.98935. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.89641/5.00823. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.90357/4.94439. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.89609/5.00321. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.89756/4.99703. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 4.90156/4.97133. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.89930/5.00022. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89478/4.99382. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89213/5.02170. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.89472/5.00826. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.89742/4.98631. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.89146/5.01619. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89385/5.01011. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.89367/4.99948. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.89388/5.00232. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.89152/5.00093. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.89094/5.00872. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.89269/4.97955. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.88844/4.99959. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89204/4.99245. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.89062/4.96923. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.88453/4.99074. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.88622/4.98806. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.88970/4.99942. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.88303/4.98638. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.88811/4.98540. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.89188/4.99771. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.88069/4.99368. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.88859/4.99224. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.88557/5.01469. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.88553/4.98814. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.88513/5.02349. Took 0.18 sec\n",
      "Epoch 72, Loss(train/val) 4.89012/4.98530. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88727/4.99044. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.88221/5.02585. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.88042/5.01100. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.88457/5.01176. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.88490/5.00624. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.88369/5.01050. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.88109/4.99287. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88090/5.00468. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.88230/5.00918. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.88660/5.02422. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.88175/5.01374. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88095/4.99697. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.87544/5.01237. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.88197/4.99162. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.88343/5.00426. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.87535/5.00649. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.89253/4.99127. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.88859/4.99506. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.89211/4.99346. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.89439/4.98436. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.87876/5.00676. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88234/5.00049. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.88539/5.00748. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.87606/5.00981. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.89032/4.98133. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.88619/4.98501. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88181/5.01731. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.14060420405767388\n",
      "Epoch 0, Loss(train/val) 4.89313/4.81981. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 4.82121/4.81373. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.81931/4.81462. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.81799/4.81529. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.81426/4.81535. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.81350/4.81551. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.81516/4.81652. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.81296/4.81561. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.81371/4.81476. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.81096/4.81280. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.80809/4.81889. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.80845/4.82130. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.80448/4.82419. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.80048/4.83400. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.80334/4.83809. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.80758/4.83110. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.80272/4.83700. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.80253/4.82969. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.80410/4.82989. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.80329/4.83819. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.80113/4.83617. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.79602/4.84027. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.80128/4.84129. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.80449/4.83493. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.79841/4.84576. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.80426/4.83118. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.79761/4.84681. Took 0.21 sec\n",
      "Epoch 27, Loss(train/val) 4.79763/4.83938. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.80009/4.83651. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.80137/4.83957. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.79280/4.84998. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.79986/4.84354. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.79725/4.84230. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.78840/4.85708. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.79333/4.84767. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.79653/4.84971. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.79280/4.84981. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.79393/4.85329. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.79771/4.84253. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.79038/4.85478. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.79350/4.84562. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.78983/4.86640. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.79794/4.85371. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.79088/4.85154. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.79458/4.84367. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.78795/4.85169. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.79210/4.85366. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.78886/4.85245. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.79293/4.84534. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.80262/4.84392. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.79863/4.84160. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.79311/4.84073. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.79251/4.84466. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.79155/4.85831. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.79513/4.84494. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.79155/4.84562. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.79250/4.84203. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.79270/4.83232. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.79518/4.83603. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.78583/4.84365. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.79101/4.84622. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.78976/4.84987. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.78691/4.85129. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.78700/4.86353. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.79324/4.83815. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.79024/4.85728. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.78442/4.86895. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.78499/4.85544. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.78823/4.85976. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.78518/4.86157. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.78555/4.85540. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.77722/4.86972. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.78687/4.86085. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.77932/4.87888. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.78133/4.90080. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.78376/4.85967. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.78841/4.85003. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.77952/4.87955. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.78955/4.85564. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.78174/4.88440. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.78291/4.86058. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.78024/4.87409. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.78010/4.85946. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.78251/4.87259. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.78744/4.84666. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.78480/4.86045. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.78482/4.87251. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.78276/4.88119. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.77768/4.90762. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.77258/4.88603. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.78078/4.89166. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.78210/4.85195. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.77759/4.87756. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.77998/4.85278. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.77755/4.88618. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.77812/4.88397. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.77323/4.90464. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.78102/4.84231. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.77309/4.94386. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.77415/4.88068. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.0905982365507463\n",
      "Epoch 0, Loss(train/val) 5.11636/5.03396. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.04316/5.03889. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.03911/5.04598. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.04030/5.05095. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 5.04109/5.04885. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.03658/5.04743. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.04095/5.04621. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.03705/5.05315. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.04034/5.05567. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.03557/5.05912. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.03750/5.05774. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.03313/5.05988. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 5.03549/5.06601. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.03174/5.06421. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.03025/5.06387. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.03536/5.06140. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.03436/5.06311. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.02588/5.08233. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.03018/5.07301. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.03149/5.07836. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.03564/5.07023. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.02393/5.08049. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.02751/5.07816. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.02350/5.07711. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.02028/5.09744. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.02601/5.06141. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.02868/5.07476. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.02666/5.07634. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.02307/5.06901. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.02379/5.07945. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.02476/5.07133. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 5.01905/5.07939. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.02056/5.07811. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.01787/5.09003. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.02697/5.07227. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.02195/5.07950. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.02252/5.07897. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.02944/5.06797. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 5.02138/5.08298. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.02642/5.07419. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.02508/5.06032. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.02427/5.06698. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.02280/5.06082. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.02879/5.06368. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.02430/5.06347. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.02924/5.04937. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.02833/5.07493. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.02059/5.08613. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.02179/5.05391. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.02458/5.06728. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.02324/5.06726. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.02170/5.08747. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.02466/5.07627. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.01870/5.08905. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.02111/5.09018. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.01706/5.10772. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.01927/5.11951. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.01172/5.12508. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.01866/5.11227. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.01517/5.11498. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.01583/5.10712. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.01334/5.11905. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.01684/5.09362. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.01576/5.11069. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.01247/5.12158. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.00985/5.11840. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.01373/5.11580. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.01125/5.12114. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.01789/5.11940. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.01474/5.10965. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.00737/5.14526. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.01321/5.10713. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.01531/5.13198. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 5.01166/5.13345. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.01155/5.13258. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.01617/5.12214. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.01000/5.12709. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.00774/5.14347. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.00731/5.11353. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.00779/5.13127. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.01144/5.11777. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.00656/5.13670. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.00769/5.14466. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.00510/5.12289. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.00988/5.12054. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.00945/5.13056. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.01420/5.12568. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.00261/5.15020. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.00469/5.12879. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.00305/5.12612. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.00932/5.12532. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.01455/5.10918. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.00611/5.15621. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 5.01273/5.14340. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 5.00744/5.12396. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.00692/5.13584. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.00671/5.12341. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 5.00587/5.14012. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.00884/5.13868. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.00782/5.13402. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.0635386119257087\n",
      "Epoch 0, Loss(train/val) 4.80767/4.88216. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.79039/4.78323. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.77827/4.79067. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.77692/4.80211. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.77902/4.80044. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.77590/4.80226. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.77461/4.81302. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.77489/4.80313. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.77670/4.80051. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.78055/4.79280. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.77479/4.79653. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.77706/4.79954. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.77546/4.79253. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.77265/4.80175. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.77706/4.80511. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.77091/4.80308. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.77162/4.80288. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.77207/4.81303. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.77347/4.79862. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.76974/4.81068. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 4.77037/4.81405. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.77307/4.80552. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.77407/4.80277. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.77321/4.80041. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.77190/4.80296. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.77023/4.80167. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.77017/4.79810. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.77098/4.80658. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.77110/4.80013. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.77296/4.80785. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.77021/4.81800. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.76832/4.81159. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77071/4.82213. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.76913/4.82495. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.76680/4.82302. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.76709/4.80107. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.76630/4.81442. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.77059/4.81342. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.76316/4.84445. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.76655/4.82279. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.76581/4.83581. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.76625/4.82066. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76571/4.84496. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.76278/4.83648. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.76187/4.84732. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.76408/4.82455. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.76112/4.86567. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.76053/4.83439. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.76085/4.84262. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.76187/4.84780. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76323/4.84006. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.75824/4.85277. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.75860/4.83496. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76040/4.83701. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76223/4.84897. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.75848/4.86082. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.75893/4.85945. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.75789/4.86464. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.75253/4.86347. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.75087/4.84150. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.76266/4.89076. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.75467/4.85652. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.75870/4.84827. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.75958/4.84448. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.75632/4.86834. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.75868/4.85225. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75900/4.83420. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.75708/4.84342. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.75582/4.85907. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.75647/4.84702. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.75233/4.85590. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.75342/4.86669. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.75385/4.85850. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.74949/4.87024. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.74927/4.87851. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.75830/4.88801. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.75807/4.85571. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.75064/4.86192. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.74849/4.89193. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.76029/4.84129. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.75581/4.87285. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.75071/4.86224. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.75036/4.87128. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.74513/4.88555. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.75714/4.86237. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.75329/4.87631. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.75692/4.88645. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.75900/4.87550. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.75523/4.83745. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.75445/4.87578. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.74918/4.88333. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.75109/4.87362. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.75423/4.87816. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.74873/4.87216. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.75635/4.86608. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75579/4.88686. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.75151/4.87052. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.74422/4.89089. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.74868/4.88581. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75187/4.90936. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.0625\n",
      "Epoch 0, Loss(train/val) 5.10669/5.07896. Took 0.22 sec\n",
      "Epoch 1, Loss(train/val) 5.06329/5.06441. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.05449/5.06886. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.05824/5.06568. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 5.05256/5.06440. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.05549/5.06119. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.04821/5.06031. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.04792/5.06229. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.04800/5.06049. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.04796/5.06235. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.04782/5.06210. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.04503/5.05963. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.04605/5.06252. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.04690/5.06203. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.04472/5.06161. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.04200/5.06142. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.04277/5.06715. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 5.04503/5.06084. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.04128/5.05671. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.04045/5.05825. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.04098/5.05971. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.03883/5.05540. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.04210/5.05925. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.04125/5.05775. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.04067/5.06277. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.04143/5.07460. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.04265/5.06919. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.04023/5.06838. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.03981/5.07302. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.03660/5.08253. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.03893/5.07879. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.03639/5.07801. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.03792/5.08100. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.03469/5.08286. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.03459/5.07918. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.03785/5.06922. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.03771/5.07420. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.03574/5.08548. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.04053/5.06699. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.03755/5.07498. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.03517/5.08214. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.03832/5.07897. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.03395/5.08351. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 5.03039/5.08859. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.03469/5.07623. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.03159/5.08672. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 5.03793/5.07529. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.03469/5.08928. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.03564/5.08635. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.03942/5.06698. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 5.03582/5.07194. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.03397/5.06251. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.03389/5.06815. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.03745/5.07226. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.03329/5.07205. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.03204/5.07172. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.03084/5.06995. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.03489/5.06648. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 5.02804/5.07871. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.02966/5.07912. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.02818/5.06279. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.03334/5.05670. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 5.03267/5.07062. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 5.03241/5.06963. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.03100/5.07637. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.02503/5.08169. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.03321/5.06891. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.03075/5.07825. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 5.03502/5.07888. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.02473/5.07191. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.02868/5.08696. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.02830/5.07870. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.02438/5.10147. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.02249/5.10990. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 5.03249/5.08615. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.02634/5.08390. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.02941/5.08833. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.02295/5.06743. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.02755/5.08237. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.02879/5.09279. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.02816/5.08681. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.02865/5.09409. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.02653/5.07238. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 5.02503/5.08024. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 5.02247/5.08805. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.02646/5.09627. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.02178/5.09453. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 5.02068/5.11409. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.02526/5.08670. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.02188/5.11588. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.01895/5.11261. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 5.02417/5.10695. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.01527/5.11576. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.02254/5.08408. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.02417/5.09880. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.01700/5.10013. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.01810/5.08717. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.02309/5.10171. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.02149/5.08066. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.02141/5.08860. Took 0.20 sec\n",
      "ACC: 0.453125, MCC: -0.14060420405767388\n",
      "Epoch 0, Loss(train/val) 4.67206/4.56443. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.59490/4.62605. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.59289/4.66337. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.59576/4.64106. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.59316/4.61340. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.58952/4.60631. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.58598/4.61225. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.58899/4.61987. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.58894/4.61659. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.58800/4.61619. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.58524/4.60225. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.58514/4.60328. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.58673/4.60561. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.58592/4.60116. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.58381/4.60608. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.58368/4.60019. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.58344/4.59803. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.58343/4.59859. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.58157/4.60445. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.58001/4.59950. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.57951/4.59265. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.57883/4.58733. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.58051/4.59186. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.57764/4.59364. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.57786/4.58890. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.57614/4.58441. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.57542/4.57938. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.57637/4.59077. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.57699/4.57990. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.57569/4.58474. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.57481/4.58219. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.57342/4.57837. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.57531/4.57524. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.57370/4.58222. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.57361/4.58247. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.57457/4.58333. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.57278/4.58328. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.57258/4.58804. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.57221/4.58188. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.57328/4.58199. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.57317/4.58690. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.57226/4.58027. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.56762/4.59008. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.57180/4.59099. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.57072/4.58161. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.57007/4.61420. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.58200/4.59658. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.57628/4.60520. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.57308/4.59448. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.57012/4.58962. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.57067/4.60230. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.56401/4.59774. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.56663/4.60372. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.56696/4.60743. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.56511/4.61001. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.56529/4.60401. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.56431/4.60056. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.56741/4.61810. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.56076/4.61363. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.56465/4.62052. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.56305/4.61788. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.56436/4.61168. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.55940/4.62157. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.56246/4.61765. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.56655/4.63944. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.56442/4.59787. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.55776/4.62749. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.56156/4.61240. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.55301/4.62545. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.55500/4.63839. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.55811/4.61790. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.55751/4.61475. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.55672/4.62388. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.55690/4.61688. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.56210/4.61419. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.55890/4.62170. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.55751/4.62508. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.55583/4.62627. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.56297/4.65788. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.57062/4.60909. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.56300/4.62521. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.56379/4.62614. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.56033/4.62468. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.56234/4.62830. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.55704/4.63298. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.55883/4.62762. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.55513/4.63349. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.55402/4.62632. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.55899/4.63575. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.55826/4.62819. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.55631/4.62380. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.56165/4.60756. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.57221/4.60231. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.57046/4.58839. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.57020/4.59182. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.56393/4.60035. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.56711/4.60061. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.56421/4.60699. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.56235/4.61715. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.56094/4.60971. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.02757337634153148\n",
      "Epoch 0, Loss(train/val) 4.84355/4.77103. Took 0.75 sec\n",
      "Epoch 1, Loss(train/val) 4.78140/4.77145. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.78080/4.76791. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.77820/4.76355. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.77762/4.76428. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.77854/4.76246. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.77586/4.76899. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.77470/4.77570. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.77398/4.77939. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.77032/4.78919. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.77245/4.78817. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.76811/4.79704. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.77577/4.78411. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.77319/4.78728. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.76916/4.78423. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.76914/4.79304. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.76685/4.79696. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.76819/4.79447. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.76705/4.79463. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.76531/4.79591. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.76833/4.79769. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.76603/4.80132. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.76603/4.80571. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.76696/4.80987. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.76611/4.80984. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.76429/4.80895. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.76450/4.80508. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.76409/4.80271. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.76102/4.80197. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.76417/4.80504. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.76301/4.80662. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.76194/4.78662. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.76265/4.81263. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.76427/4.79666. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.76340/4.79232. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.75849/4.80380. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.76459/4.80330. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.76062/4.80929. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.75736/4.78538. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.75827/4.78554. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.75821/4.78177. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.75611/4.79392. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.75907/4.78858. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.75210/4.80667. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.75829/4.80053. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.75442/4.79806. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.75553/4.79671. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.75541/4.80107. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.75812/4.79180. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.75634/4.79498. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.75428/4.79889. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.75021/4.80029. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.75312/4.79352. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.75531/4.79345. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.75178/4.79760. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.76052/4.77903. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.75266/4.78355. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.74900/4.79205. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.75517/4.78613. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.74816/4.81431. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.75394/4.79848. Took 0.21 sec\n",
      "Epoch 61, Loss(train/val) 4.75171/4.80491. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.75732/4.79033. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.74947/4.80814. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.75365/4.78249. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.75156/4.80175. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75473/4.79450. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.74961/4.78889. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.75366/4.79447. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.75369/4.78411. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.74696/4.79969. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.74608/4.80319. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.74683/4.79609. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.75194/4.78309. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.74845/4.78552. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.74594/4.79362. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.75124/4.77397. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.74705/4.78549. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.75283/4.78869. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.74539/4.77450. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.75304/4.78542. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.75386/4.80151. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.74706/4.77465. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.76185/4.77092. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.75590/4.80801. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.75350/4.77753. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.74956/4.79299. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.75141/4.79413. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.75564/4.79085. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.74502/4.80349. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.75156/4.79571. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.75170/4.79190. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.74286/4.78884. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.75321/4.78457. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.74698/4.78478. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.74209/4.78591. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.75135/4.81531. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.75154/4.78639. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.74190/4.78277. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.74453/4.78997. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.08952126702661474\n",
      "Epoch 0, Loss(train/val) 5.03240/4.99372. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.99475/5.01351. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.99180/5.03034. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.99424/5.04183. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.99517/5.02952. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.99419/5.00140. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.99142/4.99169. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.98888/4.99623. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.98614/5.00180. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.98760/5.00216. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.98687/4.99955. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.98723/4.99916. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.98707/4.99492. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.98672/4.99552. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.98545/4.98936. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.98487/4.99484. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.98479/4.98469. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.98127/4.98930. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.97801/4.98225. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.98132/4.98092. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.97824/4.97507. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.97987/4.97276. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.97420/4.97975. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.97887/4.98418. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.97680/4.97111. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.97413/4.97012. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.97249/4.96109. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.96945/4.96101. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.96920/4.97171. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.97158/4.96126. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.96726/4.95974. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.96635/4.95000. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.96446/4.95604. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.96741/4.95348. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.96846/4.96340. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.96625/4.95479. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.96287/4.94723. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.96572/4.95478. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.96640/4.96216. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.96958/4.95433. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.96401/4.94549. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.96515/4.94894. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.96113/4.94750. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.96425/4.95345. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.96198/4.95561. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.96478/4.95500. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.96748/4.96896. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.96143/4.95402. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.96306/4.94652. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.96369/4.94380. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.95944/4.94865. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.96156/4.93494. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.95885/4.99182. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.96491/4.95019. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.95536/4.99699. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.96393/4.97158. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.95871/4.97975. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.97231/4.96648. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.96136/4.96814. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.95997/4.96634. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.96022/4.96361. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.96091/4.95610. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.97167/4.96044. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.96406/4.93755. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.95672/4.93625. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.96103/4.95650. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.96913/4.99132. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.96963/4.95707. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.95988/4.96366. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.96343/4.93508. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.95948/4.95911. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.96184/4.96289. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.95725/4.94499. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.95608/4.96790. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.96403/4.94606. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.95730/4.95774. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.95775/4.95622. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.95609/4.95543. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.95945/4.96430. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.95615/4.95556. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.95645/4.93997. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.95947/4.94831. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.96131/4.95155. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.96208/4.94872. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.96111/4.94806. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.95499/4.94987. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.95066/4.95475. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.96527/4.97045. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.96934/4.97134. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.96554/4.96462. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.96773/4.97141. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.96560/4.97006. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.95903/4.95683. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.96609/4.94183. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.95861/4.94408. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.96289/4.97364. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.95531/4.95286. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.95955/4.95494. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.95436/4.96691. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.95773/4.96940. Took 0.19 sec\n",
      "ACC: 0.4375, MCC: -0.1211560890116727\n",
      "Epoch 0, Loss(train/val) 5.24992/5.14820. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.16018/5.13311. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.16040/5.13275. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.16603/5.14088. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.16018/5.14104. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.15493/5.13651. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.16022/5.13599. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.15616/5.13568. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.15645/5.13764. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.15634/5.13676. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.15587/5.13818. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.15485/5.14239. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.15137/5.14156. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 5.15166/5.14331. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 5.15044/5.14113. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.14877/5.14737. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 5.14690/5.14509. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 5.14729/5.15604. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.14686/5.15680. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.15341/5.14663. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 5.14273/5.15896. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 5.13754/5.16334. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.14436/5.15557. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.14426/5.14986. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.14142/5.16720. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.13601/5.16115. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 5.14025/5.16248. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 5.13437/5.16806. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 5.13626/5.16577. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.13634/5.16706. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.13106/5.16933. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.13304/5.17298. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.13950/5.16310. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.13866/5.17423. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.13489/5.18260. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.13438/5.16569. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.13088/5.17700. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.13370/5.17002. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 5.13197/5.18106. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.12636/5.19016. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.12821/5.18186. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 5.12765/5.18581. Took 0.21 sec\n",
      "Epoch 42, Loss(train/val) 5.12468/5.19288. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 5.13165/5.17686. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 5.12779/5.19058. Took 0.20 sec\n",
      "Epoch 45, Loss(train/val) 5.12969/5.16796. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.13049/5.17816. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.12514/5.18136. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 5.12299/5.17944. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.12328/5.17421. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.12477/5.17491. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.12533/5.18259. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.11830/5.19860. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.12217/5.18808. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.13173/5.16272. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.12117/5.19928. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.12276/5.18361. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.12305/5.17101. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.11641/5.20062. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.12193/5.17895. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.12508/5.19042. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 5.12415/5.18725. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.12301/5.18000. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.11587/5.19128. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 5.11747/5.18696. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 5.11957/5.19474. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.12211/5.18054. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 5.11606/5.18977. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.11402/5.18752. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.11834/5.19054. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.11544/5.18878. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 5.12006/5.19817. Took 0.21 sec\n",
      "Epoch 72, Loss(train/val) 5.12121/5.18525. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.11710/5.19149. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 5.11615/5.17848. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 5.11506/5.18686. Took 0.21 sec\n",
      "Epoch 76, Loss(train/val) 5.11217/5.20361. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.11074/5.20509. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 5.11586/5.16928. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.12012/5.18739. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.11865/5.19056. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.11223/5.17218. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.11708/5.17025. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.11093/5.18841. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.11510/5.19890. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.12023/5.17909. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.11557/5.18159. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.11338/5.17526. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 5.11440/5.19230. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.11744/5.17124. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 5.10546/5.19611. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 5.11049/5.18916. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.11139/5.19828. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.11305/5.19310. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 5.11228/5.19473. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.10636/5.21202. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.10966/5.20226. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.11677/5.19412. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.10262/5.21027. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.11042/5.20161. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.15694120514358612\n",
      "Epoch 0, Loss(train/val) 4.93432/4.99444. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.89900/4.86655. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.88212/4.86172. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.86592/4.86421. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.86368/4.86853. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.86459/4.87548. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.86378/4.87510. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.86538/4.87708. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.86016/4.87967. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.86161/4.88139. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.86195/4.87826. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.86109/4.87833. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.86000/4.87647. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.85844/4.88021. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85989/4.88439. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.85584/4.88789. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.85398/4.89193. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85578/4.88665. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85806/4.88193. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.85415/4.88796. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.85340/4.88994. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85392/4.89546. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85100/4.89813. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85253/4.89121. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85239/4.89350. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.85046/4.89869. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.85008/4.89842. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84944/4.90944. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.84548/4.92524. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.85074/4.94007. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84931/4.89743. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84763/4.89081. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.85050/4.89004. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84943/4.89161. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85055/4.90281. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84780/4.88726. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.84923/4.90128. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84696/4.89156. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84490/4.89821. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84462/4.91911. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84173/4.90331. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84617/4.90479. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.84380/4.90100. Took 0.21 sec\n",
      "Epoch 43, Loss(train/val) 4.84002/4.93570. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84642/4.89319. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.83905/4.91497. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84479/4.91556. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.84067/4.91598. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.84373/4.89022. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.83699/4.90434. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.83801/4.93028. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.83689/4.89917. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.83912/4.91745. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.83045/4.91596. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.83272/4.92792. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.83579/4.90070. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.83863/4.91746. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.83312/4.91587. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.83494/4.94076. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.82863/4.91745. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.82697/4.93913. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.83077/4.92423. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.83231/4.93310. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.82809/4.95070. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.83190/4.89821. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.82407/4.93228. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.82783/4.92839. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83476/4.94270. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.82392/4.92597. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.81967/4.95890. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.81909/4.95076. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83037/4.89404. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.83570/4.89650. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.82599/4.91457. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.83248/4.92848. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.81841/4.94871. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.82083/4.94374. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.82242/4.93587. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.82445/4.97420. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.82168/4.92869. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.82805/4.91145. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.81644/4.92896. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.81694/4.93234. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.82027/4.95234. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.81658/4.92887. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.82104/4.92256. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.81694/4.94190. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.82003/4.92661. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.80973/4.93125. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.83896/4.93190. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.82901/4.96707. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82519/4.91850. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82636/4.95119. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82730/4.95736. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82590/4.95431. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.82068/4.93360. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.81317/4.94681. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.80805/4.96551. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.81856/4.95791. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.81321/4.94639. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.09416472308615167\n",
      "Epoch 0, Loss(train/val) 4.75295/4.71639. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.72128/4.71360. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.73212/4.72526. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.73469/4.72776. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.72238/4.71494. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.71314/4.71840. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.71470/4.71496. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.71509/4.71922. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.71424/4.71725. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.71390/4.71538. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.71510/4.71383. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.71124/4.71383. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.71340/4.71833. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.71388/4.71324. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.71139/4.71648. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.71203/4.71587. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.71182/4.71824. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.71123/4.71738. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.71050/4.72925. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.70834/4.72171. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.70957/4.72313. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.70868/4.72752. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.70901/4.74424. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.70882/4.71616. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.70762/4.73131. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.71043/4.71647. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.70786/4.72819. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.70957/4.72339. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.71007/4.72639. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.70785/4.73459. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.70566/4.74525. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.70986/4.71846. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.70608/4.73449. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.70589/4.73205. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.70556/4.73603. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.70534/4.72174. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.70449/4.72871. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.70461/4.73559. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.70521/4.73539. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.70528/4.73829. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.70188/4.74695. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.70495/4.74613. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.70003/4.74554. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.70205/4.75934. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.70482/4.73349. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.70067/4.74681. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.70188/4.75649. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.69807/4.74131. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.70425/4.75123. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.69860/4.75368. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.69575/4.74795. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.69818/4.74602. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.69980/4.74546. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.69711/4.75121. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.69607/4.75579. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.69430/4.76228. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.69180/4.75848. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.69527/4.75872. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.69076/4.77075. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.69446/4.75878. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.69488/4.76967. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.69231/4.77140. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.68648/4.77551. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.69287/4.76561. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.69036/4.77133. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.69181/4.78464. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.69143/4.76758. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.69555/4.75055. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.69454/4.78574. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.69007/4.77785. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.69138/4.79417. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.68967/4.80329. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.68764/4.79380. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.69154/4.77971. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.69090/4.77375. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.68702/4.78621. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.69056/4.79754. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.68728/4.78895. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.68251/4.79890. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.68196/4.80881. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.68593/4.79564. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.68500/4.78555. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.69236/4.78337. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.68617/4.79037. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.68537/4.79465. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.68295/4.80461. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.68451/4.79063. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.67820/4.80955. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.68928/4.79440. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.68145/4.80703. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.68465/4.81070. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.67882/4.80712. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.68554/4.80897. Took 0.18 sec\n",
      "Epoch 93, Loss(train/val) 4.67619/4.81775. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.68153/4.81184. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.68059/4.80568. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.68339/4.81397. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.67969/4.81588. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.68347/4.80207. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.67860/4.80685. Took 0.19 sec\n",
      "ACC: 0.375, MCC: -0.2545139051903111\n",
      "Epoch 0, Loss(train/val) 4.91626/4.87766. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86022/4.86204. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.85776/4.85877. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.85679/4.85912. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.85985/4.85852. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.85995/4.85936. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.86153/4.85980. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.85876/4.86082. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.85782/4.86047. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.85640/4.86251. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.85530/4.86420. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.85732/4.86436. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85520/4.86343. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85577/4.86865. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85548/4.86881. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85869/4.86620. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85534/4.86742. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85428/4.86978. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85383/4.87170. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.85406/4.87501. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.85304/4.87367. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85143/4.88274. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.85228/4.88313. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.85277/4.88139. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.85189/4.88320. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.84975/4.88834. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.85117/4.88745. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84926/4.90218. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.84650/4.89148. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.85017/4.89816. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84753/4.89299. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.84815/4.88978. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.84865/4.89962. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84562/4.90116. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85245/4.87244. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.84817/4.87746. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84951/4.87794. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84831/4.89117. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.84165/4.92677. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.84808/4.90851. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84704/4.91713. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.84417/4.92198. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.84184/4.92280. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.84383/4.94967. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.84083/4.93058. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.83548/4.97157. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.84582/4.90884. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.83856/4.94753. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.83553/4.96113. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.83555/4.96005. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.84668/4.93718. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.83441/4.98369. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.83823/4.95144. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.83280/5.02188. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.84485/4.86841. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.85241/4.85718. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.85308/4.85957. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.84820/4.86752. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.84922/4.86311. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.84492/4.87062. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.84024/4.87015. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84319/4.92392. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.84587/4.91610. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.83571/4.99950. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.84592/4.94746. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.83740/4.93195. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.83927/4.97220. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.83965/4.95553. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.83308/4.95812. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.82899/4.99740. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.83379/4.97286. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.83758/4.93471. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.84050/4.91594. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.82884/4.96178. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.83563/4.98203. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.83711/4.96425. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.83378/4.95170. Took 0.21 sec\n",
      "Epoch 77, Loss(train/val) 4.82788/4.99516. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.83347/4.94865. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.82641/5.02045. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.83234/4.94123. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.83263/4.93189. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.82280/4.99044. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.82449/5.01252. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83033/4.96712. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.82831/4.95377. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.82107/4.99734. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.82680/5.00552. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.83159/4.96799. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.81845/4.99840. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82760/4.95102. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82357/4.98255. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.82119/5.00033. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82196/4.99301. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82349/4.96906. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.82028/4.97935. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82343/4.98883. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81386/5.00352. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.81699/5.06750. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.82567/4.96436. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.02757337634153148\n",
      "Epoch 0, Loss(train/val) 4.90863/4.89851. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.88271/4.87304. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.87782/4.87209. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87577/4.87433. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.87511/4.87858. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.87284/4.88174. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.87795/4.87780. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.87352/4.88018. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.87301/4.88100. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87332/4.88108. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.87446/4.87868. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.87209/4.88729. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.87295/4.88621. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.87133/4.88286. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87233/4.88848. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.87191/4.88575. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.87114/4.88977. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.87102/4.88825. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.87151/4.88485. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.86856/4.88903. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.86681/4.88774. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.86875/4.88341. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.86951/4.88593. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.86864/4.88538. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.86896/4.88844. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.87013/4.89030. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.86697/4.89206. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.87153/4.89035. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.86419/4.89578. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.86470/4.89613. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.86735/4.89069. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86257/4.89876. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.86554/4.89637. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.86221/4.89739. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.86600/4.89976. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.86047/4.90046. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.86214/4.92226. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.86721/4.89735. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.86338/4.90096. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.86370/4.90381. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.86422/4.90229. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.85995/4.91187. Took 0.18 sec\n",
      "Epoch 42, Loss(train/val) 4.86534/4.91760. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.86289/4.89713. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.85928/4.90334. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.85855/4.90308. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.85741/4.92053. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.86520/4.89357. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.85828/4.90350. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.85964/4.91244. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 4.85748/4.91080. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.86530/4.88557. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.87219/4.88066. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.86803/4.88769. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.86653/4.88852. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.86483/4.89628. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.85852/4.89398. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.86709/4.88823. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.86624/4.87707. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.86653/4.86967. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.86810/4.88443. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.87022/4.88942. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.86274/4.90221. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.86372/4.90090. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.86665/4.88675. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.86171/4.89734. Took 0.18 sec\n",
      "Epoch 66, Loss(train/val) 4.86449/4.89213. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.86304/4.89985. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.86608/4.89187. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.86037/4.90588. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.86518/4.90856. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.86234/4.90278. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.85931/4.91242. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.85961/4.91229. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.85825/4.93217. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.86507/4.89884. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.85915/4.92970. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.86430/4.90861. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.85217/4.92270. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.85858/4.90830. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.85962/4.92942. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.85941/4.92607. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.85331/4.91559. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.85583/4.93638. Took 0.18 sec\n",
      "Epoch 84, Loss(train/val) 4.85999/4.91472. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.85545/4.92628. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.85920/4.89240. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.86184/4.89537. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 4.85660/4.92859. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.85747/4.91827. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.85269/4.93632. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85911/4.91549. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.86285/4.89774. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.86039/4.90690. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.85762/4.92139. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.85442/4.93659. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.85990/4.90830. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.85417/4.93124. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.86018/4.92303. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.85163/4.94209. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.09202163616785992\n",
      "Epoch 0, Loss(train/val) 4.81231/4.80305. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.79731/4.80478. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.79446/4.79977. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79304/4.80337. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79410/4.80434. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79305/4.81044. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.79697/4.81174. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79961/4.80505. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.80611/4.81306. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79681/4.81899. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.78960/4.81217. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78849/4.82389. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79091/4.82039. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79148/4.81891. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78928/4.82505. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.79099/4.82689. Took 0.21 sec\n",
      "Epoch 16, Loss(train/val) 4.78953/4.82635. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.78872/4.83269. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.79201/4.82795. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.78841/4.83002. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.78938/4.83449. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.78851/4.83702. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78713/4.84244. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.78499/4.84857. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78276/4.85244. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78636/4.85454. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78607/4.86961. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78368/4.86813. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78463/4.85912. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.78142/4.86371. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 4.78095/4.87018. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.78161/4.85671. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.78195/4.87093. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.77957/4.89158. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.77814/4.88181. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77529/4.88568. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77313/4.88538. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.78042/4.89771. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77197/4.88542. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77836/4.89805. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77849/4.87674. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.77334/4.88296. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76779/4.91329. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.77500/4.87819. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77208/4.88128. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77009/4.88652. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.76882/4.87471. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.77003/4.89455. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.77314/4.89174. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.76626/4.89599. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76382/4.89945. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.76947/4.87848. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.76853/4.89601. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.77227/4.88972. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76466/4.92417. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.76763/4.93507. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.76800/4.87232. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.76086/4.91364. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.76501/4.91455. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.76211/4.95441. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.76810/4.89936. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.76425/4.89006. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.75811/4.90076. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.76344/4.90359. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76119/4.94715. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.76724/4.90777. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75459/4.93390. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.76401/4.85569. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.77217/4.84729. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.77367/4.87552. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.77235/4.87781. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.76753/4.87380. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.76625/4.88078. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.75989/4.91721. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.76379/4.90696. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.76402/4.85308. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.75707/4.87058. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.75571/4.88811. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.76309/4.92100. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.76752/4.91136. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.75790/4.90844. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.75508/4.87785. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.75694/4.91548. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.75477/4.85265. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.76599/4.86330. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.75656/4.89213. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.74597/4.91690. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.75928/4.90141. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.76003/4.88040. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.75427/4.88316. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.75511/4.91384. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.75286/4.91017. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.75935/4.88543. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.74470/4.91867. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.75580/4.94250. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.79016/4.83274. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.77151/4.86013. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.76164/4.87343. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.76484/4.86805. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75957/4.86784. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.0949157995752499\n",
      "Epoch 0, Loss(train/val) 4.73297/4.70335. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.69301/4.67210. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.68329/4.66568. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.68181/4.66874. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.68348/4.66843. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.68349/4.66881. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.68463/4.66933. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.68356/4.67026. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.68450/4.67065. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.68709/4.66973. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.68541/4.66975. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.68514/4.67173. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.68508/4.67501. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.68601/4.67688. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.68271/4.67784. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.68328/4.67752. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.68146/4.67984. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.68402/4.67896. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.68376/4.67741. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.68118/4.67911. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.68204/4.68072. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.67956/4.67717. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.68085/4.67595. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.68091/4.67936. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.68115/4.68094. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.67857/4.68098. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.67773/4.68004. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.68057/4.67956. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.67999/4.67932. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.67828/4.68115. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.68024/4.68227. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.68040/4.68511. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.68093/4.68562. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.68070/4.68474. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.67709/4.68563. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.67752/4.68603. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.67625/4.68522. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.67408/4.68971. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.67728/4.68174. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.67824/4.68258. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.67298/4.69259. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.67851/4.68181. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.67686/4.69435. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.67494/4.69155. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.67433/4.69485. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.67063/4.70493. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.67244/4.69817. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.67385/4.69233. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.67156/4.69611. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.66897/4.71024. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.67036/4.70855. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.66906/4.70589. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.66940/4.69414. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.66795/4.71741. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.67361/4.69927. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.67595/4.69540. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.67542/4.69769. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.66805/4.70666. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.67184/4.70352. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.67171/4.69598. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.66886/4.71931. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.66967/4.70395. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.66794/4.71007. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.66676/4.69921. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.66594/4.72524. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.67351/4.69216. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.66758/4.71121. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.66302/4.70999. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.66391/4.70203. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.66467/4.70969. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.66123/4.70692. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.66529/4.70475. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.66461/4.71086. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.66616/4.69546. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.66785/4.71583. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.66803/4.70019. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.66708/4.70995. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.66479/4.71279. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.66764/4.69932. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.66267/4.70785. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.66111/4.71313. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.66535/4.70184. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.66381/4.71483. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.66421/4.70772. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.66484/4.71953. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.66454/4.71239. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.66420/4.70420. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.66314/4.70918. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.66429/4.70805. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.66070/4.70264. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.66039/4.71695. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.66073/4.70855. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.65808/4.71403. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.66381/4.70198. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.66515/4.71433. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.66183/4.70453. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.66165/4.70507. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.65846/4.70480. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.66190/4.70560. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.66366/4.72304. Took 0.20 sec\n",
      "ACC: 0.421875, MCC: -0.1777495495783369\n",
      "Epoch 0, Loss(train/val) 4.91589/4.94974. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.91952/4.88727. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.90013/4.89124. Took 0.21 sec\n",
      "Epoch 3, Loss(train/val) 4.88664/4.88760. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.88752/4.88685. Took 0.21 sec\n",
      "Epoch 5, Loss(train/val) 4.89225/4.88597. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.88719/4.89051. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.88872/4.88563. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.88754/4.88550. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.88901/4.88608. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.88349/4.88834. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.88455/4.88598. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.88253/4.88685. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.88546/4.88508. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.88145/4.88464. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.88353/4.88644. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.87990/4.88762. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.88479/4.88440. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.87775/4.88597. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.88148/4.88873. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.88184/4.87672. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.88068/4.88975. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.87735/4.88987. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.87639/4.89490. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.87327/4.89363. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.87879/4.89499. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.87953/4.89632. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.87192/4.89707. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.87092/4.90053. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.87104/4.90981. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.87335/4.90221. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86978/4.90405. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.86486/4.90746. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.87606/4.89917. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.87131/4.90165. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.86538/4.91746. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.87278/4.90872. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.87022/4.90993. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.86710/4.91227. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.87109/4.90572. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.86305/4.92390. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.86837/4.90729. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.86854/4.91124. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.87021/4.91173. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.86285/4.93941. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.86980/4.91084. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.86845/4.92323. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.86095/4.92745. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.86354/4.92107. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.85879/4.93691. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.86710/4.90680. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.86552/4.90881. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.86660/4.89781. Took 0.21 sec\n",
      "Epoch 53, Loss(train/val) 4.86005/4.91959. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.86250/4.92081. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.86474/4.90862. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.85929/4.91991. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.85824/4.91547. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.86366/4.90791. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.85703/4.93022. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.85486/4.93800. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.85778/4.92209. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.85003/4.93259. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.86039/4.90234. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.85616/4.91294. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.85708/4.93658. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.85153/4.94829. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.86388/4.90530. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.85721/4.92655. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.85199/4.92700. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.85671/4.91251. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.85467/4.92663. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.85573/4.91438. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.85299/4.93644. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.86050/4.90256. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.85862/4.91816. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.85245/4.92569. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.85482/4.92865. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.85756/4.91744. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.85138/4.94435. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.85615/4.92344. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.84103/4.95426. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.85488/4.90178. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.85347/4.90913. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.85420/4.92865. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.84937/4.93248. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.84749/4.92410. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.84655/4.92798. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.84363/4.95537. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.85060/4.92474. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.84656/4.94060. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85273/4.95253. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.84314/4.93887. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.84175/4.95591. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.85094/4.91453. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.85633/4.93901. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.84188/4.96284. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.85170/4.95188. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.84657/4.93467. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.84978/4.95285. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.10677275981195139\n",
      "Epoch 0, Loss(train/val) 4.87330/4.82425. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.83845/4.82310. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.83907/4.82606. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.84157/4.83513. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.84406/4.83087. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.83772/4.82181. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.83332/4.82072. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.83134/4.81843. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.83035/4.81780. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.83283/4.82048. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.82842/4.83342. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.82961/4.82385. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.82521/4.81409. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.82880/4.82277. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.82569/4.82163. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.82634/4.82575. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.82274/4.82813. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.82415/4.82161. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.82794/4.82702. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.82771/4.82455. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.82266/4.83352. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.82238/4.83009. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.81973/4.83402. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.82305/4.84032. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82631/4.83384. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82378/4.83823. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.81884/4.82500. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.82081/4.83014. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.82036/4.82675. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.82124/4.84047. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.81792/4.83088. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.81954/4.83768. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.81609/4.84256. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.81733/4.84170. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.81992/4.83870. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82290/4.83753. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.81535/4.84250. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.81467/4.84512. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.81953/4.84702. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.81777/4.85155. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.81825/4.83531. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.81271/4.85795. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.81413/4.84151. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.81196/4.85437. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.81424/4.83732. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.81268/4.85240. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.81264/4.84417. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.81146/4.85143. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.81700/4.82980. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 4.81624/4.84191. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.81403/4.84723. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.81251/4.84169. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81202/4.85205. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.81380/4.84890. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81472/4.84569. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.81122/4.85400. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.80470/4.86008. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.81238/4.84360. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.80931/4.85643. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.81029/4.85739. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.80937/4.85990. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.80856/4.86749. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.80818/4.84312. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.80840/4.85132. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.80895/4.85847. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81025/4.85534. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.81126/4.84101. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.80474/4.84875. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.80647/4.85089. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.80609/4.85891. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.80262/4.85940. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.80614/4.86342. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.80673/4.84008. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.80678/4.85695. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81219/4.83831. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.80169/4.86093. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.80586/4.84359. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80210/4.86004. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.81043/4.84338. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.80728/4.82109. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.80821/4.85087. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.80754/4.84115. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.80524/4.84886. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80663/4.84266. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.80491/4.84047. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.80107/4.85821. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.79814/4.85768. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.80881/4.84256. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.80359/4.87017. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.79934/4.85669. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.80697/4.85763. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.80188/4.82057. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.80235/4.84191. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.80070/4.87352. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.80015/4.84799. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.80087/4.84919. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.80288/4.85690. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.80503/4.85537. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.79882/4.82845. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.79610/4.85039. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.03779786830012622\n",
      "Epoch 0, Loss(train/val) 4.88503/4.86211. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.84334/4.84148. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.83627/4.84057. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.83481/4.84369. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.83748/4.84040. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.83377/4.84213. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.83241/4.84168. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.83499/4.84144. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.83509/4.84196. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.83421/4.84354. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.83157/4.84403. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.83199/4.84407. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.82938/4.84502. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.83113/4.85339. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.82943/4.83903. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.82895/4.84549. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.82753/4.83863. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.82507/4.84136. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.82185/4.84228. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.82455/4.84337. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.81666/4.84418. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.82455/4.84944. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.81928/4.85211. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.82289/4.84035. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82365/4.85747. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82322/4.84099. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.81803/4.85751. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.81864/4.85073. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.81694/4.85190. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.81420/4.86145. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.81601/4.85122. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.81567/4.85628. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.81475/4.85986. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.81560/4.85945. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.81726/4.86126. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.81596/4.85436. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.81603/4.85766. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.81654/4.85500. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.81385/4.86168. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.81326/4.85584. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.81673/4.84015. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.81203/4.83708. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.81207/4.83875. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.80969/4.83808. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.80712/4.84753. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.80684/4.84502. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.81204/4.83515. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.81222/4.83172. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.81430/4.84134. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.81044/4.85551. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.81978/4.84242. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.80724/4.84457. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.82755/4.83486. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.81759/4.83156. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81587/4.84292. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.80794/4.83832. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.80721/4.83398. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.81014/4.82431. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81101/4.82406. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.81096/4.83481. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.80925/4.82770. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81203/4.83252. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.80822/4.83896. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.80514/4.84312. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.80715/4.83955. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.80035/4.82872. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.81339/4.82884. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.80745/4.85467. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.80108/4.85007. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.80497/4.83809. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.79960/4.84077. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.80419/4.83874. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.81132/4.83933. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.80261/4.82870. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.80636/4.84522. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.79861/4.83016. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.79970/4.84441. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80568/4.82797. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.79532/4.82259. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.81625/4.83308. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.79851/4.82757. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.79944/4.83217. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.79679/4.82610. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.79648/4.84421. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.80566/4.85743. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.79748/4.84490. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.79467/4.85419. Took 0.20 sec\n",
      "Epoch 87, Loss(train/val) 4.79413/4.85328. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.79226/4.84084. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.78719/4.88767. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82097/4.83565. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.80171/4.86470. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.80663/4.83524. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.79824/4.83211. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.79879/4.83294. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79040/4.84544. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.79713/4.84191. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.79400/4.85277. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.79191/4.85404. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.79117/4.83425. Took 0.19 sec\n",
      "ACC: 0.5625, MCC: 0.1211560890116727\n",
      "Epoch 0, Loss(train/val) 4.99248/4.90826. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.91481/4.91154. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.91110/4.90895. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.91199/4.90870. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.91311/4.90638. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.91054/4.91049. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.91276/4.90542. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.90754/4.90157. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.90705/4.90229. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.90252/4.90451. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.90588/4.90183. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.90260/4.90871. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.90078/4.90924. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.90334/4.90301. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.90491/4.89712. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.90016/4.89538. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90335/4.89861. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.89957/4.89885. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.89576/4.90251. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.90234/4.89826. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.89864/4.89816. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.89870/4.89737. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.89614/4.90345. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.89902/4.90368. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.89519/4.90385. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.89507/4.90715. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.89476/4.91093. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.89393/4.91026. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.89713/4.91542. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.89502/4.90022. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.89743/4.90272. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.89662/4.90288. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.89033/4.91706. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.89630/4.90878. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.89383/4.91129. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.88982/4.91599. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.88637/4.91289. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.89020/4.91068. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.88878/4.91922. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.88711/4.91633. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.88928/4.91418. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.88653/4.91942. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89570/4.90360. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.89066/4.90875. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.88370/4.92676. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.88618/4.91557. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.88057/4.91769. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.88426/4.91910. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.88778/4.91708. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.88406/4.91211. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.88689/4.91170. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.88629/4.92747. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.88469/4.91619. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.87941/4.93113. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.87826/4.92870. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.88434/4.92141. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.88012/4.94045. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.88384/4.92082. Took 0.18 sec\n",
      "Epoch 58, Loss(train/val) 4.88010/4.92242. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.88704/4.92668. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.88497/4.92126. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.87921/4.93444. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.87715/4.92515. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.87488/4.94583. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.87514/4.94015. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.88389/4.93208. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.88237/4.93072. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.88058/4.92673. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.87780/4.93907. Took 0.20 sec\n",
      "Epoch 69, Loss(train/val) 4.87648/4.93485. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.87556/4.94940. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.88334/4.93616. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.87748/4.94180. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88255/4.93124. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.87332/4.94768. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.87756/4.92745. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.88149/4.94325. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.87933/4.93571. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.87947/4.94037. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.87863/4.93809. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.87014/4.93923. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.87882/4.92955. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.87143/4.94440. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.87607/4.94090. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.87480/4.92417. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.87261/4.95051. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.87287/4.94690. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.87227/4.95964. Took 0.18 sec\n",
      "Epoch 88, Loss(train/val) 4.86882/4.94400. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.87793/4.94480. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.87592/4.93632. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.87245/4.94082. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.86720/4.96170. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.87196/4.94779. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.87154/4.96347. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.87312/4.94338. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.86519/4.95370. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.87093/4.94718. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.87231/4.95223. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.87070/4.95478. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.21411361031120882\n",
      "Epoch 0, Loss(train/val) 4.86567/4.86225. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.83667/4.83117. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.83545/4.84349. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.83278/4.84169. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.83134/4.84452. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.83095/4.84438. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.82942/4.83876. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.83261/4.83964. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.83086/4.83382. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.83026/4.82952. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.83177/4.82440. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.83492/4.83543. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.83883/4.83492. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.83167/4.83302. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.83026/4.83394. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.82697/4.83511. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.82977/4.83536. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.82803/4.83527. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.82838/4.83303. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.82930/4.83173. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.82552/4.83653. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.83134/4.82421. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.82843/4.82540. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.82657/4.82989. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.82763/4.83306. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82512/4.82977. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82983/4.82169. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.82700/4.81782. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.82145/4.82460. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.82150/4.82898. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.82484/4.82647. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.82122/4.82902. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.82036/4.83990. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.81872/4.84404. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.81945/4.82065. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.81855/4.83119. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.81461/4.83319. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.81656/4.82349. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.81540/4.83217. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.81370/4.83588. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.81613/4.84464. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.81509/4.82581. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.81497/4.84958. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.81165/4.84701. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.81329/4.85483. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.81722/4.84655. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.80600/4.86147. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.81178/4.82927. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.81297/4.84438. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.81182/4.85420. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.80855/4.86303. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.80674/4.87342. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81031/4.84340. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.81277/4.85942. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.81971/4.83763. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.81541/4.85113. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.81506/4.84258. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.81255/4.84528. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81229/4.85409. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.81462/4.84221. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.81380/4.85433. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.81046/4.85424. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.80920/4.85953. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.81283/4.85068. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.80680/4.85003. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.80846/4.85101. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.81049/4.85242. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.81148/4.85523. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.80857/4.85676. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.80983/4.86413. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.80842/4.85471. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.80737/4.85290. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.80406/4.88566. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.80941/4.85963. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.80117/4.86478. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.80848/4.85510. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.80856/4.86445. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80982/4.85675. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.80292/4.86078. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.80316/4.85922. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.80830/4.84396. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.80282/4.85225. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.80376/4.84727. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80593/4.85074. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.80410/4.85602. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.80454/4.86233. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.80627/4.85405. Took 0.21 sec\n",
      "Epoch 87, Loss(train/val) 4.80330/4.85270. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.81703/4.84304. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.81393/4.86527. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.80681/4.86283. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.80919/4.85805. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.80678/4.86923. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.80841/4.85260. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.81058/4.85696. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.81300/4.85830. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.80552/4.86376. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.80611/4.86205. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.80760/4.87053. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.80480/4.86877. Took 0.19 sec\n",
      "ACC: 0.40625, MCC: -0.20556755962059384\n",
      "Epoch 0, Loss(train/val) 5.21338/5.12244. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.14495/5.15054. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 5.13986/5.13305. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 5.13490/5.12802. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.13296/5.13112. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.13198/5.13569. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.13370/5.13781. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.12930/5.14111. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 5.12968/5.13425. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.12658/5.14643. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.12516/5.14689. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 5.12239/5.15322. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 5.12361/5.14809. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.12215/5.14992. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 5.11561/5.15685. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 5.12146/5.15048. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 5.12041/5.14768. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 5.11443/5.15108. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 5.11770/5.15684. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 5.12342/5.13035. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 5.11680/5.15373. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 5.11728/5.14577. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 5.11630/5.13677. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 5.10829/5.16217. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 5.11950/5.12923. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 5.12221/5.11534. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 5.11616/5.12887. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 5.11150/5.14733. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 5.11006/5.15290. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 5.11298/5.17526. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 5.12098/5.12639. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 5.11981/5.13605. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 5.11704/5.15207. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 5.10941/5.15572. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 5.11375/5.15777. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 5.10792/5.16580. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 5.10909/5.17040. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 5.10633/5.17344. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 5.10095/5.18118. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 5.11186/5.16231. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 5.10653/5.15652. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 5.09757/5.19588. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 5.10549/5.18707. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 5.10557/5.17890. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 5.10201/5.18767. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 5.09655/5.20001. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 5.09895/5.18955. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 5.09892/5.17958. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 5.10587/5.20984. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 5.10817/5.16349. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 5.10000/5.17916. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 5.09462/5.18259. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 5.09931/5.18661. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 5.09552/5.18614. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 5.10045/5.18369. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 5.10126/5.18457. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 5.09375/5.19848. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 5.09413/5.19956. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 5.09292/5.18179. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 5.10094/5.21531. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 5.10203/5.17953. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 5.09990/5.19476. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 5.09736/5.19678. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 5.09553/5.19905. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 5.09725/5.18741. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 5.10225/5.16399. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 5.10958/5.17247. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 5.10361/5.16707. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 5.10022/5.18357. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 5.09664/5.18832. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 5.09350/5.18780. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 5.09629/5.17622. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 5.09693/5.17505. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 5.09955/5.17336. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 5.09865/5.18452. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 5.09648/5.20895. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 5.09790/5.18084. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 5.09138/5.19875. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 5.09007/5.19615. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 5.09087/5.20907. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 5.09758/5.18779. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 5.09368/5.19991. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 5.09732/5.22097. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 5.09711/5.20213. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 5.09030/5.18985. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 5.09493/5.16896. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 5.09592/5.17131. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 5.09528/5.17921. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 5.08677/5.19991. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 5.08948/5.19719. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 5.08403/5.20620. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 5.08274/5.20224. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 5.09150/5.18144. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 5.08681/5.20609. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 5.08525/5.22159. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 5.08930/5.19324. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 5.08718/5.20525. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 5.08637/5.19237. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 5.08481/5.20542. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 5.08133/5.17618. Took 0.20 sec\n",
      "ACC: 0.46875, MCC: -0.051799406700962544\n",
      "Epoch 0, Loss(train/val) 5.06746/4.98595. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.99992/4.99142. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.99622/5.00968. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.99567/5.00871. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.99584/5.01274. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.99422/5.01418. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.99566/5.01592. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.99584/5.01252. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.99531/5.00733. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.99374/5.00147. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.99541/4.99324. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.99171/4.98979. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.99038/4.99090. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.98890/4.98967. Took 0.21 sec\n",
      "Epoch 14, Loss(train/val) 4.98938/4.98916. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.98865/4.99539. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.98454/5.00633. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.99235/5.00049. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.98846/4.99980. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.98958/4.98694. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.98724/4.99567. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.98572/4.99680. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.98499/5.00010. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.98381/5.00209. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.98297/5.01571. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.98291/5.00585. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.98577/4.99147. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.98247/5.00705. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.98451/4.99698. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.97922/5.01007. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.98046/5.01526. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.98298/5.00523. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.98312/5.00765. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.98198/5.01518. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.97893/5.01939. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.98200/5.01708. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.97557/5.02598. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.98006/4.97907. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.97772/5.00430. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.98112/4.99826. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.97903/5.02953. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.97944/5.01200. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.98027/5.01202. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.97503/5.01806. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.97637/5.01344. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.97924/5.00475. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.97915/5.02242. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.97272/5.02718. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.97461/5.02647. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.98149/5.00043. Took 0.18 sec\n",
      "Epoch 50, Loss(train/val) 4.99017/4.98657. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.99203/4.98040. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.98269/4.99885. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.97931/4.99336. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.98583/4.99653. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.98721/4.99188. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.98309/4.99258. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.98458/4.99356. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.98079/4.99214. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.98099/4.99444. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.98120/4.99722. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.98399/5.00501. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.97984/5.00859. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.98194/5.00150. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.98118/5.00988. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 4.98372/5.00640. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.97845/5.00192. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.98191/4.99874. Took 0.18 sec\n",
      "Epoch 68, Loss(train/val) 4.97936/5.00098. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.97523/5.01426. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.97365/5.01518. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.97611/5.00074. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.98114/5.00706. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.97625/5.00603. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.97456/5.01006. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.97901/4.99657. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.97676/4.99921. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 4.97503/5.00997. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.97464/5.01123. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.97764/5.01826. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.97656/5.00790. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.97328/5.01004. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.97922/5.00491. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.97326/5.00683. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.96872/5.03556. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.97393/5.02025. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.97524/5.01519. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.96766/5.03028. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.97174/5.01717. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.98232/5.00511. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.98107/5.00444. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.97941/5.00180. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.97848/5.01139. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.97508/5.01480. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.97115/5.01566. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 4.97542/5.02973. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.96965/5.02883. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.96700/5.03418. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.97354/5.03249. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.97497/5.01884. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.02757337634153148\n",
      "Epoch 0, Loss(train/val) 4.94001/4.93433. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.86790/4.87147. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.86350/4.86882. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.86415/4.87005. Took 0.18 sec\n",
      "Epoch 4, Loss(train/val) 4.86701/4.87297. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.86297/4.87390. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.86718/4.87343. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.86039/4.87952. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.86234/4.88056. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.86086/4.88712. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.85913/4.89730. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.85084/4.90637. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.85771/4.90493. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.85157/4.90843. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.85291/4.91722. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.85139/4.91184. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.85388/4.92775. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.85347/4.92492. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.85031/4.93899. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.84744/4.94351. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.84775/4.94874. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.85053/4.93514. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.84816/4.94800. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.84682/4.93798. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.84824/4.94753. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.84439/4.95084. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.84438/4.94392. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.84599/4.94238. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.84431/4.95167. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.84749/4.93951. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.84351/4.94160. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.84730/4.93934. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.84424/4.95060. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.84425/4.95222. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.84206/4.95216. Took 0.18 sec\n",
      "Epoch 35, Loss(train/val) 4.84463/4.93660. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.84214/4.95026. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.84097/4.94601. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.83919/4.94521. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.83313/4.95499. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.84294/4.93198. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.84106/4.94348. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.83556/4.95998. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.83677/4.94693. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.82984/4.97933. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.83704/4.94762. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.83453/4.96289. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.83745/4.94153. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.84012/4.95884. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.83597/4.94979. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.83558/4.95870. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.83295/4.96926. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.83380/4.95058. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.83366/4.96265. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.83276/4.94130. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.82718/4.97785. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.82653/4.96521. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.82805/4.95563. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.83601/4.94639. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.83714/4.95459. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.82899/4.98671. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.82743/4.96755. Took 0.18 sec\n",
      "Epoch 62, Loss(train/val) 4.82808/4.94438. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.82814/4.95762. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.82575/4.95465. Took 0.18 sec\n",
      "Epoch 65, Loss(train/val) 4.82570/4.97505. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.82423/4.95498. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.83240/4.93827. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.82808/4.95940. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.83101/4.92561. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.82977/4.94543. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.82422/4.96802. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.82563/4.96746. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.82875/4.93701. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.82383/4.95320. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.82470/4.93860. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.81983/4.96233. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.82273/4.96362. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.82516/4.95103. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.82005/4.96297. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 4.82914/4.95236. Took 0.18 sec\n",
      "Epoch 81, Loss(train/val) 4.84230/4.96516. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.83921/4.95858. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84163/4.95258. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.83039/4.95899. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.83384/4.95447. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.83663/4.94057. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.83376/4.92709. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 4.83430/4.94627. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.82643/4.94779. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.82589/4.92829. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.82223/4.94746. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.81563/4.95148. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.82459/4.95976. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.82004/4.94578. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 4.81918/4.93490. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.82221/4.95708. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.81707/4.96671. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.82122/4.95722. Took 0.18 sec\n",
      "Epoch 99, Loss(train/val) 4.81925/4.92592. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.025861699363244256\n",
      "Epoch 0, Loss(train/val) 4.96239/4.99331. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.94586/4.98910. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.93900/5.03959. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.95824/4.97901. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.95250/4.93196. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.92880/4.94076. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.92890/4.94713. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.93418/4.94410. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.92870/4.94358. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.92816/4.95047. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.92707/4.96432. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.93067/4.92853. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.92988/4.93750. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.92799/4.96945. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.92548/4.95089. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.92327/4.95840. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.92576/4.95000. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.92256/4.98527. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.92422/4.94630. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.92197/4.98347. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.92060/4.96884. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.91905/4.98830. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.91922/4.97763. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.91820/4.98193. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.91840/4.98196. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.91737/4.97363. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.91652/4.98494. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.91760/4.98352. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.91683/4.98832. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.91254/4.98720. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.91588/4.97957. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.91541/4.98558. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.91759/4.98415. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.91226/4.99679. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.91768/4.99106. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.91534/4.98192. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.91351/4.98599. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.91465/4.99606. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.92067/4.96079. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.91552/4.98684. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.91706/4.98352. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.91412/4.98061. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.91564/4.99052. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.91267/4.98792. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.91162/4.99124. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.91288/4.98096. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.91590/4.98838. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.91225/4.98685. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.91182/4.99640. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.91328/4.98292. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.91219/4.99593. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.91190/4.99464. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.91425/4.99610. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.91429/4.98494. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.91437/5.00528. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.91255/4.98221. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.91222/4.99133. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.91110/5.00162. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.90880/4.99389. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.91275/5.00097. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.91147/4.98394. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.91029/5.00594. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.90974/4.98752. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.91260/5.00356. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.90952/5.00510. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.91253/4.99068. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.91115/4.99280. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.90817/5.00790. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.91183/4.98395. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.90838/4.99507. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.90951/5.01600. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.91007/4.99771. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.91155/4.98284. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.91113/5.00840. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.90583/4.99274. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.91220/5.00236. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.90822/4.99528. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.91371/4.99754. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.91133/4.99894. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.91225/4.99180. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.90930/4.99757. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.90881/5.00638. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.90786/5.00481. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.90903/4.99696. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.90922/5.00631. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.90911/5.00026. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.90742/5.02461. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.90585/5.02335. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.90624/4.99899. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.90941/5.01648. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.90809/5.00357. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.90607/4.99649. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.90502/5.03377. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.90895/4.98907. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.90849/4.99891. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.90439/4.99586. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.90775/4.99324. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.90765/5.00562. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.90303/5.00201. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.90625/5.01391. Took 0.19 sec\n",
      "ACC: 0.59375, MCC: 0.19088542889273333\n",
      "Epoch 0, Loss(train/val) 4.68944/4.68441. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.66866/4.67420. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.66311/4.67681. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.66434/4.67668. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.66167/4.68009. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.65987/4.67911. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.66145/4.68102. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.66373/4.68147. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.66379/4.68510. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.66268/4.68520. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.65940/4.68612. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.65719/4.68658. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.66079/4.68440. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.65797/4.68830. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.65714/4.69053. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.65779/4.68523. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.65578/4.69045. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.65925/4.68523. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.65840/4.68117. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.65701/4.67970. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.65358/4.68023. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.65429/4.68401. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.65821/4.68351. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.65515/4.67891. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.65553/4.68088. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.65053/4.68578. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.65503/4.68349. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.65644/4.68834. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.65415/4.68799. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.65255/4.68881. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.65181/4.69257. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.65052/4.69663. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.65624/4.68825. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.65553/4.67027. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.65962/4.66706. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.65762/4.66909. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.65694/4.66834. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.65612/4.67095. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.65312/4.67724. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.65057/4.67281. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.65477/4.67119. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.65299/4.67522. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.65278/4.67749. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.65530/4.67788. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.64915/4.67453. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.65159/4.67433. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.64440/4.67168. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.65205/4.68630. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.65267/4.68178. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.65958/4.67515. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.66073/4.67018. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.65799/4.67424. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.65483/4.68046. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.65593/4.68377. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.65717/4.68306. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.65743/4.68275. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.65320/4.68278. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.64870/4.67719. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.65139/4.67997. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.64766/4.68333. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.65508/4.69115. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.65383/4.68628. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.65423/4.67548. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.65569/4.67793. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.65105/4.68423. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.65705/4.68368. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.65154/4.68164. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.65100/4.67784. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.64899/4.68194. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.65122/4.67409. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.64712/4.67434. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.64955/4.68086. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.64238/4.68054. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.64921/4.67985. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.64725/4.68268. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.64182/4.68490. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.64919/4.67787. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.64893/4.68307. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.64575/4.68115. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.64940/4.69512. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.64422/4.67768. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.64409/4.68845. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.64216/4.69638. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.64282/4.69843. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.64571/4.68625. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.64841/4.68996. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.64167/4.68988. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.64864/4.69308. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.64235/4.68774. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.64493/4.68920. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.64110/4.69025. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.64429/4.66336. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.64891/4.66907. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.64721/4.68860. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.64548/4.68359. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.64208/4.68826. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.64152/4.69195. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.64360/4.69299. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.64151/4.69509. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.64365/4.69440. Took 0.19 sec\n",
      "ACC: 0.5, MCC: 0.0\n",
      "Epoch 0, Loss(train/val) 4.85776/4.77729. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.79926/4.77834. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.80803/4.79273. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.80748/4.78559. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79908/4.77155. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79010/4.76970. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.78876/4.77011. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79148/4.77085. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.78907/4.76960. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79020/4.77126. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.78993/4.77130. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78595/4.77041. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.78983/4.77045. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.78729/4.77324. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78719/4.77216. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78565/4.77232. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78623/4.77042. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78649/4.77063. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78405/4.77230. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78430/4.77377. Took 0.21 sec\n",
      "Epoch 20, Loss(train/val) 4.78228/4.77818. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78407/4.77730. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78134/4.77712. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78066/4.77309. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.77913/4.78314. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78030/4.78103. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78316/4.78217. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.77456/4.78396. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78060/4.78489. Took 0.18 sec\n",
      "Epoch 29, Loss(train/val) 4.77510/4.78010. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.77715/4.78731. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77766/4.78137. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.77628/4.78219. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.77682/4.78476. Took 0.20 sec\n",
      "Epoch 34, Loss(train/val) 4.77462/4.78576. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77278/4.79519. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77125/4.79878. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.77337/4.79293. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.77516/4.80405. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77458/4.79691. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77489/4.79927. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 4.76678/4.81475. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.77235/4.79940. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.76946/4.79794. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77313/4.83265. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77672/4.80315. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77357/4.81640. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.76997/4.80764. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.77110/4.81457. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.76921/4.82674. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.76540/4.80939. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.77125/4.80569. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.76645/4.81983. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76756/4.82371. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76747/4.80807. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.76560/4.81331. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.76637/4.81519. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76737/4.80544. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.75972/4.81650. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.76022/4.82125. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.76572/4.81957. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.76297/4.80999. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.75752/4.82149. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.76490/4.81683. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76502/4.81217. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.75524/4.82482. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75711/4.82510. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.76778/4.80670. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.76180/4.81466. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.75694/4.83118. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.75994/4.83539. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.76530/4.82737. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.75416/4.82337. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.75365/4.84074. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.76314/4.82998. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.76103/4.83376. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.75177/4.83282. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.75810/4.83133. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.75473/4.84073. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.75697/4.83026. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.75042/4.83887. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.74877/4.84464. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.75222/4.83972. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.75270/4.83567. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.75853/4.81293. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.75126/4.84441. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.75538/4.83312. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.74624/4.84159. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.75092/4.85314. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.76218/4.82270. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.74441/4.85571. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.74939/4.85710. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.75035/4.83044. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.75064/4.82579. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.74872/4.80769. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.74486/4.83264. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.75944/4.83648. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.75414/4.81398. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.75036/4.83767. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.74348/4.84026. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.0629940788348712\n",
      "Epoch 0, Loss(train/val) 4.97330/4.86460. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.87873/4.85695. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.87793/4.85790. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.87967/4.86379. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.87777/4.86458. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.87757/4.87178. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.87686/4.87534. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.87515/4.87365. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.87759/4.88003. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87585/4.87635. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.87500/4.87401. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.87078/4.87172. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.87326/4.87167. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.86927/4.87788. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87017/4.87147. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.87182/4.86825. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.87187/4.87338. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.87097/4.87194. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.86880/4.87280. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.86831/4.87819. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.87185/4.86561. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.87435/4.87570. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.86976/4.87315. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.86859/4.87293. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.86664/4.87379. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.86561/4.86979. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.86657/4.87188. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.86870/4.87475. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.86652/4.87806. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.86546/4.87795. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.86782/4.88439. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86678/4.88318. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.86249/4.89186. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.86488/4.87240. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.86222/4.88337. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.86239/4.88144. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.86216/4.89882. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.86046/4.88873. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.86364/4.88180. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.86452/4.86889. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.86033/4.88081. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.86622/4.88276. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.85875/4.87753. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.85893/4.86997. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.85979/4.86769. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.85974/4.88220. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.85933/4.87470. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.85265/4.86670. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.86069/4.87557. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.85633/4.88496. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.85945/4.86479. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.85727/4.87274. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.85180/4.86588. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.85494/4.85807. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.86621/4.86145. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.86098/4.85297. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.85983/4.85584. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.84964/4.85423. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.85546/4.84499. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.85652/4.87010. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85600/4.84651. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.85370/4.84767. Took 0.20 sec\n",
      "Epoch 62, Loss(train/val) 4.85518/4.83405. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.85143/4.84307. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.85589/4.86617. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.84704/4.84390. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.85257/4.85310. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.85382/4.86852. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.85216/4.85293. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84654/4.85378. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.85136/4.84212. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.85603/4.84639. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.84736/4.87212. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84688/4.86628. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.85518/4.86548. Took 0.18 sec\n",
      "Epoch 75, Loss(train/val) 4.84677/4.83841. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.85342/4.85326. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.84700/4.84093. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.85144/4.85444. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.85418/4.84947. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.85021/4.83220. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.85044/4.86375. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.84759/4.84928. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84774/4.84035. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.84881/4.84705. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.85618/4.84679. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.85016/4.84828. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.84799/4.85394. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.84536/4.84432. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.83325/4.83758. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.84560/4.85434. Took 0.21 sec\n",
      "Epoch 91, Loss(train/val) 4.84916/4.83151. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.84625/4.85300. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.84518/4.83143. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.84250/4.84430. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.84857/4.85737. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.83958/4.83419. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.84846/4.84522. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.84185/4.83805. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.85191/4.85746. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.1014574359634967\n",
      "Epoch 0, Loss(train/val) 4.91086/4.82023. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.79477/4.79811. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.79390/4.79629. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.79275/4.79979. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79202/4.80058. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.79284/4.79976. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.79197/4.79429. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79093/4.79190. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79217/4.79289. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79230/4.79540. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79041/4.78932. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.78988/4.79024. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79082/4.79014. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.78729/4.78840. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78411/4.78901. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.78504/4.78743. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.78472/4.78090. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78815/4.79126. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78524/4.79436. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78280/4.79020. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.78197/4.79302. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.78550/4.79705. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.77829/4.83360. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78872/4.81753. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.78238/4.80899. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.78145/4.81345. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78116/4.80778. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.77810/4.82575. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.78288/4.81726. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.78124/4.82031. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.77649/4.82444. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.77790/4.81296. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.78041/4.80718. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.77893/4.80947. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.77650/4.81064. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77917/4.81379. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77786/4.81416. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.77620/4.81172. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77270/4.82323. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77283/4.81812. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.77704/4.81445. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.77241/4.80892. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.76987/4.80073. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.77740/4.82096. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77161/4.81703. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.77360/4.81634. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.77084/4.81697. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.77612/4.84059. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.77671/4.79972. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.76561/4.81632. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76929/4.82705. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.77158/4.81804. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.76844/4.82584. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76642/4.81545. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.76501/4.80874. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.77057/4.81131. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.77034/4.80937. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76353/4.80944. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.76319/4.83856. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.76482/4.82025. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.76739/4.79327. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.76171/4.82668. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.76159/4.80154. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.76343/4.82215. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.76515/4.78650. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.76624/4.80831. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.76181/4.81027. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.76339/4.80025. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.76164/4.79360. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.76086/4.81039. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.75984/4.80096. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.76935/4.81718. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.76624/4.80660. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.76367/4.79296. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.76004/4.81008. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.75899/4.80761. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.75318/4.81031. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.75836/4.79878. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.76217/4.81633. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.76164/4.80763. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.76166/4.80436. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.75949/4.80883. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.76347/4.81958. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.75734/4.81206. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.75569/4.81399. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.75866/4.83127. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.75660/4.82047. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.75423/4.80607. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.75539/4.83130. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.75823/4.81847. Took 0.21 sec\n",
      "Epoch 90, Loss(train/val) 4.75354/4.81424. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.75726/4.81106. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.75709/4.82035. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.75478/4.82717. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.75956/4.81179. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.75174/4.79771. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.76012/4.81516. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.76075/4.80816. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.75566/4.81405. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.75029/4.82111. Took 0.19 sec\n",
      "ACC: 0.359375, MCC: -0.2506422767984621\n",
      "Epoch 0, Loss(train/val) 4.89686/4.81362. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.80300/4.80251. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.79517/4.80237. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.79711/4.80324. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.79792/4.80667. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.79500/4.80599. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.79513/4.80550. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.79447/4.80871. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.79538/4.81090. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.79777/4.81336. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.79174/4.81374. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.79325/4.81936. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.79298/4.82016. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.79289/4.81985. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.78873/4.82314. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.78917/4.81917. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.79350/4.81617. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.78795/4.81909. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.78845/4.81836. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.78377/4.81183. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.78243/4.81448. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.78505/4.81663. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.78273/4.81871. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.78502/4.82499. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.77773/4.82908. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.77987/4.82710. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.78339/4.82328. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.78012/4.83188. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.77768/4.82740. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.77825/4.83135. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.77979/4.81590. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.77707/4.82886. Took 0.21 sec\n",
      "Epoch 32, Loss(train/val) 4.77809/4.82860. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.78602/4.81470. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.78027/4.82915. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.77789/4.83053. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.77562/4.82742. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.77322/4.83803. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.77521/4.82994. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.77475/4.82920. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.77198/4.85262. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.77457/4.83122. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.77366/4.82846. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.76970/4.82672. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.77295/4.82474. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.76847/4.83065. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.76805/4.82152. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.77171/4.83424. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.77520/4.83196. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.77086/4.82673. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.76539/4.83523. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.76492/4.84139. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.76757/4.83194. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.76485/4.83757. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.76991/4.83952. Took 0.20 sec\n",
      "Epoch 55, Loss(train/val) 4.77072/4.83318. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.76494/4.84136. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.76582/4.82816. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.76460/4.82999. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.76831/4.82421. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.76743/4.81882. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.76586/4.82200. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.76928/4.82546. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.75691/4.83396. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.75888/4.85053. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.76173/4.81875. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.76633/4.81093. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.76683/4.82017. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.76593/4.82712. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.76297/4.82980. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.76666/4.80502. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.76322/4.82261. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.76888/4.81851. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.76525/4.81952. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.76147/4.82571. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.75855/4.82367. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.75892/4.80999. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.76134/4.82596. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.76906/4.81256. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.76179/4.82182. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.76040/4.81600. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.75895/4.81779. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.76599/4.81748. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.76275/4.82189. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.76510/4.81291. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.75974/4.82273. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.75927/4.81825. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.76725/4.81594. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.76607/4.81577. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.76687/4.85133. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.76699/4.81486. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.76528/4.81238. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.76478/4.80595. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.76019/4.80158. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.76279/4.81876. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.76693/4.80599. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.75868/4.80568. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.76121/4.81296. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.75551/4.81549. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.76508/4.80363. Took 0.19 sec\n",
      "ACC: 0.546875, MCC: 0.08845235543978211\n",
      "Epoch 0, Loss(train/val) 4.93131/4.86633. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.86635/4.85205. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.85657/4.84452. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.84948/4.85083. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.85047/4.85224. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.84577/4.85451. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.84415/4.84939. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.84385/4.86085. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.84314/4.85709. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.84358/4.85985. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.84543/4.86379. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.84009/4.86709. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.83856/4.87163. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.83596/4.87092. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.83953/4.87205. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.83628/4.87378. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.83315/4.87860. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.83492/4.87355. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.83411/4.87035. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.83441/4.87694. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.83300/4.86752. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.83141/4.87124. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.82846/4.86525. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.82743/4.86446. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.83079/4.87096. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.82806/4.86109. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.82447/4.86571. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.82646/4.85818. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.82478/4.86680. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.82741/4.85895. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.82739/4.86000. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.82475/4.86124. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.82534/4.86609. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.82244/4.87349. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.82597/4.85491. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.82699/4.85951. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.82349/4.87131. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.82422/4.86958. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.82563/4.86350. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.82552/4.86875. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.82564/4.87006. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.82030/4.87873. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.82323/4.86693. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.82156/4.86929. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.82327/4.86979. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.82516/4.85778. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.81840/4.86462. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.81497/4.87834. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.81916/4.86429. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.82222/4.85958. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.81962/4.86540. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.81440/4.87356. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.81796/4.87459. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.82035/4.87211. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.82356/4.86327. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.81449/4.87725. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.81334/4.87411. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.81858/4.86462. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.81212/4.87886. Took 0.18 sec\n",
      "Epoch 59, Loss(train/val) 4.81743/4.86956. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.81781/4.86102. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.80771/4.87817. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.82119/4.86383. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.81231/4.87604. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.81605/4.86995. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.81311/4.85784. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.81918/4.85037. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.81229/4.86912. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.81958/4.86004. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.81597/4.86443. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.81983/4.87007. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.81582/4.85556. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.81266/4.85715. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.81256/4.86920. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.81494/4.85602. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.81232/4.86320. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.80703/4.88149. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.81044/4.86550. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.80806/4.85937. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.81270/4.85537. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.80765/4.86468. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.81242/4.85958. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.80951/4.87167. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.81228/4.85621. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.80639/4.86348. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.80975/4.85483. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.81154/4.86719. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.81366/4.86378. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.80967/4.86052. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.80735/4.84986. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.80607/4.85980. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.80742/4.85221. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.81235/4.84742. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.80650/4.88206. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.81120/4.85888. Took 0.20 sec\n",
      "Epoch 95, Loss(train/val) 4.80494/4.87910. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.80919/4.85337. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.80496/4.85427. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.80599/4.85416. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.80883/4.84687. Took 0.19 sec\n",
      "ACC: 0.3125, MCC: -0.40711742805652384\n",
      "Epoch 0, Loss(train/val) 4.78509/4.70095. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.70482/4.70687. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.70174/4.71266. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.69981/4.71528. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.70014/4.72032. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.70228/4.72757. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.69945/4.72061. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.70033/4.72997. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.69755/4.72697. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.69974/4.72101. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.70155/4.72341. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.70014/4.72100. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.69539/4.73049. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.69800/4.73114. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.69337/4.74047. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.69377/4.73803. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.69412/4.73352. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.69371/4.74041. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.69646/4.73836. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.69349/4.74737. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.69183/4.73278. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.70112/4.71168. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.69868/4.71500. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.69642/4.72710. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.69316/4.74097. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.69323/4.75763. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.69226/4.75618. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.69258/4.75467. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.69072/4.76046. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.69271/4.75305. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.69149/4.76304. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.68787/4.76474. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.68586/4.76860. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.68630/4.77961. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.69845/4.72128. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.69613/4.75414. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.69229/4.73511. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.68919/4.76435. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.69081/4.75649. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.68648/4.79085. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.68788/4.76056. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.68460/4.80007. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.68641/4.78438. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.68415/4.79618. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.68112/4.79307. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.68170/4.80491. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.67930/4.78517. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.68130/4.80208. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.68418/4.74556. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.69360/4.73051. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.69367/4.75513. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.68797/4.75595. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.68854/4.78140. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.68924/4.75286. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.68442/4.79600. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.68397/4.78335. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.68424/4.79381. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.67976/4.78865. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.68171/4.78654. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.68365/4.79028. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.67568/4.81364. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.68140/4.79156. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.67633/4.76787. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.68027/4.77774. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.67397/4.78948. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.67885/4.79224. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.67410/4.82185. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.67295/4.80238. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.67669/4.80767. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.67908/4.78105. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.67375/4.81695. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.67324/4.80884. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.67731/4.80046. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.67200/4.79380. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.67419/4.81924. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.66805/4.80570. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.67232/4.77781. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.66876/4.81637. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.67278/4.80201. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.67130/4.80079. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.66854/4.79864. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.67181/4.78743. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.66711/4.80470. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.67226/4.80398. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.66930/4.79902. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.66288/4.84419. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.67148/4.81075. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.66835/4.81432. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.66912/4.82433. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.66383/4.81761. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.66798/4.82188. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.67204/4.82037. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.66960/4.79878. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.66588/4.81647. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.66848/4.80137. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.66208/4.81778. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.66203/4.82968. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.66061/4.84312. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.66429/4.81748. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.66272/4.84277. Took 0.19 sec\n",
      "ACC: 0.375, MCC: -0.2545139051903111\n",
      "Epoch 0, Loss(train/val) 4.77815/4.74252. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.76532/4.73954. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.75255/4.73872. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.75418/4.73526. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.75568/4.74127. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.75879/4.73959. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.76504/4.74256. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.76938/4.76836. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.76113/4.77901. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.75689/4.76478. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.75351/4.75553. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.75738/4.76457. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.75668/4.77030. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.75687/4.77448. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.75387/4.77336. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.75143/4.77192. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.75363/4.76965. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.75540/4.77427. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.75478/4.76935. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.75248/4.75385. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.75098/4.76434. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.75219/4.77293. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.75258/4.77463. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.75038/4.78203. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.75364/4.78369. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.75077/4.78612. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.74723/4.79368. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.74875/4.80095. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.74838/4.78891. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.74643/4.80609. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.74610/4.79074. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.74614/4.80834. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.74339/4.81136. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.75232/4.78135. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.75553/4.77200. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.74897/4.78331. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.74600/4.80837. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.74374/4.80136. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.74387/4.80594. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.74533/4.80847. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.73891/4.81986. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.75284/4.78384. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.75239/4.77332. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.74733/4.78754. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.74779/4.79352. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.74478/4.80662. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.74511/4.78887. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.74934/4.79424. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.74826/4.79100. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.74539/4.80142. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.74376/4.80687. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.73636/4.80228. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.73696/4.78986. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.73790/4.80679. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.74206/4.77702. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.74588/4.78524. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.73608/4.80996. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.74181/4.78547. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.74732/4.78948. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.73525/4.80080. Took 0.21 sec\n",
      "Epoch 60, Loss(train/val) 4.73966/4.77510. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.73868/4.80270. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.73745/4.80510. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.73721/4.82559. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.72951/4.83098. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.72646/4.80971. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.75129/4.76623. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.74737/4.75886. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.74724/4.77595. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.74687/4.78135. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.73944/4.80294. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.74396/4.79662. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.74023/4.79903. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.74420/4.79983. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.74153/4.78719. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.74635/4.77864. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.73911/4.78415. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.73786/4.79654. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.74016/4.78346. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.73997/4.79675. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.74580/4.78214. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.73229/4.80757. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.73478/4.80133. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.73266/4.82329. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.73326/4.80607. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.73310/4.83338. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.73171/4.81444. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.73565/4.81823. Took 0.21 sec\n",
      "Epoch 88, Loss(train/val) 4.73576/4.79495. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 4.73944/4.80698. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.73391/4.80339. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.73849/4.78784. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.74333/4.79807. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.73455/4.79991. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.73537/4.82063. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.73124/4.83190. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 4.73725/4.81010. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.73289/4.80526. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.73153/4.80502. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.73797/4.82168. Took 0.19 sec\n",
      "ACC: 0.5, MCC: -0.015873015873015872\n",
      "Epoch 0, Loss(train/val) 5.00659/4.90187. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.90897/4.92681. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.89921/4.91636. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.90024/4.90137. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.89815/4.89588. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.89919/4.90155. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.89960/4.90600. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.89449/4.90905. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.89777/4.90743. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.89268/4.91522. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.89836/4.92269. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.88990/4.92196. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.89227/4.93445. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.89313/4.92081. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.89030/4.92809. Took 0.20 sec\n",
      "Epoch 15, Loss(train/val) 4.88820/4.92966. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.89020/4.91916. Took 0.18 sec\n",
      "Epoch 17, Loss(train/val) 4.88986/4.93368. Took 0.18 sec\n",
      "Epoch 18, Loss(train/val) 4.88951/4.91099. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.88960/4.94848. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.88340/4.92666. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.88591/4.92574. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.88064/4.92750. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.88356/4.92985. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.88388/4.95423. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.88558/4.93422. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.87752/4.95286. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.87948/4.93547. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.88921/4.93092. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.89058/4.93158. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.88296/4.92368. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.87420/4.95482. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.89198/4.89811. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.88591/4.91745. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 4.88645/4.93186. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.88527/4.93218. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.87842/4.92801. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.88121/4.95003. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.87594/4.93418. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.88072/4.92560. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.87345/4.93785. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.87602/4.94703. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.87189/4.94322. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.87704/4.94005. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.87170/4.94778. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.87307/4.93256. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.87144/4.94845. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.87214/4.94887. Took 0.18 sec\n",
      "Epoch 48, Loss(train/val) 4.87407/4.94201. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.86805/4.95367. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.87184/4.94030. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.86816/4.96637. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.87267/4.94192. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.87400/4.94700. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.87770/4.97036. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.87141/4.98727. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.87198/4.94910. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.86498/4.99423. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.86834/4.98325. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.86251/4.98669. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.86667/4.98712. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.87176/4.95048. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.86706/4.97445. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.86298/4.99333. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.86565/4.95271. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.87570/4.94883. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.87282/4.97246. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.87304/4.94172. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.87748/4.94347. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.86907/4.96579. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.85961/4.98599. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.87114/4.96194. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.85803/5.01672. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.86384/4.96955. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.86293/4.99200. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.86793/4.95790. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.86367/4.97891. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.86305/5.00046. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.86545/4.95857. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.86182/4.96075. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.85906/4.98255. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.85950/5.00428. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.85838/5.01286. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84488/5.04822. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.85494/4.98283. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.86137/4.97776. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.85660/5.00422. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.86384/4.94717. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.85862/4.98327. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.85749/4.97486. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.84521/5.01333. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.85413/4.97737. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.85909/5.02731. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.85183/5.00518. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.85157/4.99142. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.85573/4.98249. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.86638/4.98797. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.86499/4.96382. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.87071/4.95433. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.87261/4.92738. Took 0.19 sec\n",
      "ACC: 0.5, MCC: -0.004945946830869485\n",
      "Epoch 0, Loss(train/val) 4.78042/4.74911. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.76193/4.74891. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.75832/4.74367. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.75244/4.74356. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.74903/4.74254. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.74745/4.74193. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.74904/4.74349. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.74718/4.74388. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.74721/4.74520. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.74600/4.74421. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.74866/4.74336. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.74814/4.74541. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.74598/4.74608. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.74416/4.74653. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.74404/4.74945. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.74484/4.75491. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.74322/4.76554. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.74506/4.76452. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 4.74490/4.74420. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.74304/4.74851. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.74305/4.75099. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.74190/4.75185. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.74265/4.75105. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.74244/4.75062. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.74156/4.76237. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.74201/4.75757. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.74178/4.76445. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.74256/4.76018. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.74259/4.75909. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.73817/4.76623. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.74108/4.76778. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.74117/4.76802. Took 0.18 sec\n",
      "Epoch 32, Loss(train/val) 4.73506/4.77337. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.73926/4.77072. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.73701/4.77059. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.73952/4.77047. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.74048/4.76928. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.74063/4.77092. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.73895/4.75550. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.73779/4.76185. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.74139/4.76727. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.74375/4.76344. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.73988/4.77798. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.74017/4.76814. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.73748/4.77228. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.73340/4.79364. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.73494/4.78117. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.73864/4.77407. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.73597/4.78393. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.73855/4.78724. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.73589/4.78551. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.73649/4.79357. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.73469/4.80059. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.73476/4.78209. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.72911/4.77789. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.73449/4.79718. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.73707/4.78337. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.73374/4.77667. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.73351/4.78959. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.73077/4.79916. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.73251/4.79811. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.73418/4.79529. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.73479/4.78917. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.73160/4.79668. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.73070/4.79626. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.73154/4.78972. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.73127/4.80080. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.72906/4.80484. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.73173/4.80787. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.72964/4.82152. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.73022/4.80607. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.72661/4.82082. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.72635/4.83551. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.72479/4.81416. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.72958/4.80290. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.73375/4.81320. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.72611/4.83903. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.73136/4.81476. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.72065/4.85918. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.72850/4.80536. Took 0.18 sec\n",
      "Epoch 80, Loss(train/val) 4.72856/4.82749. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.72406/4.83901. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.72795/4.82010. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.72149/4.84362. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.72794/4.81452. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.72427/4.83929. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.72689/4.79867. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.72136/4.82039. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.72170/4.85594. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.73623/4.81212. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.73192/4.82047. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.72391/4.84487. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.72515/4.81847. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.72106/4.83828. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.71772/4.84312. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.72307/4.82353. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.72292/4.85036. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.72555/4.84469. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.72194/4.82340. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.72755/4.80276. Took 0.19 sec\n",
      "ACC: 0.53125, MCC: 0.0629940788348712\n",
      "Epoch 0, Loss(train/val) 5.01208/4.97370. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.98120/4.97905. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.98435/4.98406. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 4.98617/5.00209. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.98821/5.01474. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.99076/4.99118. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.98348/4.97706. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.97569/4.98038. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.97663/4.98291. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.97849/4.98416. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.97956/4.98304. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.97824/4.97916. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.97745/4.98348. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.97597/4.98022. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.97632/4.98186. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.97425/4.98187. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.97642/4.98124. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.97707/4.98231. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.97411/4.98111. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.97526/4.98156. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.97398/4.98113. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.97667/4.98332. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.97706/4.98140. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.97649/4.98210. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.97516/4.98202. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.97344/4.98430. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.97419/4.98292. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.97322/4.98609. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.97171/4.98713. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.97254/4.98736. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.97123/4.97920. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.97170/4.98235. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.97147/4.98191. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.96685/4.98415. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.97075/4.98713. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.97054/4.97919. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.96788/4.98255. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.97016/4.98607. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.96991/4.98882. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.97009/4.98205. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.96453/4.99170. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.96789/4.99367. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.96747/4.98448. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.96405/4.99570. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.96332/4.99016. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.96656/4.99578. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.96555/4.98944. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.96570/4.99109. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.96203/5.00307. Took 0.21 sec\n",
      "Epoch 49, Loss(train/val) 4.96532/4.99901. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.96572/4.99128. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.96205/5.00183. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.96349/4.99562. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.96235/5.00888. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.96115/4.99877. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.96415/4.99680. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.95883/5.01253. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.96087/5.00373. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.96569/5.00064. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.95806/5.00151. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.96020/5.01122. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.95865/5.00517. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.95865/5.01508. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.95989/5.00703. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.95667/5.02178. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.96094/4.99648. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.95712/5.00906. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.95600/5.00929. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.95579/5.01383. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.95797/5.01179. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.95548/5.00776. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.95492/5.01717. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.96476/5.01514. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.95833/5.01320. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.95306/5.01655. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.95314/5.01527. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.95403/5.02270. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.95350/5.03303. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.95689/5.01784. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.95856/5.02378. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.95495/5.01381. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.95498/5.01482. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.95223/5.01410. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.95160/5.00307. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.95728/5.00908. Took 0.21 sec\n",
      "Epoch 85, Loss(train/val) 4.95795/5.01057. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.95462/5.01197. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.94909/5.01518. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.95009/5.00472. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.95698/5.00399. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.95255/5.01711. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.95324/5.00866. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.95508/5.01402. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.94563/5.02097. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.94910/5.01631. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.95766/5.00500. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.94610/5.03124. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.95737/5.01543. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.95034/5.02272. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.95100/5.02718. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.06643282473893375\n",
      "Epoch 0, Loss(train/val) 4.69094/4.65312. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.67236/4.65427. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.66927/4.65811. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.67249/4.66468. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.68300/4.64952. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.67032/4.65449. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.66010/4.64775. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.66185/4.64507. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.66197/4.64358. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.66224/4.64601. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.66006/4.64215. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.66142/4.64079. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.66182/4.64441. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.65780/4.64153. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.65834/4.64319. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.66104/4.64066. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.65965/4.63953. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.65778/4.63971. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.65666/4.63732. Took 0.20 sec\n",
      "Epoch 19, Loss(train/val) 4.65370/4.63733. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.65770/4.63982. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.65638/4.64240. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.65530/4.64141. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.65665/4.64570. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.65735/4.64614. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.65325/4.64779. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.65609/4.64940. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.65437/4.64743. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.65617/4.64764. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.65228/4.64836. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.65326/4.64964. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.65368/4.64973. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.64916/4.65050. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.65233/4.64704. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.65283/4.64525. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.65155/4.64184. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.65268/4.65087. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.65339/4.64676. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.65000/4.65392. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.65540/4.64552. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.64938/4.64571. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.65037/4.64891. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.64768/4.65733. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.64942/4.64867. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.65008/4.65777. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.65071/4.64149. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.64832/4.64919. Took 0.20 sec\n",
      "Epoch 47, Loss(train/val) 4.65059/4.64185. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.64846/4.66044. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.64792/4.64477. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.65027/4.64835. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.64664/4.64438. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.64604/4.66062. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.64876/4.64981. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.64749/4.65278. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.64651/4.64763. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.64952/4.65144. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.64581/4.65142. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.64608/4.64471. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.64403/4.64515. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.64748/4.65664. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.64490/4.65361. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.65214/4.65923. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.64389/4.65169. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.64133/4.64915. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.64605/4.66834. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.64298/4.64333. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.64542/4.67092. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.64672/4.64087. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.64864/4.64993. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.64854/4.64999. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.64234/4.64820. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.64564/4.64931. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.64349/4.64707. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.64432/4.66401. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.64388/4.64227. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.64134/4.64837. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.64459/4.63439. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.64344/4.64384. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.64154/4.64170. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.64176/4.65111. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.64660/4.64082. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.64185/4.64845. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.64059/4.63537. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.64208/4.64104. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.64303/4.64047. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.63873/4.66302. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.63748/4.66748. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.64326/4.65454. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.64299/4.66357. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.64422/4.66841. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.63830/4.67242. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.64980/4.65775. Took 0.20 sec\n",
      "Epoch 93, Loss(train/val) 4.63942/4.64304. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.64316/4.66646. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.63939/4.64635. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.63951/4.65112. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.64310/4.65091. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.64309/4.64543. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.64004/4.64308. Took 0.20 sec\n",
      "ACC: 0.453125, MCC: -0.10677275981195139\n",
      "Epoch 0, Loss(train/val) 5.08054/5.04864. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 5.01459/5.00610. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 5.00836/4.99670. Took 0.20 sec\n",
      "Epoch 3, Loss(train/val) 5.00590/4.99309. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 5.00746/4.99128. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 5.00709/4.99063. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 5.01111/4.99434. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 5.00537/4.99585. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 5.00503/4.99459. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 5.00449/4.99203. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 5.00064/4.98963. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 5.00095/4.98876. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 5.00020/4.99225. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 5.00034/4.98942. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.99997/4.99281. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.99791/5.00061. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 5.00015/4.99115. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.99776/4.99415. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.99682/4.99434. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.99666/4.99406. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.99324/4.99734. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.99515/4.99668. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.99297/5.00391. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.99147/4.99463. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.99540/4.99137. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.99243/4.99362. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.99122/4.99107. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.99537/4.98586. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.99384/4.98394. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.99270/4.98510. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.99035/4.98745. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.99101/4.98522. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.99160/4.98505. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.99023/4.98562. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.99364/4.98725. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.99091/4.98600. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.98786/4.97500. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.99394/4.98117. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.99195/4.98541. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.99383/4.98481. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.99495/4.98220. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.99397/4.98242. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.98799/4.98525. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.98915/4.97800. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.98775/4.98323. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.98697/4.98713. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.98726/4.97699. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.99237/4.98312. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.98908/4.97682. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.98483/4.98054. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.98971/4.98132. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.98405/4.97286. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.98666/4.98308. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.98554/4.98846. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.98531/4.98837. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.98384/4.98566. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.98441/4.98220. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.98479/4.99215. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.98487/4.98594. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.98269/4.98374. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.98179/5.00287. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.98671/4.98779. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.98340/4.99779. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.98223/5.00180. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.98492/4.99071. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.98268/5.00318. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.98601/4.98624. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.98234/4.99072. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.98172/4.99357. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.98276/5.00148. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.98213/4.99191. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.98545/4.98326. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.97790/5.01052. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.98465/4.97812. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.98104/4.99995. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.98370/4.97682. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.98273/5.00310. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.97935/5.00257. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.97668/4.99917. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.98043/5.00259. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.98095/5.00542. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.97770/4.99812. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.97842/5.00713. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.98268/4.98713. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.97618/5.00633. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.97849/4.99773. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.97766/4.98376. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.98355/5.00370. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.98250/4.99249. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.97788/4.99816. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.98125/4.98493. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.98024/4.99911. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.98171/4.98652. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.97966/5.00282. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.97907/4.99188. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.97996/4.98957. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.98095/4.98583. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.97327/5.00802. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.97839/4.99000. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.97768/4.99480. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.06362847629757777\n",
      "Epoch 0, Loss(train/val) 4.95110/4.87643. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.88809/4.89458. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.89960/4.90939. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.90616/4.91469. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.89671/4.89625. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.88030/4.90402. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.87815/4.91319. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.88097/4.91191. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.87910/4.90823. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.87885/4.91179. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.87839/4.91153. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.87793/4.91098. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.87528/4.90813. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.87343/4.90986. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.87465/4.90640. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.87246/4.91450. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.87382/4.90304. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.86783/4.90524. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.87261/4.90629. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.87008/4.90695. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.86835/4.90313. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.86860/4.89928. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.86951/4.89565. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.86655/4.89741. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.86393/4.90323. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.86937/4.89970. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.86530/4.90298. Took 0.20 sec\n",
      "Epoch 27, Loss(train/val) 4.86319/4.91164. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.86447/4.90634. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.86517/4.90103. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.86185/4.90550. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.86503/4.89681. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.86201/4.91169. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.86186/4.89827. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.85946/4.88964. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.86044/4.89608. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.85747/4.90334. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.86488/4.88389. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.85851/4.92019. Took 0.20 sec\n",
      "Epoch 39, Loss(train/val) 4.86171/4.89743. Took 0.21 sec\n",
      "Epoch 40, Loss(train/val) 4.85887/4.90954. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.86590/4.88125. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.85850/4.89003. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.85551/4.91675. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.85533/4.90006. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.85394/4.90646. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.85670/4.89717. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.86123/4.90419. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.85809/4.89175. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.86108/4.88945. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.85384/4.90356. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.85350/4.92074. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.85359/4.92149. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.85560/4.89905. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.85435/4.90171. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.85293/4.90665. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.84746/4.88776. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.85563/4.87377. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.85446/4.90606. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.85444/4.91294. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.85536/4.88810. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.84718/4.88892. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.84796/4.89292. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.84817/4.91225. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.85719/4.93099. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.85235/4.87243. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.85137/4.88392. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.84622/4.88356. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.85091/4.91354. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.84218/4.91587. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.84936/4.89435. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.85222/4.90400. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.84717/4.89966. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.84744/4.89042. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.84831/4.91301. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.84658/4.92218. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.85063/4.89201. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.85264/4.89834. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.84323/4.90562. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.84326/4.89156. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.84098/4.94949. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.84988/4.90539. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.84828/4.89138. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.84199/4.88839. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.84408/4.92417. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.84724/4.89458. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.84725/4.90521. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.84573/4.91241. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.84320/4.90201. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.84469/4.90265. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.85008/4.89049. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.84557/4.89926. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.84564/4.91314. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.83863/4.91336. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.84645/4.91531. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.84394/4.89558. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.84031/4.89606. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.83752/4.89646. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.84259/4.90914. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.83532/4.91654. Took 0.19 sec\n",
      "ACC: 0.484375, MCC: -0.06052275326688024\n",
      "Epoch 0, Loss(train/val) 4.89558/4.81220. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.83627/4.82204. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.82920/4.84367. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.83005/4.82659. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.82502/4.81500. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.81809/4.82049. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.81917/4.82733. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.82060/4.82972. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.82139/4.83405. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.81944/4.83265. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.81701/4.82835. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.81447/4.83508. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.81523/4.83066. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.81307/4.83474. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.81416/4.83515. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.81835/4.83138. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.81385/4.82855. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.81407/4.83001. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.81259/4.83650. Took 0.21 sec\n",
      "Epoch 19, Loss(train/val) 4.81300/4.82982. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.81002/4.83517. Took 0.20 sec\n",
      "Epoch 21, Loss(train/val) 4.81073/4.83678. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.81036/4.83976. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.81284/4.83675. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.81169/4.83845. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.81181/4.83460. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.81067/4.83586. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.81105/4.83647. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.81226/4.83621. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.81045/4.84380. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.80755/4.84067. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.80972/4.83637. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.80797/4.84048. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.80824/4.83833. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.80939/4.83336. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.80850/4.83962. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.81096/4.83670. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.80884/4.84146. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.80514/4.84311. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.80678/4.84972. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.80467/4.84759. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.80525/4.83747. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.81142/4.83942. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.80691/4.83209. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.80772/4.83758. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.80621/4.83407. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.80545/4.84651. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.80373/4.83799. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.80482/4.84161. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.80697/4.83668. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.80147/4.83873. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.80608/4.84832. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.80472/4.83195. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.80267/4.84875. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.80565/4.83595. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.80318/4.85709. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.80864/4.83234. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.80474/4.84843. Took 0.21 sec\n",
      "Epoch 58, Loss(train/val) 4.80234/4.85020. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.80707/4.83834. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.80652/4.83000. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.80327/4.84569. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.80130/4.84629. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.80443/4.84431. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.80327/4.84144. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.80718/4.84408. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.80099/4.85572. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.80066/4.85076. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.80331/4.85688. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.80647/4.84631. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.80784/4.84270. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.80169/4.85214. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.80461/4.83968. Took 0.20 sec\n",
      "Epoch 73, Loss(train/val) 4.80301/4.84779. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.80303/4.85797. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.80477/4.84207. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.80082/4.86191. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.80331/4.84914. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.80159/4.84992. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.79947/4.85737. Took 0.20 sec\n",
      "Epoch 80, Loss(train/val) 4.81234/4.84614. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.81332/4.83649. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.80718/4.84188. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.80642/4.85234. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.80766/4.84972. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.80387/4.85783. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.80301/4.85288. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.80656/4.85871. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.80160/4.85408. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.79922/4.86091. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.79816/4.86026. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.80352/4.85484. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.80383/4.84921. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.79717/4.85887. Took 0.20 sec\n",
      "Epoch 94, Loss(train/val) 4.79936/4.85403. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.79735/4.85719. Took 0.18 sec\n",
      "Epoch 96, Loss(train/val) 4.79931/4.85207. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.79657/4.87152. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.80296/4.85764. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.79898/4.85435. Took 0.19 sec\n",
      "ACC: 0.640625, MCC: 0.282494169258455\n",
      "Epoch 0, Loss(train/val) 4.92654/4.92696. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.93333/4.91175. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.91426/4.94130. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.92624/4.93734. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.92828/4.90567. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.91779/4.90474. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.90980/4.90447. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.91403/4.90510. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.91302/4.90374. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.91171/4.90309. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.90907/4.90361. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.90934/4.90278. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.90672/4.90307. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.90941/4.90343. Took 0.20 sec\n",
      "Epoch 14, Loss(train/val) 4.90823/4.90177. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.90749/4.90140. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90663/4.90439. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.90648/4.90288. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.90249/4.90518. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.90456/4.91918. Took 0.20 sec\n",
      "Epoch 20, Loss(train/val) 4.91083/4.90894. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.90356/4.91283. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.90694/4.91187. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.90572/4.91480. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.90584/4.91484. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.90291/4.91594. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.90397/4.91729. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.90036/4.92253. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90150/4.92763. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.90056/4.92957. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90140/4.92391. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.90321/4.92400. Took 0.20 sec\n",
      "Epoch 32, Loss(train/val) 4.89798/4.92332. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.90358/4.92731. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.89877/4.92804. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.89780/4.93804. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.89762/4.92915. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.89966/4.93664. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89385/4.93314. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.89802/4.92439. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90036/4.93367. Took 0.21 sec\n",
      "Epoch 41, Loss(train/val) 4.90109/4.93097. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89663/4.94862. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.89745/4.92845. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.89877/4.92839. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.89083/4.94873. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.89988/4.93063. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89581/4.94029. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89335/4.93677. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.89475/4.93572. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.89125/4.95003. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.89494/4.93348. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89275/4.93214. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.89383/4.94667. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.89336/4.95422. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.89415/4.94993. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.88856/4.96207. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.89076/4.96112. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.89847/4.93003. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89338/4.94559. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.88934/4.94658. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.89407/4.93900. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.89302/4.93199. Took 0.20 sec\n",
      "Epoch 63, Loss(train/val) 4.89033/4.94200. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.88728/4.96647. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.89194/4.93631. Took 0.20 sec\n",
      "Epoch 66, Loss(train/val) 4.89258/4.94311. Took 0.21 sec\n",
      "Epoch 67, Loss(train/val) 4.89031/4.94453. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.88869/4.95961. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.89228/4.93701. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.89639/4.93969. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.89752/4.92864. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.88789/4.93682. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.90043/4.93386. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.90273/4.92920. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.89728/4.93970. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.89536/4.94292. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.89424/4.96439. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.89322/4.93822. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.89500/4.95381. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.89404/4.93772. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.88975/4.93524. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.88844/4.96851. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.89285/4.92861. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.90207/4.91864. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.89951/4.92711. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.89530/4.93311. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.89603/4.90731. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.89538/4.91161. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.89544/4.91924. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.89223/4.92983. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.89296/4.91796. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.88691/4.93866. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.88811/4.93361. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.89271/4.94358. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.89718/4.91607. Took 0.20 sec\n",
      "Epoch 96, Loss(train/val) 4.89375/4.93095. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.89285/4.90950. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.89382/4.93143. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88801/4.94610. Took 0.19 sec\n",
      "ACC: 0.609375, MCC: 0.21971768720102058\n",
      "Epoch 0, Loss(train/val) 4.96191/4.92201. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.91883/4.92853. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.91532/4.91445. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.91126/4.91240. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.90937/4.91010. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.90925/4.90971. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.90889/4.91012. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.90432/4.90865. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.90422/4.91087. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.89933/4.91154. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.90517/4.91256. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.89911/4.91046. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.90196/4.90996. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.90030/4.91270. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.89979/4.91788. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.90033/4.91766. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90644/4.92590. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.90024/4.92892. Took 0.21 sec\n",
      "Epoch 18, Loss(train/val) 4.90349/4.93239. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.89908/4.93431. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.89493/4.93234. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.89500/4.92451. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.89127/4.92941. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 4.89536/4.93064. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.89661/4.93549. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.89077/4.94101. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.89069/4.94551. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.90460/4.94417. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90780/4.95705. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.90384/4.95216. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.90273/4.96013. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.90865/4.96384. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.90632/4.96325. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.90151/4.95635. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.90231/4.95588. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.89841/4.95171. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.89838/4.95648. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.89790/4.94971. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.89455/4.95374. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90208/4.94316. Took 0.18 sec\n",
      "Epoch 40, Loss(train/val) 4.89528/4.96473. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.89303/4.95532. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.89528/4.97041. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.90496/4.95378. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.90255/4.96129. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.89084/4.96127. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.90188/4.97495. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.89933/4.95933. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.89230/4.96643. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.89782/4.97983. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.90375/4.97457. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.91101/4.94266. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.89727/4.95666. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.90050/4.97069. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.89819/4.97178. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.89532/4.96057. Took 0.20 sec\n",
      "Epoch 56, Loss(train/val) 4.88939/4.95931. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.89290/4.97339. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.88797/4.96561. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89118/4.96360. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.88591/4.96748. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.88805/4.95727. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.89090/4.96158. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.88480/4.98673. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.88430/4.96591. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.87913/4.97197. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.88327/4.97539. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.88746/4.95784. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.88218/4.97292. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.88509/4.96217. Took 0.18 sec\n",
      "Epoch 70, Loss(train/val) 4.89096/4.95632. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.89758/4.95672. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.89342/4.96969. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.88996/4.97493. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.89060/4.95858. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.88848/4.95748. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.88167/4.96794. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.88391/4.97212. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.88466/4.97601. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.88779/4.96410. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.88309/4.97619. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.88494/4.97521. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.88482/4.95498. Took 0.20 sec\n",
      "Epoch 83, Loss(train/val) 4.88241/4.97031. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.88128/4.97377. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.88263/4.96228. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.88331/4.97278. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.87909/4.98557. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.88648/4.97247. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.87964/4.98197. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.87330/4.98302. Took 0.20 sec\n",
      "Epoch 91, Loss(train/val) 4.88639/4.98322. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.87715/4.96971. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.88244/4.99929. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.87848/4.97128. Took 0.18 sec\n",
      "Epoch 95, Loss(train/val) 4.87873/4.99061. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.87397/4.98085. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.88392/4.97243. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.87761/4.99250. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88052/4.97540. Took 0.19 sec\n",
      "ACC: 0.46875, MCC: -0.01642880193633814\n",
      "Epoch 0, Loss(train/val) 4.80235/4.78396. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.76054/4.77623. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.75582/4.76890. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.75515/4.77143. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.75341/4.77373. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.75112/4.77578. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.75418/4.77620. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.75289/4.77862. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.75638/4.77625. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.75499/4.77912. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.74603/4.78222. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.75275/4.78061. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.74777/4.78510. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.74896/4.78434. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.74710/4.77731. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.74775/4.78030. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.74610/4.77805. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.74573/4.77600. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.74186/4.77848. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.74358/4.78959. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.74498/4.77709. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.74324/4.77780. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.73913/4.79505. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.75251/4.78297. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.74426/4.77035. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.74425/4.81017. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.74339/4.80677. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.74256/4.79477. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.74347/4.79407. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.73907/4.79914. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 4.74086/4.80323. Took 0.20 sec\n",
      "Epoch 31, Loss(train/val) 4.74345/4.79674. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.73977/4.80507. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.74672/4.79754. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.74866/4.77501. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.74480/4.78272. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.74053/4.79515. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.73845/4.80846. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.73498/4.80430. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.73864/4.80437. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.73439/4.80852. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.73830/4.76638. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.74568/4.75028. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.73735/4.76244. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.73641/4.77542. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.73121/4.80863. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.73229/4.80950. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.73414/4.82534. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.73131/4.80609. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.73343/4.80903. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.73557/4.81760. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.73127/4.78081. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.73396/4.77462. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.73129/4.77176. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.73078/4.80416. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.74119/4.79227. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.73660/4.81828. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.73507/4.77951. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.73855/4.77484. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.73244/4.77837. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.73356/4.78999. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.72731/4.80206. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.73069/4.79240. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.72555/4.79198. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.72899/4.78496. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.73122/4.79305. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.72884/4.79121. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.73540/4.79561. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.72900/4.79874. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.73168/4.80033. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.72609/4.78576. Took 0.20 sec\n",
      "Epoch 71, Loss(train/val) 4.72540/4.79132. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.72625/4.80050. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.72440/4.80733. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.72535/4.80068. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.72442/4.80777. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.73147/4.80075. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.73217/4.79917. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.73013/4.78276. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.72351/4.79566. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.73458/4.78025. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.72215/4.80565. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.74036/4.78104. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.72610/4.79021. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.73434/4.80300. Took 0.20 sec\n",
      "Epoch 85, Loss(train/val) 4.73732/4.79359. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.73155/4.78836. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.72714/4.79480. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.72476/4.80547. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.72890/4.79745. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.72593/4.80719. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.73285/4.79961. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.72458/4.79906. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.72428/4.80895. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.72505/4.80181. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.73068/4.79221. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.73066/4.80704. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.73005/4.79941. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.71881/4.81281. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.72526/4.80723. Took 0.19 sec\n",
      "ACC: 0.515625, MCC: 0.04350001325672093\n",
      "Epoch 0, Loss(train/val) 4.80147/4.79971. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.78236/4.78492. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.77995/4.76758. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.77384/4.76064. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.76651/4.76064. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.76386/4.76148. Took 0.20 sec\n",
      "Epoch 6, Loss(train/val) 4.76438/4.76219. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.76613/4.76373. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.76516/4.76784. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.76523/4.77109. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.76279/4.77290. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.76096/4.77800. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.76079/4.78569. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.75723/4.79504. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.75551/4.79946. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.75940/4.78216. Took 0.20 sec\n",
      "Epoch 16, Loss(train/val) 4.75651/4.79910. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.75563/4.79121. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.75702/4.80318. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.75627/4.80021. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.74939/4.82831. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.75247/4.82105. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.75259/4.80871. Took 0.20 sec\n",
      "Epoch 23, Loss(train/val) 4.74675/4.83450. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.74961/4.82138. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.74853/4.82618. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.74698/4.81207. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.74503/4.82167. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.74497/4.82259. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.74327/4.81600. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.73948/4.85403. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.74661/4.82651. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.74399/4.81742. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.74045/4.84136. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.73874/4.83163. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.73280/4.87643. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.74828/4.81466. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.73591/4.83283. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.73608/4.83766. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.73788/4.83314. Took 0.20 sec\n",
      "Epoch 40, Loss(train/val) 4.73723/4.84850. Took 0.20 sec\n",
      "Epoch 41, Loss(train/val) 4.73441/4.82389. Took 0.20 sec\n",
      "Epoch 42, Loss(train/val) 4.73981/4.82697. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.73322/4.88330. Took 0.18 sec\n",
      "Epoch 44, Loss(train/val) 4.73931/4.83550. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.73474/4.85032. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.73596/4.84442. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.73170/4.85606. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.74065/4.82192. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.73442/4.87447. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.73356/4.85177. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.72888/4.89295. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.73044/4.84902. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.72654/4.84290. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.73199/4.86028. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.73431/4.85076. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.72753/4.85110. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.73341/4.84922. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.73053/4.83212. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.72545/4.85964. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.73068/4.83781. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.72165/4.85944. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.73042/4.85797. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.73276/4.84860. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.72911/4.85162. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.72503/4.85436. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.72522/4.86247. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.72410/4.84809. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.73731/4.86125. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.72413/4.82881. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.72816/4.85899. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.72444/4.87845. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.72685/4.86375. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.71974/4.85911. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.72591/4.85328. Took 0.20 sec\n",
      "Epoch 75, Loss(train/val) 4.72491/4.89236. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.72079/4.84517. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.72575/4.86616. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.71978/4.86152. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.72500/4.86104. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.72633/4.85883. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.72039/4.85842. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.71181/4.86510. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.71774/4.86403. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.71823/4.87482. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.71735/4.87832. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.72116/4.85992. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.72159/4.86535. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.71882/4.87059. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.71524/4.86169. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.72537/4.86891. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.72073/4.87405. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.71248/4.88731. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.71942/4.83944. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.71773/4.88433. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.71139/4.85700. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.72024/4.88879. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.71294/4.84717. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.71990/4.87950. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.71970/4.88958. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.09202163616785992\n",
      "Epoch 0, Loss(train/val) 4.88554/5.00295. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.83005/4.92731. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.82214/4.91169. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.81925/4.90400. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.81647/4.89765. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.81455/4.90470. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.81613/4.90565. Took 0.20 sec\n",
      "Epoch 7, Loss(train/val) 4.81645/4.89636. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.81595/4.89443. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.81433/4.88960. Took 0.20 sec\n",
      "Epoch 10, Loss(train/val) 4.81199/4.88550. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.81251/4.87886. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.81109/4.87610. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.80770/4.88491. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.80863/4.90083. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.80652/4.89407. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.80533/4.91346. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.80475/4.93584. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.80535/4.90496. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.80385/4.90586. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.80364/4.89460. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.80215/4.88683. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.80116/4.89246. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.80359/4.89970. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.80080/4.89200. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.79901/4.89279. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.80007/4.90796. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.80145/4.90836. Took 0.20 sec\n",
      "Epoch 28, Loss(train/val) 4.79686/4.90529. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.80031/4.91842. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.79939/4.91382. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.79820/4.89655. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.79927/4.90373. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.79979/4.90982. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.79277/4.91027. Took 0.20 sec\n",
      "Epoch 35, Loss(train/val) 4.79857/4.90271. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.79690/4.89502. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.79642/4.90746. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.79875/4.90280. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.79216/4.90597. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.79835/4.91579. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.79501/4.90647. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.79476/4.94370. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.79447/4.94837. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.79789/4.93579. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.79515/4.91307. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.79266/4.89943. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.79196/4.92079. Took 0.20 sec\n",
      "Epoch 48, Loss(train/val) 4.79032/4.92651. Took 0.20 sec\n",
      "Epoch 49, Loss(train/val) 4.80256/4.85763. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.80808/4.88032. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.80490/4.82924. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.80194/4.88189. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.80353/4.86924. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.79752/4.87677. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.79743/4.87869. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.79508/4.86968. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.79550/4.89062. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.79833/4.90573. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.79925/4.87891. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.79656/4.88954. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.79729/4.88895. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.79986/4.87779. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.79758/4.87729. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.79509/4.89268. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.79395/4.88431. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.79762/4.87993. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.79290/4.90189. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.79054/4.90646. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.79300/4.90791. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.79194/4.93383. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.79099/4.93328. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.79025/4.91607. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.79420/4.91565. Took 0.20 sec\n",
      "Epoch 74, Loss(train/val) 4.78995/4.91257. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.78825/4.91114. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.79137/4.92169. Took 0.18 sec\n",
      "Epoch 77, Loss(train/val) 4.79077/4.90114. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.79652/4.90041. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.78906/4.92848. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.78521/4.94459. Took 0.20 sec\n",
      "Epoch 81, Loss(train/val) 4.78650/4.92795. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.78693/4.93512. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.78786/4.93538. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.79132/4.90358. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.78765/4.92691. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.78678/4.94051. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.78761/4.94180. Took 0.20 sec\n",
      "Epoch 88, Loss(train/val) 4.78444/4.96834. Took 0.18 sec\n",
      "Epoch 89, Loss(train/val) 4.78312/4.95966. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.78476/4.96627. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.78631/4.92610. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.78940/4.91965. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.78219/4.94939. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.78436/4.93648. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.78187/4.97531. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.78960/4.94465. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.78001/4.95354. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.78446/4.93120. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.78461/4.97036. Took 0.19 sec\n",
      "ACC: 0.453125, MCC: -0.09416472308615167\n",
      "Epoch 0, Loss(train/val) 4.98765/4.96409. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.96747/4.97997. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.96173/4.98930. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.96128/4.98477. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.96145/4.97625. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.95834/4.97099. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.95302/4.96974. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.95337/4.97168. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.95647/4.97165. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.95358/4.97211. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.95501/4.97343. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.95416/4.97216. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.95421/4.97155. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.95376/4.97146. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.95219/4.96702. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.95188/4.96768. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.95137/4.97070. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.95261/4.97485. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.95375/4.97856. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.95328/4.97627. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.95220/4.97394. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.94725/4.98035. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.95087/4.99485. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.94886/4.98555. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.94789/4.98601. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.94602/4.98899. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.94658/4.99290. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.94941/4.98994. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.94379/4.99596. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.94707/4.99824. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.94683/5.00074. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.94697/4.98547. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.94465/5.00324. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.94464/5.00269. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.94424/5.00167. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.94074/5.00601. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.94429/5.00546. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.94663/5.00166. Took 0.20 sec\n",
      "Epoch 38, Loss(train/val) 4.94457/4.98268. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.94361/5.00122. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.94103/4.99251. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.94155/5.01042. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.94311/5.00160. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.93988/5.01621. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.93914/5.01429. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.93694/5.01398. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.93451/5.02453. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.93833/5.01605. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.93486/5.02858. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.93602/5.01503. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.93212/5.02928. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.93656/5.01479. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.93631/5.01501. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.93451/5.02606. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.93672/5.02997. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.94032/4.99347. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.93343/5.02187. Took 0.20 sec\n",
      "Epoch 57, Loss(train/val) 4.93127/5.02393. Took 0.20 sec\n",
      "Epoch 58, Loss(train/val) 4.93147/5.04397. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.93791/5.02960. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.93766/5.01404. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.93631/5.01163. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.93133/5.03261. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.93111/5.03926. Took 0.20 sec\n",
      "Epoch 64, Loss(train/val) 4.93470/5.03109. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.93226/5.04713. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.93777/5.00806. Took 0.20 sec\n",
      "Epoch 67, Loss(train/val) 4.93249/5.01907. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.92779/5.03728. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.92736/5.07086. Took 0.20 sec\n",
      "Epoch 70, Loss(train/val) 4.93308/5.02645. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.93403/5.03250. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.93058/5.02465. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.93160/5.03461. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.93421/5.02963. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.93306/5.03495. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.93342/5.03342. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.93080/5.03934. Took 0.20 sec\n",
      "Epoch 78, Loss(train/val) 4.92861/5.03025. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.93112/5.07938. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.93240/5.02342. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.93439/5.02093. Took 0.20 sec\n",
      "Epoch 82, Loss(train/val) 4.92808/5.03639. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.92801/5.04058. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.92942/5.05559. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.93057/5.04491. Took 0.20 sec\n",
      "Epoch 86, Loss(train/val) 4.92819/5.02785. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.92623/5.06276. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.93190/5.05225. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.92746/5.04460. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.93046/5.03649. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.92380/5.05950. Took 0.20 sec\n",
      "Epoch 92, Loss(train/val) 4.92522/5.03278. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.92933/5.04585. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.93027/5.02897. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.92631/5.05738. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.92638/5.03543. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.92630/5.04149. Took 0.20 sec\n",
      "Epoch 98, Loss(train/val) 4.92511/5.05166. Took 0.20 sec\n",
      "Epoch 99, Loss(train/val) 4.92186/5.06194. Took 0.19 sec\n",
      "ACC: 0.421875, MCC: -0.10981749844451165\n",
      "Epoch 0, Loss(train/val) 4.95667/4.92074. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.91451/4.91025. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.91893/4.90726. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.91862/4.91066. Took 0.20 sec\n",
      "Epoch 4, Loss(train/val) 4.91503/4.91446. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.91608/4.92123. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.91364/4.92078. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.91419/4.92249. Took 0.20 sec\n",
      "Epoch 8, Loss(train/val) 4.91390/4.92219. Took 0.19 sec\n",
      "Epoch 9, Loss(train/val) 4.91275/4.92156. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.91298/4.92119. Took 0.20 sec\n",
      "Epoch 11, Loss(train/val) 4.91483/4.91600. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.91217/4.91281. Took 0.20 sec\n",
      "Epoch 13, Loss(train/val) 4.91335/4.91084. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.91153/4.91610. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.90877/4.90508. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.90993/4.90622. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.91065/4.90780. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.90728/4.92471. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.90896/4.91068. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.90742/4.90857. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.90639/4.91453. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.90528/4.91275. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.90319/4.91667. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.90500/4.91423. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.90391/4.91554. Took 0.20 sec\n",
      "Epoch 26, Loss(train/val) 4.90394/4.91186. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.90290/4.91940. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.90234/4.91969. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.90547/4.91027. Took 0.20 sec\n",
      "Epoch 30, Loss(train/val) 4.90472/4.91747. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.89967/4.92125. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.90210/4.89484. Took 0.20 sec\n",
      "Epoch 33, Loss(train/val) 4.91003/4.90161. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.90731/4.90990. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.90606/4.90400. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.90467/4.90897. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.90751/4.91395. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.90565/4.91314. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.90261/4.91244. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.90204/4.91318. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.90306/4.90938. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.90202/4.92155. Took 0.20 sec\n",
      "Epoch 43, Loss(train/val) 4.90061/4.91163. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.90172/4.90730. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.90211/4.91614. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.90001/4.91983. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.90856/4.88382. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.90513/4.88406. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.90432/4.89181. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.90137/4.88773. Took 0.20 sec\n",
      "Epoch 51, Loss(train/val) 4.90509/4.88983. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.90253/4.89704. Took 0.20 sec\n",
      "Epoch 53, Loss(train/val) 4.90082/4.90117. Took 0.20 sec\n",
      "Epoch 54, Loss(train/val) 4.89994/4.90203. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.89961/4.89966. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.90105/4.90458. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.90147/4.89639. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.89908/4.90397. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.89787/4.90706. Took 0.19 sec\n",
      "Epoch 60, Loss(train/val) 4.90082/4.89490. Took 0.19 sec\n",
      "Epoch 61, Loss(train/val) 4.90002/4.90787. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.89940/4.90880. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.90335/4.90222. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.90133/4.91572. Took 0.20 sec\n",
      "Epoch 65, Loss(train/val) 4.89796/4.92505. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.89650/4.92461. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.89589/4.91875. Took 0.20 sec\n",
      "Epoch 68, Loss(train/val) 4.89798/4.91475. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.89891/4.90471. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.89788/4.91728. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.89687/4.91760. Took 0.20 sec\n",
      "Epoch 72, Loss(train/val) 4.89471/4.91604. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.89549/4.91789. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.89897/4.89908. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.89867/4.91610. Took 0.19 sec\n",
      "Epoch 76, Loss(train/val) 4.89342/4.91953. Took 0.20 sec\n",
      "Epoch 77, Loss(train/val) 4.89529/4.91303. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.89419/4.92414. Took 0.20 sec\n",
      "Epoch 79, Loss(train/val) 4.89497/4.90964. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.89754/4.92311. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.89507/4.91894. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.89126/4.91465. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.89398/4.90974. Took 0.19 sec\n",
      "Epoch 84, Loss(train/val) 4.89671/4.91501. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.89211/4.90812. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.89023/4.90683. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.89238/4.91593. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.89010/4.90693. Took 0.20 sec\n",
      "Epoch 89, Loss(train/val) 4.89481/4.90474. Took 0.19 sec\n",
      "Epoch 90, Loss(train/val) 4.89533/4.90222. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.89055/4.91993. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.89000/4.91063. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.89254/4.91203. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.88721/4.91743. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.89139/4.90545. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.89041/4.91836. Took 0.19 sec\n",
      "Epoch 97, Loss(train/val) 4.88824/4.90383. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.89301/4.90802. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.88942/4.91122. Took 0.20 sec\n",
      "ACC: 0.515625, MCC: 0.02757337634153148\n",
      "Epoch 0, Loss(train/val) 5.06151/4.94190. Took 0.21 sec\n",
      "Epoch 1, Loss(train/val) 4.95763/4.93831. Took 0.20 sec\n",
      "Epoch 2, Loss(train/val) 4.94616/4.94289. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.94408/4.94279. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.94672/4.94065. Took 0.19 sec\n",
      "Epoch 5, Loss(train/val) 4.94228/4.93628. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.93809/4.93435. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.93653/4.93829. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.93598/4.94248. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.93306/4.94491. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.93492/4.94705. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.93395/4.94724. Took 0.20 sec\n",
      "Epoch 12, Loss(train/val) 4.93240/4.95044. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.92866/4.94858. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.93043/4.94985. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.92930/4.95036. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.92949/4.95347. Took 0.20 sec\n",
      "Epoch 17, Loss(train/val) 4.92983/4.95413. Took 0.20 sec\n",
      "Epoch 18, Loss(train/val) 4.92717/4.95165. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.92879/4.95285. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.92946/4.95529. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.92578/4.94474. Took 0.20 sec\n",
      "Epoch 22, Loss(train/val) 4.92703/4.94902. Took 0.19 sec\n",
      "Epoch 23, Loss(train/val) 4.92614/4.94416. Took 0.20 sec\n",
      "Epoch 24, Loss(train/val) 4.93031/4.94845. Took 0.20 sec\n",
      "Epoch 25, Loss(train/val) 4.92696/4.94917. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.92680/4.94502. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.92897/4.94522. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.92587/4.94539. Took 0.20 sec\n",
      "Epoch 29, Loss(train/val) 4.92738/4.94397. Took 0.19 sec\n",
      "Epoch 30, Loss(train/val) 4.92504/4.94404. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.92477/4.94634. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.92482/4.94108. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.92713/4.93961. Took 0.19 sec\n",
      "Epoch 34, Loss(train/val) 4.92559/4.93938. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.92548/4.94385. Took 0.20 sec\n",
      "Epoch 36, Loss(train/val) 4.92374/4.94022. Took 0.19 sec\n",
      "Epoch 37, Loss(train/val) 4.92430/4.94087. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.92262/4.93879. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.92295/4.93661. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.92221/4.93515. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.93017/4.92978. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.93499/4.93717. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.92994/4.93351. Took 0.19 sec\n",
      "Epoch 44, Loss(train/val) 4.93003/4.93255. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.92961/4.93056. Took 0.20 sec\n",
      "Epoch 46, Loss(train/val) 4.93056/4.93461. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.92347/4.93716. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.92314/4.93441. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.92278/4.93238. Took 0.20 sec\n",
      "Epoch 50, Loss(train/val) 4.93321/4.94454. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.92531/4.94126. Took 0.19 sec\n",
      "Epoch 52, Loss(train/val) 4.92539/4.94625. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.92310/4.95163. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.92124/4.94489. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.92265/4.95162. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.91904/4.94186. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.91708/4.94780. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.91702/4.95145. Took 0.20 sec\n",
      "Epoch 59, Loss(train/val) 4.91800/4.94732. Took 0.20 sec\n",
      "Epoch 60, Loss(train/val) 4.91925/4.95545. Took 0.20 sec\n",
      "Epoch 61, Loss(train/val) 4.91629/4.96060. Took 0.19 sec\n",
      "Epoch 62, Loss(train/val) 4.92266/4.94396. Took 0.19 sec\n",
      "Epoch 63, Loss(train/val) 4.91737/4.94948. Took 0.19 sec\n",
      "Epoch 64, Loss(train/val) 4.91525/4.95579. Took 0.19 sec\n",
      "Epoch 65, Loss(train/val) 4.91975/4.96077. Took 0.19 sec\n",
      "Epoch 66, Loss(train/val) 4.92380/4.93959. Took 0.19 sec\n",
      "Epoch 67, Loss(train/val) 4.92507/4.93931. Took 0.19 sec\n",
      "Epoch 68, Loss(train/val) 4.91949/4.95196. Took 0.19 sec\n",
      "Epoch 69, Loss(train/val) 4.91652/4.95602. Took 0.19 sec\n",
      "Epoch 70, Loss(train/val) 4.91475/4.95859. Took 0.19 sec\n",
      "Epoch 71, Loss(train/val) 4.91324/4.94986. Took 0.19 sec\n",
      "Epoch 72, Loss(train/val) 4.91170/4.96275. Took 0.19 sec\n",
      "Epoch 73, Loss(train/val) 4.91201/4.95656. Took 0.19 sec\n",
      "Epoch 74, Loss(train/val) 4.91476/4.95137. Took 0.19 sec\n",
      "Epoch 75, Loss(train/val) 4.91581/4.94125. Took 0.20 sec\n",
      "Epoch 76, Loss(train/val) 4.91142/4.96484. Took 0.19 sec\n",
      "Epoch 77, Loss(train/val) 4.91173/4.97673. Took 0.19 sec\n",
      "Epoch 78, Loss(train/val) 4.91453/4.95294. Took 0.19 sec\n",
      "Epoch 79, Loss(train/val) 4.91625/4.95119. Took 0.19 sec\n",
      "Epoch 80, Loss(train/val) 4.91101/4.96563. Took 0.19 sec\n",
      "Epoch 81, Loss(train/val) 4.91201/4.97629. Took 0.19 sec\n",
      "Epoch 82, Loss(train/val) 4.91382/4.96660. Took 0.19 sec\n",
      "Epoch 83, Loss(train/val) 4.90925/4.97739. Took 0.20 sec\n",
      "Epoch 84, Loss(train/val) 4.90733/4.96752. Took 0.19 sec\n",
      "Epoch 85, Loss(train/val) 4.91402/4.96704. Took 0.19 sec\n",
      "Epoch 86, Loss(train/val) 4.91139/4.96941. Took 0.19 sec\n",
      "Epoch 87, Loss(train/val) 4.91146/4.94496. Took 0.19 sec\n",
      "Epoch 88, Loss(train/val) 4.91359/4.95007. Took 0.19 sec\n",
      "Epoch 89, Loss(train/val) 4.91840/4.94428. Took 0.20 sec\n",
      "Epoch 90, Loss(train/val) 4.90983/4.96731. Took 0.19 sec\n",
      "Epoch 91, Loss(train/val) 4.90698/4.97198. Took 0.19 sec\n",
      "Epoch 92, Loss(train/val) 4.90900/4.96168. Took 0.19 sec\n",
      "Epoch 93, Loss(train/val) 4.90544/4.97917. Took 0.19 sec\n",
      "Epoch 94, Loss(train/val) 4.90518/4.97709. Took 0.19 sec\n",
      "Epoch 95, Loss(train/val) 4.90958/4.98829. Took 0.19 sec\n",
      "Epoch 96, Loss(train/val) 4.91160/4.94960. Took 0.20 sec\n",
      "Epoch 97, Loss(train/val) 4.91609/4.97204. Took 0.19 sec\n",
      "Epoch 98, Loss(train/val) 4.91351/4.97581. Took 0.19 sec\n",
      "Epoch 99, Loss(train/val) 4.90805/4.95535. Took 0.19 sec\n",
      "ACC: 0.578125, MCC: 0.14180020247260586\n",
      "Epoch 0, Loss(train/val) 4.98379/4.88536. Took 0.20 sec\n",
      "Epoch 1, Loss(train/val) 4.88911/4.88219. Took 0.19 sec\n",
      "Epoch 2, Loss(train/val) 4.88888/4.88752. Took 0.19 sec\n",
      "Epoch 3, Loss(train/val) 4.88669/4.89442. Took 0.19 sec\n",
      "Epoch 4, Loss(train/val) 4.89231/4.89101. Took 0.20 sec\n",
      "Epoch 5, Loss(train/val) 4.88758/4.89524. Took 0.19 sec\n",
      "Epoch 6, Loss(train/val) 4.88859/4.89425. Took 0.19 sec\n",
      "Epoch 7, Loss(train/val) 4.88605/4.89697. Took 0.19 sec\n",
      "Epoch 8, Loss(train/val) 4.88666/4.89546. Took 0.20 sec\n",
      "Epoch 9, Loss(train/val) 4.88475/4.89403. Took 0.19 sec\n",
      "Epoch 10, Loss(train/val) 4.88626/4.89462. Took 0.19 sec\n",
      "Epoch 11, Loss(train/val) 4.88577/4.89641. Took 0.19 sec\n",
      "Epoch 12, Loss(train/val) 4.88673/4.88974. Took 0.19 sec\n",
      "Epoch 13, Loss(train/val) 4.88711/4.88860. Took 0.19 sec\n",
      "Epoch 14, Loss(train/val) 4.88375/4.88873. Took 0.19 sec\n",
      "Epoch 15, Loss(train/val) 4.88239/4.89252. Took 0.19 sec\n",
      "Epoch 16, Loss(train/val) 4.88325/4.89663. Took 0.19 sec\n",
      "Epoch 17, Loss(train/val) 4.88108/4.89632. Took 0.19 sec\n",
      "Epoch 18, Loss(train/val) 4.87914/4.89540. Took 0.19 sec\n",
      "Epoch 19, Loss(train/val) 4.87894/4.89317. Took 0.19 sec\n",
      "Epoch 20, Loss(train/val) 4.87941/4.89112. Took 0.19 sec\n",
      "Epoch 21, Loss(train/val) 4.88685/4.89417. Took 0.19 sec\n",
      "Epoch 22, Loss(train/val) 4.88294/4.89678. Took 0.18 sec\n",
      "Epoch 23, Loss(train/val) 4.87779/4.89747. Took 0.19 sec\n",
      "Epoch 24, Loss(train/val) 4.87969/4.89701. Took 0.19 sec\n",
      "Epoch 25, Loss(train/val) 4.87626/4.90155. Took 0.19 sec\n",
      "Epoch 26, Loss(train/val) 4.87738/4.90600. Took 0.19 sec\n",
      "Epoch 27, Loss(train/val) 4.87529/4.90857. Took 0.19 sec\n",
      "Epoch 28, Loss(train/val) 4.87478/4.90224. Took 0.19 sec\n",
      "Epoch 29, Loss(train/val) 4.87132/4.90732. Took 0.21 sec\n",
      "Epoch 30, Loss(train/val) 4.87566/4.91002. Took 0.19 sec\n",
      "Epoch 31, Loss(train/val) 4.87486/4.90187. Took 0.19 sec\n",
      "Epoch 32, Loss(train/val) 4.87235/4.90479. Took 0.19 sec\n",
      "Epoch 33, Loss(train/val) 4.86970/4.90973. Took 0.18 sec\n",
      "Epoch 34, Loss(train/val) 4.87258/4.90476. Took 0.19 sec\n",
      "Epoch 35, Loss(train/val) 4.87238/4.90317. Took 0.19 sec\n",
      "Epoch 36, Loss(train/val) 4.87282/4.89961. Took 0.20 sec\n",
      "Epoch 37, Loss(train/val) 4.86898/4.90734. Took 0.19 sec\n",
      "Epoch 38, Loss(train/val) 4.86922/4.90626. Took 0.19 sec\n",
      "Epoch 39, Loss(train/val) 4.87500/4.90839. Took 0.19 sec\n",
      "Epoch 40, Loss(train/val) 4.87424/4.89836. Took 0.19 sec\n",
      "Epoch 41, Loss(train/val) 4.86567/4.90713. Took 0.19 sec\n",
      "Epoch 42, Loss(train/val) 4.86811/4.90167. Took 0.19 sec\n",
      "Epoch 43, Loss(train/val) 4.86566/4.90125. Took 0.20 sec\n",
      "Epoch 44, Loss(train/val) 4.86776/4.90677. Took 0.19 sec\n",
      "Epoch 45, Loss(train/val) 4.86703/4.90088. Took 0.19 sec\n",
      "Epoch 46, Loss(train/val) 4.86640/4.90171. Took 0.19 sec\n",
      "Epoch 47, Loss(train/val) 4.87008/4.89281. Took 0.19 sec\n",
      "Epoch 48, Loss(train/val) 4.86650/4.90549. Took 0.19 sec\n",
      "Epoch 49, Loss(train/val) 4.85846/4.90766. Took 0.19 sec\n",
      "Epoch 50, Loss(train/val) 4.86845/4.89598. Took 0.19 sec\n",
      "Epoch 51, Loss(train/val) 4.86479/4.90100. Took 0.20 sec\n",
      "Epoch 52, Loss(train/val) 4.86113/4.89800. Took 0.19 sec\n",
      "Epoch 53, Loss(train/val) 4.86816/4.89935. Took 0.19 sec\n",
      "Epoch 54, Loss(train/val) 4.86675/4.90102. Took 0.19 sec\n",
      "Epoch 55, Loss(train/val) 4.85961/4.91933. Took 0.19 sec\n",
      "Epoch 56, Loss(train/val) 4.86770/4.89013. Took 0.19 sec\n",
      "Epoch 57, Loss(train/val) 4.87079/4.88880. Took 0.19 sec\n",
      "Epoch 58, Loss(train/val) 4.86961/4.88975. Took 0.19 sec\n",
      "Epoch 59, Loss(train/val) 4.86488/4.89058. Took 0.19 sec\n"
     ]
    }
   ],
   "source": [
    "## 실행 파일\n",
    "args.data_list = os.listdir(\"C:\\\\Users\\\\USER\\\\JupyterProjects\\\\DTML\\\\data\\\\kdd17\\\\ourpped\")\n",
    "\n",
    "\n",
    "with open(args.save_file_path + '\\\\' + 'DTML_result_t.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"model\", \"stock\", \"entire_exp_time\",  \"avg_test_ACC\", \"avg_test_ACC_std\", \"avg_test_MCC\"])\n",
    "\n",
    "    for data in args.data_list:\n",
    "        \n",
    "        stock = data.split('.')[0]\n",
    "\n",
    "        est = time.time()\n",
    "        setattr(args, 'symbol', stock)\n",
    "        args.new_file_path = args.save_file_path + '\\\\' + \"DTML_\" + args.symbol\n",
    "        os.makedirs(args.new_file_path)\n",
    "        \n",
    "        \n",
    "        csv_read = stock_csv_read(data,args.x_frames,args.y_frames)\n",
    "        split_data_list = csv_read.cv_split()\n",
    "        \n",
    "        ACC_cv = []\n",
    "        for i, data in enumerate(split_data_list):\n",
    "            args.split_file_path = args.new_file_path + \"\\\\\" + str(i) +\"th_iter\"\n",
    "            os.makedirs(args.split_file_path)\n",
    "            # 0번째에 index 1번째에 stock 1개가 input으로 들어감\n",
    "            trainset = StockDataset(data[0])\n",
    "            valset = StockDataset(data[1])\n",
    "            testset = StockDataset(data[2])\n",
    "        \n",
    "\n",
    "            partition = {'train': trainset, 'val': valset, 'test': testset}\n",
    "\n",
    "\n",
    "            setting, result = experiment(partition, args)\n",
    "            eet = time.time()\n",
    "            entire_exp_time = eet - est\n",
    "\n",
    "            fig = plt.figure()\n",
    "            plt.plot(result['train_losses'])\n",
    "            plt.plot(result['val_losses'])\n",
    "            plt.legend(['train_losses', 'val_losses'], fontsize=15)\n",
    "            plt.xlabel('epoch', fontsize=15)\n",
    "            plt.ylabel('loss', fontsize=15)\n",
    "            plt.grid()\n",
    "            plt.savefig(args.split_file_path + '\\\\' + str(args.symbol) + '_fig' + '.png')\n",
    "            plt.close(fig)\n",
    "            ACC_cv.append(result['ACC'])\n",
    "            # csv파일에 기록하기\n",
    "        ACC_cv_ar = np.array(ACC_cv)\n",
    "        acc_avg = np.mean(ACC_cv_ar)\n",
    "        acc_std = np.std(ACC_cv_ar)\n",
    "\n",
    "        wr.writerow([\"DTML\", args.symbol, entire_exp_time, acc_avg, acc_std, result['MCC']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4bb3f-2033-4b97-8031-19370a89d75b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_64",
   "language": "python",
   "name": "py38_64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
